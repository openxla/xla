# Copyright 2023 The OpenXLA Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
version: 3
results {
  device: "CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 9.10.0"
  hlo: "(bf16[128,1024,1024]{2,1,0}, s8[33554432]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\"__cublas$gemm\", backend_config={\"device_type\":\"DEVICE_TYPE_INVALID\",\"force_earliest_schedule\":false,\"gemm_backend_config\":{\"alpha_imag\":0,\"alpha_real\":1,\"autotune_workspace_size\":\"0\",\"beta\":0,\"damax_output\":false,\"dot_dimension_numbers\":{\"lhs_batch_dimensions\":[\"0\"],\"lhs_contracting_dimensions\":[\"2\"],\"rhs_batch_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"]},\"epilogue\":\"DEFAULT\",\"grad_x\":false,\"grad_y\":false,\"lhs_stride\":\"1048576\",\"precision_config\":{\"algorithm\":\"ALG_UNSET\",\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"rhs_stride\":\"1048576\"},\"operation_queue_id\":\"0\",\"reification_cost\":[],\"wait_on_operation_queues\":[]}"
  result {
    gemm {
      algorithm: -1
    }
    run_time {
      nanos: 1
    }
  }
}
results {
  device: "CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 9.10.0"
  hlo: "{\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\n  tmp_1 = bf16[] constant({...})\n  tmp_2 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_1), dimensions={}\n  tmp_3 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_2)\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_3)\n  tmp_5 = bf16[4,32,1024,1024]{3,2,1,0} transpose(bf16[4,32,1024,1024]{3,2,1,0} tmp_4), dimensions={0,1,3,2}\n  tmp_6 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_5)\n  tmp_7 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_7)\n  tmp_9 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_6, bf16[128,1024,1024]{2,1,0} tmp_8), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n  ROOT tmp_10 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_9)\n}"
  result {
    other {
      name: "CUBLAS_FISSION"
      config {
        type_url: "type.googleapis.com/xla.AutotuneResult.GemmKey"
        value: "\010\377\377\377\377\377\377\377\377\377\001"
      }
    }
  }
}
results {
  device: "CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 9.10.0"
  hlo: "{\n  tmp_0 = bf16[3,32,1024,4,1024]{4,3,2,1,0} parameter(0)\n  tmp_1 = bf16[3,4,32,1024,1024]{4,3,2,1,0} transpose(bf16[3,32,1024,4,1024]{4,3,2,1,0} tmp_0), dimensions={0,3,1,2,4}\n  tmp_2 = bf16[1,3,32,1024]{3,2,1,0} parameter(1)\n  tmp_3 = bf16[3,32,1024]{2,1,0} bitcast(bf16[1,3,32,1024]{3,2,1,0} tmp_2)\n  tmp_4 = bf16[3,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[3,32,1024]{2,1,0} tmp_3), dimensions={0,2,3}\n  tmp_5 = bf16[3,4,32,1024,1024]{4,3,2,1,0} add(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_1, bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_4)\n  tmp_6 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_5), slice={[1:2], [0:4], [0:32], [0:1024], [0:1024]}\n  tmp_7 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_5), slice={[0:1], [0:4], [0:32], [0:1024], [0:1024]}\n  ROOT tmp_8 = (bf16[1,4,32,1024,1024]{4,3,2,1,0}, bf16[1,4,32,1024,1024]{4,3,2,1,0}) tuple(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_6, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_7)\n}"
  result {
    other {
      name: "NATIVE_EMITTER"
      config {
        type_url: "type.googleapis.com/xla.gpu.NativeEmitterBackendConfig"
      }
    }
  }
}
results {
  device: "CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 9.10.0"
  hlo: "{\n  tmp_0 = bf16[3,32,1024,4,1024]{4,3,2,1,0} parameter(0)\n  tmp_1 = bf16[3,4,32,1024,1024]{4,3,2,1,0} transpose(bf16[3,32,1024,4,1024]{4,3,2,1,0} tmp_0), dimensions={0,3,1,2,4}\n  tmp_2 = bf16[1,3,32,1024]{3,2,1,0} parameter(1)\n  tmp_3 = bf16[3,32,1024]{2,1,0} bitcast(bf16[1,3,32,1024]{3,2,1,0} tmp_2)\n  tmp_4 = bf16[3,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[3,32,1024]{2,1,0} tmp_3), dimensions={0,2,3}\n  tmp_5 = bf16[3,4,32,1024,1024]{4,3,2,1,0} add(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_1, bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_4)\n  tmp_6 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_5), slice={[1:2], [0:4], [0:32], [0:1024], [0:1024]}\n  tmp_7 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_5), slice={[0:1], [0:4], [0:32], [0:1024], [0:1024]}\n  tmp_8 = bf16[] constant({...})\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_8), dimensions={}\n  tmp_10 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_7, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\n  tmp_11 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_10)\n  tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} transpose(bf16[4,32,1024,1024]{3,2,1,0} tmp_11), dimensions={0,1,3,2}\n  ROOT tmp_13 = (bf16[1,4,32,1024,1024]{4,3,2,1,0}, bf16[4,32,1024,1024]{3,2,1,0}) tuple(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_6, bf16[4,32,1024,1024]{3,2,1,0} tmp_12)\n}"
  result {
    other {
      name: "NATIVE_EMITTER"
      config {
        type_url: "type.googleapis.com/xla.gpu.NativeEmitterBackendConfig"
      }
    }
  }
}