{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJplJGtYEeWE"
      },
      "source": [
        "# Megascale Network Performance Analysis\n",
        "\n",
        "Provided an Xprof profile, this notebook will generate several graphs and metrics that can help in identifying and understanding potential performance issues.\n",
        "\n",
        "**Use:** Upload one or more xprof profiles to use as input to this Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJBVL8_pfXFD"
      },
      "source": [
        "## 1) Install dependencies and generate required data structures\n",
        "The cells below must be run before jumping to other sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGoK5a1-H1eF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install necessary packages\n",
        "\n",
        "import shutil\n",
        "\n",
        "if shutil.which(\"pip\") is None:\n",
        "  print(\"pip is not installed. Skipping package installation.\")\n",
        "else:\n",
        "  # The nightly release of JAX is required until the official release contains the necessary ProfileData API.\n",
        "  !pip install --pre -U libtpu-nightly -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "  !pip install --pre -U jax jaxlib -i https://us-python.pkg.dev/ml-oss-artifacts-published/jax/simple/\n",
        "  !pip install git+https://github.com/jax-ml/jax\n",
        "  print(\"Finished package installation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veDCCvuobKCD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define helper functions\n",
        "\n",
        "from IPython import display\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def bytes_to_human(n, precision=2):\n",
        "  \"\"\"Convert bytes to a human-readable string (e.g., B, KiB, MiB, GiB).\"\"\"\n",
        "  if n < 0:\n",
        "    return \"-NaN\"\n",
        "\n",
        "  # Define the units and the base for the conversion\n",
        "  units = [\"B\", \"KiB\", \"MiB\", \"GiB\", \"TiB\", \"PiB\", \"EiB\"]\n",
        "  base = 1024\n",
        "\n",
        "  # Special case for bytes\n",
        "  if n < base:\n",
        "    return f\"{n} {units[0]}\"\n",
        "\n",
        "  # Find the correct unit and divide by the base\n",
        "  for i, unit in enumerate(units):\n",
        "    if n < base ** (i + 1):\n",
        "      return f\"{n / (base**i):.{precision}f} {unit}\"\n",
        "\n",
        "  # Handle extremely large numbers\n",
        "  return f\"{n / base**(len(units)-1):.{precision}f} {units[-1]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload xprof profile(s)\n",
        "\n",
        "Using the \"Files\" button in the vertical menu on the left hand side. Upload one or more xprof profiles."
      ],
      "metadata": {
        "id": "WSTeD5t-ySoi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oVyPFMsrXbN",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# @title Generate a DataFrame for each profile.\n",
        "\n",
        "profile_paths = '/content/example_profile_1.pb'  # @param {'type': 'string', isTemplate: true}\n",
        "labels = 'profile1'  # @param {'type':'string', isTemplate: true}\n",
        "\n",
        "profile_paths = profile_paths.split(',')\n",
        "labels = labels.split(',')\n",
        "assert len(profile_paths) == len(labels)\n",
        "\n",
        "dfs = []\n",
        "for i, xplane_path in enumerate(profile_paths):\n",
        "  plane = jax.profiler.ProfileData.from_file(xplane_path).find_plane_with_name(\n",
        "      '/host:CPU'\n",
        "  )\n",
        "  rows = []\n",
        "  for line in plane.lines:\n",
        "    if not line.name.startswith('MegascaleEM2_Worker'):\n",
        "      continue\n",
        "    for event in line.events:\n",
        "      if event.name == 'MegaScale: Communication Transport Receive':\n",
        "        stats = dict(event.stats)\n",
        "        source_id = f'{stats[\"dcn_source_slice_id\"]}-{stats[\"dcn_source_per_slice_device_id\"]}'\n",
        "        destination_id = f'{stats[\"dcn_destination_slice_id\"]}-{stats[\"dcn_destination_per_slice_device_id\"]}'\n",
        "        latency_us = stats['duration_us']\n",
        "        start_ns = event.start_ns\n",
        "        end_ns = start_ns + latency_us * 1000\n",
        "        timestamp = pd.to_datetime(end_ns, unit='ns')\n",
        "\n",
        "        rows.append([\n",
        "            latency_us,\n",
        "            stats['payload_size_bytes'],\n",
        "            source_id,\n",
        "            destination_id,\n",
        "            start_ns,\n",
        "            end_ns,\n",
        "            timestamp,\n",
        "        ])\n",
        "  df = pd.DataFrame(\n",
        "      rows,\n",
        "      columns=[\n",
        "          'latency_us',\n",
        "          'bytes',\n",
        "          'src',\n",
        "          'dst',\n",
        "          'start_ns',\n",
        "          'end_ns',\n",
        "          'timestamp',\n",
        "      ],\n",
        "  )\n",
        "  df.set_index('timestamp', inplace=True)\n",
        "  df.attrs['label'] = labels[i]\n",
        "  dfs.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8elIbiKnaZNG"
      },
      "source": [
        "## 2) Examine transfer latencies\n",
        "\n",
        "Check for outliers or persistently high latency. Small transfers should have lower latency than large ones.\n",
        "\n",
        "Possible sources of high latency are network slowdowns or individual host slowness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVPICBp7qTR6",
        "cellView": "form",
        "collapsed": true,
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# @title Network transfer latency\n",
        "\n",
        "for _, df in enumerate(dfs):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  series_names = df['bytes'].unique()\n",
        "  # Sort series_names numerically\n",
        "  series_names_sorted = sorted(series_names)\n",
        "  for i, series_name in enumerate(series_names_sorted):\n",
        "    series_data = df[df['bytes'] == series_name]\n",
        "    plt.scatter(\n",
        "        series_data.index,\n",
        "        series_data['latency_us'] / 1000,\n",
        "        label=f'{bytes_to_human(series_name)}',\n",
        "        s=20,\n",
        "    )\n",
        "\n",
        "  plt.title(f'Network transfer latency over time for {df.attrs.get(\"label\")}')\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Latency (ms)')\n",
        "  plt.legend(title='Transfer Size')\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkDraMKXYyEL"
      },
      "source": [
        "## 3) Examine the distribution of transfer sizes.\n",
        "\n",
        "Sanity check the transfer size distribution. Generally we want fewer larger transfers, not a high number of small ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr1knpVzrKs4",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Distribution of transfer sizes\n",
        "\n",
        "for _, df in enumerate(dfs):\n",
        "  grouped = df.groupby('bytes')\n",
        "  count_by_bytes = grouped.size()\n",
        "\n",
        "data = {\n",
        "    'Buffer size': [],\n",
        "    'Count': [],\n",
        "    'Percentage': [],\n",
        "}\n",
        "for key, value in count_by_bytes.items():\n",
        "  percentage = (value / count_by_bytes.sum()) * 100\n",
        "  data['Buffer size'].append(bytes_to_human(key))\n",
        "  data['Count'].append(value)\n",
        "  data['Percentage'].append(f'{percentage:.2f}')\n",
        "\n",
        "display.display(pd.DataFrame(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g45h6NffZv8n"
      },
      "source": [
        "## 4) Examine inflight transfer count over time\n",
        "\n",
        "This indicates how many pending collectives there are at a given point in time throughout the profiling time window. If this chart is spiky or remains consistently high then the program may not be well optimized for compute/communication overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D91NXuv5rIe0",
        "cellView": "form",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# @title Inflight transfers by size.\n",
        "\n",
        "agg_window_ms = 100\n",
        "\n",
        "for i, df in enumerate(dfs):\n",
        "  grouped_by_size = df.groupby('bytes')\n",
        "\n",
        "  legend_labels = []\n",
        "  resampled_series_list = []\n",
        "\n",
        "  for size, group_df in grouped_by_size:\n",
        "    # Create time series for the start and end of each transfer for this size.\n",
        "    # At the start of a transfer, the inflight count increases by 1.\n",
        "    # At the end of a transfer, the inflight count decreases by 1.\n",
        "    inflight_changes = pd.concat([\n",
        "        pd.Series(1, index=pd.to_datetime(group_df['start_ns'], unit='ns')),\n",
        "        pd.Series(-1, index=pd.to_datetime(group_df['end_ns'], unit='ns')),\n",
        "    ]).sort_index()\n",
        "\n",
        "    # Handle duplicate timestamps.\n",
        "    inflight_changes = (\n",
        "        inflight_changes.groupby(inflight_changes.index).sum().sort_index()\n",
        "    )\n",
        "\n",
        "    # Calculate the cumulative sum to get the number of inflight transfers over time for this size.\n",
        "    cumulative_inflight = inflight_changes.cumsum()\n",
        "\n",
        "    # Resample the cumulative inflight count to the desired time window and take the max.\n",
        "    inflight_count_per_window = (\n",
        "        cumulative_inflight.resample(f'{agg_window_ms}ms').max().fillna(0)\n",
        "    )\n",
        "\n",
        "    # Add the series.\n",
        "    resampled_series_list.append(inflight_count_per_window)\n",
        "    legend_labels.append(f'{bytes_to_human(int(size))}')\n",
        "\n",
        "  # Concatenate all resampled series into a single DataFrame and reindex to a common time index.\n",
        "  combined_df = pd.concat(resampled_series_list, axis=1).fillna(0)\n",
        "  time_labels_datetime = combined_df.index\n",
        "  inflight_data_for_stacking = (\n",
        "      combined_df.values.T\n",
        "  )  # Transpose to get data in the correct shape for stackplot.\n",
        "\n",
        "  # Plot the results as a stacked area chart.\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.stackplot(\n",
        "      time_labels_datetime, inflight_data_for_stacking, labels=legend_labels\n",
        "  )\n",
        "  plt.title(f'Inflight Transfers By Size For {labels[i]}')\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Inflight transfers')\n",
        "  plt.legend(title='Transfer Size (bytes)')\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioWK-TjLpFTz"
      },
      "source": [
        "## 5) Examine network throughput\n",
        "\n",
        "Optionally, you may enter the per-task maximum bandwidth for the platform to see it in the graph. For example, if you're using TPU v6e with two tasks per machine then the per-task bandwidth is 4 NICs per machine * 200 Gbps per NIC / 2 tasks per machine = 400 Gbps.\n",
        "\n",
        "If the throughput is consistently near the platform's theoretical max then this indicates that this workload is network-bound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4pMQkrSqbtq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Network transfer throughput over time.\n",
        "\n",
        "max_bandwidth_gbps = 100  # @param {'type':'number', isTemplate: true}\n",
        "\n",
        "agg_window_ms = 1  # @param {'type':'number', isTemplate: true}\n",
        "\n",
        "for _, df in enumerate(dfs):\n",
        "  df['average_bandwidth_gbps'] = (\n",
        "      (df['bytes'] * 8) / (df['latency_us'] * 1e-6) / 1e9\n",
        "  )\n",
        "\n",
        "  # Create a time series for the start and end of each transfer.\n",
        "  # At the start of a transfer, bandwidth increases by average_bandwidth_gbps.\n",
        "  # At the end of a transfer, bandwidth decreases by average_bandwidth_gbps.\n",
        "  bandwidth_changes = pd.concat([\n",
        "      pd.Series(\n",
        "          df['average_bandwidth_gbps'].values,\n",
        "          index=pd.to_datetime(df['start_ns'], unit='ns'),\n",
        "      ),\n",
        "      pd.Series(\n",
        "          -df['average_bandwidth_gbps'].values,\n",
        "          index=pd.to_datetime(df['end_ns'], unit='ns'),\n",
        "      ),\n",
        "  ]).sort_index()\n",
        "\n",
        "  # Handle duplicate timestamps.\n",
        "  bandwidth_changes = (\n",
        "      bandwidth_changes.groupby(bandwidth_changes.index).sum().sort_index()\n",
        "  )\n",
        "\n",
        "  # Calculate the cumulative bandwidth over time.\n",
        "  cumulative_bandwidth = bandwidth_changes.cumsum()\n",
        "\n",
        "  # To get a correct time-weighted average, we first create a dense time series\n",
        "  # by upsampling and forward-filling, then we downsample and take the mean.\n",
        "  # Note: The upsampling frequency should be high enough to capture the data's dynamics.\n",
        "  dense_bandwidth = cumulative_bandwidth.resample(\n",
        "      f'{agg_window_ms/100}ms'\n",
        "  ).ffill()\n",
        "  average_bandwidth_per_window = (\n",
        "      dense_bandwidth.resample(f'{agg_window_ms}ms').mean().fillna(0)\n",
        "  )\n",
        "\n",
        "  # Plot the results\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.plot(\n",
        "      average_bandwidth_per_window.index, average_bandwidth_per_window.values\n",
        "  )\n",
        "\n",
        "  if max_bandwidth_gbps > 0:\n",
        "    plt.axhline(\n",
        "        y=max_bandwidth_gbps,\n",
        "        color='r',\n",
        "        linestyle='--',\n",
        "        label=f'Platform Max Bandwidth ({max_bandwidth_gbps} Gbps)',\n",
        "    )\n",
        "  plt.title(f'Network Throughput (Gbps) for {df.attrs.get(\"label\")}')\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Throughput (Gbps)')\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Examine {source, destination} pairs for slow transfers\n",
        "\n",
        "Returns the source and destination global device ID ({slice}-{per_slice_device_id}) pairs for transfers that occur after `min_start_time_ns` and that take longer than `min_latency_us`.\n",
        "\n",
        "This data can help identify bad network cards, overloaded network switches, sub-optimal sharding, etc."
      ],
      "metadata": {
        "id": "QzLe-2QdqIU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clHtPqHWFuJM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Long tail host pairs\n",
        "\n",
        "min_start_time_ns = 0  # @param\n",
        "min_latency_us = 1000  # @param\n",
        "\n",
        "for i, df in enumerate(dfs):\n",
        "  long_tail = (\n",
        "      df.loc[df['start_ns'] > min_start_time_ns]\n",
        "      .loc[df['latency_us'] > min_latency_us]\n",
        "      .replace({'src': i, 'dst': i})\n",
        "      .groupby(['src', 'dst'])\n",
        "      .size()\n",
        "      .reset_index(name='counts')\n",
        "  )\n",
        "  print(f'Long tail host src-dest pair for {labels[i]}')\n",
        "  display.display(long_tail)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
