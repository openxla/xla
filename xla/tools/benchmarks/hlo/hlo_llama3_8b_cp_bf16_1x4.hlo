HloModule jit_train_step, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias), {8}: (8, {}, may-alias), {9}: (9, {}, may-alias), {10}: (10, {}, may-alias), {11}: (11, {}, may-alias), {12}: (12, {}, may-alias), {13}: (13, {}, may-alias), {14}: (14, {}, may-alias), {15}: (15, {}, may-alias), {16}: (16, {}, may-alias), {17}: (17, {}, may-alias), {18}: (18, {}, may-alias), {19}: (19, {}, may-alias), {20}: (20, {}, may-alias), {21}: (21, {}, may-alias), {22}: (22, {}, may-alias), {23}: (23, {}, may-alias), {24}: (24, {}, may-alias), {25}: (25, {}, may-alias), {26}: (26, {}, may-alias), {27}: (27, {}, may-alias), {28}: (28, {}, may-alias), {29}: (29, {}, may-alias), {30}: (30, {}, may-alias), {31}: (31, {}, may-alias), {32}: (32, {}, may-alias), {33}: (33, {}, may-alias), {34}: (34, {}, may-alias), {35}: (35, {}, may-alias), {36}: (36, {}, may-alias), {37}: (37, {}, may-alias), {38}: (38, {}, may-alias) }, entry_computation_layout={(s32[], f32[4096]{0}, f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, /*index=5*/f32[4096,32]{1,0}, f32[4096,32]{1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, /*index=10*/f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, f32[128256,1024]{1,0}, s32[], f32[4096]{0}, /*index=15*/f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, f32[4096,32]{1,0}, f32[4096,32]{1,0}, /*index=20*/f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, /*index=25*/f32[128256,1024]{1,0}, f32[4096]{0}, f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, /*index=30*/f32[4096,32]{1,0}, f32[4096,32]{1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, /*index=35*/f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, f32[128256,1024]{1,0}, s32[], s32[4,2048]{1,0}, /*index=40*/s32[4,2048]{1,0}, s32[4,2048]{1,0}, s32[4,2048]{1,0})->(s32[], f32[4096]{0}, f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, /*index=5*/f32[4096,32]{1,0}, f32[4096,32]{1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, /*index=10*/f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, f32[128256,1024]{1,0}, s32[], f32[4096]{0}, /*index=15*/f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, f32[4096,32]{1,0}, f32[4096,32]{1,0}, /*index=20*/f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, /*index=25*/f32[128256,1024]{1,0}, f32[4096]{0}, f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, /*index=30*/f32[4096,32]{1,0}, f32[4096,32]{1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, /*index=35*/f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, f32[128256,1024]{1,0}, s32[], f32[], /*index=40*/f32[], f32[], f32[], f32[], f32[], /*index=45*/s32[])}, allow_spmd_sharding_propagation_to_parameters={false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false}, allow_spmd_sharding_propagation_to_output={false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true}, num_partitions=4

%region_53.54 (reduce_sum.345: f32[], reduce_sum.346: f32[]) -> f32[] {
  %reduce_sum.345 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.346 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.225 = f32[] add(%reduce_sum.345, %reduce_sum.346), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_54.55 (reduce_sum.352: f32[], reduce_sum.353: f32[]) -> f32[] {
  %reduce_sum.352 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.353 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.231 = f32[] add(%reduce_sum.352, %reduce_sum.353), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_55.56 (reduce_sum.359: f32[], reduce_sum.360: f32[]) -> f32[] {
  %reduce_sum.359 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.360 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.232 = f32[] add(%reduce_sum.359, %reduce_sum.360), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_56.57 (reduce_sum.366: f32[], reduce_sum.367: f32[]) -> f32[] {
  %reduce_sum.366 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.367 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.238 = f32[] add(%reduce_sum.366, %reduce_sum.367), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_57.58 (reduce_sum.373: f32[], reduce_sum.374: f32[]) -> f32[] {
  %reduce_sum.373 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.374 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.239 = f32[] add(%reduce_sum.373, %reduce_sum.374), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_58.59 (reduce_sum.380: f32[], reduce_sum.381: f32[]) -> f32[] {
  %reduce_sum.380 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.381 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.245 = f32[] add(%reduce_sum.380, %reduce_sum.381), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_59.60 (reduce_sum.387: f32[], reduce_sum.388: f32[]) -> f32[] {
  %reduce_sum.387 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.388 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.246 = f32[] add(%reduce_sum.387, %reduce_sum.388), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_60.61 (reduce_sum.394: f32[], reduce_sum.395: f32[]) -> f32[] {
  %reduce_sum.394 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.395 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.252 = f32[] add(%reduce_sum.394, %reduce_sum.395), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_61.62 (reduce_sum.401: f32[], reduce_sum.402: f32[]) -> f32[] {
  %reduce_sum.401 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.402 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.253 = f32[] add(%reduce_sum.401, %reduce_sum.402), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_62.63 (reduce_sum.408: f32[], reduce_sum.409: f32[]) -> f32[] {
  %reduce_sum.408 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.409 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.259 = f32[] add(%reduce_sum.408, %reduce_sum.409), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_63.64 (reduce_sum.415: f32[], reduce_sum.416: f32[]) -> f32[] {
  %reduce_sum.415 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.416 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.260 = f32[] add(%reduce_sum.415, %reduce_sum.416), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_64.65 (reduce_sum.422: f32[], reduce_sum.423: f32[]) -> f32[] {
  %reduce_sum.422 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.423 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.266 = f32[] add(%reduce_sum.422, %reduce_sum.423), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_0.1 (reduce_sum.16: s32[], reduce_sum.17: s32[]) -> s32[] {
  %reduce_sum.16 = s32[] parameter(0), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  %reduce_sum.17 = s32[] parameter(1), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  ROOT %reduce_sum.18 = s32[] add(%reduce_sum.16, %reduce_sum.17), metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/train.py" source_line=174 source_end_line=174 source_column=18 source_end_column=18}
}

%region_0.1.clone (reduce_sum.413: s32[], reduce_sum.414: s32[]) -> s32[] {
  %reduce_sum.413 = s32[] parameter(0), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  %reduce_sum.414 = s32[] parameter(1), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  ROOT %reduce_sum.420 = s32[] add(%reduce_sum.413, %reduce_sum.414), metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/train.py" source_line=174 source_end_line=174 source_column=18 source_end_column=18}
}

%region_2.0 (reduce_sum.22: f32[], reduce_sum.23: f32[]) -> f32[] {
  %reduce_sum.22 = f32[] parameter(0), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/reduce_sum"}
  %reduce_sum.23 = f32[] parameter(1), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/reduce_sum"}
  ROOT %reduce_sum.24 = f32[] add(%reduce_sum.22, %reduce_sum.23), metadata={op_name="layers/reduce_sum" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
}

%region_0.3._custom_call_lowering_rule (reduce_window_sum.3: s32[], reduce_window_sum.4: s32[]) -> s32[] {
  %reduce_window_sum.3 = s32[] parameter(0), metadata={op_name="reduce_window_sum"}
  %reduce_window_sum.4 = s32[] parameter(1), metadata={op_name="reduce_window_sum"}
  ROOT %reduce_window_sum.5 = s32[] add(%reduce_window_sum.3, %reduce_window_sum.4), metadata={op_name="reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
}

%region_1.6._custom_call_lowering_rule (arg_empty_tuple.1: ()) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_empty_tuple.1 = () parameter(0)
  %constant.111 = bf16[] constant(0)
  %broadcast.194 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.111), dimensions={}
  %constant.110 = f32[] constant(-inf)
  %broadcast.193 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.110), dimensions={}
  ROOT %tuple.13 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%broadcast.194, %broadcast.193)
}

%region_2.7._custom_call_lowering_rule (arg_tuple.8: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/u32[2], f32[0], f32[0], f32[0], f32[0], /*index=10*/f32[0], f32[0])) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_tuple.8 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.154 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.8), index=2
  %get-tuple-element.155 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.8), index=3
  %constant.112 = bf16[0]{0} constant({})
  %get-tuple-element.156 = bf16[0]{0} get-tuple-element(%arg_tuple.8), index=4
  %get-tuple-element.157 = u32[2]{0} get-tuple-element(%arg_tuple.8), index=5
  %constant.113 = s32[1]{0} constant({0})
  %get-tuple-element.152 = s32[2]{0} get-tuple-element(%arg_tuple.8), index=0
  %constant.114 = s32[] constant(2048)
  %broadcast.196 = s32[2]{0} broadcast(%constant.114), dimensions={}
  %sub.44 = s32[2]{0} subtract(%get-tuple-element.152, %broadcast.196), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %constant.115 = s32[] constant(0)
  %broadcast.195 = s32[2]{0} broadcast(%constant.115), dimensions={}
  %lt.12 = pred[2]{0} compare(%sub.44, %broadcast.195), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.54 = s32[2]{0} select(%lt.12, %broadcast.195, %sub.44), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.13 = pred[2]{0} compare(%select_n.54, %broadcast.196), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.55 = s32[2]{0} select(%lt.13, %select_n.54, %broadcast.196), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.16 = pred[2]{0} compare(%select_n.55, %broadcast.195), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.56 = s32[2]{0} select(%lt.16, %broadcast.195, %select_n.55), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.1 = s32[2]{0} reduce-window(%select_n.56, %constant.115), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.18 = s32[3]{0} concatenate(%constant.113, %reduce_window_sum.1), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.153 = s32[2]{0} get-tuple-element(%arg_tuple.8), index=1
  %sub.45 = s32[2]{0} subtract(%get-tuple-element.153, %broadcast.196), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %lt.14 = pred[2]{0} compare(%sub.45, %broadcast.195), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.57 = s32[2]{0} select(%lt.14, %broadcast.195, %sub.45), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.15 = pred[2]{0} compare(%select_n.57, %broadcast.196), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.58 = s32[2]{0} select(%lt.15, %select_n.57, %broadcast.196), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.17 = pred[2]{0} compare(%select_n.58, %broadcast.195), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.59 = s32[2]{0} select(%lt.17, %broadcast.195, %select_n.58), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.2 = s32[2]{0} reduce-window(%select_n.59, %constant.115), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.19 = s32[3]{0} concatenate(%constant.113, %reduce_window_sum.2), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.158 = f32[0]{0} get-tuple-element(%arg_tuple.8), index=6
  %get-tuple-element.159 = f32[0]{0} get-tuple-element(%arg_tuple.8), index=7
  %get-tuple-element.160 = f32[0]{0} get-tuple-element(%arg_tuple.8), index=8
  %get-tuple-element.161 = f32[0]{0} get-tuple-element(%arg_tuple.8), index=9
  %get-tuple-element.162 = f32[0]{0} get-tuple-element(%arg_tuple.8), index=10
  %get-tuple-element.163 = f32[0]{0} get-tuple-element(%arg_tuple.8), index=11
  %te_fused_attn_forward_ffi.10 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, u8[1]{0}) custom-call(%get-tuple-element.154, %get-tuple-element.155, %constant.112, %get-tuple-element.156, %get-tuple-element.157, /*index=5*/%concatenate.18, %concatenate.19, %get-tuple-element.158, %get-tuple-element.159, %get-tuple-element.160, /*index=10*/%get-tuple-element.161, %get-tuple-element.162, %get-tuple-element.163), custom_call_target="te_fused_attn_forward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u32[2]{0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_forward_ffi.11 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.10), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %te_fused_attn_forward_ffi.12 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.10), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  ROOT %tuple.14 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%te_fused_attn_forward_ffi.11, %te_fused_attn_forward_ffi.12)
}

%region_3.10._custom_call_lowering_rule (arg_empty_tuple.3: ()) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_empty_tuple.3 = () parameter(0)
  %constant.218 = bf16[] constant(0)
  %broadcast.198 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.218), dimensions={}
  %constant.217 = f32[] constant(-inf)
  %broadcast.197 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.217), dimensions={}
  ROOT %tuple.15 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%broadcast.198, %broadcast.197)
}

%region_4.11._custom_call_lowering_rule (arg_tuple.9: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/u32[2], f32[0], f32[0], f32[0], f32[0], /*index=10*/f32[0], f32[0])) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_tuple.9 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.166 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.9), index=2
  %get-tuple-element.167 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.9), index=3
  %constant.219 = bf16[0]{0} constant({})
  %get-tuple-element.168 = bf16[0]{0} get-tuple-element(%arg_tuple.9), index=4
  %get-tuple-element.169 = u32[2]{0} get-tuple-element(%arg_tuple.9), index=5
  %constant.220 = s32[1]{0} constant({0})
  %get-tuple-element.164 = s32[2]{0} get-tuple-element(%arg_tuple.9), index=0
  %constant.225 = s32[] constant(4096)
  %broadcast.201 = s32[2]{0} broadcast(%constant.225), dimensions={}
  %sub.46 = s32[2]{0} subtract(%get-tuple-element.164, %broadcast.201), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %constant.223 = s32[] constant(0)
  %broadcast.200 = s32[2]{0} broadcast(%constant.223), dimensions={}
  %lt.24 = pred[2]{0} compare(%sub.46, %broadcast.200), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.60 = s32[2]{0} select(%lt.24, %broadcast.200, %sub.46), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %constant.221 = s32[] constant(2048)
  %broadcast.199 = s32[2]{0} broadcast(%constant.221), dimensions={}
  %lt.25 = pred[2]{0} compare(%select_n.60, %broadcast.199), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.61 = s32[2]{0} select(%lt.25, %select_n.60, %broadcast.199), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.28 = pred[2]{0} compare(%select_n.61, %broadcast.200), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.62 = s32[2]{0} select(%lt.28, %broadcast.200, %select_n.61), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.6 = s32[2]{0} reduce-window(%select_n.62, %constant.223), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.20 = s32[3]{0} concatenate(%constant.220, %reduce_window_sum.6), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.165 = s32[2]{0} get-tuple-element(%arg_tuple.9), index=1
  %sub.47 = s32[2]{0} subtract(%get-tuple-element.165, %broadcast.201), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %lt.26 = pred[2]{0} compare(%sub.47, %broadcast.200), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.63 = s32[2]{0} select(%lt.26, %broadcast.200, %sub.47), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.27 = pred[2]{0} compare(%select_n.63, %broadcast.199), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.64 = s32[2]{0} select(%lt.27, %select_n.63, %broadcast.199), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.29 = pred[2]{0} compare(%select_n.64, %broadcast.200), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.65 = s32[2]{0} select(%lt.29, %broadcast.200, %select_n.64), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.8 = s32[2]{0} reduce-window(%select_n.65, %constant.223), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.21 = s32[3]{0} concatenate(%constant.220, %reduce_window_sum.8), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.170 = f32[0]{0} get-tuple-element(%arg_tuple.9), index=6
  %get-tuple-element.171 = f32[0]{0} get-tuple-element(%arg_tuple.9), index=7
  %get-tuple-element.172 = f32[0]{0} get-tuple-element(%arg_tuple.9), index=8
  %get-tuple-element.173 = f32[0]{0} get-tuple-element(%arg_tuple.9), index=9
  %get-tuple-element.174 = f32[0]{0} get-tuple-element(%arg_tuple.9), index=10
  %get-tuple-element.175 = f32[0]{0} get-tuple-element(%arg_tuple.9), index=11
  %te_fused_attn_forward_ffi.20 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, u8[1]{0}) custom-call(%get-tuple-element.166, %get-tuple-element.167, %constant.219, %get-tuple-element.168, %get-tuple-element.169, /*index=5*/%concatenate.20, %concatenate.21, %get-tuple-element.170, %get-tuple-element.171, %get-tuple-element.172, /*index=10*/%get-tuple-element.173, %get-tuple-element.174, %get-tuple-element.175), custom_call_target="te_fused_attn_forward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u32[2]{0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_forward_ffi.21 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.20), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %te_fused_attn_forward_ffi.22 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.20), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  ROOT %tuple.16 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%te_fused_attn_forward_ffi.21, %te_fused_attn_forward_ffi.22)
}

%region_5.12._custom_call_lowering_rule (arg_empty_tuple.5: ()) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_empty_tuple.5 = () parameter(0)
  %constant.229 = bf16[] constant(0)
  %broadcast.203 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.229), dimensions={}
  %constant.228 = f32[] constant(-inf)
  %broadcast.202 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.228), dimensions={}
  ROOT %tuple.17 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%broadcast.203, %broadcast.202)
}

%region_6.13._custom_call_lowering_rule (arg_tuple.10: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/u32[2], f32[0], f32[0], f32[0], f32[0], /*index=10*/f32[0], f32[0])) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_tuple.10 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.178 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.10), index=2
  %get-tuple-element.179 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.10), index=3
  %constant.230 = bf16[0]{0} constant({})
  %get-tuple-element.180 = bf16[0]{0} get-tuple-element(%arg_tuple.10), index=4
  %get-tuple-element.181 = u32[2]{0} get-tuple-element(%arg_tuple.10), index=5
  %constant.231 = s32[1]{0} constant({0})
  %get-tuple-element.176 = s32[2]{0} get-tuple-element(%arg_tuple.10), index=0
  %constant.236 = s32[] constant(6144)
  %broadcast.206 = s32[2]{0} broadcast(%constant.236), dimensions={}
  %sub.48 = s32[2]{0} subtract(%get-tuple-element.176, %broadcast.206), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %constant.234 = s32[] constant(0)
  %broadcast.205 = s32[2]{0} broadcast(%constant.234), dimensions={}
  %lt.36 = pred[2]{0} compare(%sub.48, %broadcast.205), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.66 = s32[2]{0} select(%lt.36, %broadcast.205, %sub.48), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %constant.232 = s32[] constant(2048)
  %broadcast.204 = s32[2]{0} broadcast(%constant.232), dimensions={}
  %lt.37 = pred[2]{0} compare(%select_n.66, %broadcast.204), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.67 = s32[2]{0} select(%lt.37, %select_n.66, %broadcast.204), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.40 = pred[2]{0} compare(%select_n.67, %broadcast.205), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.68 = s32[2]{0} select(%lt.40, %broadcast.205, %select_n.67), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.9 = s32[2]{0} reduce-window(%select_n.68, %constant.234), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.22 = s32[3]{0} concatenate(%constant.231, %reduce_window_sum.9), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.177 = s32[2]{0} get-tuple-element(%arg_tuple.10), index=1
  %sub.49 = s32[2]{0} subtract(%get-tuple-element.177, %broadcast.206), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %lt.38 = pred[2]{0} compare(%sub.49, %broadcast.205), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.69 = s32[2]{0} select(%lt.38, %broadcast.205, %sub.49), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.39 = pred[2]{0} compare(%select_n.69, %broadcast.204), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.70 = s32[2]{0} select(%lt.39, %select_n.69, %broadcast.204), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.41 = pred[2]{0} compare(%select_n.70, %broadcast.205), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.71 = s32[2]{0} select(%lt.41, %broadcast.205, %select_n.70), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.10 = s32[2]{0} reduce-window(%select_n.71, %constant.234), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.23 = s32[3]{0} concatenate(%constant.231, %reduce_window_sum.10), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.182 = f32[0]{0} get-tuple-element(%arg_tuple.10), index=6
  %get-tuple-element.183 = f32[0]{0} get-tuple-element(%arg_tuple.10), index=7
  %get-tuple-element.184 = f32[0]{0} get-tuple-element(%arg_tuple.10), index=8
  %get-tuple-element.185 = f32[0]{0} get-tuple-element(%arg_tuple.10), index=9
  %get-tuple-element.186 = f32[0]{0} get-tuple-element(%arg_tuple.10), index=10
  %get-tuple-element.187 = f32[0]{0} get-tuple-element(%arg_tuple.10), index=11
  %te_fused_attn_forward_ffi.30 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, u8[1]{0}) custom-call(%get-tuple-element.178, %get-tuple-element.179, %constant.230, %get-tuple-element.180, %get-tuple-element.181, /*index=5*/%concatenate.22, %concatenate.23, %get-tuple-element.182, %get-tuple-element.183, %get-tuple-element.184, /*index=10*/%get-tuple-element.185, %get-tuple-element.186, %get-tuple-element.187), custom_call_target="te_fused_attn_forward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u32[2]{0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_forward_ffi.31 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.30), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %te_fused_attn_forward_ffi.32 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.30), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  ROOT %tuple.18 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%te_fused_attn_forward_ffi.31, %te_fused_attn_forward_ffi.32)
}

%region_3.3 (reduce_sum.25: f32[], reduce_sum.29: f32[]) -> f32[] {
  %reduce_sum.25 = f32[] parameter(0), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/reduce_sum"}
  %reduce_sum.29 = f32[] parameter(1), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/reduce_sum"}
  ROOT %reduce_sum.30 = f32[] add(%reduce_sum.25, %reduce_sum.29), metadata={op_name="layers/reduce_sum" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
}

%region_1.4_spmd (param: (s32[], bf16[2,2048,4096], bf16[32,2,2048,4096], f32[32,4096], f32[32,4096], /*index=5*/f32[32,1024,32,128], bf16[2,2048,1,64], bf16[2,2048,1,64], f32[32,1024,8,128], bf16[2,2048,1,64], /*index=10*/bf16[2,2048,1,64], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,14336], f32[32,1024,14336], /*index=15*/f32[32,14336,1024])) -> (s32[], bf16[2,2048,4096], bf16[32,2,2048,4096], f32[32,4096], f32[32,4096], /*index=5*/f32[32,1024,32,128], bf16[2,2048,1,64], bf16[2,2048,1,64], f32[32,1024,8,128], bf16[2,2048,1,64], /*index=10*/bf16[2,2048,1,64], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,14336], f32[32,1024,14336], /*index=15*/f32[32,14336,1024]) {
  %param = (s32[], bf16[2,2048,4096]{2,1,0}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{1,0}, f32[32,4096]{1,0}, /*index=5*/f32[32,1024,32,128]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, /*index=10*/bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,1024,14336]{2,1,0}, /*index=15*/f32[32,14336,1024]{2,1,0}) parameter(0)
  %get-tuple-element.188 = s32[] get-tuple-element(%param), index=0
  %constant.239 = s32[] constant(1)
  %add.150 = s32[] add(%get-tuple-element.188, %constant.239), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.189 = f32[32,4096]{1,0} get-tuple-element(%param), index=3
  %constant.240 = s32[] constant(0)
  %dynamic_slice.38 = f32[1,4096]{1,0} dynamic-slice(%get-tuple-element.189, %get-tuple-element.188, %constant.240), dynamic_slice_sizes={1,4096}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.249 = bf16[1,4096]{1,0} convert(%dynamic_slice.38), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=78 source_end_line=78 source_column=12 source_end_column=12}
  %convert_element_type.250 = bf16[4096]{0} reshape(%convert_element_type.249), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=78 source_end_line=78 source_column=12 source_end_column=12}
  %dot_general.124 = bf16[2,2048,4096]{2,1,0} broadcast(%convert_element_type.250), dimensions={2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %get-tuple-element.190 = bf16[2,2048,4096]{2,1,0} get-tuple-element(%param), index=1
  %sharding_constraint.118 = bf16[2,2048,4096]{2,1,0} copy(%get-tuple-element.190), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %get-tuple-element.191 = f32[32,4096]{1,0} get-tuple-element(%param), index=4
  %dynamic_slice.39 = f32[1,4096]{1,0} dynamic-slice(%get-tuple-element.191, %get-tuple-element.188, %constant.240), dynamic_slice_sizes={1,4096}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.251 = bf16[1,4096]{1,0} convert(%dynamic_slice.39), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=78 source_end_line=78 source_column=12 source_end_column=12}
  %convert_element_type.252 = bf16[4096]{0} reshape(%convert_element_type.251), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=78 source_end_line=78 source_column=12 source_end_column=12}
  %dot_general.125 = bf16[2,2048,4096]{2,1,0} broadcast(%convert_element_type.252), dimensions={2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %convert_element_type.253 = f32[2,2048,4096]{2,1,0} convert(%sharding_constraint.118), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=66 source_end_line=66 source_column=8 source_end_column=8}
  %square.90 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.253, %convert_element_type.253), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/square" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %constant.241 = f32[] constant(0)
  %reduce = f32[2,2048]{1,0} reduce(%square.90, %constant.241), dimensions={2}, to_apply=%region_2.0, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/reduce_sum" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
  %constant.242 = f32[] constant(0.000244140625)
  %closed_call.8 = f32[2,2048]{1,0} broadcast(%constant.242), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call"}
  %div.95 = f32[2,2048]{1,0} multiply(%reduce, %closed_call.8), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/div" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
  %constant.243 = f32[] constant(1e-05)
  %closed_call.9 = f32[2,2048]{1,0} broadcast(%constant.243), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call"}
  %add.151 = f32[2,2048]{1,0} add(%div.95, %closed_call.9), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/add" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=34 source_end_column=34}
  %rsqrt.16 = f32[2,2048]{1,0} rsqrt(%add.151), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/rsqrt" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=24 source_end_column=24}
  %mul.702 = f32[2,2048,4096]{2,1,0} broadcast(%rsqrt.16), dimensions={0,1}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %mul.703 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.253, %mul.702), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %convert_element_type.254 = bf16[2,2048,4096]{2,1,0} convert(%mul.703), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=8 source_end_column=8}
  %dot_general.126 = bf16[2,2048,4096]{2,1,0} multiply(%dot_general.125, %convert_element_type.254), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %sharding_constraint.119 = bf16[2,2048,4096]{2,1,0} copy(%dot_general.126), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %sharding_constraint.120 = bf16[2,2048,4096]{2,1,0} copy(%sharding_constraint.119), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %get-tuple-element.192 = f32[32,1024,32,128]{3,2,1,0} get-tuple-element(%param), index=5
  %dynamic-slice = f32[1,1024,32,128]{3,2,1,0} dynamic-slice(%get-tuple-element.192, %get-tuple-element.188, %constant.240, %constant.240, %constant.240), dynamic_slice_sizes={1,1024,32,128}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.255 = bf16[1,1024,32,128]{3,2,1,0} convert(%dynamic-slice), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %convert_element_type.256 = bf16[1024,32,128]{2,1,0} reshape(%convert_element_type.255), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %all-gather = bf16[4096,32,128]{2,1,0} all-gather(%convert_element_type.256), channel_id=1, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dot.5 = bf16[2,2048,32,128]{3,2,1,0} dot(%sharding_constraint.120, %all-gather), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %slice.8 = bf16[2,2048,32,64]{3,2,1,0} slice(%dot.5), slice={[0:2], [0:2048], [0:32], [0:64]}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/split" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=337 source_end_line=337 source_column=30 source_end_column=30}
  %get-tuple-element.193 = bf16[2,2048,1,64]{3,2,1,0} get-tuple-element(%param), index=6
  %mul.704 = bf16[2,2048,64]{2,1,0} reshape(%get-tuple-element.193), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=17 source_end_column=17}
  %mul.705 = bf16[2,2048,32,64]{3,2,1,0} broadcast(%mul.704), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=17 source_end_column=17}
  %mul.706 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.8, %mul.705), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=17 source_end_column=17}
  %slice.9 = bf16[2,2048,32,64]{3,2,1,0} slice(%dot.5), slice={[0:2], [0:2048], [0:32], [64:128]}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/split" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=337 source_end_line=337 source_column=30 source_end_column=30}
  %get-tuple-element.194 = bf16[2,2048,1,64]{3,2,1,0} get-tuple-element(%param), index=7
  %mul.707 = bf16[2,2048,64]{2,1,0} reshape(%get-tuple-element.194), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=36 source_end_column=36}
  %mul.708 = bf16[2,2048,32,64]{3,2,1,0} broadcast(%mul.707), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=36 source_end_column=36}
  %mul.709 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.9, %mul.708), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=36 source_end_column=36}
  %sub.50 = bf16[2,2048,32,64]{3,2,1,0} subtract(%mul.706, %mul.709), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sub" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=17 source_end_column=17}
  %mul.712 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.9, %mul.705), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=339 source_end_line=339 source_column=18 source_end_column=18}
  %mul.715 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.8, %mul.708), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=339 source_end_line=339 source_column=38 source_end_column=38}
  %add.152 = bf16[2,2048,32,64]{3,2,1,0} add(%mul.712, %mul.715), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/add" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=339 source_end_line=339 source_column=18 source_end_column=18}
  %concatenate.24 = bf16[2,2048,32,128]{3,2,1,0} concatenate(%sub.50, %add.152), dimensions={3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/concatenate" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=343 source_end_line=343 source_column=12 source_end_column=12}
  %sharding_constraint.121 = bf16[2,2048,32,128]{3,2,1,0} copy(%concatenate.24), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %get-tuple-element.195 = f32[32,1024,8,128]{3,2,1,0} get-tuple-element(%param), index=8
  %dynamic-slice.3 = f32[1,1024,8,128]{3,2,1,0} dynamic-slice(%get-tuple-element.195, %get-tuple-element.188, %constant.240, %constant.240, %constant.240), dynamic_slice_sizes={1,1024,8,128}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.257 = bf16[1,1024,8,128]{3,2,1,0} convert(%dynamic-slice.3), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %convert_element_type.258 = bf16[1024,8,128]{2,1,0} reshape(%convert_element_type.257), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %all-gather.1 = bf16[4096,8,128]{2,1,0} all-gather(%convert_element_type.258), channel_id=2, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dot.6 = bf16[2,2048,8,128]{3,2,1,0} dot(%sharding_constraint.120, %all-gather.1), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %slice.10 = bf16[2,2048,8,64]{3,2,1,0} slice(%dot.6), slice={[0:2], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/split" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=337 source_end_line=337 source_column=30 source_end_column=30}
  %get-tuple-element.196 = bf16[2,2048,1,64]{3,2,1,0} get-tuple-element(%param), index=9
  %mul.716 = bf16[2,2048,64]{2,1,0} reshape(%get-tuple-element.196), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=17 source_end_column=17}
  %mul.717 = bf16[2,2048,8,64]{3,2,1,0} broadcast(%mul.716), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=17 source_end_column=17}
  %mul.718 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.10, %mul.717), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=17 source_end_column=17}
  %slice.11 = bf16[2,2048,8,64]{3,2,1,0} slice(%dot.6), slice={[0:2], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/split" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=337 source_end_line=337 source_column=30 source_end_column=30}
  %get-tuple-element.197 = bf16[2,2048,1,64]{3,2,1,0} get-tuple-element(%param), index=10
  %mul.719 = bf16[2,2048,64]{2,1,0} reshape(%get-tuple-element.197), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=36 source_end_column=36}
  %mul.720 = bf16[2,2048,8,64]{3,2,1,0} broadcast(%mul.719), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=36 source_end_column=36}
  %mul.721 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.11, %mul.720), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=36 source_end_column=36}
  %sub.51 = bf16[2,2048,8,64]{3,2,1,0} subtract(%mul.718, %mul.721), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sub" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=338 source_end_line=338 source_column=17 source_end_column=17}
  %mul.724 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.11, %mul.717), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=339 source_end_line=339 source_column=18 source_end_column=18}
  %mul.727 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.10, %mul.720), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=339 source_end_line=339 source_column=38 source_end_column=38}
  %add.153 = bf16[2,2048,8,64]{3,2,1,0} add(%mul.724, %mul.727), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/add" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=339 source_end_line=339 source_column=18 source_end_column=18}
  %concatenate.25 = bf16[2,2048,8,128]{3,2,1,0} concatenate(%sub.51, %add.153), dimensions={3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/concatenate" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=343 source_end_line=343 source_column=12 source_end_column=12}
  %sharding_constraint.123 = bf16[2,2048,8,128]{3,2,1,0} copy(%concatenate.25), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %broadcast_in_dim.82 = bf16[2,2048,1,8,128]{4,3,2,1,0} reshape(%sharding_constraint.123), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %get-tuple-element.198 = f32[32,1024,8,128]{3,2,1,0} get-tuple-element(%param), index=11
  %dynamic-slice.6 = f32[1,1024,8,128]{3,2,1,0} dynamic-slice(%get-tuple-element.198, %get-tuple-element.188, %constant.240, %constant.240, %constant.240), dynamic_slice_sizes={1,1024,8,128}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.259 = bf16[1,1024,8,128]{3,2,1,0} convert(%dynamic-slice.6), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %convert_element_type.260 = bf16[1024,8,128]{2,1,0} reshape(%convert_element_type.259), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %all-gather.2 = bf16[4096,8,128]{2,1,0} all-gather(%convert_element_type.260), channel_id=3, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dot.7 = bf16[2,2048,8,128]{3,2,1,0} dot(%sharding_constraint.120, %all-gather.2), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %sharding_constraint.124 = bf16[2,2048,8,128]{3,2,1,0} copy(%dot.7), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %broadcast_in_dim.83 = bf16[2,2048,1,8,128]{4,3,2,1,0} reshape(%sharding_constraint.124), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %concatenate.26 = bf16[2,2048,2,8,128]{4,3,2,1,0} concatenate(%broadcast_in_dim.82, %broadcast_in_dim.83), dimensions={2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.289 = bf16[0]{0} constant({})
  %constant.290 = u32[] constant(0)
  %broadcast.207 = u32[8]{0} broadcast(%constant.290), dimensions={}, metadata={op_name="broadcast.39"}
  %constant.293 = s32[4]{0} constant({0, 2, 4, 6}), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %partition-id = u32[] partition-id()
  %dynamic-slice.7 = s32[1]{0} dynamic-slice(%constant.293, %partition-id), dynamic_slice_sizes={1}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %reshape.307 = s32[] reshape(%dynamic-slice.7), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %dynamic-slice.8 = u32[2]{0} dynamic-slice(%broadcast.207, %reshape.307), dynamic_slice_sizes={2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.295 = s32[1]{0} constant({0}), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.291 = s32[] constant(8192)
  %broadcast.208 = s32[2]{0} broadcast(%constant.291), dimensions={}, metadata={op_name="broadcast.38"}
  %broadcast.209 = s32[2]{0} broadcast(%constant.240), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.52 = s32[2]{0} subtract(%broadcast.208, %broadcast.209), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %lt.42 = pred[2]{0} compare(%sub.52, %broadcast.209), direction=LT, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %select_n.72 = s32[2]{0} select(%lt.42, %broadcast.209, %sub.52), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.298 = s32[] constant(2048), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %broadcast.210 = s32[2]{0} broadcast(%constant.298), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %lt.43 = pred[2]{0} compare(%select_n.72, %broadcast.210), direction=LT, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %select_n.73 = s32[2]{0} select(%lt.43, %select_n.72, %broadcast.210), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %lt.46 = pred[2]{0} compare(%select_n.73, %broadcast.209), direction=LT, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %select_n.74 = s32[2]{0} select(%lt.46, %broadcast.209, %select_n.73), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %reduce_window_sum.11 = s32[2]{0} reduce-window(%select_n.74, %constant.240), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %concatenate.27 = s32[3]{0} concatenate(%constant.295, %reduce_window_sum.11), dimensions={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.292 = f32[0]{0} constant({})
  %te_fused_attn_forward_ffi.35 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, u8[1]{0}) custom-call(%sharding_constraint.121, %concatenate.26, %constant.289, %constant.289, %dynamic-slice.8, /*index=5*/%concatenate.27, %concatenate.27, %constant.292, %constant.292, %constant.292, /*index=10*/%constant.292, %constant.292, %constant.292), custom_call_target="te_fused_attn_forward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u32[2]{0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 2 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_forward_ffi.36 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.35), index=0, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %convert_element_type.261 = f32[2,2048,32,128]{3,2,1,0} convert(%te_fused_attn_forward_ffi.36), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.302 = f32[] constant(1), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %broadcast.211 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.302), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.303 = u32[] constant(1), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %axis_index.5 = u32[] divide(%partition-id, %constant.303), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.304 = u32[] constant(4), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %axis_index.6 = u32[] remainder(%axis_index.5, %constant.304), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %axis_index.7 = s32[] convert(%axis_index.6), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %ge.3 = pred[] compare(%axis_index.7, %constant.239), direction=GE, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %convert_element_type.262 = s32[] convert(%ge.3), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.15 = () tuple(), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %ppermute.3 = bf16[2,2048,2,8,128]{4,3,2,1,0} collective-permute(%concatenate.26), channel_id=4, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.16 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) tuple(%broadcast.208, %broadcast.208, %sharding_constraint.121, %ppermute.3, %constant.289, /*index=5*/%dynamic-slice.8, %constant.292, %constant.292, %constant.292, %constant.292, /*index=10*/%constant.292, %constant.292), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.17 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) conditional(%convert_element_type.262, %cond.15, %cond.16), branch_computations={%region_1.6._custom_call_lowering_rule, %region_2.7._custom_call_lowering_rule}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.19 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%cond.17), index=1, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %te_fused_attn_forward_ffi.37 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.35), index=1, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.54 = f32[2,32,2048,1]{3,2,1,0} subtract(%cond.19, %te_fused_attn_forward_ffi.37), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.16 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.54), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %exp.10 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.16), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %add.154 = f32[2,32,2048,1]{3,2,1,0} add(%exp.10, %broadcast.211), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %div.96 = f32[2,32,2048,1]{3,2,1,0} divide(%broadcast.211, %add.154), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %transpose.130 = f32[2,2048,32,1]{3,1,2,0} transpose(%div.96), dimensions={0,2,1,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.728 = f32[2,2048,32,1]{3,2,1,0} broadcast(%transpose.130), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.729 = f32[2,2048,32]{2,1,0} reshape(%mul.728), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.730 = f32[2,2048,32,128]{3,2,1,0} broadcast(%mul.729), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.18 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.17), index=0, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %convert_element_type.263 = f32[2,2048,32,128]{3,2,1,0} convert(%cond.18), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.55 = f32[2,2048,32,128]{3,2,1,0} subtract(%convert_element_type.261, %convert_element_type.263), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.731 = f32[2,2048,32,128]{3,2,1,0} multiply(%mul.730, %sub.55), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.56 = f32[2,2048,32,128]{3,2,1,0} subtract(%convert_element_type.261, %mul.731), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.306 = s32[] constant(2), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %ge.4 = pred[] compare(%axis_index.7, %constant.306), direction=GE, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %convert_element_type.264 = s32[] convert(%ge.4), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.20 = () tuple(), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %ppermute.4 = bf16[2,2048,2,8,128]{4,3,2,1,0} collective-permute(%ppermute.3), channel_id=4, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.21 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) tuple(%broadcast.208, %broadcast.208, %sharding_constraint.121, %ppermute.4, %constant.289, /*index=5*/%dynamic-slice.8, %constant.292, %constant.292, %constant.292, %constant.292, /*index=10*/%constant.292, %constant.292), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.22 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) conditional(%convert_element_type.264, %cond.20, %cond.21), branch_computations={%region_3.10._custom_call_lowering_rule, %region_4.11._custom_call_lowering_rule}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.24 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%cond.22), index=1, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.57 = f32[2,32,2048,1]{3,2,1,0} subtract(%te_fused_attn_forward_ffi.37, %cond.19), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.17 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.57), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %broadcast.212 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.241), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.58 = f32[2,32,2048,1]{3,2,1,0} subtract(%neg.17, %broadcast.212), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %ne.4 = pred[2,32,2048,1]{3,2,1,0} compare(%sub.58, %sub.58), direction=NE, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %max.3 = f32[2,32,2048,1]{3,2,1,0} maximum(%neg.17, %broadcast.212), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %abs.2 = f32[2,32,2048,1]{3,2,1,0} abs(%sub.58), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.18 = f32[2,32,2048,1]{3,2,1,0} negate(%abs.2), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %exp.11 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.18), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %log1p.2 = f32[2,32,2048,1]{3,2,1,0} log-plus-one(%exp.11), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %add.155 = f32[2,32,2048,1]{3,2,1,0} add(%max.3, %log1p.2), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %select_n.78 = f32[2,32,2048,1]{3,2,1,0} select(%ne.4, %neg.17, %add.155), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.19 = f32[2,32,2048,1]{3,2,1,0} negate(%select_n.78), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.59 = f32[2,32,2048,1]{3,2,1,0} subtract(%te_fused_attn_forward_ffi.37, %neg.19), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.60 = f32[2,32,2048,1]{3,2,1,0} subtract(%cond.24, %sub.59), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.20 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.60), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %exp.12 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.20), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %add.156 = f32[2,32,2048,1]{3,2,1,0} add(%exp.12, %broadcast.211), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %div.97 = f32[2,32,2048,1]{3,2,1,0} divide(%broadcast.211, %add.156), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %transpose.131 = f32[2,2048,32,1]{3,1,2,0} transpose(%div.97), dimensions={0,2,1,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.732 = f32[2,2048,32,1]{3,2,1,0} broadcast(%transpose.131), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.733 = f32[2,2048,32]{2,1,0} reshape(%mul.732), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.734 = f32[2,2048,32,128]{3,2,1,0} broadcast(%mul.733), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.23 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.22), index=0, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %convert_element_type.265 = f32[2,2048,32,128]{3,2,1,0} convert(%cond.23), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.61 = f32[2,2048,32,128]{3,2,1,0} subtract(%sub.56, %convert_element_type.265), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.735 = f32[2,2048,32,128]{3,2,1,0} multiply(%mul.734, %sub.61), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.62 = f32[2,2048,32,128]{3,2,1,0} subtract(%sub.56, %mul.735), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %constant.308 = s32[] constant(3), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %ge.5 = pred[] compare(%axis_index.7, %constant.308), direction=GE, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %convert_element_type.266 = s32[] convert(%ge.5), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.25 = () tuple(), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %ppermute.5 = bf16[2,2048,2,8,128]{4,3,2,1,0} collective-permute(%ppermute.4), channel_id=4, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.26 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) tuple(%broadcast.208, %broadcast.208, %sharding_constraint.121, %ppermute.5, %constant.289, /*index=5*/%dynamic-slice.8, %constant.292, %constant.292, %constant.292, %constant.292, /*index=10*/%constant.292, %constant.292), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.27 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) conditional(%convert_element_type.266, %cond.25, %cond.26), branch_computations={%region_5.12._custom_call_lowering_rule, %region_6.13._custom_call_lowering_rule}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.29 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%cond.27), index=1, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.63 = f32[2,32,2048,1]{3,2,1,0} subtract(%sub.59, %cond.24), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.21 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.63), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.64 = f32[2,32,2048,1]{3,2,1,0} subtract(%neg.21, %broadcast.212), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %ne.5 = pred[2,32,2048,1]{3,2,1,0} compare(%sub.64, %sub.64), direction=NE, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %max.4 = f32[2,32,2048,1]{3,2,1,0} maximum(%neg.21, %broadcast.212), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %abs.3 = f32[2,32,2048,1]{3,2,1,0} abs(%sub.64), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.22 = f32[2,32,2048,1]{3,2,1,0} negate(%abs.3), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %exp.13 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.22), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %log1p.3 = f32[2,32,2048,1]{3,2,1,0} log-plus-one(%exp.13), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %add.157 = f32[2,32,2048,1]{3,2,1,0} add(%max.4, %log1p.3), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %select_n.79 = f32[2,32,2048,1]{3,2,1,0} select(%ne.5, %neg.21, %add.157), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.23 = f32[2,32,2048,1]{3,2,1,0} negate(%select_n.79), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.65 = f32[2,32,2048,1]{3,2,1,0} subtract(%sub.59, %neg.23), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.66 = f32[2,32,2048,1]{3,2,1,0} subtract(%cond.29, %sub.65), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %neg.24 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.66), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %exp.14 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.24), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %add.350 = f32[2,32,2048,1]{3,2,1,0} add(%exp.14, %broadcast.211), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %div.98 = f32[2,32,2048,1]{3,2,1,0} divide(%broadcast.211, %add.350), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %transpose.132 = f32[2,2048,32,1]{3,1,2,0} transpose(%div.98), dimensions={0,2,1,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.736 = f32[2,2048,32,1]{3,2,1,0} broadcast(%transpose.132), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.737 = f32[2,2048,32]{2,1,0} reshape(%mul.736), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.738 = f32[2,2048,32,128]{3,2,1,0} broadcast(%mul.737), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %cond.28 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.27), index=0, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %convert_element_type.267 = f32[2,2048,32,128]{3,2,1,0} convert(%cond.28), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.67 = f32[2,2048,32,128]{3,2,1,0} subtract(%sub.62, %convert_element_type.267), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %mul.739 = f32[2,2048,32,128]{3,2,1,0} multiply(%mul.738, %sub.67), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sub.68 = f32[2,2048,32,128]{3,2,1,0} subtract(%sub.62, %mul.739), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %convert_element_type.268 = bf16[2,2048,32,128]{3,2,1,0} convert(%sub.68), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/attention.py" source_line=863 source_end_line=863 source_column=13 source_end_column=13}
  %sharding_constraint.125 = bf16[2,2048,32,128]{3,2,1,0} copy(%convert_element_type.268), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %get-tuple-element.203 = f32[32,32,128,1024]{3,2,1,0} get-tuple-element(%param), index=12
  %dynamic-slice.9 = f32[1,32,128,1024]{3,2,1,0} dynamic-slice(%get-tuple-element.203, %get-tuple-element.188, %constant.240, %constant.240, %constant.240), dynamic_slice_sizes={1,32,128,1024}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.269 = bf16[1,32,128,1024]{3,2,1,0} convert(%dynamic-slice.9), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %convert_element_type.270 = bf16[32,128,1024]{2,1,0} reshape(%convert_element_type.269), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %all-gather.4 = bf16[32,128,4096]{2,1,0} all-gather(%convert_element_type.270), channel_id=6, replica_groups=[1,4]<=[4], dimensions={2}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dot.8 = bf16[2,2048,4096]{2,1,0} dot(%sharding_constraint.125, %all-gather.4), lhs_contracting_dims={2,3}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %sharding_constraint.126 = bf16[2,2048,4096]{2,1,0} copy(%dot.8), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %add.352 = bf16[2,2048,4096]{2,1,0} add(%sharding_constraint.118, %sharding_constraint.126), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/add" source_file="/opt/maxtext/src/MaxText/layers/llama2.py" source_line=174 source_end_line=174 source_column=26 source_end_column=26}
  %convert_element_type.271 = f32[2,2048,4096]{2,1,0} convert(%add.352), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=66 source_end_line=66 source_column=8 source_end_column=8}
  %square.91 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.271, %convert_element_type.271), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/square" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %reduce.1 = f32[2,2048]{1,0} reduce(%square.91, %constant.241), dimensions={2}, to_apply=%region_3.3, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/reduce_sum" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
  %div.99 = f32[2,2048]{1,0} multiply(%reduce.1, %closed_call.8), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/div" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
  %add.353 = f32[2,2048]{1,0} add(%div.99, %closed_call.9), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/add" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=34 source_end_column=34}
  %rsqrt.17 = f32[2,2048]{1,0} rsqrt(%add.353), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/rsqrt" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=24 source_end_column=24}
  %mul.740 = f32[2,2048,4096]{2,1,0} broadcast(%rsqrt.17), dimensions={0,1}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %mul.741 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.271, %mul.740), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %convert_element_type.272 = bf16[2,2048,4096]{2,1,0} convert(%mul.741), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=8 source_end_column=8}
  %dot_general.127 = bf16[2,2048,4096]{2,1,0} multiply(%dot_general.124, %convert_element_type.272), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %sharding_constraint.127 = bf16[2,2048,4096]{2,1,0} copy(%dot_general.127), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %get-tuple-element.204 = f32[32,1024,14336]{2,1,0} get-tuple-element(%param), index=13
  %dynamic-slice.10 = f32[1,1024,14336]{2,1,0} dynamic-slice(%get-tuple-element.204, %get-tuple-element.188, %constant.240, %constant.240), dynamic_slice_sizes={1,1024,14336}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.273 = bf16[1,1024,14336]{2,1,0} convert(%dynamic-slice.10), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %convert_element_type.274 = bf16[1024,14336]{1,0} reshape(%convert_element_type.273), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %all-gather.5 = bf16[4096,14336]{1,0} all-gather(%convert_element_type.274), channel_id=7, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dot.9 = bf16[2,2048,14336]{2,1,0} dot(%sharding_constraint.127, %all-gather.5), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %constant.317 = bf16[] constant(1)
  %jit_silu_.6 = bf16[2,2048,14336]{2,1,0} broadcast(%constant.317), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/jit(silu)"}
  %neg.28 = bf16[2,2048,14336]{2,1,0} negate(%dot.9), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/jit(silu)/neg" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %exp.16 = bf16[2,2048,14336]{2,1,0} exponential(%neg.28), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/jit(silu)/exp" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %add.354 = bf16[2,2048,14336]{2,1,0} add(%exp.16, %jit_silu_.6), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/jit(silu)/add" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %div.100 = bf16[2,2048,14336]{2,1,0} divide(%jit_silu_.6, %add.354), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/jit(silu)/div" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %mul.742 = bf16[2,2048,14336]{2,1,0} multiply(%dot.9, %div.100), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/jit(silu)/mul" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %get-tuple-element.205 = f32[32,1024,14336]{2,1,0} get-tuple-element(%param), index=14
  %dynamic-slice.11 = f32[1,1024,14336]{2,1,0} dynamic-slice(%get-tuple-element.205, %get-tuple-element.188, %constant.240, %constant.240), dynamic_slice_sizes={1,1024,14336}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.275 = bf16[1,1024,14336]{2,1,0} convert(%dynamic-slice.11), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %convert_element_type.276 = bf16[1024,14336]{1,0} reshape(%convert_element_type.275), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %all-gather.6 = bf16[4096,14336]{1,0} all-gather(%convert_element_type.276), channel_id=8, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dot.10 = bf16[2,2048,14336]{2,1,0} dot(%sharding_constraint.127, %all-gather.6), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %mul.743 = bf16[2,2048,14336]{2,1,0} multiply(%mul.742, %dot.10), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=522 source_end_line=522 source_column=8 source_end_column=8}
  %sharding_constraint.128 = bf16[2,2048,14336]{2,1,0} copy(%mul.743), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %get-tuple-element.206 = f32[32,14336,1024]{2,1,0} get-tuple-element(%param), index=15
  %dynamic-slice.12 = f32[1,14336,1024]{2,1,0} dynamic-slice(%get-tuple-element.206, %get-tuple-element.188, %constant.240, %constant.240), dynamic_slice_sizes={1,14336,1024}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.277 = bf16[1,14336,1024]{2,1,0} convert(%dynamic-slice.12), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %convert_element_type.278 = bf16[14336,1024]{1,0} reshape(%convert_element_type.277), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %all-gather.7 = bf16[14336,4096]{1,0} all-gather(%convert_element_type.278), channel_id=9, replica_groups=[1,4]<=[4], dimensions={1}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dot.11 = bf16[2,2048,4096]{2,1,0} dot(%sharding_constraint.128, %all-gather.7), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %sharding_constraint.129 = bf16[2,2048,4096]{2,1,0} copy(%dot.11), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %add.355 = bf16[2,2048,4096]{2,1,0} add(%sharding_constraint.129, %add.352), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/add" source_file="/opt/maxtext/src/MaxText/layers/llama2.py" source_line=193 source_end_line=193 source_column=19 source_end_column=19}
  %sharding_constraint.130 = bf16[2,2048,4096]{2,1,0} copy(%add.355), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/closed_call/layers/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %get-tuple-element.207 = bf16[32,2,2048,4096]{3,2,1,0} get-tuple-element(%param), index=2
  %broadcast_in_dim.90 = bf16[1,2,2048,4096]{3,2,1,0} reshape(%sharding_constraint.118), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic-update-slice = bf16[32,2,2048,4096]{3,2,1,0} dynamic-update-slice(%get-tuple-element.207, %broadcast_in_dim.90, %get-tuple-element.188, %constant.240, %constant.240, /*index=5*/%constant.240), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  ROOT %tuple.21 = (s32[], bf16[2,2048,4096]{2,1,0}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{1,0}, f32[32,4096]{1,0}, /*index=5*/f32[32,1024,32,128]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, /*index=10*/bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,1024,14336]{2,1,0}, /*index=15*/f32[32,14336,1024]{2,1,0}) tuple(%add.150, %sharding_constraint.130, %dynamic-update-slice, %get-tuple-element.189, %get-tuple-element.191, /*index=5*/%get-tuple-element.192, %get-tuple-element.193, %get-tuple-element.194, %get-tuple-element.195, %get-tuple-element.196, /*index=10*/%get-tuple-element.197, %get-tuple-element.198, %get-tuple-element.203, %get-tuple-element.204, %get-tuple-element.205, /*index=15*/%get-tuple-element.206)
}

%region_4.5_spmd (param.1: (s32[], bf16[2,2048,4096], bf16[32,2,2048,4096], f32[32,4096], f32[32,4096], /*index=5*/f32[32,1024,32,128], bf16[2,2048,1,64], bf16[2,2048,1,64], f32[32,1024,8,128], bf16[2,2048,1,64], /*index=10*/bf16[2,2048,1,64], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,14336], f32[32,1024,14336], /*index=15*/f32[32,14336,1024])) -> pred[] {
  %param.1 = (s32[], bf16[2,2048,4096]{2,1,0}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{1,0}, f32[32,4096]{1,0}, /*index=5*/f32[32,1024,32,128]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, /*index=10*/bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,1024,14336]{2,1,0}, /*index=15*/f32[32,14336,1024]{2,1,0}) parameter(0)
  %get-tuple-element.208 = s32[] get-tuple-element(%param.1), index=0
  %constant.325 = s32[] constant(32)
  ROOT %lt.18 = pred[] compare(%get-tuple-element.208, %constant.325), direction=LT, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while/cond/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
}

%region_5.6 (reduce_sum.36: f32[], reduce_sum.37: f32[]) -> f32[] {
  %reduce_sum.36 = f32[] parameter(0), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reduce_sum"}
  %reduce_sum.37 = f32[] parameter(1), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reduce_sum"}
  ROOT %reduce_sum.38 = f32[] add(%reduce_sum.36, %reduce_sum.37), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reduce_sum" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
}

%region_6.7 (reduce_max.0: f32[], reduce_max.1: f32[]) -> f32[] {
  %reduce_max.0 = f32[] parameter(0), metadata={op_name="jit(train_step)/jvp()/reduce_max"}
  %reduce_max.1 = f32[] parameter(1), metadata={op_name="jit(train_step)/jvp()/reduce_max"}
  ROOT %reduce_max.2 = f32[] maximum(%reduce_max.0, %reduce_max.1), metadata={op_name="jit(train_step)/jvp()/reduce_max" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=552 source_end_line=552 source_column=14 source_end_column=14}
}

%region_7.8 (reduce_sum.39: f32[], reduce_sum.43: f32[]) -> f32[] {
  %reduce_sum.39 = f32[] parameter(0), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  %reduce_sum.43 = f32[] parameter(1), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  ROOT %reduce_sum.44 = f32[] add(%reduce_sum.39, %reduce_sum.43), metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=555 source_end_line=555 source_column=12 source_end_column=12}
}

%add.9.clone (x.19: bf16[], y.19: bf16[]) -> bf16[] {
  %x.19 = bf16[] parameter(0)
  %y.19 = bf16[] parameter(1)
  ROOT %add.397 = bf16[] add(%x.19, %y.19)
}

%region_8.9 (reduce_sum.45: f32[], reduce_sum.46: f32[]) -> f32[] {
  %reduce_sum.45 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.46 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.50 = f32[] add(%reduce_sum.45, %reduce_sum.46), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_9.10 (reduce_sum.51: f32[], reduce_sum.52: f32[]) -> f32[] {
  %reduce_sum.51 = f32[] parameter(0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reduce_sum"}
  %reduce_sum.52 = f32[] parameter(1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reduce_sum"}
  ROOT %reduce_sum.53 = f32[] add(%reduce_sum.51, %reduce_sum.52), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reduce_sum" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
}

%region_11.11 (reduce_sum.57: f32[], reduce_sum.58: f32[]) -> f32[] {
  %reduce_sum.57 = f32[] parameter(0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/reduce_sum"}
  %reduce_sum.58 = f32[] parameter(1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/reduce_sum"}
  ROOT %reduce_sum.59 = f32[] add(%reduce_sum.57, %reduce_sum.58), metadata={op_name="checkpoint/rematted_computation/layers/reduce_sum" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
}

%region_0.3._custom_call_lowering_rule.1 (reduce_window_sum.0: s32[], reduce_window_sum.7: s32[]) -> s32[] {
  %reduce_window_sum.0 = s32[] parameter(0), metadata={op_name="reduce_window_sum"}
  %reduce_window_sum.7 = s32[] parameter(1), metadata={op_name="reduce_window_sum"}
  ROOT %reduce_window_sum.13 = s32[] add(%reduce_window_sum.0, %reduce_window_sum.7), metadata={op_name="reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
}

%region_1.6._custom_call_lowering_rule.1 (arg_empty_tuple.0: ()) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_empty_tuple.0 = () parameter(0)
  %constant.327 = bf16[] constant(0)
  %broadcast.217 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.327), dimensions={}
  %constant.326 = f32[] constant(-inf)
  %broadcast.216 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.326), dimensions={}
  ROOT %tuple.22 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%broadcast.217, %broadcast.216)
}

%region_2.7._custom_call_lowering_rule.1 (arg_tuple.11: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/u32[2], f32[0], f32[0], f32[0], f32[0], /*index=10*/f32[0], f32[0])) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_tuple.11 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.211 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.11), index=2
  %get-tuple-element.212 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.11), index=3
  %constant.328 = bf16[0]{0} constant({})
  %get-tuple-element.213 = bf16[0]{0} get-tuple-element(%arg_tuple.11), index=4
  %get-tuple-element.214 = u32[2]{0} get-tuple-element(%arg_tuple.11), index=5
  %constant.329 = s32[1]{0} constant({0})
  %get-tuple-element.209 = s32[2]{0} get-tuple-element(%arg_tuple.11), index=0
  %constant.330 = s32[] constant(2048)
  %broadcast.219 = s32[2]{0} broadcast(%constant.330), dimensions={}
  %sub.72 = s32[2]{0} subtract(%get-tuple-element.209, %broadcast.219), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %constant.331 = s32[] constant(0)
  %broadcast.218 = s32[2]{0} broadcast(%constant.331), dimensions={}
  %lt.19 = pred[2]{0} compare(%sub.72, %broadcast.218), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.81 = s32[2]{0} select(%lt.19, %broadcast.218, %sub.72), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.20 = pred[2]{0} compare(%select_n.81, %broadcast.219), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.82 = s32[2]{0} select(%lt.20, %select_n.81, %broadcast.219), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.23 = pred[2]{0} compare(%select_n.82, %broadcast.218), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.83 = s32[2]{0} select(%lt.23, %broadcast.218, %select_n.82), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.14 = s32[2]{0} reduce-window(%select_n.83, %constant.331), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.29 = s32[3]{0} concatenate(%constant.329, %reduce_window_sum.14), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.210 = s32[2]{0} get-tuple-element(%arg_tuple.11), index=1
  %sub.73 = s32[2]{0} subtract(%get-tuple-element.210, %broadcast.219), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %lt.21 = pred[2]{0} compare(%sub.73, %broadcast.218), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.84 = s32[2]{0} select(%lt.21, %broadcast.218, %sub.73), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.22 = pred[2]{0} compare(%select_n.84, %broadcast.219), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.85 = s32[2]{0} select(%lt.22, %select_n.84, %broadcast.219), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.30 = pred[2]{0} compare(%select_n.85, %broadcast.218), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.86 = s32[2]{0} select(%lt.30, %broadcast.218, %select_n.85), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.15 = s32[2]{0} reduce-window(%select_n.86, %constant.331), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.30 = s32[3]{0} concatenate(%constant.329, %reduce_window_sum.15), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.215 = f32[0]{0} get-tuple-element(%arg_tuple.11), index=6
  %get-tuple-element.216 = f32[0]{0} get-tuple-element(%arg_tuple.11), index=7
  %get-tuple-element.217 = f32[0]{0} get-tuple-element(%arg_tuple.11), index=8
  %get-tuple-element.218 = f32[0]{0} get-tuple-element(%arg_tuple.11), index=9
  %get-tuple-element.219 = f32[0]{0} get-tuple-element(%arg_tuple.11), index=10
  %get-tuple-element.220 = f32[0]{0} get-tuple-element(%arg_tuple.11), index=11
  %te_fused_attn_forward_ffi.0 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, u8[1]{0}) custom-call(%get-tuple-element.211, %get-tuple-element.212, %constant.328, %get-tuple-element.213, %get-tuple-element.214, /*index=5*/%concatenate.29, %concatenate.30, %get-tuple-element.215, %get-tuple-element.216, %get-tuple-element.217, /*index=10*/%get-tuple-element.218, %get-tuple-element.219, %get-tuple-element.220), custom_call_target="te_fused_attn_forward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u32[2]{0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_forward_ffi.1 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.0), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %te_fused_attn_forward_ffi.2 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.0), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  ROOT %tuple.23 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%te_fused_attn_forward_ffi.1, %te_fused_attn_forward_ffi.2)
}

%region_3.10._custom_call_lowering_rule.1 (arg_empty_tuple.2: ()) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_empty_tuple.2 = () parameter(0)
  %constant.337 = bf16[] constant(0)
  %broadcast.221 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.337), dimensions={}
  %constant.336 = f32[] constant(-inf)
  %broadcast.220 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.336), dimensions={}
  ROOT %tuple.24 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%broadcast.221, %broadcast.220)
}

%region_4.11._custom_call_lowering_rule.1 (arg_tuple.12: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/u32[2], f32[0], f32[0], f32[0], f32[0], /*index=10*/f32[0], f32[0])) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_tuple.12 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.223 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.12), index=2
  %get-tuple-element.224 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.12), index=3
  %constant.338 = bf16[0]{0} constant({})
  %get-tuple-element.225 = bf16[0]{0} get-tuple-element(%arg_tuple.12), index=4
  %get-tuple-element.226 = u32[2]{0} get-tuple-element(%arg_tuple.12), index=5
  %constant.339 = s32[1]{0} constant({0})
  %get-tuple-element.221 = s32[2]{0} get-tuple-element(%arg_tuple.12), index=0
  %constant.344 = s32[] constant(4096)
  %broadcast.224 = s32[2]{0} broadcast(%constant.344), dimensions={}
  %sub.74 = s32[2]{0} subtract(%get-tuple-element.221, %broadcast.224), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %constant.342 = s32[] constant(0)
  %broadcast.223 = s32[2]{0} broadcast(%constant.342), dimensions={}
  %lt.31 = pred[2]{0} compare(%sub.74, %broadcast.223), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.87 = s32[2]{0} select(%lt.31, %broadcast.223, %sub.74), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %constant.340 = s32[] constant(2048)
  %broadcast.222 = s32[2]{0} broadcast(%constant.340), dimensions={}
  %lt.32 = pred[2]{0} compare(%select_n.87, %broadcast.222), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.88 = s32[2]{0} select(%lt.32, %select_n.87, %broadcast.222), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.35 = pred[2]{0} compare(%select_n.88, %broadcast.223), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.89 = s32[2]{0} select(%lt.35, %broadcast.223, %select_n.88), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.16 = s32[2]{0} reduce-window(%select_n.89, %constant.342), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.31 = s32[3]{0} concatenate(%constant.339, %reduce_window_sum.16), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.222 = s32[2]{0} get-tuple-element(%arg_tuple.12), index=1
  %sub.75 = s32[2]{0} subtract(%get-tuple-element.222, %broadcast.224), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %lt.33 = pred[2]{0} compare(%sub.75, %broadcast.223), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.90 = s32[2]{0} select(%lt.33, %broadcast.223, %sub.75), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.34 = pred[2]{0} compare(%select_n.90, %broadcast.222), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.91 = s32[2]{0} select(%lt.34, %select_n.90, %broadcast.222), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.48 = pred[2]{0} compare(%select_n.91, %broadcast.223), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.92 = s32[2]{0} select(%lt.48, %broadcast.223, %select_n.91), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.17 = s32[2]{0} reduce-window(%select_n.92, %constant.342), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.32 = s32[3]{0} concatenate(%constant.339, %reduce_window_sum.17), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.227 = f32[0]{0} get-tuple-element(%arg_tuple.12), index=6
  %get-tuple-element.228 = f32[0]{0} get-tuple-element(%arg_tuple.12), index=7
  %get-tuple-element.229 = f32[0]{0} get-tuple-element(%arg_tuple.12), index=8
  %get-tuple-element.230 = f32[0]{0} get-tuple-element(%arg_tuple.12), index=9
  %get-tuple-element.231 = f32[0]{0} get-tuple-element(%arg_tuple.12), index=10
  %get-tuple-element.232 = f32[0]{0} get-tuple-element(%arg_tuple.12), index=11
  %te_fused_attn_forward_ffi.3 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, u8[1]{0}) custom-call(%get-tuple-element.223, %get-tuple-element.224, %constant.338, %get-tuple-element.225, %get-tuple-element.226, /*index=5*/%concatenate.31, %concatenate.32, %get-tuple-element.227, %get-tuple-element.228, %get-tuple-element.229, /*index=10*/%get-tuple-element.230, %get-tuple-element.231, %get-tuple-element.232), custom_call_target="te_fused_attn_forward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u32[2]{0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_forward_ffi.4 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.3), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %te_fused_attn_forward_ffi.5 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.3), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  ROOT %tuple.25 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%te_fused_attn_forward_ffi.4, %te_fused_attn_forward_ffi.5)
}

%region_5.12._custom_call_lowering_rule.1 (arg_empty_tuple.4: ()) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_empty_tuple.4 = () parameter(0)
  %constant.348 = bf16[] constant(0)
  %broadcast.226 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.348), dimensions={}
  %constant.347 = f32[] constant(-inf)
  %broadcast.225 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.347), dimensions={}
  ROOT %tuple.26 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%broadcast.226, %broadcast.225)
}

%region_6.13._custom_call_lowering_rule.1 (arg_tuple.13: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/u32[2], f32[0], f32[0], f32[0], f32[0], /*index=10*/f32[0], f32[0])) -> (bf16[2,2048,32,128], f32[2,32,2048,1]) {
  %arg_tuple.13 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.235 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.13), index=2
  %get-tuple-element.236 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.13), index=3
  %constant.349 = bf16[0]{0} constant({})
  %get-tuple-element.237 = bf16[0]{0} get-tuple-element(%arg_tuple.13), index=4
  %get-tuple-element.238 = u32[2]{0} get-tuple-element(%arg_tuple.13), index=5
  %constant.350 = s32[1]{0} constant({0})
  %get-tuple-element.233 = s32[2]{0} get-tuple-element(%arg_tuple.13), index=0
  %constant.355 = s32[] constant(6144)
  %broadcast.229 = s32[2]{0} broadcast(%constant.355), dimensions={}
  %sub.76 = s32[2]{0} subtract(%get-tuple-element.233, %broadcast.229), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %constant.353 = s32[] constant(0)
  %broadcast.228 = s32[2]{0} broadcast(%constant.353), dimensions={}
  %lt.49 = pred[2]{0} compare(%sub.76, %broadcast.228), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.93 = s32[2]{0} select(%lt.49, %broadcast.228, %sub.76), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %constant.351 = s32[] constant(2048)
  %broadcast.227 = s32[2]{0} broadcast(%constant.351), dimensions={}
  %lt.50 = pred[2]{0} compare(%select_n.93, %broadcast.227), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.94 = s32[2]{0} select(%lt.50, %select_n.93, %broadcast.227), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.53 = pred[2]{0} compare(%select_n.94, %broadcast.228), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.95 = s32[2]{0} select(%lt.53, %broadcast.228, %select_n.94), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.18 = s32[2]{0} reduce-window(%select_n.95, %constant.353), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.33 = s32[3]{0} concatenate(%constant.350, %reduce_window_sum.18), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.234 = s32[2]{0} get-tuple-element(%arg_tuple.13), index=1
  %sub.77 = s32[2]{0} subtract(%get-tuple-element.234, %broadcast.229), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %lt.51 = pred[2]{0} compare(%sub.77, %broadcast.228), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.96 = s32[2]{0} select(%lt.51, %broadcast.228, %sub.77), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.52 = pred[2]{0} compare(%select_n.96, %broadcast.227), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.97 = s32[2]{0} select(%lt.52, %select_n.96, %broadcast.227), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.54 = pred[2]{0} compare(%select_n.97, %broadcast.228), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %select_n.98 = s32[2]{0} select(%lt.54, %broadcast.228, %select_n.97), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.19 = s32[2]{0} reduce-window(%select_n.98, %constant.353), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.34 = s32[3]{0} concatenate(%constant.350, %reduce_window_sum.19), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %get-tuple-element.239 = f32[0]{0} get-tuple-element(%arg_tuple.13), index=6
  %get-tuple-element.240 = f32[0]{0} get-tuple-element(%arg_tuple.13), index=7
  %get-tuple-element.241 = f32[0]{0} get-tuple-element(%arg_tuple.13), index=8
  %get-tuple-element.242 = f32[0]{0} get-tuple-element(%arg_tuple.13), index=9
  %get-tuple-element.243 = f32[0]{0} get-tuple-element(%arg_tuple.13), index=10
  %get-tuple-element.244 = f32[0]{0} get-tuple-element(%arg_tuple.13), index=11
  %te_fused_attn_forward_ffi.6 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, u8[1]{0}) custom-call(%get-tuple-element.235, %get-tuple-element.236, %constant.349, %get-tuple-element.237, %get-tuple-element.238, /*index=5*/%concatenate.33, %concatenate.34, %get-tuple-element.239, %get-tuple-element.240, %get-tuple-element.241, /*index=10*/%get-tuple-element.242, %get-tuple-element.243, %get-tuple-element.244), custom_call_target="te_fused_attn_forward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u32[2]{0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_forward_ffi.7 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.6), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  %te_fused_attn_forward_ffi.8 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.6), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_forward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1927 source_end_line=1929 source_column=60 source_end_column=21}
  ROOT %tuple.27 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) tuple(%te_fused_attn_forward_ffi.7, %te_fused_attn_forward_ffi.8)
}

%region_12.12 (reduce_sum.60: f32[], reduce_sum.64: f32[]) -> f32[] {
  %reduce_sum.60 = f32[] parameter(0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/reduce_sum"}
  %reduce_sum.64 = f32[] parameter(1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/reduce_sum"}
  ROOT %reduce_sum.65 = f32[] add(%reduce_sum.60, %reduce_sum.64), metadata={op_name="checkpoint/rematted_computation/layers/reduce_sum" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
}

%region_13.13 (reduce_sum.66: f32[], reduce_sum.67: f32[]) -> f32[] {
  %reduce_sum.66 = f32[] parameter(0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/reduce_sum"}
  %reduce_sum.67 = f32[] parameter(1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/reduce_sum"}
  ROOT %reduce_sum.71 = f32[] add(%reduce_sum.66, %reduce_sum.67), metadata={op_name="checkpoint/layers/reduce_sum" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
}

%region_1.7._custom_call_lowering_rule (arg_empty_tuple.6: ()) -> (bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0]) {
  %arg_empty_tuple.6 = () parameter(0)
  %constant.359 = bf16[] constant(0)
  %broadcast.231 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.359), dimensions={}
  %broadcast.230 = bf16[2,2048,2,8,128]{4,3,2,1,0} broadcast(%constant.359), dimensions={}
  %constant.358 = bf16[0]{0} constant({})
  ROOT %tuple.28 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) tuple(%broadcast.231, %broadcast.230, %constant.358)
}

%region_0.3._custom_call_lowering_rule.2 (reduce_window_sum.20: s32[], reduce_window_sum.21: s32[]) -> s32[] {
  %reduce_window_sum.20 = s32[] parameter(0), metadata={op_name="reduce_window_sum"}
  %reduce_window_sum.21 = s32[] parameter(1), metadata={op_name="reduce_window_sum"}
  ROOT %reduce_window_sum.22 = s32[] add(%reduce_window_sum.20, %reduce_window_sum.21), metadata={op_name="reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
}

%region_2.8._custom_call_lowering_rule (arg_tuple.14: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/f32[2,32,2048,1], u32[2,4], bf16[2,2048,32,128], bf16[2,2048,32,128], f32[0], /*index=10*/f32[0], f32[0], f32[0], f32[0], f32[0])) -> (bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0]) {
  %arg_tuple.14 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.247 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.14), index=2
  %get-tuple-element.248 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.14), index=3
  %constant.361 = bf16[0]{0} constant({})
  %get-tuple-element.249 = bf16[0]{0} get-tuple-element(%arg_tuple.14), index=4
  %get-tuple-element.250 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%arg_tuple.14), index=5
  %get-tuple-element.251 = u32[2,4]{1,0} get-tuple-element(%arg_tuple.14), index=6
  %get-tuple-element.252 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.14), index=7
  %get-tuple-element.253 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.14), index=8
  %constant.362 = s32[1]{0} constant({0})
  %get-tuple-element.245 = s32[2]{0} get-tuple-element(%arg_tuple.14), index=0
  %constant.363 = s32[] constant(2048)
  %broadcast.233 = s32[2]{0} broadcast(%constant.363), dimensions={}
  %sub.78 = s32[2]{0} subtract(%get-tuple-element.245, %broadcast.233), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %constant.364 = s32[] constant(0)
  %broadcast.232 = s32[2]{0} broadcast(%constant.364), dimensions={}
  %lt.55 = pred[2]{0} compare(%sub.78, %broadcast.232), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.99 = s32[2]{0} select(%lt.55, %broadcast.232, %sub.78), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.56 = pred[2]{0} compare(%select_n.99, %broadcast.233), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.100 = s32[2]{0} select(%lt.56, %select_n.99, %broadcast.233), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.59 = pred[2]{0} compare(%select_n.100, %broadcast.232), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.101 = s32[2]{0} select(%lt.59, %broadcast.232, %select_n.100), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.23 = s32[2]{0} reduce-window(%select_n.101, %constant.364), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.2, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.35 = s32[3]{0} concatenate(%constant.362, %reduce_window_sum.23), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %get-tuple-element.246 = s32[2]{0} get-tuple-element(%arg_tuple.14), index=1
  %sub.79 = s32[2]{0} subtract(%get-tuple-element.246, %broadcast.233), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %lt.57 = pred[2]{0} compare(%sub.79, %broadcast.232), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.102 = s32[2]{0} select(%lt.57, %broadcast.232, %sub.79), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.58 = pred[2]{0} compare(%select_n.102, %broadcast.233), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.103 = s32[2]{0} select(%lt.58, %select_n.102, %broadcast.233), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.60 = pred[2]{0} compare(%select_n.103, %broadcast.232), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.104 = s32[2]{0} select(%lt.60, %broadcast.232, %select_n.103), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.24 = s32[2]{0} reduce-window(%select_n.104, %constant.364), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.2, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.36 = s32[3]{0} concatenate(%constant.362, %reduce_window_sum.24), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %get-tuple-element.254 = f32[0]{0} get-tuple-element(%arg_tuple.14), index=9
  %get-tuple-element.255 = f32[0]{0} get-tuple-element(%arg_tuple.14), index=10
  %get-tuple-element.256 = f32[0]{0} get-tuple-element(%arg_tuple.14), index=11
  %get-tuple-element.257 = f32[0]{0} get-tuple-element(%arg_tuple.14), index=12
  %get-tuple-element.258 = f32[0]{0} get-tuple-element(%arg_tuple.14), index=13
  %get-tuple-element.259 = f32[0]{0} get-tuple-element(%arg_tuple.14), index=14
  %te_fused_attn_backward_ffi.12 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u8[134742144]{0}) custom-call(%get-tuple-element.247, %get-tuple-element.248, %constant.361, %get-tuple-element.249, %get-tuple-element.250, /*index=5*/%get-tuple-element.251, %get-tuple-element.252, %get-tuple-element.253, %concatenate.35, %concatenate.36, /*index=10*/%get-tuple-element.254, %get-tuple-element.255, %get-tuple-element.256, %get-tuple-element.257, %get-tuple-element.258, /*index=15*/%get-tuple-element.259), custom_call_target="te_fused_attn_backward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_backward_ffi.13 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_backward_ffi.12), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %te_fused_attn_backward_ffi.14 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%te_fused_attn_backward_ffi.12), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %te_fused_attn_backward_ffi.16 = bf16[0]{0} get-tuple-element(%te_fused_attn_backward_ffi.12), index=3, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  ROOT %tuple.29 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) tuple(%te_fused_attn_backward_ffi.13, %te_fused_attn_backward_ffi.14, %te_fused_attn_backward_ffi.16)
}

%region_3.9._custom_call_lowering_rule (arg_empty_tuple.7: ()) -> (bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0]) {
  %arg_empty_tuple.7 = () parameter(0)
  %constant.370 = bf16[] constant(0)
  %broadcast.235 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.370), dimensions={}
  %broadcast.234 = bf16[2,2048,2,8,128]{4,3,2,1,0} broadcast(%constant.370), dimensions={}
  %constant.369 = bf16[0]{0} constant({})
  ROOT %tuple.30 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) tuple(%broadcast.235, %broadcast.234, %constant.369)
}

%region_4.10._custom_call_lowering_rule (arg_tuple.15: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/f32[2,32,2048,1], u32[2,4], bf16[2,2048,32,128], bf16[2,2048,32,128], f32[0], /*index=10*/f32[0], f32[0], f32[0], f32[0], f32[0])) -> (bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0]) {
  %arg_tuple.15 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.262 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.15), index=2
  %get-tuple-element.263 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.15), index=3
  %constant.372 = bf16[0]{0} constant({})
  %get-tuple-element.264 = bf16[0]{0} get-tuple-element(%arg_tuple.15), index=4
  %get-tuple-element.265 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%arg_tuple.15), index=5
  %get-tuple-element.266 = u32[2,4]{1,0} get-tuple-element(%arg_tuple.15), index=6
  %get-tuple-element.267 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.15), index=7
  %get-tuple-element.268 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.15), index=8
  %constant.373 = s32[1]{0} constant({0})
  %get-tuple-element.260 = s32[2]{0} get-tuple-element(%arg_tuple.15), index=0
  %constant.378 = s32[] constant(4096)
  %broadcast.238 = s32[2]{0} broadcast(%constant.378), dimensions={}
  %sub.80 = s32[2]{0} subtract(%get-tuple-element.260, %broadcast.238), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %constant.376 = s32[] constant(0)
  %broadcast.237 = s32[2]{0} broadcast(%constant.376), dimensions={}
  %lt.61 = pred[2]{0} compare(%sub.80, %broadcast.237), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.105 = s32[2]{0} select(%lt.61, %broadcast.237, %sub.80), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %constant.374 = s32[] constant(2048)
  %broadcast.236 = s32[2]{0} broadcast(%constant.374), dimensions={}
  %lt.62 = pred[2]{0} compare(%select_n.105, %broadcast.236), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.106 = s32[2]{0} select(%lt.62, %select_n.105, %broadcast.236), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.65 = pred[2]{0} compare(%select_n.106, %broadcast.237), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.107 = s32[2]{0} select(%lt.65, %broadcast.237, %select_n.106), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.25 = s32[2]{0} reduce-window(%select_n.107, %constant.376), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.2, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.37 = s32[3]{0} concatenate(%constant.373, %reduce_window_sum.25), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %get-tuple-element.261 = s32[2]{0} get-tuple-element(%arg_tuple.15), index=1
  %sub.81 = s32[2]{0} subtract(%get-tuple-element.261, %broadcast.238), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %lt.63 = pred[2]{0} compare(%sub.81, %broadcast.237), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.108 = s32[2]{0} select(%lt.63, %broadcast.237, %sub.81), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.64 = pred[2]{0} compare(%select_n.108, %broadcast.236), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.109 = s32[2]{0} select(%lt.64, %select_n.108, %broadcast.236), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.66 = pred[2]{0} compare(%select_n.109, %broadcast.237), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.110 = s32[2]{0} select(%lt.66, %broadcast.237, %select_n.109), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.26 = s32[2]{0} reduce-window(%select_n.110, %constant.376), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.2, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.38 = s32[3]{0} concatenate(%constant.373, %reduce_window_sum.26), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %get-tuple-element.269 = f32[0]{0} get-tuple-element(%arg_tuple.15), index=9
  %get-tuple-element.270 = f32[0]{0} get-tuple-element(%arg_tuple.15), index=10
  %get-tuple-element.271 = f32[0]{0} get-tuple-element(%arg_tuple.15), index=11
  %get-tuple-element.272 = f32[0]{0} get-tuple-element(%arg_tuple.15), index=12
  %get-tuple-element.273 = f32[0]{0} get-tuple-element(%arg_tuple.15), index=13
  %get-tuple-element.274 = f32[0]{0} get-tuple-element(%arg_tuple.15), index=14
  %te_fused_attn_backward_ffi.24 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u8[134742144]{0}) custom-call(%get-tuple-element.262, %get-tuple-element.263, %constant.372, %get-tuple-element.264, %get-tuple-element.265, /*index=5*/%get-tuple-element.266, %get-tuple-element.267, %get-tuple-element.268, %concatenate.37, %concatenate.38, /*index=10*/%get-tuple-element.269, %get-tuple-element.270, %get-tuple-element.271, %get-tuple-element.272, %get-tuple-element.273, /*index=15*/%get-tuple-element.274), custom_call_target="te_fused_attn_backward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_backward_ffi.25 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_backward_ffi.24), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %te_fused_attn_backward_ffi.26 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%te_fused_attn_backward_ffi.24), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %te_fused_attn_backward_ffi.28 = bf16[0]{0} get-tuple-element(%te_fused_attn_backward_ffi.24), index=3, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  ROOT %tuple.31 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) tuple(%te_fused_attn_backward_ffi.25, %te_fused_attn_backward_ffi.26, %te_fused_attn_backward_ffi.28)
}

%region_5.11._custom_call_lowering_rule (arg_empty_tuple.8: ()) -> (bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0]) {
  %arg_empty_tuple.8 = () parameter(0)
  %constant.382 = bf16[] constant(0)
  %broadcast.240 = bf16[2,2048,32,128]{3,2,1,0} broadcast(%constant.382), dimensions={}
  %broadcast.239 = bf16[2,2048,2,8,128]{4,3,2,1,0} broadcast(%constant.382), dimensions={}
  %constant.381 = bf16[0]{0} constant({})
  ROOT %tuple.32 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) tuple(%broadcast.240, %broadcast.239, %constant.381)
}

%region_6.12._custom_call_lowering_rule (arg_tuple.16: (s32[2], s32[2], bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0], /*index=5*/f32[2,32,2048,1], u32[2,4], bf16[2,2048,32,128], bf16[2,2048,32,128], f32[0], /*index=10*/f32[0], f32[0], f32[0], f32[0], f32[0])) -> (bf16[2,2048,32,128], bf16[2,2048,2,8,128], bf16[0]) {
  %arg_tuple.16 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}) parameter(0)
  %get-tuple-element.277 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.16), index=2
  %get-tuple-element.278 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%arg_tuple.16), index=3
  %constant.384 = bf16[0]{0} constant({})
  %get-tuple-element.279 = bf16[0]{0} get-tuple-element(%arg_tuple.16), index=4
  %get-tuple-element.280 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%arg_tuple.16), index=5
  %get-tuple-element.281 = u32[2,4]{1,0} get-tuple-element(%arg_tuple.16), index=6
  %get-tuple-element.282 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.16), index=7
  %get-tuple-element.283 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%arg_tuple.16), index=8
  %constant.385 = s32[1]{0} constant({0})
  %get-tuple-element.275 = s32[2]{0} get-tuple-element(%arg_tuple.16), index=0
  %constant.390 = s32[] constant(6144)
  %broadcast.243 = s32[2]{0} broadcast(%constant.390), dimensions={}
  %sub.82 = s32[2]{0} subtract(%get-tuple-element.275, %broadcast.243), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %constant.388 = s32[] constant(0)
  %broadcast.242 = s32[2]{0} broadcast(%constant.388), dimensions={}
  %lt.67 = pred[2]{0} compare(%sub.82, %broadcast.242), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.111 = s32[2]{0} select(%lt.67, %broadcast.242, %sub.82), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %constant.386 = s32[] constant(2048)
  %broadcast.241 = s32[2]{0} broadcast(%constant.386), dimensions={}
  %lt.68 = pred[2]{0} compare(%select_n.111, %broadcast.241), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.112 = s32[2]{0} select(%lt.68, %select_n.111, %broadcast.241), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.71 = pred[2]{0} compare(%select_n.112, %broadcast.242), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.113 = s32[2]{0} select(%lt.71, %broadcast.242, %select_n.112), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.27 = s32[2]{0} reduce-window(%select_n.113, %constant.388), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.2, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.39 = s32[3]{0} concatenate(%constant.385, %reduce_window_sum.27), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %get-tuple-element.276 = s32[2]{0} get-tuple-element(%arg_tuple.16), index=1
  %sub.83 = s32[2]{0} subtract(%get-tuple-element.276, %broadcast.243), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/sub" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %lt.69 = pred[2]{0} compare(%sub.83, %broadcast.242), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.114 = s32[2]{0} select(%lt.69, %broadcast.242, %sub.83), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %lt.70 = pred[2]{0} compare(%select_n.114, %broadcast.241), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.115 = s32[2]{0} select(%lt.70, %select_n.114, %broadcast.241), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1757 source_end_line=1759 source_column=26 source_end_column=9}
  %lt.72 = pred[2]{0} compare(%select_n.115, %broadcast.242), direction=LT, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/lt" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %select_n.116 = s32[2]{0} select(%lt.72, %broadcast.242, %select_n.115), metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=1756 source_end_line=1756 source_column=30 source_end_column=88}
  %reduce_window_sum.28 = s32[2]{0} reduce-window(%select_n.116, %constant.388), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.2, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/jit(_cumsum_with_promotion)/generate_cu_seqlen/reduce_window_sum" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=246 source_end_line=246 source_column=16 source_end_column=71}
  %concatenate.40 = s32[3]{0} concatenate(%constant.385, %reduce_window_sum.28), dimensions={0}, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/concatenate" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %get-tuple-element.284 = f32[0]{0} get-tuple-element(%arg_tuple.16), index=9
  %get-tuple-element.285 = f32[0]{0} get-tuple-element(%arg_tuple.16), index=10
  %get-tuple-element.286 = f32[0]{0} get-tuple-element(%arg_tuple.16), index=11
  %get-tuple-element.287 = f32[0]{0} get-tuple-element(%arg_tuple.16), index=12
  %get-tuple-element.288 = f32[0]{0} get-tuple-element(%arg_tuple.16), index=13
  %get-tuple-element.289 = f32[0]{0} get-tuple-element(%arg_tuple.16), index=14
  %te_fused_attn_backward_ffi.36 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u8[134742144]{0}) custom-call(%get-tuple-element.277, %get-tuple-element.278, %constant.384, %get-tuple-element.279, %get-tuple-element.280, /*index=5*/%get-tuple-element.281, %get-tuple-element.282, %get-tuple-element.283, %concatenate.39, %concatenate.40, /*index=10*/%get-tuple-element.284, %get-tuple-element.285, %get-tuple-element.286, %get-tuple-element.287, %get-tuple-element.288, /*index=15*/%get-tuple-element.289), custom_call_target="te_fused_attn_backward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 0 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_backward_ffi.37 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_backward_ffi.36), index=0, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %te_fused_attn_backward_ffi.38 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%te_fused_attn_backward_ffi.36), index=1, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  %te_fused_attn_backward_ffi.40 = bf16[0]{0} get-tuple-element(%te_fused_attn_backward_ffi.36), index=3, metadata={op_name="tmp_xla_computation/cond/branch_0_fun/cond/branch_1_fun/te_fused_attn_backward_ffi" source_file="/usr/local/lib/python3.12/dist-packages/transformer_engine/jax/cpp_extensions/attention.py" source_line=2157 source_end_line=2159 source_column=66 source_end_column=21}
  ROOT %tuple.33 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) tuple(%te_fused_attn_backward_ffi.37, %te_fused_attn_backward_ffi.38, %te_fused_attn_backward_ffi.40)
}

%region_14.14 (reduce_sum.72: f32[], reduce_sum.73: f32[]) -> f32[] {
  %reduce_sum.72 = f32[] parameter(0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/reduce_sum"}
  %reduce_sum.73 = f32[] parameter(1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/reduce_sum"}
  ROOT %reduce_sum.74 = f32[] add(%reduce_sum.72, %reduce_sum.73), metadata={op_name="checkpoint/layers/reduce_sum" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
}

%add.clone (x.1: bf16[], y.1: bf16[]) -> bf16[] {
  %x.1 = bf16[] parameter(0)
  %y.1 = bf16[] parameter(1)
  ROOT %add.357 = bf16[] add(%x.1, %y.1)
}

%add.1.clone (x.3: bf16[], y.3: bf16[]) -> bf16[] {
  %x.3 = bf16[] parameter(0)
  %y.3 = bf16[] parameter(1)
  ROOT %add.359 = bf16[] add(%x.3, %y.3)
}

%add.2.clone (x.5: bf16[], y.5: bf16[]) -> bf16[] {
  %x.5 = bf16[] parameter(0)
  %y.5 = bf16[] parameter(1)
  ROOT %add.361 = bf16[] add(%x.5, %y.5)
}

%add.3.clone (x.7: bf16[], y.7: bf16[]) -> bf16[] {
  %x.7 = bf16[] parameter(0)
  %y.7 = bf16[] parameter(1)
  ROOT %add.363 = bf16[] add(%x.7, %y.7)
}

%add.4.clone (x.9: bf16[], y.9: bf16[]) -> bf16[] {
  %x.9 = bf16[] parameter(0)
  %y.9 = bf16[] parameter(1)
  ROOT %add.365 = bf16[] add(%x.9, %y.9)
}

%add.5.clone (x.11: bf16[], y.11: bf16[]) -> bf16[] {
  %x.11 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.367 = bf16[] add(%x.11, %y.11)
}

%add.6.clone (x.13: bf16[], y.13: bf16[]) -> bf16[] {
  %x.13 = bf16[] parameter(0)
  %y.13 = bf16[] parameter(1)
  ROOT %add.369 = bf16[] add(%x.13, %y.13)
}

%add.7.clone (x.15: bf16[], y.15: bf16[]) -> bf16[] {
  %x.15 = bf16[] parameter(0)
  %y.15 = bf16[] parameter(1)
  ROOT %add.371 = bf16[] add(%x.15, %y.15)
}

%add.8.clone (x.17: bf16[], y.17: bf16[]) -> bf16[] {
  %x.17 = bf16[] parameter(0)
  %y.17 = bf16[] parameter(1)
  ROOT %add.373 = bf16[] add(%x.17, %y.17)
}

%region_10.15_spmd (param.2: (s32[], bf16[2,2048,4096], f32[32,1024,14336], f32[32,1024,14336], f32[32,14336,1024], /*index=5*/f32[32,4096], f32[32,4096], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,32,128], /*index=10*/f32[32,1024,8,128], f32[32,4096], bf16[32,2,2048,4096], f32[32,4096], f32[32,1024,32,128], /*index=15*/s32[2,2048], f32[32,1024,8,128], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,14336], /*index=20*/f32[32,14336,1024], f32[32,1024,14336])) -> (s32[], bf16[2,2048,4096], f32[32,1024,14336], f32[32,1024,14336], f32[32,14336,1024], /*index=5*/f32[32,4096], f32[32,4096], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,32,128], /*index=10*/f32[32,1024,8,128], f32[32,4096], bf16[32,2,2048,4096], f32[32,4096], f32[32,1024,32,128], /*index=15*/s32[2,2048], f32[32,1024,8,128], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,14336], /*index=20*/f32[32,14336,1024], f32[32,1024,14336]) {
  %param.2 = (s32[], bf16[2,2048,4096]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,14336,1024]{2,1,0}, /*index=5*/f32[32,4096]{1,0}, f32[32,4096]{1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,32,128]{3,2,1,0}, /*index=10*/f32[32,1024,8,128]{3,2,1,0}, f32[32,4096]{1,0}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{1,0}, f32[32,1024,32,128]{3,2,1,0}, /*index=15*/s32[2,2048]{1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,14336]{2,1,0}, /*index=20*/f32[32,14336,1024]{2,1,0}, f32[32,1024,14336]{2,1,0}) parameter(0)
  %get-tuple-element.290 = s32[] get-tuple-element(%param.2), index=0
  %constant.393 = s32[] constant(1)
  %add.374 = s32[] add(%get-tuple-element.290, %constant.393), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.291 = bf16[2,2048,4096]{2,1,0} get-tuple-element(%param.2), index=1
  %sharding_constraint.131 = bf16[2,2048,4096]{2,1,0} copy(%get-tuple-element.291), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.292 = f32[32,4096]{1,0} get-tuple-element(%param.2), index=11
  %constant.394 = s32[] constant(31)
  %sub.84 = s32[] subtract(%constant.394, %get-tuple-element.290), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/sub" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %constant.395 = s32[] constant(0)
  %dynamic_slice.40 = f32[1,4096]{1,0} dynamic-slice(%get-tuple-element.292, %sub.84, %constant.395), dynamic_slice_sizes={1,4096}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.279 = bf16[1,4096]{1,0} convert(%dynamic_slice.40), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.280 = bf16[4096]{0} reshape(%convert_element_type.279), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot_general.128 = bf16[2,2048,4096]{2,1,0} broadcast(%convert_element_type.280), dimensions={2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.293 = bf16[32,2,2048,4096]{3,2,1,0} get-tuple-element(%param.2), index=12
  %dynamic-slice.13 = bf16[1,2,2048,4096]{3,2,1,0} dynamic-slice(%get-tuple-element.293, %sub.84, %constant.395, %constant.395, %constant.395), dynamic_slice_sizes={1,2,2048,4096}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %squeeze.0 = bf16[2,2048,4096]{2,1,0} reshape(%dynamic-slice.13), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %sharding_constraint.132 = bf16[2,2048,4096]{2,1,0} copy(%squeeze.0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.294 = f32[32,4096]{1,0} get-tuple-element(%param.2), index=13
  %dynamic_slice.41 = f32[1,4096]{1,0} dynamic-slice(%get-tuple-element.294, %sub.84, %constant.395), dynamic_slice_sizes={1,4096}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.281 = bf16[1,4096]{1,0} convert(%dynamic_slice.41), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.282 = bf16[4096]{0} reshape(%convert_element_type.281), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot_general.129 = bf16[2,2048,4096]{2,1,0} broadcast(%convert_element_type.282), dimensions={2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.283 = f32[2,2048,4096]{2,1,0} convert(%sharding_constraint.132), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %square.92 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.283, %convert_element_type.283), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/square" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.399 = f32[] constant(0)
  %reduce.2 = f32[2,2048]{1,0} reduce(%square.92, %constant.399), dimensions={2}, to_apply=%region_11.11, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/reduce_sum" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.400 = f32[] constant(0.000244140625)
  %closed_call.10 = f32[2,2048]{1,0} broadcast(%constant.400), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call"}
  %div.101 = f32[2,2048]{1,0} multiply(%reduce.2, %closed_call.10), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/div" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.401 = f32[] constant(1e-05)
  %closed_call.11 = f32[2,2048]{1,0} broadcast(%constant.401), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call"}
  %add.375 = f32[2,2048]{1,0} add(%div.101, %closed_call.11), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.376 = f32[2,2048,1]{2,1,0} reshape(%add.375), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %rsqrt.18 = f32[2,2048,1]{2,1,0} rsqrt(%add.376), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/rsqrt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.744 = f32[2,2048]{1,0} reshape(%rsqrt.18), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.745 = f32[2,2048,4096]{2,1,0} broadcast(%mul.744), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.746 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.283, %mul.745), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.284 = bf16[2,2048,4096]{2,1,0} convert(%mul.746), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot_general.130 = bf16[2,2048,4096]{2,1,0} multiply(%dot_general.129, %convert_element_type.284), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.133 = bf16[2,2048,4096]{2,1,0} copy(%dot_general.130), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.134 = bf16[2,2048,4096]{2,1,0} copy(%sharding_constraint.133), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.295 = f32[32,1024,32,128]{3,2,1,0} get-tuple-element(%param.2), index=14
  %dynamic-slice.14 = f32[1,1024,32,128]{3,2,1,0} dynamic-slice(%get-tuple-element.295, %sub.84, %constant.395, %constant.395, %constant.395), dynamic_slice_sizes={1,1024,32,128}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.285 = bf16[1,1024,32,128]{3,2,1,0} convert(%dynamic-slice.14), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.286 = bf16[1024,32,128]{2,1,0} reshape(%convert_element_type.285), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-gather.8 = bf16[4096,32,128]{2,1,0} all-gather(%convert_element_type.286), channel_id=10, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.12 = bf16[2,2048,32,128]{3,2,1,0} dot(%sharding_constraint.134, %all-gather.8), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %slice.12 = bf16[2,2048,32,64]{3,2,1,0} slice(%dot.12), slice={[0:2], [0:2048], [0:32], [0:64]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/split" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.296 = s32[2,2048]{1,0} get-tuple-element(%param.2), index=15
  %convert_element_type.287 = f32[2,2048]{1,0} convert(%get-tuple-element.296), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.102 = f32[2,2048,1,64]{3,2,1,0} broadcast(%convert_element_type.287), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/div" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.414 = f32[] constant(500000)
  %closed_call.28 = f32[64]{0} broadcast(%constant.414), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call"}
  %iota.21 = s32[64]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/iota" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.415 = s32[] constant(2)
  %closed_call.29 = s32[64]{0} broadcast(%constant.415), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call"}
  %mul.747 = s32[64]{0} multiply(%iota.21, %closed_call.29), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.288 = f32[64]{0} convert(%mul.747), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.416 = f32[] constant(0.0078125)
  %closed_call.30 = f32[64]{0} broadcast(%constant.416), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call"}
  %div.103 = f32[64]{0} multiply(%convert_element_type.288, %closed_call.30), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/div" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %pow.18 = f32[64]{0} power(%closed_call.28, %div.103), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/pow" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.104 = f32[2,2048,1,64]{3,2,1,0} broadcast(%pow.18), dimensions={3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/div" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.105 = f32[2,2048,1,64]{3,2,1,0} divide(%div.102, %div.104), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/div" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cos.12 = f32[2,2048,1,64]{3,2,1,0} cosine(%div.105), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/cos" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.289 = bf16[2,2048,1,64]{3,2,1,0} convert(%cos.12), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.748 = bf16[2,2048,64]{2,1,0} reshape(%convert_element_type.289), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.749 = bf16[2,2048,32,64]{3,2,1,0} broadcast(%mul.748), dimensions={0,1,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.750 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.12, %mul.749), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %slice.13 = bf16[2,2048,32,64]{3,2,1,0} slice(%dot.12), slice={[0:2], [0:2048], [0:32], [64:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/split" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sin.10 = f32[2,2048,1,64]{3,2,1,0} sine(%div.105), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sin" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.290 = bf16[2,2048,1,64]{3,2,1,0} convert(%sin.10), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.751 = bf16[2,2048,64]{2,1,0} reshape(%convert_element_type.290), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.752 = bf16[2,2048,32,64]{3,2,1,0} broadcast(%mul.751), dimensions={0,1,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.753 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.13, %mul.752), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.85 = bf16[2,2048,32,64]{3,2,1,0} subtract(%mul.750, %mul.753), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sub" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.756 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.13, %mul.749), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.759 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.12, %mul.752), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.377 = bf16[2,2048,32,64]{3,2,1,0} add(%mul.756, %mul.759), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.41 = bf16[2,2048,32,128]{3,2,1,0} concatenate(%sub.85, %add.377), dimensions={3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/concatenate" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.135 = bf16[2,2048,32,128]{3,2,1,0} copy(%concatenate.41), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.297 = f32[32,1024,8,128]{3,2,1,0} get-tuple-element(%param.2), index=16
  %dynamic-slice.17 = f32[1,1024,8,128]{3,2,1,0} dynamic-slice(%get-tuple-element.297, %sub.84, %constant.395, %constant.395, %constant.395), dynamic_slice_sizes={1,1024,8,128}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.291 = bf16[1,1024,8,128]{3,2,1,0} convert(%dynamic-slice.17), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.292 = bf16[1024,8,128]{2,1,0} reshape(%convert_element_type.291), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-gather.9 = bf16[4096,8,128]{2,1,0} all-gather(%convert_element_type.292), channel_id=11, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.13 = bf16[2,2048,8,128]{3,2,1,0} dot(%sharding_constraint.134, %all-gather.9), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %slice.14 = bf16[2,2048,8,64]{3,2,1,0} slice(%dot.13), slice={[0:2], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/split" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.762 = bf16[2,2048,8,64]{3,2,1,0} broadcast(%mul.748), dimensions={0,1,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.763 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.14, %mul.762), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %slice.15 = bf16[2,2048,8,64]{3,2,1,0} slice(%dot.13), slice={[0:2], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/split" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.765 = bf16[2,2048,8,64]{3,2,1,0} broadcast(%mul.751), dimensions={0,1,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.766 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.15, %mul.765), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.86 = bf16[2,2048,8,64]{3,2,1,0} subtract(%mul.763, %mul.766), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sub" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.769 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.15, %mul.762), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.772 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.14, %mul.765), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.378 = bf16[2,2048,8,64]{3,2,1,0} add(%mul.769, %mul.772), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.42 = bf16[2,2048,8,128]{3,2,1,0} concatenate(%sub.86, %add.378), dimensions={3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/concatenate" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.137 = bf16[2,2048,8,128]{3,2,1,0} copy(%concatenate.42), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.127 = bf16[2,2048,1,8,128]{4,3,2,1,0} reshape(%sharding_constraint.137), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.298 = f32[32,1024,8,128]{3,2,1,0} get-tuple-element(%param.2), index=17
  %dynamic-slice.20 = f32[1,1024,8,128]{3,2,1,0} dynamic-slice(%get-tuple-element.298, %sub.84, %constant.395, %constant.395, %constant.395), dynamic_slice_sizes={1,1024,8,128}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.297 = bf16[1,1024,8,128]{3,2,1,0} convert(%dynamic-slice.20), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.298 = bf16[1024,8,128]{2,1,0} reshape(%convert_element_type.297), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-gather.10 = bf16[4096,8,128]{2,1,0} all-gather(%convert_element_type.298), channel_id=12, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.14 = bf16[2,2048,8,128]{3,2,1,0} dot(%sharding_constraint.134, %all-gather.10), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.138 = bf16[2,2048,8,128]{3,2,1,0} copy(%dot.14), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.128 = bf16[2,2048,1,8,128]{4,3,2,1,0} reshape(%sharding_constraint.138), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.43 = bf16[2,2048,2,8,128]{4,3,2,1,0} concatenate(%broadcast_in_dim.127, %broadcast_in_dim.128), dimensions={2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.450 = bf16[0]{0} constant({})
  %constant.451 = u32[] constant(0)
  %closed_call.31 = u32[8]{0} broadcast(%constant.451), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call"}
  %constant.454 = s32[4]{0} constant({0, 2, 4, 6}), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %partition-id.2 = u32[] partition-id()
  %dynamic-slice.21 = s32[1]{0} dynamic-slice(%constant.454, %partition-id.2), dynamic_slice_sizes={1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %reshape.312 = s32[] reshape(%dynamic-slice.21), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.22 = u32[2]{0} dynamic-slice(%closed_call.31, %reshape.312), dynamic_slice_sizes={2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.456 = s32[1]{0} constant({0}), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.452 = s32[] constant(8192)
  %closed_call.32 = s32[2]{0} broadcast(%constant.452), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call"}
  %broadcast.244 = s32[2]{0} broadcast(%constant.395), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.87 = s32[2]{0} subtract(%closed_call.32, %broadcast.244), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %lt.73 = pred[2]{0} compare(%sub.87, %broadcast.244), direction=LT, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %select_n.117 = s32[2]{0} select(%lt.73, %broadcast.244, %sub.87), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.459 = s32[] constant(2048), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast.245 = s32[2]{0} broadcast(%constant.459), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %lt.74 = pred[2]{0} compare(%select_n.117, %broadcast.245), direction=LT, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %select_n.118 = s32[2]{0} select(%lt.74, %select_n.117, %broadcast.245), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %lt.75 = pred[2]{0} compare(%select_n.118, %broadcast.244), direction=LT, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %select_n.119 = s32[2]{0} select(%lt.75, %broadcast.244, %select_n.118), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %reduce_window_sum.29 = s32[2]{0} reduce-window(%select_n.119, %constant.395), window={size=2 pad=1_0}, to_apply=%region_0.3._custom_call_lowering_rule.1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.44 = s32[3]{0} concatenate(%constant.456, %reduce_window_sum.29), dimensions={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.453 = f32[0]{0} constant({})
  %te_fused_attn_forward_ffi.9 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, u8[1]{0}) custom-call(%sharding_constraint.135, %concatenate.43, %constant.450, %constant.450, %dynamic-slice.22, /*index=5*/%concatenate.44, %concatenate.44, %constant.453, %constant.453, %constant.453, /*index=10*/%constant.453, %constant.453, %constant.453), custom_call_target="te_fused_attn_forward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u32[2]{0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 2 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_forward_ffi.13 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.9), index=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.299 = f32[2,2048,32,128]{3,2,1,0} convert(%te_fused_attn_forward_ffi.13), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.463 = f32[] constant(1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast.246 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.463), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.464 = u32[] constant(1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %axis_index.1 = u32[] divide(%partition-id.2, %constant.464), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.465 = u32[] constant(4), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %axis_index.2 = u32[] remainder(%axis_index.1, %constant.465), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %axis_index.3 = s32[] convert(%axis_index.2), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ge.0 = pred[] compare(%axis_index.3, %constant.393), direction=GE, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.300 = s32[] convert(%ge.0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.0 = () tuple(), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ppermute.0 = bf16[2,2048,2,8,128]{4,3,2,1,0} collective-permute(%concatenate.43), channel_id=13, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.1 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) tuple(%closed_call.32, %closed_call.32, %sharding_constraint.135, %ppermute.0, %constant.450, /*index=5*/%dynamic-slice.22, %constant.453, %constant.453, %constant.453, %constant.453, /*index=10*/%constant.453, %constant.453), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.2 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) conditional(%convert_element_type.300, %cond.0, %cond.1), branch_computations={%region_1.6._custom_call_lowering_rule.1, %region_2.7._custom_call_lowering_rule.1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.3 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%cond.2), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %te_fused_attn_forward_ffi.14 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%te_fused_attn_forward_ffi.9), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.89 = f32[2,32,2048,1]{3,2,1,0} subtract(%cond.3, %te_fused_attn_forward_ffi.14), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.29 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.89), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %exp.17 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.29), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.379 = f32[2,32,2048,1]{3,2,1,0} add(%exp.17, %broadcast.246), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.274 = f32[2,32,2048,1]{3,2,1,0} divide(%broadcast.246, %add.379), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %transpose.133 = f32[2,2048,32,1]{3,1,2,0} transpose(%div.274), dimensions={0,2,1,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.773 = f32[2,2048,32,1]{3,2,1,0} broadcast(%transpose.133), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.774 = f32[2,2048,32]{2,1,0} reshape(%mul.773), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.775 = f32[2,2048,32,128]{3,2,1,0} broadcast(%mul.774), dimensions={0,1,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.4 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.2), index=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.301 = f32[2,2048,32,128]{3,2,1,0} convert(%cond.4), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.90 = f32[2,2048,32,128]{3,2,1,0} subtract(%convert_element_type.299, %convert_element_type.301), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.776 = f32[2,2048,32,128]{3,2,1,0} multiply(%mul.775, %sub.90), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.91 = f32[2,2048,32,128]{3,2,1,0} subtract(%convert_element_type.299, %mul.776), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ge.1 = pred[] compare(%axis_index.3, %constant.415), direction=GE, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.302 = s32[] convert(%ge.1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.5 = () tuple(), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ppermute.1 = bf16[2,2048,2,8,128]{4,3,2,1,0} collective-permute(%ppermute.0), channel_id=13, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.6 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) tuple(%closed_call.32, %closed_call.32, %sharding_constraint.135, %ppermute.1, %constant.450, /*index=5*/%dynamic-slice.22, %constant.453, %constant.453, %constant.453, %constant.453, /*index=10*/%constant.453, %constant.453), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.7 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) conditional(%convert_element_type.302, %cond.5, %cond.6), branch_computations={%region_3.10._custom_call_lowering_rule.1, %region_4.11._custom_call_lowering_rule.1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.8 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%cond.7), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.92 = f32[2,32,2048,1]{3,2,1,0} subtract(%te_fused_attn_forward_ffi.14, %cond.3), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.30 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.92), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast.247 = f32[2,32,2048,1]{3,2,1,0} broadcast(%constant.399), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.93 = f32[2,32,2048,1]{3,2,1,0} subtract(%neg.30, %broadcast.247), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ne.7 = pred[2,32,2048,1]{3,2,1,0} compare(%sub.93, %sub.93), direction=NE, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %max.6 = f32[2,32,2048,1]{3,2,1,0} maximum(%neg.30, %broadcast.247), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %abs.0 = f32[2,32,2048,1]{3,2,1,0} abs(%sub.93), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.31 = f32[2,32,2048,1]{3,2,1,0} negate(%abs.0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %exp.18 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.31), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %log1p.0 = f32[2,32,2048,1]{3,2,1,0} log-plus-one(%exp.18), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.380 = f32[2,32,2048,1]{3,2,1,0} add(%max.6, %log1p.0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %select_n.123 = f32[2,32,2048,1]{3,2,1,0} select(%ne.7, %neg.30, %add.380), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.32 = f32[2,32,2048,1]{3,2,1,0} negate(%select_n.123), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.94 = f32[2,32,2048,1]{3,2,1,0} subtract(%te_fused_attn_forward_ffi.14, %neg.32), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.95 = f32[2,32,2048,1]{3,2,1,0} subtract(%cond.8, %sub.94), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.33 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.95), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %exp.19 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.33), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.381 = f32[2,32,2048,1]{3,2,1,0} add(%exp.19, %broadcast.246), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.275 = f32[2,32,2048,1]{3,2,1,0} divide(%broadcast.246, %add.381), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %transpose.134 = f32[2,2048,32,1]{3,1,2,0} transpose(%div.275), dimensions={0,2,1,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.777 = f32[2,2048,32,1]{3,2,1,0} broadcast(%transpose.134), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.778 = f32[2,2048,32]{2,1,0} reshape(%mul.777), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.779 = f32[2,2048,32,128]{3,2,1,0} broadcast(%mul.778), dimensions={0,1,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.9 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.7), index=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.303 = f32[2,2048,32,128]{3,2,1,0} convert(%cond.9), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.96 = f32[2,2048,32,128]{3,2,1,0} subtract(%sub.91, %convert_element_type.303), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.780 = f32[2,2048,32,128]{3,2,1,0} multiply(%mul.779, %sub.96), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.97 = f32[2,2048,32,128]{3,2,1,0} subtract(%sub.91, %mul.780), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.469 = s32[] constant(3), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ge.2 = pred[] compare(%axis_index.3, %constant.469), direction=GE, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.304 = s32[] convert(%ge.2), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.10 = () tuple(), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ppermute.2 = bf16[2,2048,2,8,128]{4,3,2,1,0} collective-permute(%ppermute.1), channel_id=13, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.11 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/u32[2]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}) tuple(%closed_call.32, %closed_call.32, %sharding_constraint.135, %ppermute.2, %constant.450, /*index=5*/%dynamic-slice.22, %constant.453, %constant.453, %constant.453, %constant.453, /*index=10*/%constant.453, %constant.453), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.12 = (bf16[2,2048,32,128]{3,2,1,0}, f32[2,32,2048,1]{3,2,1,0}) conditional(%convert_element_type.304, %cond.10, %cond.11), branch_computations={%region_5.12._custom_call_lowering_rule.1, %region_6.13._custom_call_lowering_rule.1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.13 = f32[2,32,2048,1]{3,2,1,0} get-tuple-element(%cond.12), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.98 = f32[2,32,2048,1]{3,2,1,0} subtract(%sub.94, %cond.8), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.34 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.98), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.99 = f32[2,32,2048,1]{3,2,1,0} subtract(%neg.34, %broadcast.247), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ne.8 = pred[2,32,2048,1]{3,2,1,0} compare(%sub.99, %sub.99), direction=NE, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %max.7 = f32[2,32,2048,1]{3,2,1,0} maximum(%neg.34, %broadcast.247), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %abs.1 = f32[2,32,2048,1]{3,2,1,0} abs(%sub.99), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.35 = f32[2,32,2048,1]{3,2,1,0} negate(%abs.1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %exp.20 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.35), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %log1p.1 = f32[2,32,2048,1]{3,2,1,0} log-plus-one(%exp.20), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.382 = f32[2,32,2048,1]{3,2,1,0} add(%max.7, %log1p.1), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %select_n.124 = f32[2,32,2048,1]{3,2,1,0} select(%ne.8, %neg.34, %add.382), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.36 = f32[2,32,2048,1]{3,2,1,0} negate(%select_n.124), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.100 = f32[2,32,2048,1]{3,2,1,0} subtract(%sub.94, %neg.36), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.101 = f32[2,32,2048,1]{3,2,1,0} subtract(%cond.13, %sub.100), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.37 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.101), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %exp.21 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.37), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.383 = f32[2,32,2048,1]{3,2,1,0} add(%exp.21, %broadcast.246), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.276 = f32[2,32,2048,1]{3,2,1,0} divide(%broadcast.246, %add.383), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %transpose.135 = f32[2,2048,32,1]{3,1,2,0} transpose(%div.276), dimensions={0,2,1,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.781 = f32[2,2048,32,1]{3,2,1,0} broadcast(%transpose.135), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.782 = f32[2,2048,32]{2,1,0} reshape(%mul.781), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.783 = f32[2,2048,32,128]{3,2,1,0} broadcast(%mul.782), dimensions={0,1,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.14 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.12), index=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.305 = f32[2,2048,32,128]{3,2,1,0} convert(%cond.14), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.102 = f32[2,2048,32,128]{3,2,1,0} subtract(%sub.97, %convert_element_type.305), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.784 = f32[2,2048,32,128]{3,2,1,0} multiply(%mul.783, %sub.102), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.103 = f32[2,2048,32,128]{3,2,1,0} subtract(%sub.97, %mul.784), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.306 = bf16[2,2048,32,128]{3,2,1,0} convert(%sub.103), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.139 = bf16[2,2048,32,128]{3,2,1,0} copy(%convert_element_type.306), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.300 = f32[32,32,128,1024]{3,2,1,0} get-tuple-element(%param.2), index=18
  %dynamic-slice.23 = f32[1,32,128,1024]{3,2,1,0} dynamic-slice(%get-tuple-element.300, %sub.84, %constant.395, %constant.395, %constant.395), dynamic_slice_sizes={1,32,128,1024}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.307 = bf16[1,32,128,1024]{3,2,1,0} convert(%dynamic-slice.23), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.308 = bf16[32,128,1024]{2,1,0} reshape(%convert_element_type.307), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-gather.11 = bf16[32,128,4096]{2,1,0} all-gather(%convert_element_type.308), channel_id=14, replica_groups=[1,4]<=[4], dimensions={2}, use_global_device_ids=true, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.15 = bf16[2,2048,4096]{2,1,0} dot(%sharding_constraint.139, %all-gather.11), lhs_contracting_dims={2,3}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.140 = bf16[2,2048,4096]{2,1,0} copy(%dot.15), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.385 = bf16[2,2048,4096]{2,1,0} add(%sharding_constraint.132, %sharding_constraint.140), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.309 = f32[2,2048,4096]{2,1,0} convert(%add.385), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %square.93 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.309, %convert_element_type.309), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/square" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %reduce.3 = f32[2,2048]{1,0} reduce(%square.93, %constant.399), dimensions={2}, to_apply=%region_12.12, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/reduce_sum" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.277 = f32[2,2048]{1,0} multiply(%reduce.3, %closed_call.10), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/div" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.386 = f32[2,2048]{1,0} add(%div.277, %closed_call.11), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.387 = f32[2,2048,1]{2,1,0} reshape(%add.386), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %rsqrt.19 = f32[2,2048,1]{2,1,0} rsqrt(%add.387), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/rsqrt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.785 = f32[2,2048]{1,0} reshape(%rsqrt.19), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.786 = f32[2,2048,4096]{2,1,0} broadcast(%mul.785), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.787 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.309, %mul.786), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.310 = bf16[2,2048,4096]{2,1,0} convert(%mul.787), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot_general.131 = bf16[2,2048,4096]{2,1,0} multiply(%dot_general.128, %convert_element_type.310), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.141 = bf16[2,2048,4096]{2,1,0} copy(%dot_general.131), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.301 = f32[32,1024,14336]{2,1,0} get-tuple-element(%param.2), index=19
  %dynamic-slice.24 = f32[1,1024,14336]{2,1,0} dynamic-slice(%get-tuple-element.301, %sub.84, %constant.395, %constant.395), dynamic_slice_sizes={1,1024,14336}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.311 = bf16[1,1024,14336]{2,1,0} convert(%dynamic-slice.24), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.312 = bf16[1024,14336]{1,0} reshape(%convert_element_type.311), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-gather.12 = bf16[4096,14336]{1,0} all-gather(%convert_element_type.312), channel_id=15, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.16 = bf16[2,2048,14336]{2,1,0} dot(%sharding_constraint.141, %all-gather.12), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.478 = bf16[] constant(1)
  %jit_silu_.12 = bf16[2,2048,14336]{2,1,0} broadcast(%constant.478), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/jit(silu)"}
  %neg.41 = bf16[2,2048,14336]{2,1,0} negate(%dot.16), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/jit(silu)/neg" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %exp.23 = bf16[2,2048,14336]{2,1,0} exponential(%neg.41), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/jit(silu)/exp" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %add.388 = bf16[2,2048,14336]{2,1,0} add(%exp.23, %jit_silu_.12), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/jit(silu)/add" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %div.278 = bf16[2,2048,14336]{2,1,0} divide(%jit_silu_.12, %add.388), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/jit(silu)/div" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %mul.788 = bf16[2,2048,14336]{2,1,0} multiply(%dot.16, %div.278), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/jit(silu)/mul" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %sharding_constraint.142 = bf16[2,2048,4096]{2,1,0} copy(%sharding_constraint.131), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.302 = f32[32,14336,1024]{2,1,0} get-tuple-element(%param.2), index=20
  %dynamic-slice.25 = f32[1,14336,1024]{2,1,0} dynamic-slice(%get-tuple-element.302, %sub.84, %constant.395, %constant.395), dynamic_slice_sizes={1,14336,1024}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.313 = bf16[1,14336,1024]{2,1,0} convert(%dynamic-slice.25), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.314 = bf16[14336,1024]{1,0} reshape(%convert_element_type.313), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-gather.13 = bf16[14336,4096]{1,0} all-gather(%convert_element_type.314), channel_id=16, replica_groups=[1,4]<=[4], dimensions={1}, use_global_device_ids=true, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.17 = bf16[2,2048,14336]{2,1,0} dot(%sharding_constraint.142, %all-gather.13), lhs_contracting_dims={2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.143 = bf16[2,2048,14336]{2,1,0} copy(%dot.17), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.789 = bf16[2,2048,14336]{2,1,0} multiply(%mul.788, %sharding_constraint.143), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.303 = f32[32,1024,14336]{2,1,0} get-tuple-element(%param.2), index=21
  %dynamic-slice.26 = f32[1,1024,14336]{2,1,0} dynamic-slice(%get-tuple-element.303, %sub.84, %constant.395, %constant.395), dynamic_slice_sizes={1,1024,14336}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.315 = bf16[1,1024,14336]{2,1,0} convert(%dynamic-slice.26), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.316 = bf16[1024,14336]{1,0} reshape(%convert_element_type.315), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-gather.14 = bf16[4096,14336]{1,0} all-gather(%convert_element_type.316), channel_id=17, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.18 = bf16[2,2048,4096]{2,1,0} dot(%mul.789, %all-gather.14), lhs_contracting_dims={2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.19 = bf16[2,2048,14336]{2,1,0} dot(%sharding_constraint.141, %all-gather.14), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.790 = bf16[2,2048,14336]{2,1,0} multiply(%sharding_constraint.143, %dot.19), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.791 = bf16[2,2048,14336]{2,1,0} multiply(%mul.790, %div.278), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/jit(silu)/mul" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %mul.792 = bf16[2,2048,14336]{2,1,0} multiply(%dot.16, %mul.790), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/jit(silu)/mul" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %sub.107 = bf16[2,2048,14336]{2,1,0} subtract(%jit_silu_.12, %div.278), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/jit(silu)/sub" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %mul.793 = bf16[2,2048,14336]{2,1,0} multiply(%div.278, %sub.107), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/jit(silu)/mul" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %mul.794 = bf16[2,2048,14336]{2,1,0} multiply(%mul.792, %mul.793), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/jit(silu)/mul" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %add_any.39 = bf16[2,2048,14336]{2,1,0} add(%mul.791, %mul.794), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/jit(silu)/add_any" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=518 source_end_line=518 source_column=12 source_end_column=12}
  %dot.20 = bf16[2,2048,4096]{2,1,0} dot(%add_any.39, %all-gather.12), lhs_contracting_dims={2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.40 = bf16[2,2048,4096]{2,1,0} add(%dot.18, %dot.20), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.144 = bf16[2,2048,4096]{2,1,0} copy(%add_any.40), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %transpose.136 = bf16[4096,2,2048]{0,2,1} transpose(%sharding_constraint.144), dimensions={2,0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot_general.132 = bf16[4096,2,2048]{2,1,0} broadcast(%convert_element_type.280), dimensions={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot_general.133 = bf16[4096,2,2048]{0,2,1} multiply(%transpose.136, %dot_general.132), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.317 = f32[4096,2,2048]{0,2,1} convert(%dot_general.133), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.318 = f32[2,2048,4096]{2,1,0} transpose(%convert_element_type.317), dimensions={1,2,0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.797 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.318, %mul.786), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.798 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.309, %convert_element_type.318), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %reduce.4 = f32[2,2048]{1,0} reduce(%mul.798, %constant.399), dimensions={2}, to_apply=%region_13.13, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/reduce_sum" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %reshape.313 = f32[2,2048,1]{2,1,0} reshape(%reduce.4), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/reshape" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.279 = f32[2,2048,1]{2,1,0} divide(%rsqrt.19, %add.387), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/div" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.483 = f32[] constant(-0.5)
  %closed_call.33 = f32[2,2048,1]{2,1,0} broadcast(%constant.483), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call"}
  %mul.799 = f32[2,2048,1]{2,1,0} multiply(%div.279, %closed_call.33), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.800 = f32[2,2048,1]{2,1,0} multiply(%reshape.313, %mul.799), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.484 = f32[] constant(0.00048828125)
  %mul.801 = f32[2,2048,1]{2,1,0} broadcast(%constant.484), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.802 = f32[2,2048,1]{2,1,0} multiply(%mul.800, %mul.801), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.803 = f32[2,2048]{1,0} reshape(%mul.802), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.804 = f32[2,2048,4096]{2,1,0} broadcast(%mul.803), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.805 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.309, %mul.804), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.41 = f32[2,2048,4096]{2,1,0} add(%mul.797, %mul.805), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.319 = bf16[2,2048,4096]{2,1,0} convert(%add_any.41), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.42 = bf16[2,2048,4096]{2,1,0} add(%sharding_constraint.131, %convert_element_type.319), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.143 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} reshape(%concatenate.43), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.496 = bf16[] constant(0), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast.253 = bf16[2,2048,1,8,128]{4,3,2,1,0} broadcast(%constant.496), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.49 = bf16[2,2048,2,8,128]{4,3,2,1,0} concatenate(%broadcast.253, %broadcast.253), dimensions={2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.144 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} reshape(%concatenate.49), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.50 = bf16[2,2,2048,2,8,128]{5,4,3,2,1,0} concatenate(%broadcast_in_dim.143, %broadcast_in_dim.144), dimensions={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ppermute.6 = bf16[2,2,2048,2,8,128]{5,4,3,2,1,0} collective-permute(%concatenate.50), channel_id=18, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.36 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} slice(%ppermute.6), slice={[0:1], [0:2], [0:2048], [0:2], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.2 = bf16[2,2048,2,8,128]{4,3,2,1,0} reshape(%split.36), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.145 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} reshape(%squeeze.2), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.37 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} slice(%ppermute.6), slice={[1:2], [0:2], [0:2048], [0:2], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.3 = bf16[2,2048,2,8,128]{4,3,2,1,0} reshape(%split.37), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.104 = f32[2,32,2048,1]{3,2,1,0} subtract(%sub.100, %cond.13), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.38 = f32[2,32,2048,1]{3,2,1,0} negate(%sub.104), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.105 = f32[2,32,2048,1]{3,2,1,0} subtract(%neg.38, %broadcast.247), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ne.9 = pred[2,32,2048,1]{3,2,1,0} compare(%sub.105, %sub.105), direction=NE, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %max.8 = f32[2,32,2048,1]{3,2,1,0} maximum(%neg.38, %broadcast.247), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %abs.5 = f32[2,32,2048,1]{3,2,1,0} abs(%sub.105), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.39 = f32[2,32,2048,1]{3,2,1,0} negate(%abs.5), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %exp.22 = f32[2,32,2048,1]{3,2,1,0} exponential(%neg.39), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %log1p.5 = f32[2,32,2048,1]{3,2,1,0} log-plus-one(%exp.22), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.384 = f32[2,32,2048,1]{3,2,1,0} add(%max.8, %log1p.5), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %select_n.125 = f32[2,32,2048,1]{3,2,1,0} select(%ne.9, %neg.38, %add.384), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.40 = f32[2,32,2048,1]{3,2,1,0} negate(%select_n.125), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sub.106 = f32[2,32,2048,1]{3,2,1,0} subtract(%sub.100, %neg.40), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast.250 = u32[2,4]{1,0} broadcast(%constant.451), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.145 = u32[2,4]{1,0} copy(%broadcast.250), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/DotProductAttention_0/_FusedDotProductAttention_0/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.146 = bf16[2,2048,4096]{2,1,0} copy(%add_any.42), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.21 = bf16[2,2048,32,128]{3,2,1,0} dot(%sharding_constraint.146, %all-gather.11), lhs_contracting_dims={2}, rhs_contracting_dims={2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.147 = bf16[2,2048,32,128]{3,2,1,0} copy(%dot.21), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %te_fused_attn_backward_ffi.42 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, u8[134742144]{0}) custom-call(%sharding_constraint.135, %concatenate.43, %constant.450, %constant.450, %sub.106, /*index=5*/%sharding_constraint.145, %convert_element_type.306, %sharding_constraint.147, %concatenate.44, %concatenate.44, /*index=10*/%constant.453, %constant.453, %constant.453, %constant.453, %constant.453, /*index=15*/%constant.453), custom_call_target="te_fused_attn_backward_ffi", operand_layout_constraints={bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, bf16[0]{0}, f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, s32[3]{0}, s32[3]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}}, api_version=API_VERSION_TYPED_FFI, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}, backend_config={attn_heads = 32 : i64, bias_batch = 0 : i64, bias_heads = 0 : i64, bias_type = 0 : i64, deterministic = false, dropout_probability = 0.000000e+00 : f64, input_batch = 2 : i64, is_training = true, kv_max_seqlen = 2048 : i64, mask_type = 2 : i64, max_segments_per_seq = 1 : i64, num_gqa_groups = 8 : i64, q_max_seqlen = 2048 : i64, qk_head_dim = 128 : i64, qkv_layout = 7 : i64, scaling_factor = 1.000000e+00 : f64, v_head_dim = 128 : i64, window_size_left = -1 : i64, window_size_right = -1 : i64}
  %te_fused_attn_backward_ffi.44 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%te_fused_attn_backward_ffi.42), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.390 = bf16[2,2048,2,8,128]{4,3,2,1,0} add(%squeeze.3, %te_fused_attn_backward_ffi.44), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.146 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} reshape(%add.390), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.51 = bf16[2,2,2048,2,8,128]{5,4,3,2,1,0} concatenate(%broadcast_in_dim.145, %broadcast_in_dim.146), dimensions={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ppermute.7 = bf16[2,2,2048,2,8,128]{5,4,3,2,1,0} collective-permute(%concatenate.51), channel_id=18, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.38 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} slice(%ppermute.7), slice={[0:1], [0:2], [0:2048], [0:2], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.4 = bf16[2,2048,2,8,128]{4,3,2,1,0} reshape(%split.38), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.147 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} reshape(%squeeze.4), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.39 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} slice(%ppermute.7), slice={[1:2], [0:2], [0:2048], [0:2], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.5 = bf16[2,2048,2,8,128]{4,3,2,1,0} reshape(%split.39), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.30 = () tuple(), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.31 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}) tuple(%closed_call.32, %closed_call.32, %sharding_constraint.135, %squeeze.2, %constant.450, /*index=5*/%sub.106, %sharding_constraint.145, %convert_element_type.306, %sharding_constraint.147, %constant.453, /*index=10*/%constant.453, %constant.453, %constant.453, %constant.453, %constant.453), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.32 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) conditional(%convert_element_type.300, %cond.30, %cond.31), branch_computations={%region_1.7._custom_call_lowering_rule, %region_2.8._custom_call_lowering_rule}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.39 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%cond.32), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.392 = bf16[2,2048,2,8,128]{4,3,2,1,0} add(%squeeze.5, %cond.39), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.148 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} reshape(%add.392), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.52 = bf16[2,2,2048,2,8,128]{5,4,3,2,1,0} concatenate(%broadcast_in_dim.147, %broadcast_in_dim.148), dimensions={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ppermute.8 = bf16[2,2,2048,2,8,128]{5,4,3,2,1,0} collective-permute(%concatenate.52), channel_id=18, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.40 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} slice(%ppermute.8), slice={[0:1], [0:2], [0:2048], [0:2], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.8 = bf16[2,2048,2,8,128]{4,3,2,1,0} reshape(%split.40), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.149 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} reshape(%squeeze.8), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.41 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} slice(%ppermute.8), slice={[1:2], [0:2], [0:2048], [0:2], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.6 = bf16[2,2048,2,8,128]{4,3,2,1,0} reshape(%split.41), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.34 = () tuple(), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.35 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}) tuple(%closed_call.32, %closed_call.32, %sharding_constraint.135, %squeeze.4, %constant.450, /*index=5*/%sub.106, %sharding_constraint.145, %convert_element_type.306, %sharding_constraint.147, %constant.453, /*index=10*/%constant.453, %constant.453, %constant.453, %constant.453, %constant.453), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.36 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) conditional(%convert_element_type.302, %cond.34, %cond.35), branch_computations={%region_3.9._custom_call_lowering_rule, %region_4.10._custom_call_lowering_rule}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.43 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%cond.36), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.394 = bf16[2,2048,2,8,128]{4,3,2,1,0} add(%squeeze.6, %cond.43), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.150 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} reshape(%add.394), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.53 = bf16[2,2,2048,2,8,128]{5,4,3,2,1,0} concatenate(%broadcast_in_dim.149, %broadcast_in_dim.150), dimensions={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ppermute.9 = bf16[2,2,2048,2,8,128]{5,4,3,2,1,0} collective-permute(%concatenate.53), channel_id=18, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.43 = bf16[1,2,2048,2,8,128]{5,4,3,2,1,0} slice(%ppermute.9), slice={[1:2], [0:2], [0:2048], [0:2], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.18 = bf16[2,2048,2,8,128]{4,3,2,1,0} reshape(%split.43), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.38 = () tuple(), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.40 = (s32[2]{0}, s32[2]{0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}, /*index=5*/f32[2,32,2048,1]{3,2,1,0}, u32[2,4]{1,0}, bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,32,128]{3,2,1,0}, f32[0]{0}, /*index=10*/f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}, f32[0]{0}) tuple(%closed_call.32, %closed_call.32, %sharding_constraint.135, %squeeze.8, %constant.450, /*index=5*/%sub.106, %sharding_constraint.145, %convert_element_type.306, %sharding_constraint.147, %constant.453, /*index=10*/%constant.453, %constant.453, %constant.453, %constant.453, %constant.453), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.41 = (bf16[2,2048,32,128]{3,2,1,0}, bf16[2,2048,2,8,128]{4,3,2,1,0}, bf16[0]{0}) conditional(%convert_element_type.304, %cond.38, %cond.40), branch_computations={%region_5.11._custom_call_lowering_rule, %region_6.12._custom_call_lowering_rule}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.44 = bf16[2,2048,2,8,128]{4,3,2,1,0} get-tuple-element(%cond.41), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.395 = bf16[2,2048,2,8,128]{4,3,2,1,0} add(%squeeze.18, %cond.44), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %ppermute.10 = bf16[2,2048,2,8,128]{4,3,2,1,0} collective-permute(%add.395), channel_id=18, source_target_pairs={{0,1},{1,2},{2,3},{3,0}}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.45 = bf16[2,2048,1,8,128]{4,3,2,1,0} slice(%ppermute.10), slice={[0:2], [0:2048], [1:2], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.20 = bf16[2,2048,8,128]{3,2,1,0} reshape(%split.45), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.148 = bf16[2,2048,8,128]{3,2,1,0} copy(%squeeze.20), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.22 = bf16[2,2048,4096]{2,1,0} dot(%sharding_constraint.148, %all-gather.10), lhs_contracting_dims={2,3}, rhs_contracting_dims={1,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %split.44 = bf16[2,2048,1,8,128]{4,3,2,1,0} slice(%ppermute.10), slice={[0:2], [0:2048], [0:1], [0:8], [0:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %squeeze.19 = bf16[2,2048,8,128]{3,2,1,0} reshape(%split.44), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.149 = bf16[2,2048,8,128]{3,2,1,0} copy(%squeeze.19), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %slice.16 = bf16[2,2048,8,64]{3,2,1,0} slice(%sharding_constraint.149), slice={[0:2], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/split" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.808 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.16, %mul.765), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %slice.17 = bf16[2,2048,8,64]{3,2,1,0} slice(%sharding_constraint.149), slice={[0:2], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/split" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.811 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.17, %mul.762), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.43 = bf16[2,2048,8,64]{3,2,1,0} add(%mul.808, %mul.811), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.814 = bf16[2,2048,8,64]{3,2,1,0} multiply(%slice.16, %mul.762), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.42 = bf16[2,2048,8,64]{3,2,1,0} negate(%slice.17), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/neg" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.817 = bf16[2,2048,8,64]{3,2,1,0} multiply(%neg.42, %mul.765), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.44 = bf16[2,2048,8,64]{3,2,1,0} add(%mul.814, %mul.817), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.54 = bf16[2,2048,8,128]{3,2,1,0} concatenate(%add_any.43, %add_any.44), dimensions={3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/concatenate" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.23 = bf16[2,2048,4096]{2,1,0} dot(%concatenate.54, %all-gather.9), lhs_contracting_dims={2,3}, rhs_contracting_dims={1,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.45 = bf16[2,2048,4096]{2,1,0} add(%dot.22, %dot.23), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.150 = bf16[2,2048,4096]{2,1,0} copy(%add_any.45), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %te_fused_attn_backward_ffi.43 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%te_fused_attn_backward_ffi.42), index=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.33 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.32), index=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.389 = bf16[2,2048,32,128]{3,2,1,0} add(%te_fused_attn_backward_ffi.43, %cond.33), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.37 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.36), index=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.391 = bf16[2,2048,32,128]{3,2,1,0} add(%add.389, %cond.37), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %cond.42 = bf16[2,2048,32,128]{3,2,1,0} get-tuple-element(%cond.41), index=0, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add.393 = bf16[2,2048,32,128]{3,2,1,0} add(%add.391, %cond.42), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/DotProductAttention_0/_FusedDotProductAttention_0/custom_partitioning" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.151 = bf16[2,2048,32,128]{3,2,1,0} copy(%add.393), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %slice.18 = bf16[2,2048,32,64]{3,2,1,0} slice(%sharding_constraint.151), slice={[0:2], [0:2048], [0:32], [64:128]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/split" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.820 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.18, %mul.752), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %slice.19 = bf16[2,2048,32,64]{3,2,1,0} slice(%sharding_constraint.151), slice={[0:2], [0:2048], [0:32], [0:64]}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/split" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.823 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.19, %mul.749), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.46 = bf16[2,2048,32,64]{3,2,1,0} add(%mul.820, %mul.823), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.826 = bf16[2,2048,32,64]{3,2,1,0} multiply(%slice.18, %mul.749), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %neg.43 = bf16[2,2048,32,64]{3,2,1,0} negate(%slice.19), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/neg" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.829 = bf16[2,2048,32,64]{3,2,1,0} multiply(%neg.43, %mul.752), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.47 = bf16[2,2048,32,64]{3,2,1,0} add(%mul.826, %mul.829), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %concatenate.55 = bf16[2,2048,32,128]{3,2,1,0} concatenate(%add_any.46, %add_any.47), dimensions={3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/concatenate" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.24 = bf16[2,2048,4096]{2,1,0} dot(%concatenate.55, %all-gather.8), lhs_contracting_dims={2,3}, rhs_contracting_dims={1,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.152 = bf16[2,2048,4096]{2,1,0} copy(%dot.24), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.48 = bf16[2,2048,4096]{2,1,0} add(%sharding_constraint.150, %sharding_constraint.152), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.153 = bf16[2,2048,4096]{2,1,0} copy(%add_any.48), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %transpose.137 = bf16[4096,2,2048]{0,2,1} transpose(%sharding_constraint.153), dimensions={2,0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot_general.134 = bf16[4096,2,2048]{2,1,0} broadcast(%convert_element_type.282), dimensions={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot_general.135 = bf16[4096,2,2048]{0,2,1} multiply(%transpose.137, %dot_general.134), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.323 = f32[4096,2,2048]{0,2,1} convert(%dot_general.135), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.324 = f32[2,2048,4096]{2,1,0} transpose(%convert_element_type.323), dimensions={1,2,0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.832 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.324, %mul.745), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.833 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.283, %convert_element_type.324), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %reduce.5 = f32[2,2048]{1,0} reduce(%mul.833, %constant.399), dimensions={2}, to_apply=%region_14.14, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/reduce_sum" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %reshape.318 = f32[2,2048,1]{2,1,0} reshape(%reduce.5), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/reshape" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %div.280 = f32[2,2048,1]{2,1,0} divide(%rsqrt.18, %add.376), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/div" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.834 = f32[2,2048,1]{2,1,0} multiply(%div.280, %closed_call.33), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.835 = f32[2,2048,1]{2,1,0} multiply(%reshape.318, %mul.834), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.836 = f32[2,2048,1]{2,1,0} multiply(%mul.835, %mul.801), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.837 = f32[2,2048]{1,0} reshape(%mul.836), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.838 = f32[2,2048,4096]{2,1,0} broadcast(%mul.837), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %mul.839 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.283, %mul.838), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.49 = f32[2,2048,4096]{2,1,0} add(%mul.832, %mul.839), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.325 = bf16[2,2048,4096]{2,1,0} convert(%add_any.49), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %add_any.50 = bf16[2,2048,4096]{2,1,0} add(%add_any.42, %convert_element_type.325), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/add_any" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.154 = bf16[2,2048,4096]{2,1,0} copy(%add_any.50), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %get-tuple-element.309 = f32[32,1024,14336]{2,1,0} get-tuple-element(%param.2), index=2
  %dot.25 = bf16[4096,14336]{1,0} dot(%sharding_constraint.141, %add_any.39), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce = bf16[4096,14336]{1,0} all-reduce(%dot.25), channel_id=19, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %constant.537 = s32[4]{0} constant({0, 1024, 2048, 3072}), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.31 = s32[1]{0} dynamic-slice(%constant.537, %partition-id.2), dynamic_slice_sizes={1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %reshape.319 = s32[] reshape(%dynamic-slice.31), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.32 = bf16[1024,14336]{1,0} dynamic-slice(%all-reduce, %reshape.319, %constant.395), dynamic_slice_sizes={1024,14336}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.326 = f32[1024,14336]{1,0} convert(%dynamic-slice.32), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.151 = f32[1,1024,14336]{2,1,0} reshape(%convert_element_type.326), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic-update-slice.1 = f32[32,1024,14336]{2,1,0} dynamic-update-slice(%get-tuple-element.309, %broadcast_in_dim.151, %sub.84, %constant.395, %constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.310 = f32[32,1024,14336]{2,1,0} get-tuple-element(%param.2), index=3
  %dot.26 = bf16[4096,14336]{1,0} dot(%sharding_constraint.141, %mul.789), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce.1 = bf16[4096,14336]{1,0} all-reduce(%dot.26), channel_id=20, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.1.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.34 = bf16[1024,14336]{1,0} dynamic-slice(%all-reduce.1, %reshape.319, %constant.395), dynamic_slice_sizes={1024,14336}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.327 = f32[1024,14336]{1,0} convert(%dynamic-slice.34), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.152 = f32[1,1024,14336]{2,1,0} reshape(%convert_element_type.327), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic-update-slice.2 = f32[32,1024,14336]{2,1,0} dynamic-update-slice(%get-tuple-element.310, %broadcast_in_dim.152, %sub.84, %constant.395, %constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.311 = f32[32,14336,1024]{2,1,0} get-tuple-element(%param.2), index=4
  %mul.840 = bf16[2,2048,14336]{2,1,0} multiply(%mul.788, %dot.19), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/mul" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %sharding_constraint.155 = bf16[2,2048,14336]{2,1,0} copy(%mul.840), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dot.27 = bf16[14336,4096]{1,0} dot(%sharding_constraint.155, %sharding_constraint.142), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce.2 = bf16[14336,4096]{1,0} all-reduce(%dot.27), channel_id=21, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.2.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.36 = bf16[14336,1024]{1,0} dynamic-slice(%all-reduce.2, %constant.395, %reshape.319), dynamic_slice_sizes={14336,1024}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.328 = f32[14336,1024]{1,0} convert(%dynamic-slice.36), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.153 = f32[1,14336,1024]{2,1,0} reshape(%convert_element_type.328), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic-update-slice.3 = f32[32,14336,1024]{2,1,0} dynamic-update-slice(%get-tuple-element.311, %broadcast_in_dim.153, %sub.84, %constant.395, %constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.312 = f32[32,4096]{1,0} get-tuple-element(%param.2), index=5
  %dot.28 = bf16[4096]{0} dot(%transpose.136, %convert_element_type.310), lhs_batch_dims={0}, lhs_contracting_dims={1,2}, rhs_batch_dims={2}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce.3 = bf16[4096]{0} all-reduce(%dot.28), channel_id=22, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.3.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.329 = f32[4096]{0} convert(%all-reduce.3), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.154 = f32[1,4096]{1,0} reshape(%convert_element_type.329), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic_update_slice.20 = f32[32,4096]{1,0} dynamic-update-slice(%get-tuple-element.312, %broadcast_in_dim.154, %sub.84, %constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.313 = f32[32,4096]{1,0} get-tuple-element(%param.2), index=6
  %dot.29 = bf16[4096]{0} dot(%transpose.137, %convert_element_type.284), lhs_batch_dims={0}, lhs_contracting_dims={1,2}, rhs_batch_dims={2}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce.4 = bf16[4096]{0} all-reduce(%dot.29), channel_id=23, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.4.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/i...k,...k->i...k/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.330 = f32[4096]{0} convert(%all-reduce.4), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.155 = f32[1,4096]{1,0} reshape(%convert_element_type.330), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic_update_slice.21 = f32[32,4096]{1,0} dynamic-update-slice(%get-tuple-element.313, %broadcast_in_dim.155, %sub.84, %constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.314 = f32[32,1024,8,128]{3,2,1,0} get-tuple-element(%param.2), index=7
  %dot.30 = bf16[8,128,4096]{2,1,0} dot(%concatenate.54, %sharding_constraint.134), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce.5 = bf16[8,128,4096]{2,1,0} all-reduce(%dot.30), channel_id=24, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.5.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.38 = bf16[8,128,1024]{2,1,0} dynamic-slice(%all-reduce.5, %constant.395, %constant.395, %reshape.319), dynamic_slice_sizes={8,128,1024}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.331 = f32[8,128,1024]{2,1,0} convert(%dynamic-slice.38), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.332 = f32[1024,8,128]{0,2,1} transpose(%convert_element_type.331), dimensions={2,0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.156 = f32[1,1024,8,128]{3,2,1,0} reshape(%convert_element_type.332), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic-update-slice.4 = f32[32,1024,8,128]{3,2,1,0} dynamic-update-slice(%get-tuple-element.314, %broadcast_in_dim.156, %sub.84, %constant.395, %constant.395, /*index=5*/%constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.315 = f32[32,32,128,1024]{3,2,1,0} get-tuple-element(%param.2), index=8
  %dot.31 = bf16[4096,32,128]{2,1,0} dot(%sharding_constraint.146, %sharding_constraint.139), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce.6 = bf16[4096,32,128]{2,1,0} all-reduce(%dot.31), channel_id=25, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.6.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.40 = bf16[1024,32,128]{2,1,0} dynamic-slice(%all-reduce.6, %reshape.319, %constant.395, %constant.395), dynamic_slice_sizes={1024,32,128}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.333 = f32[1024,32,128]{2,1,0} convert(%dynamic-slice.40), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.334 = f32[32,128,1024]{1,0,2} transpose(%convert_element_type.333), dimensions={1,2,0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.157 = f32[1,32,128,1024]{3,2,1,0} reshape(%convert_element_type.334), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic-update-slice.5 = f32[32,32,128,1024]{3,2,1,0} dynamic-update-slice(%get-tuple-element.315, %broadcast_in_dim.157, %sub.84, %constant.395, %constant.395, /*index=5*/%constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.316 = f32[32,1024,32,128]{3,2,1,0} get-tuple-element(%param.2), index=9
  %dot.32 = bf16[32,128,4096]{2,1,0} dot(%concatenate.55, %sharding_constraint.134), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce.7 = bf16[32,128,4096]{2,1,0} all-reduce(%dot.32), channel_id=26, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.7.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.42 = bf16[32,128,1024]{2,1,0} dynamic-slice(%all-reduce.7, %constant.395, %constant.395, %reshape.319), dynamic_slice_sizes={32,128,1024}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.335 = f32[32,128,1024]{2,1,0} convert(%dynamic-slice.42), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.336 = f32[1024,32,128]{0,2,1} transpose(%convert_element_type.335), dimensions={2,0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.158 = f32[1,1024,32,128]{3,2,1,0} reshape(%convert_element_type.336), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic-update-slice.6 = f32[32,1024,32,128]{3,2,1,0} dynamic-update-slice(%get-tuple-element.316, %broadcast_in_dim.158, %sub.84, %constant.395, %constant.395, /*index=5*/%constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.317 = f32[32,1024,8,128]{3,2,1,0} get-tuple-element(%param.2), index=10
  %dot.33 = bf16[8,128,4096]{2,1,0} dot(%sharding_constraint.148, %sharding_constraint.134), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %all-reduce.8 = bf16[8,128,4096]{2,1,0} all-reduce(%dot.33), channel_id=27, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.8.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %dynamic-slice.44 = bf16[8,128,1024]{2,1,0} dynamic-slice(%all-reduce.8, %constant.395, %constant.395, %reshape.319), dynamic_slice_sizes={8,128,1024}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/dot_general" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.337 = f32[8,128,1024]{2,1,0} convert(%dynamic-slice.44), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %convert_element_type.338 = f32[1024,8,128]{0,2,1} transpose(%convert_element_type.337), dimensions={2,0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/closed_call/checkpoint/layers/convert_element_type" source_file="/opt/flax/flax/core/axes_scan.py" source_line=156 source_end_line=156 source_column=29 source_end_column=29}
  %broadcast_in_dim.159 = f32[1,1024,8,128]{3,2,1,0} reshape(%convert_element_type.338), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dynamic-update-slice.7 = f32[32,1024,8,128]{3,2,1,0} dynamic-update-slice(%get-tuple-element.317, %broadcast_in_dim.159, %sub.84, %constant.395, %constant.395, /*index=5*/%constant.395), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  ROOT %tuple.41 = (s32[], bf16[2,2048,4096]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,14336,1024]{2,1,0}, /*index=5*/f32[32,4096]{1,0}, f32[32,4096]{1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,32,128]{3,2,1,0}, /*index=10*/f32[32,1024,8,128]{3,2,1,0}, f32[32,4096]{1,0}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{1,0}, f32[32,1024,32,128]{3,2,1,0}, /*index=15*/s32[2,2048]{1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,14336]{2,1,0}, /*index=20*/f32[32,14336,1024]{2,1,0}, f32[32,1024,14336]{2,1,0}) tuple(%add.374, %sharding_constraint.154, %dynamic-update-slice.1, %dynamic-update-slice.2, %dynamic-update-slice.3, /*index=5*/%dynamic_update_slice.20, %dynamic_update_slice.21, %dynamic-update-slice.4, %dynamic-update-slice.5, %dynamic-update-slice.6, /*index=10*/%dynamic-update-slice.7, %get-tuple-element.292, %get-tuple-element.293, %get-tuple-element.294, %get-tuple-element.295, /*index=15*/%get-tuple-element.296, %get-tuple-element.297, %get-tuple-element.298, %get-tuple-element.300, %get-tuple-element.301, /*index=20*/%get-tuple-element.302, %get-tuple-element.303)
}

%region_15.16_spmd (param.3: (s32[], bf16[2,2048,4096], f32[32,1024,14336], f32[32,1024,14336], f32[32,14336,1024], /*index=5*/f32[32,4096], f32[32,4096], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,32,128], /*index=10*/f32[32,1024,8,128], f32[32,4096], bf16[32,2,2048,4096], f32[32,4096], f32[32,1024,32,128], /*index=15*/s32[2,2048], f32[32,1024,8,128], f32[32,1024,8,128], f32[32,32,128,1024], f32[32,1024,14336], /*index=20*/f32[32,14336,1024], f32[32,1024,14336])) -> pred[] {
  %param.3 = (s32[], bf16[2,2048,4096]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,14336,1024]{2,1,0}, /*index=5*/f32[32,4096]{1,0}, f32[32,4096]{1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,32,128]{3,2,1,0}, /*index=10*/f32[32,1024,8,128]{3,2,1,0}, f32[32,4096]{1,0}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{1,0}, f32[32,1024,32,128]{3,2,1,0}, /*index=15*/s32[2,2048]{1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,14336]{2,1,0}, /*index=20*/f32[32,14336,1024]{2,1,0}, f32[32,1024,14336]{2,1,0}) parameter(0)
  %get-tuple-element.318 = s32[] get-tuple-element(%param.3), index=0
  %constant.589 = s32[] constant(32)
  ROOT %lt.85 = pred[] compare(%get-tuple-element.318, %constant.589), direction=LT, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while/cond/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
}

%region_16.17 (reduce_sum.86: f32[], reduce_sum.87: f32[]) -> f32[] {
  %reduce_sum.86 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.87 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.88 = f32[] add(%reduce_sum.86, %reduce_sum.87), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_16.17.clone (reduce_sum.421: f32[], reduce_sum.504: f32[]) -> f32[] {
  %reduce_sum.421 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.504 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.505 = f32[] add(%reduce_sum.421, %reduce_sum.504), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_17.18 (reduce_sum.93: f32[], reduce_sum.94: f32[]) -> f32[] {
  %reduce_sum.93 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.94 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.95 = f32[] add(%reduce_sum.93, %reduce_sum.94), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_17.18.clone (reduce_sum.506: f32[], reduce_sum.507: f32[]) -> f32[] {
  %reduce_sum.506 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.507 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.508 = f32[] add(%reduce_sum.506, %reduce_sum.507), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_18.19 (reduce_sum.100: f32[], reduce_sum.101: f32[]) -> f32[] {
  %reduce_sum.100 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.101 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.102 = f32[] add(%reduce_sum.100, %reduce_sum.101), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_18.19.clone (reduce_sum.509: f32[], reduce_sum.510: f32[]) -> f32[] {
  %reduce_sum.509 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.510 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.511 = f32[] add(%reduce_sum.509, %reduce_sum.510), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_19.20 (reduce_sum.85: f32[], reduce_sum.92: f32[]) -> f32[] {
  %reduce_sum.85 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.92 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.99 = f32[] add(%reduce_sum.85, %reduce_sum.92), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_20.21 (reduce_sum.114: f32[], reduce_sum.115: f32[]) -> f32[] {
  %reduce_sum.114 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.115 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.106 = f32[] add(%reduce_sum.114, %reduce_sum.115), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_21.22 (reduce_sum.121: f32[], reduce_sum.122: f32[]) -> f32[] {
  %reduce_sum.121 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.122 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.113 = f32[] add(%reduce_sum.121, %reduce_sum.122), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_21.22.clone (reduce_sum.512: f32[], reduce_sum.513: f32[]) -> f32[] {
  %reduce_sum.512 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.513 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.514 = f32[] add(%reduce_sum.512, %reduce_sum.513), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_22.23 (reduce_sum.128: f32[], reduce_sum.129: f32[]) -> f32[] {
  %reduce_sum.128 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.129 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.119 = f32[] add(%reduce_sum.128, %reduce_sum.129), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_22.23.clone (reduce_sum.515: f32[], reduce_sum.516: f32[]) -> f32[] {
  %reduce_sum.515 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.516 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.517 = f32[] add(%reduce_sum.515, %reduce_sum.516), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_23.24 (reduce_sum.135: f32[], reduce_sum.136: f32[]) -> f32[] {
  %reduce_sum.135 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.136 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.120 = f32[] add(%reduce_sum.135, %reduce_sum.136), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_23.24.clone (reduce_sum.518: f32[], reduce_sum.519: f32[]) -> f32[] {
  %reduce_sum.518 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.519 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.520 = f32[] add(%reduce_sum.518, %reduce_sum.519), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_24.25 (reduce_sum.142: f32[], reduce_sum.143: f32[]) -> f32[] {
  %reduce_sum.142 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.143 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.126 = f32[] add(%reduce_sum.142, %reduce_sum.143), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_24.25.clone (reduce_sum.521: f32[], reduce_sum.522: f32[]) -> f32[] {
  %reduce_sum.521 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.522 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.523 = f32[] add(%reduce_sum.521, %reduce_sum.522), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%add.10.clone (x.21: bf16[], y.21: bf16[]) -> bf16[] {
  %x.21 = bf16[] parameter(0)
  %y.21 = bf16[] parameter(1)
  ROOT %add.399 = bf16[] add(%x.21, %y.21)
}

%region_25.26 (reduce_sum.149: f32[], reduce_sum.150: f32[]) -> f32[] {
  %reduce_sum.149 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.150 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.127 = f32[] add(%reduce_sum.149, %reduce_sum.150), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_25.26.clone (reduce_sum.524: f32[], reduce_sum.525: f32[]) -> f32[] {
  %reduce_sum.524 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.525 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.526 = f32[] add(%reduce_sum.524, %reduce_sum.525), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%add.11.clone (x.23: bf16[], y.23: bf16[]) -> bf16[] {
  %x.23 = bf16[] parameter(0)
  %y.23 = bf16[] parameter(1)
  ROOT %add.401 = bf16[] add(%x.23, %y.23)
}

%region_26.27 (reduce_sum.156: f32[], reduce_sum.157: f32[]) -> f32[] {
  %reduce_sum.156 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.157 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.133 = f32[] add(%reduce_sum.156, %reduce_sum.157), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_26.27.clone (reduce_sum.527: f32[], reduce_sum.528: f32[]) -> f32[] {
  %reduce_sum.527 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.528 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.529 = f32[] add(%reduce_sum.527, %reduce_sum.528), metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
}

%region_27.28 (reduce_sum.163: f32[], reduce_sum.164: f32[]) -> f32[] {
  %reduce_sum.163 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.164 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.134 = f32[] add(%reduce_sum.163, %reduce_sum.164), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_28.29 (reduce_sum.170: f32[], reduce_sum.171: f32[]) -> f32[] {
  %reduce_sum.170 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.171 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.140 = f32[] add(%reduce_sum.170, %reduce_sum.171), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_28.29.clone (reduce_sum.530: f32[], reduce_sum.531: f32[]) -> f32[] {
  %reduce_sum.530 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.531 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.532 = f32[] add(%reduce_sum.530, %reduce_sum.531), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_29.30 (reduce_sum.177: f32[], reduce_sum.178: f32[]) -> f32[] {
  %reduce_sum.177 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.178 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.141 = f32[] add(%reduce_sum.177, %reduce_sum.178), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_29.30.clone (reduce_sum.533: f32[], reduce_sum.534: f32[]) -> f32[] {
  %reduce_sum.533 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.534 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.535 = f32[] add(%reduce_sum.533, %reduce_sum.534), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_30.31 (reduce_sum.184: f32[], reduce_sum.185: f32[]) -> f32[] {
  %reduce_sum.184 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.185 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.147 = f32[] add(%reduce_sum.184, %reduce_sum.185), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_30.31.clone (reduce_sum.536: f32[], reduce_sum.537: f32[]) -> f32[] {
  %reduce_sum.536 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.537 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.538 = f32[] add(%reduce_sum.536, %reduce_sum.537), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_31.32 (reduce_sum.191: f32[], reduce_sum.192: f32[]) -> f32[] {
  %reduce_sum.191 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.192 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.148 = f32[] add(%reduce_sum.191, %reduce_sum.192), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_32.33 (reduce_sum.198: f32[], reduce_sum.199: f32[]) -> f32[] {
  %reduce_sum.198 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.199 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.154 = f32[] add(%reduce_sum.198, %reduce_sum.199), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_33.34 (reduce_sum.205: f32[], reduce_sum.206: f32[]) -> f32[] {
  %reduce_sum.205 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.206 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.155 = f32[] add(%reduce_sum.205, %reduce_sum.206), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_33.34.clone (reduce_sum.539: f32[], reduce_sum.540: f32[]) -> f32[] {
  %reduce_sum.539 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.540 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.541 = f32[] add(%reduce_sum.539, %reduce_sum.540), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_34.35 (reduce_sum.212: f32[], reduce_sum.213: f32[]) -> f32[] {
  %reduce_sum.212 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.213 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.161 = f32[] add(%reduce_sum.212, %reduce_sum.213), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_34.35.clone (reduce_sum.542: f32[], reduce_sum.543: f32[]) -> f32[] {
  %reduce_sum.542 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.543 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.544 = f32[] add(%reduce_sum.542, %reduce_sum.543), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_35.36 (reduce_sum.219: f32[], reduce_sum.220: f32[]) -> f32[] {
  %reduce_sum.219 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.220 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.162 = f32[] add(%reduce_sum.219, %reduce_sum.220), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_35.36.clone (reduce_sum.545: f32[], reduce_sum.546: f32[]) -> f32[] {
  %reduce_sum.545 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.546 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.547 = f32[] add(%reduce_sum.545, %reduce_sum.546), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_36.37 (reduce_sum.226: f32[], reduce_sum.227: f32[]) -> f32[] {
  %reduce_sum.226 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.227 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.168 = f32[] add(%reduce_sum.226, %reduce_sum.227), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_36.37.clone (reduce_sum.548: f32[], reduce_sum.549: f32[]) -> f32[] {
  %reduce_sum.548 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.549 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.550 = f32[] add(%reduce_sum.548, %reduce_sum.549), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_37.38 (reduce_sum.233: f32[], reduce_sum.234: f32[]) -> f32[] {
  %reduce_sum.233 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.234 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.169 = f32[] add(%reduce_sum.233, %reduce_sum.234), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_37.38.clone (reduce_sum.551: f32[], reduce_sum.552: f32[]) -> f32[] {
  %reduce_sum.551 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.552 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.553 = f32[] add(%reduce_sum.551, %reduce_sum.552), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_38.39 (reduce_sum.240: f32[], reduce_sum.241: f32[]) -> f32[] {
  %reduce_sum.240 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.241 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.175 = f32[] add(%reduce_sum.240, %reduce_sum.241), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_38.39.clone (reduce_sum.554: f32[], reduce_sum.555: f32[]) -> f32[] {
  %reduce_sum.554 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.555 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.556 = f32[] add(%reduce_sum.554, %reduce_sum.555), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_39.40 (reduce_sum.247: f32[], reduce_sum.248: f32[]) -> f32[] {
  %reduce_sum.247 = f32[] parameter(0), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  %reduce_sum.248 = f32[] parameter(1), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  ROOT %reduce_sum.176 = f32[] add(%reduce_sum.247, %reduce_sum.248), metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=557 source_end_line=557 source_column=10 source_end_column=10}
}

%region_40.41 (reduce_sum.254: f32[], reduce_sum.255: f32[]) -> f32[] {
  %reduce_sum.254 = f32[] parameter(0), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  %reduce_sum.255 = f32[] parameter(1), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  ROOT %reduce_sum.182 = f32[] add(%reduce_sum.254, %reduce_sum.255), metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/train.py" source_line=153 source_end_line=153 source_column=19 source_end_column=19}
}

%region_40.41.clone (reduce_sum.557: f32[], reduce_sum.558: f32[]) -> f32[] {
  %reduce_sum.557 = f32[] parameter(0), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  %reduce_sum.558 = f32[] parameter(1), metadata={op_name="jit(train_step)/jvp()/reduce_sum"}
  ROOT %reduce_sum.559 = f32[] add(%reduce_sum.557, %reduce_sum.558), metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/train.py" source_line=153 source_end_line=153 source_column=19 source_end_column=19}
}

%region_41.42 (reduce_sum.261: f32[], reduce_sum.262: f32[]) -> f32[] {
  %reduce_sum.261 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.262 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.183 = f32[] add(%reduce_sum.261, %reduce_sum.262), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_42.43 (reduce_sum.268: f32[], reduce_sum.269: f32[]) -> f32[] {
  %reduce_sum.268 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.269 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.189 = f32[] add(%reduce_sum.268, %reduce_sum.269), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_42.43.clone (reduce_sum.560: f32[], reduce_sum.561: f32[]) -> f32[] {
  %reduce_sum.560 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.561 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.562 = f32[] add(%reduce_sum.560, %reduce_sum.561), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_43.44 (reduce_sum.275: f32[], reduce_sum.276: f32[]) -> f32[] {
  %reduce_sum.275 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.276 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.190 = f32[] add(%reduce_sum.275, %reduce_sum.276), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_43.44.clone (reduce_sum.563: f32[], reduce_sum.564: f32[]) -> f32[] {
  %reduce_sum.563 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.564 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.565 = f32[] add(%reduce_sum.563, %reduce_sum.564), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_44.45 (reduce_sum.282: f32[], reduce_sum.283: f32[]) -> f32[] {
  %reduce_sum.282 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.283 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.196 = f32[] add(%reduce_sum.282, %reduce_sum.283), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_44.45.clone (reduce_sum.566: f32[], reduce_sum.567: f32[]) -> f32[] {
  %reduce_sum.566 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.567 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.568 = f32[] add(%reduce_sum.566, %reduce_sum.567), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_45.46 (reduce_sum.289: f32[], reduce_sum.290: f32[]) -> f32[] {
  %reduce_sum.289 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.290 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.197 = f32[] add(%reduce_sum.289, %reduce_sum.290), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_46.47 (reduce_sum.296: f32[], reduce_sum.297: f32[]) -> f32[] {
  %reduce_sum.296 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.297 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.203 = f32[] add(%reduce_sum.296, %reduce_sum.297), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_47.48 (reduce_sum.303: f32[], reduce_sum.304: f32[]) -> f32[] {
  %reduce_sum.303 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.304 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.204 = f32[] add(%reduce_sum.303, %reduce_sum.304), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_47.48.clone (reduce_sum.569: f32[], reduce_sum.570: f32[]) -> f32[] {
  %reduce_sum.569 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.570 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.571 = f32[] add(%reduce_sum.569, %reduce_sum.570), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_48.49 (reduce_sum.310: f32[], reduce_sum.311: f32[]) -> f32[] {
  %reduce_sum.310 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.311 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.210 = f32[] add(%reduce_sum.310, %reduce_sum.311), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_48.49.clone (reduce_sum.572: f32[], reduce_sum.573: f32[]) -> f32[] {
  %reduce_sum.572 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.573 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.574 = f32[] add(%reduce_sum.572, %reduce_sum.573), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_49.50 (reduce_sum.317: f32[], reduce_sum.318: f32[]) -> f32[] {
  %reduce_sum.317 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.318 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.211 = f32[] add(%reduce_sum.317, %reduce_sum.318), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_49.50.clone (reduce_sum.575: f32[], reduce_sum.576: f32[]) -> f32[] {
  %reduce_sum.575 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.576 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.577 = f32[] add(%reduce_sum.575, %reduce_sum.576), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_50.51 (reduce_sum.324: f32[], reduce_sum.325: f32[]) -> f32[] {
  %reduce_sum.324 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.325 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.217 = f32[] add(%reduce_sum.324, %reduce_sum.325), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_50.51.clone (reduce_sum.578: f32[], reduce_sum.579: f32[]) -> f32[] {
  %reduce_sum.578 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.579 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.580 = f32[] add(%reduce_sum.578, %reduce_sum.579), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_51.52 (reduce_sum.331: f32[], reduce_sum.332: f32[]) -> f32[] {
  %reduce_sum.331 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.332 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.218 = f32[] add(%reduce_sum.331, %reduce_sum.332), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_51.52.clone (reduce_sum.581: f32[], reduce_sum.582: f32[]) -> f32[] {
  %reduce_sum.581 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.582 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.583 = f32[] add(%reduce_sum.581, %reduce_sum.582), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_52.53 (reduce_sum.338: f32[], reduce_sum.339: f32[]) -> f32[] {
  %reduce_sum.338 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.339 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.224 = f32[] add(%reduce_sum.338, %reduce_sum.339), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_52.53.clone (reduce_sum.584: f32[], reduce_sum.585: f32[]) -> f32[] {
  %reduce_sum.584 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.585 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.586 = f32[] add(%reduce_sum.584, %reduce_sum.585), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_54.55.clone (reduce_sum.587: f32[], reduce_sum.588: f32[]) -> f32[] {
  %reduce_sum.587 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.588 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.589 = f32[] add(%reduce_sum.587, %reduce_sum.588), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_55.56.clone (reduce_sum.590: f32[], reduce_sum.591: f32[]) -> f32[] {
  %reduce_sum.590 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.591 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.592 = f32[] add(%reduce_sum.590, %reduce_sum.591), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_56.57.clone (reduce_sum.593: f32[], reduce_sum.594: f32[]) -> f32[] {
  %reduce_sum.593 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.594 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.595 = f32[] add(%reduce_sum.593, %reduce_sum.594), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_59.60.clone (reduce_sum.596: f32[], reduce_sum.597: f32[]) -> f32[] {
  %reduce_sum.596 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.597 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.598 = f32[] add(%reduce_sum.596, %reduce_sum.597), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_60.61.clone (reduce_sum.599: f32[], reduce_sum.600: f32[]) -> f32[] {
  %reduce_sum.599 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.600 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.601 = f32[] add(%reduce_sum.599, %reduce_sum.600), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_61.62.clone (reduce_sum.602: f32[], reduce_sum.603: f32[]) -> f32[] {
  %reduce_sum.602 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.603 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.604 = f32[] add(%reduce_sum.602, %reduce_sum.603), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_62.63.clone (reduce_sum.605: f32[], reduce_sum.606: f32[]) -> f32[] {
  %reduce_sum.605 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.606 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.607 = f32[] add(%reduce_sum.605, %reduce_sum.606), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_63.64.clone (reduce_sum.608: f32[], reduce_sum.609: f32[]) -> f32[] {
  %reduce_sum.608 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.609 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.610 = f32[] add(%reduce_sum.608, %reduce_sum.609), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

%region_64.65.clone (reduce_sum.611: f32[], reduce_sum.612: f32[]) -> f32[] {
  %reduce_sum.611 = f32[] parameter(0), metadata={op_name="jit(train_step)/reduce_sum"}
  %reduce_sum.612 = f32[] parameter(1), metadata={op_name="jit(train_step)/reduce_sum"}
  ROOT %reduce_sum.613 = f32[] add(%reduce_sum.611, %reduce_sum.612), metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
}

ENTRY %main.66_spmd (param.4: s32[], param.5: f32[4096], param.17: f32[1024,32,14336], param.18: f32[1024,32,14336], param.19: f32[14336,32,1024], param.10: f32[4096,32], param.11: f32[4096,32], param.14: f32[1024,32,8,128], param.16: f32[32,32,128,1024], param.12: f32[1024,32,32,128], param.15: f32[1024,32,8,128], param.20: f32[1024,128256], param.9: f32[128256,1024], param.23: s32[], param.22: f32[4096], param.25: f32[1024,32,14336], param.27: f32[1024,32,14336], param.29: f32[14336,32,1024], param.31: f32[4096,32], param.33: f32[4096,32], param.35: f32[1024,32,8,128], param.37: f32[32,32,128,1024], param.39: f32[1024,32,32,128], param.41: f32[1024,32,8,128], param.43: f32[1024,128256], param.45: f32[128256,1024], param.24: f32[4096], param.26: f32[1024,32,14336], param.28: f32[1024,32,14336], param.30: f32[14336,32,1024], param.32: f32[4096,32], param.34: f32[4096,32], param.36: f32[1024,32,8,128], param.38: f32[32,32,128,1024], param.40: f32[1024,32,32,128], param.42: f32[1024,32,8,128], param.44: f32[1024,128256], param.46: f32[128256,1024], param.6: s32[], param.8: s32[4,2048], param.13: s32[4,2048], param.21: s32[4,2048], param.7: s32[4,2048]) -> (s32[], f32[4096], f32[1024,32,14336], f32[1024,32,14336], f32[14336,32,1024], /*index=5*/f32[4096,32], f32[4096,32], f32[1024,32,8,128], f32[32,32,128,1024], f32[1024,32,32,128], /*index=10*/f32[1024,32,8,128], f32[1024,128256], f32[128256,1024], s32[], f32[4096], /*index=15*/f32[1024,32,14336], f32[1024,32,14336], f32[14336,32,1024], f32[4096,32], f32[4096,32], /*index=20*/f32[1024,32,8,128], f32[32,32,128,1024], f32[1024,32,32,128], f32[1024,32,8,128], f32[1024,128256], /*index=25*/f32[128256,1024], f32[4096], f32[1024,32,14336], f32[1024,32,14336], f32[14336,32,1024], /*index=30*/f32[4096,32], f32[4096,32], f32[1024,32,8,128], f32[32,32,128,1024], f32[1024,32,32,128], /*index=35*/f32[1024,32,8,128], f32[1024,128256], f32[128256,1024], s32[], f32[], /*index=40*/f32[], f32[], f32[], f32[], f32[], /*index=45*/s32[]) {
  %param.4 = s32[] parameter(0), sharding={replicated}, metadata={op_name="state.step"}
  %constant.590 = s32[] constant(1)
  %add.402 = s32[] add(%param.4, %constant.590), metadata={op_name="jit(train_step)/add" source_file="/opt/flax/flax/training/train_state.py" source_line=118 source_end_line=118 source_column=11 source_end_column=11}
  %param.5 = f32[4096]{0} parameter(1), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'decoder_norm\'][\'scale\']"}
  %param.6 = s32[] parameter(38), sharding={replicated}, metadata={op_name="state.opt_state[2].count"}
  %constant.591 = s32[] constant(2)
  %lt.86 = pred[] compare(%param.6, %constant.591), direction=LT, metadata={op_name="jit(train_step)/lt" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_join.py" source_line=43 source_end_line=43 source_column=25 source_end_column=25}
  %constant.592 = f32[] constant(1)
  %constant.593 = s32[] constant(0)
  %max.9 = s32[] maximum(%constant.593, %param.6), metadata={op_name="jit(train_step)/jit(clip)/max" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=143 source_end_line=143 source_column=12 source_end_column=12}
  %min.3 = s32[] minimum(%constant.591, %max.9), metadata={op_name="jit(train_step)/jit(clip)/min" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=143 source_end_line=143 source_column=12 source_end_column=12}
  %convert_element_type.339 = f32[] convert(%min.3), metadata={op_name="jit(train_step)/convert_element_type" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=144 source_end_line=144 source_column=15 source_end_column=15}
  %constant.594 = f32[] constant(0.5)
  %div.281 = f32[] multiply(%convert_element_type.339, %constant.594), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=144 source_end_line=144 source_column=15 source_end_column=15}
  %sub.110 = f32[] subtract(%constant.592, %div.281), metadata={op_name="jit(train_step)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=144 source_end_line=144 source_column=11 source_end_column=11}
  %constant.595 = f32[] constant(-3e-05)
  %mul.841 = f32[] multiply(%sub.110, %constant.595), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=145 source_end_line=145 source_column=11 source_end_column=11}
  %constant.596 = f32[] constant(3e-05)
  %add.403 = f32[] add(%mul.841, %constant.596), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=145 source_end_line=145 source_column=11 source_end_column=11}
  %constant.597 = s32[] constant(-2)
  %sub.111 = s32[] add(%param.6, %constant.597), metadata={op_name="jit(train_step)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_join.py" source_line=43 source_end_line=43 source_column=59 source_end_column=59}
  %convert_element_type.340 = f32[] convert(%sub.111), metadata={op_name="jit(train_step)/convert_element_type" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1116 source_end_line=1116 source_column=12 source_end_column=12}
  %constant.598 = f32[] constant(0.174532935)
  %mul.842 = f32[] multiply(%convert_element_type.340, %constant.598), metadata={op_name="jit(train_step)/mul" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1117 source_end_line=1117 source_column=25 source_end_column=25}
  %cos.14 = f32[] cosine(%mul.842), metadata={op_name="jit(train_step)/cos" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1117 source_end_line=1117 source_column=17 source_end_column=17}
  %add.404 = f32[] add(%cos.14, %constant.592), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1117 source_end_line=1117 source_column=17 source_end_column=17}
  %constant.599 = f32[] constant(1.5e-05)
  %mul.843 = f32[] multiply(%add.404, %constant.599), metadata={op_name="jit(train_step)/mul" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1118 source_end_line=1118 source_column=11 source_end_column=11}
  %mul.844 = f32[] multiply(%add.404, %constant.594), metadata={op_name="jit(train_step)/mul" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1117 source_end_line=1117 source_column=10 source_end_column=10}
  %sub.112 = f32[] subtract(%constant.592, %mul.844), metadata={op_name="jit(train_step)/sub" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1118 source_end_line=1118 source_column=37 source_end_column=37}
  %constant.600 = f32[] constant(3e-06)
  %mul.845 = f32[] multiply(%sub.112, %constant.600), metadata={op_name="jit(train_step)/mul" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1118 source_end_line=1118 source_column=25 source_end_column=25}
  %add.405 = f32[] add(%mul.843, %mul.845), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/maxtext_utils.py" source_line=1118 source_end_line=1118 source_column=11 source_end_column=11}
  %select_n.132 = f32[] select(%lt.86, %add.403, %add.405), metadata={op_name="jit(train_step)/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_join.py" source_line=43 source_end_line=43 source_column=15 source_end_column=15}
  %constant.601 = f32[] constant(-1)
  %mul.846 = f32[] multiply(%select_n.132, %constant.601), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=964 source_end_line=964 source_column=43 source_end_column=43}
  %mul.847 = f32[4096]{0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %param.7 = s32[4,2048]{1,0} parameter(42), sharding={devices=[1,4]<=[4]}, metadata={op_name="data[\'targets_segmentation\']"}
  %slice.20 = s32[2,2048]{1,0} slice(%param.7), slice={[0:2], [0:2048]}, metadata={op_name="jit(train_step)/jvp()/slice" source_file="/opt/maxtext/src/MaxText/train.py" source_line=104 source_end_line=104 source_column=16 source_end_column=16}
  %broadcast.254 = s32[2,2048]{1,0} broadcast(%constant.593), dimensions={}, metadata={op_name="broadcast.84"}
  %ne.10 = pred[2,2048]{1,0} compare(%slice.20, %broadcast.254), direction=NE, metadata={op_name="jit(train_step)/jvp()/ne" source_file="/opt/maxtext/src/MaxText/train.py" source_line=152 source_end_line=152 source_column=21 source_end_column=21}
  %all-gather.15 = pred[2,8192]{1,0} all-gather(%ne.10), channel_id=29, replica_groups=[1,4]<=[4], dimensions={1}, use_global_device_ids=true, metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=152 source_end_line=152 source_column=13 source_end_column=13}
  %convert_element_type.341 = s32[2,2048]{1,0} convert(%ne.10), metadata={op_name="jit(train_step)/jvp()/convert_element_type" source_file="/opt/maxtext/src/MaxText/train.py" source_line=174 source_end_line=174 source_column=18 source_end_column=18}
  %reduce.6 = s32[] reduce(%convert_element_type.341, %constant.593), dimensions={0,1}, to_apply=%region_0.1, metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/train.py" source_line=174 source_end_line=174 source_column=18 source_end_column=18}
  %all-reduce.9 = s32[] all-reduce(%reduce.6), channel_id=28, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_0.1.clone, metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/train.py" source_line=174 source_end_line=174 source_column=18 source_end_column=18}
  %convert_element_type.342 = f32[] convert(%all-reduce.9), metadata={op_name="jit(train_step)/jvp()/convert_element_type" source_file="/opt/maxtext/src/MaxText/train.py" source_line=186 source_end_line=186 source_column=25 source_end_column=25}
  %constant.607 = f32[] constant(1e-08)
  %add.406 = f32[] add(%convert_element_type.342, %constant.607), metadata={op_name="jit(train_step)/jvp()/add" source_file="/opt/maxtext/src/MaxText/train.py" source_line=186 source_end_line=186 source_column=25 source_end_column=25}
  %div.282 = f32[] divide(%constant.592, %add.406), metadata={op_name="jit(train_step)/transpose(jvp())/div" source_file="/opt/maxtext/src/MaxText/train.py" source_line=186 source_end_line=186 source_column=11 source_end_column=11}
  %broadcast_in_dim.160 = f32[2,8192]{1,0} broadcast(%div.282), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp())/broadcast_in_dim" source_file="/opt/maxtext/src/MaxText/train.py" source_line=153 source_end_line=153 source_column=19 source_end_column=19}
  %constant.608 = f32[] constant(0)
  %broadcast.255 = f32[2,8192]{1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="broadcast.42"}
  %mul.848 = f32[2,8192]{1,0} select(%all-gather.15, %broadcast_in_dim.160, %broadcast.255), metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=152 source_end_line=152 source_column=13 source_end_column=13}
  %sharding_constraint.156 = f32[2,8192]{1,0} copy(%mul.848), metadata={op_name="jit(train_step)/transpose(jvp())/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %constant.610 = s32[4]{0} constant({0, 2048, 4096, 6144}), metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %partition-id.4 = u32[] partition-id()
  %dynamic-slice.46 = s32[1]{0} dynamic-slice(%constant.610, %partition-id.4), dynamic_slice_sizes={1}, metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %reshape.327 = s32[] reshape(%dynamic-slice.46), metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %dynamic-slice.47 = f32[2,2048]{1,0} dynamic-slice(%sharding_constraint.156, %constant.593, %reshape.327), dynamic_slice_sizes={2,2048}, metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %mul.849 = f32[2,2048,128256]{2,1,0} broadcast(%dynamic-slice.47), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %convert_element_type.343 = bf16[4096]{0} convert(%param.5), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=78 source_end_line=78 source_column=12 source_end_column=12}
  %dot_general.136 = bf16[2,2048,4096]{2,1,0} broadcast(%convert_element_type.343), dimensions={2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %param.8 = s32[4,2048]{1,0} parameter(39), sharding={devices=[1,4]<=[4]}, metadata={op_name="data[\'inputs\']"}
  %slice.21 = s32[2,2048]{1,0} slice(%param.8), slice={[0:2], [0:2048]}, metadata={op_name="jit(train_step)/jvp()/slice" source_file="/opt/maxtext/src/MaxText/train.py" source_line=104 source_end_line=104 source_column=16 source_end_column=16}
  %eq.13 = s32[2,2048,128256]{2,1,0} broadcast(%slice.21), dimensions={0,1}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/eq" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=166 source_end_line=166 source_column=26 source_end_column=26}
  %iota.23 = s32[2,2048,128256]{2,1,0} iota(), iota_dimension=2, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/eq" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=166 source_end_line=166 source_column=26 source_end_column=26}
  %eq.28 = pred[2,2048,128256]{2,1,0} compare(%eq.13, %iota.23), direction=EQ, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/eq" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=166 source_end_line=166 source_column=26 source_end_column=26}
  %convert_element_type.344 = bf16[2,2048,128256]{2,1,0} convert(%eq.28), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=166 source_end_line=166 source_column=16 source_end_column=16}
  %param.9 = f32[128256,1024]{1,0} parameter(12), sharding={devices=[1,4]<=[4]}, metadata={op_name="state.params[\'params\'][\'token_embedder\'][\'embedding\']"}
  %convert_element_type.345 = bf16[128256,1024]{1,0} convert(%param.9), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=150 source_end_line=150 source_column=16 source_end_column=16}
  %all-gather.16 = bf16[128256,4096]{1,0} all-gather(%convert_element_type.345), channel_id=30, replica_groups=[1,4]<=[4], dimensions={1}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/dot_general" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=167 source_end_line=167 source_column=15 source_end_column=15}
  %dot.34 = bf16[2,2048,4096]{2,1,0} dot(%convert_element_type.344, %all-gather.16), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/dot_general" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=167 source_end_line=167 source_column=15 source_end_column=15}
  %constant.616 = bf16[] constant(0)
  %broadcast_in_dim.161 = bf16[32,2,2048,4096]{3,2,1,0} broadcast(%constant.616), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %param.10 = f32[4096,32]{1,0} parameter(5), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'post_self_attention_layer_norm\'][\'scale\']"}
  %transpose.138 = f32[32,4096]{0,1} transpose(%param.10), dimensions={1,0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %param.11 = f32[4096,32]{1,0} parameter(6), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'pre_self_attention_layer_norm\'][\'scale\']"}
  %transpose.139 = f32[32,4096]{0,1} transpose(%param.11), dimensions={1,0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %param.12 = f32[1024,32,32,128]{3,2,1,0} parameter(9), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'query\'][\'kernel\']"}
  %transpose.140 = f32[32,1024,32,128]{3,2,0,1} transpose(%param.12), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %param.13 = s32[4,2048]{1,0} parameter(40), sharding={devices=[1,4]<=[4]}, metadata={op_name="data[\'inputs_position\']"}
  %slice.22 = s32[2,2048]{1,0} slice(%param.13), slice={[0:2], [0:2048]}, metadata={op_name="jit(train_step)/jvp()/slice" source_file="/opt/maxtext/src/MaxText/train.py" source_line=104 source_end_line=104 source_column=16 source_end_column=16}
  %convert_element_type.346 = f32[2,2048]{1,0} convert(%slice.22), metadata={op_name="jit(train_step)/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=334 source_end_line=334 source_column=19 source_end_column=19}
  %div.283 = f32[2,2048,1,64]{3,2,1,0} broadcast(%convert_element_type.346), dimensions={0,1}, metadata={op_name="jit(train_step)/layers/div" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=334 source_end_line=334 source_column=19 source_end_column=19}
  %constant.622 = f32[] constant(500000)
  %broadcast.256 = f32[64]{0} broadcast(%constant.622), dimensions={}, metadata={op_name="broadcast.87"}
  %iota.24 = s32[64]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/layers/iota" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=300 source_end_line=300 source_column=19 source_end_column=19}
  %broadcast.257 = s32[64]{0} broadcast(%constant.591), dimensions={}, metadata={op_name="broadcast.89"}
  %mul.850 = s32[64]{0} multiply(%iota.24, %broadcast.257), metadata={op_name="jit(train_step)/layers/mul" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=300 source_end_line=300 source_column=15 source_end_column=15}
  %convert_element_type.347 = f32[64]{0} convert(%mul.850), metadata={op_name="jit(train_step)/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=300 source_end_line=300 source_column=15 source_end_column=15}
  %constant.623 = f32[] constant(0.0078125)
  %broadcast.258 = f32[64]{0} broadcast(%constant.623), dimensions={}, metadata={op_name="broadcast.44"}
  %div.284 = f32[64]{0} multiply(%convert_element_type.347, %broadcast.258), metadata={op_name="jit(train_step)/layers/div" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=300 source_end_line=300 source_column=15 source_end_column=15}
  %pow.20 = f32[64]{0} power(%broadcast.256, %div.284), metadata={op_name="jit(train_step)/layers/pow" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=301 source_end_line=301 source_column=37 source_end_column=37}
  %div.285 = f32[2,2048,1,64]{3,2,1,0} broadcast(%pow.20), dimensions={3}, metadata={op_name="jit(train_step)/layers/div" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=334 source_end_line=334 source_column=19 source_end_column=19}
  %div.286 = f32[2,2048,1,64]{3,2,1,0} divide(%div.283, %div.285), metadata={op_name="jit(train_step)/layers/div" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=334 source_end_line=334 source_column=19 source_end_column=19}
  %cos.15 = f32[2,2048,1,64]{3,2,1,0} cosine(%div.286), metadata={op_name="jit(train_step)/layers/cos" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=336 source_end_line=336 source_column=10 source_end_column=10}
  %convert_element_type.348 = bf16[2,2048,1,64]{3,2,1,0} convert(%cos.15), metadata={op_name="jit(train_step)/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=336 source_end_line=336 source_column=10 source_end_column=10}
  %sin.12 = f32[2,2048,1,64]{3,2,1,0} sine(%div.286), metadata={op_name="jit(train_step)/layers/sin" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=335 source_end_line=335 source_column=10 source_end_column=10}
  %convert_element_type.349 = bf16[2,2048,1,64]{3,2,1,0} convert(%sin.12), metadata={op_name="jit(train_step)/layers/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=335 source_end_line=335 source_column=10 source_end_column=10}
  %param.14 = f32[1024,32,8,128]{3,2,1,0} parameter(7), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'key\'][\'kernel\']"}
  %transpose.141 = f32[32,1024,8,128]{3,2,0,1} transpose(%param.14), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %param.15 = f32[1024,32,8,128]{3,2,1,0} parameter(10), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'value\'][\'kernel\']"}
  %transpose.142 = f32[32,1024,8,128]{3,2,0,1} transpose(%param.15), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %param.16 = f32[32,32,128,1024]{3,2,1,0} parameter(8), sharding={devices=[1,1,1,4]<=[4]}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'out\'][\'kernel\']"}
  %transpose.143 = f32[32,32,128,1024]{3,2,0,1} transpose(%param.16), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %param.17 = f32[1024,32,14336]{2,1,0} parameter(2), sharding={devices=[4,1,1]<=[4]}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_0\'][\'kernel\']"}
  %transpose.144 = f32[32,1024,14336]{2,0,1} transpose(%param.17), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %param.18 = f32[1024,32,14336]{2,1,0} parameter(3), sharding={devices=[4,1,1]<=[4]}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_1\'][\'kernel\']"}
  %transpose.145 = f32[32,1024,14336]{2,0,1} transpose(%param.18), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %param.19 = f32[14336,32,1024]{2,1,0} parameter(4), sharding={devices=[1,1,4]<=[4]}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wo\'][\'kernel\']"}
  %transpose.146 = f32[32,14336,1024]{2,0,1} transpose(%param.19), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %tuple.42 = (s32[], bf16[2,2048,4096]{2,1,0}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{0,1}, f32[32,4096]{0,1}, /*index=5*/f32[32,1024,32,128]{3,2,0,1}, bf16[2,2048,1,64]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,0,1}, bf16[2,2048,1,64]{3,2,1,0}, /*index=10*/bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,0,1}, f32[32,32,128,1024]{3,2,0,1}, f32[32,1024,14336]{2,0,1}, f32[32,1024,14336]{2,0,1}, /*index=15*/f32[32,14336,1024]{2,0,1}) tuple(%constant.593, %dot.34, %broadcast_in_dim.161, %transpose.138, %transpose.139, /*index=5*/%transpose.140, %convert_element_type.348, %convert_element_type.349, %transpose.141, %convert_element_type.348, /*index=10*/%convert_element_type.349, %transpose.142, %transpose.143, %transpose.144, %transpose.145, /*index=15*/%transpose.146), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %while.36 = (s32[], bf16[2,2048,4096]{2,1,0}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{0,1}, f32[32,4096]{0,1}, /*index=5*/f32[32,1024,32,128]{3,2,0,1}, bf16[2,2048,1,64]{3,2,1,0}, bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,0,1}, bf16[2,2048,1,64]{3,2,1,0}, /*index=10*/bf16[2,2048,1,64]{3,2,1,0}, f32[32,1024,8,128]{3,2,0,1}, f32[32,32,128,1024]{3,2,0,1}, f32[32,1024,14336]{2,0,1}, f32[32,1024,14336]{2,0,1}, /*index=15*/f32[32,14336,1024]{2,0,1}) while(%tuple.42), condition=%region_4.5_spmd, body=%region_1.4_spmd, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.319 = bf16[2,2048,4096]{2,1,0} get-tuple-element(%while.36), index=1, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %convert_element_type.354 = f32[2,2048,4096]{2,1,0} convert(%get-tuple-element.319), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=66 source_end_line=66 source_column=8 source_end_column=8}
  %square.94 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.354, %convert_element_type.354), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/square" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %reduce.7 = f32[2,2048]{1,0} reduce(%square.94, %constant.608), dimensions={2}, to_apply=%region_5.6, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reduce_sum" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
  %constant.624 = f32[] constant(0.000244140625)
  %broadcast.259 = f32[2,2048]{1,0} broadcast(%constant.624), dimensions={}, metadata={op_name="broadcast.126"}
  %div.291 = f32[2,2048]{1,0} multiply(%reduce.7, %broadcast.259), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/div" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=12 source_end_column=12}
  %constant.625 = f32[] constant(1e-05)
  %add.407 = f32[2,2048]{1,0} broadcast(%constant.625), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/add" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=34 source_end_column=34}
  %add.408 = f32[2,2048]{1,0} add(%div.291, %add.407), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/add" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=34 source_end_column=34}
  %add.409 = f32[2,2048,1]{2,1,0} reshape(%add.408), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/add" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=34 source_end_column=34}
  %rsqrt.20 = f32[2,2048,1]{2,1,0} rsqrt(%add.409), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/rsqrt" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=24 source_end_column=24}
  %mul.852 = f32[2,2048]{1,0} reshape(%rsqrt.20), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %mul.853 = f32[2,2048,4096]{2,1,0} broadcast(%mul.852), dimensions={0,1}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %mul.854 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.354, %mul.853), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %convert_element_type.355 = bf16[2,2048,4096]{2,1,0} convert(%mul.854), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=8 source_end_column=8}
  %dot_general.137 = bf16[2,2048,4096]{2,1,0} multiply(%dot_general.136, %convert_element_type.355), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %param.20 = f32[1024,128256]{1,0} parameter(11), sharding={devices=[4,1]<=[4]}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'logits_dense\'][\'kernel\']"}
  %convert_element_type.356 = bf16[1024,128256]{1,0} convert(%param.20), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %all-gather.17 = bf16[4096,128256]{1,0} all-gather(%convert_element_type.356), channel_id=31, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dot.35 = bf16[2,2048,128256]{2,1,0} dot(%dot_general.137, %all-gather.17), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %convert_element_type.357 = f32[2,2048,128256]{2,1,0} convert(%dot.35), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/decoders.py" source_line=674 source_end_line=674 source_column=15 source_end_column=15}
  %constant.626 = f32[] constant(-inf)
  %reduce.8 = f32[2,2048]{1,0} reduce(%convert_element_type.357, %constant.626), dimensions={2}, to_apply=%region_6.7, metadata={op_name="jit(train_step)/jvp()/reduce_max" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=552 source_end_line=552 source_column=14 source_end_column=14}
  %sub.113 = f32[2,2048,128256]{2,1,0} broadcast(%reduce.8), dimensions={0,1}, metadata={op_name="jit(train_step)/jvp()/sub" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=553 source_end_line=553 source_column=12 source_end_column=12}
  %sub.114 = f32[2,2048,128256]{2,1,0} subtract(%convert_element_type.357, %sub.113), metadata={op_name="jit(train_step)/jvp()/sub" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=553 source_end_line=553 source_column=12 source_end_column=12}
  %exp.24 = f32[2,2048,128256]{2,1,0} exponential(%sub.114), metadata={op_name="jit(train_step)/jvp()/exp" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=554 source_end_line=554 source_column=16 source_end_column=16}
  %reduce.9 = f32[2,2048]{1,0} reduce(%exp.24, %constant.608), dimensions={2}, to_apply=%region_7.8, metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=555 source_end_line=555 source_column=12 source_end_column=12}
  %log.6 = f32[2,2048]{1,0} log(%reduce.9), metadata={op_name="jit(train_step)/jvp()/log" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=559 source_end_line=559 source_column=22 source_end_column=22}
  %add.410 = f32[2,2048]{1,0} add(%log.6, %reduce.8), metadata={op_name="jit(train_step)/jvp()/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=559 source_end_line=559 source_column=22 source_end_column=22}
  %broadcast.260 = f32[2,2048]{1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="broadcast.152"}
  %mul.855 = f32[2,2048]{1,0} multiply(%add.410, %broadcast.260), metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %add.411 = f32[2,2048]{1,0} broadcast(%constant.592), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp())/add" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %add.412 = f32[2,2048]{1,0} add(%mul.855, %add.411), metadata={op_name="jit(train_step)/transpose(jvp())/add" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %mul.856 = f32[2,2048,128256]{2,1,0} broadcast(%add.412), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %mul.857 = f32[2,2048,128256]{2,1,0} multiply(%mul.856, %exp.24), metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %div.292 = f32[2,2048,128256]{2,1,0} broadcast(%reduce.9), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp())/div" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %div.293 = f32[2,2048,128256]{2,1,0} divide(%mul.857, %div.292), metadata={op_name="jit(train_step)/transpose(jvp())/div" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %param.21 = s32[4,2048]{1,0} parameter(41), sharding={devices=[1,4]<=[4]}, metadata={op_name="data[\'targets\']"}
  %slice.23 = s32[2,2048]{1,0} slice(%param.21), slice={[0:2], [0:2048]}, metadata={op_name="jit(train_step)/jvp()/slice" source_file="/opt/maxtext/src/MaxText/train.py" source_line=104 source_end_line=104 source_column=16 source_end_column=16}
  %eq.29 = s32[2,2048,128256]{2,1,0} broadcast(%slice.23), dimensions={0,1}, metadata={op_name="jit(train_step)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/src/MaxText/train.py" source_line=143 source_end_line=143 source_column=24 source_end_column=24}
  %iota.26 = s32[2,2048,128256]{2,1,0} iota(), iota_dimension=2, metadata={op_name="jit(train_step)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/src/MaxText/train.py" source_line=143 source_end_line=143 source_column=24 source_end_column=24}
  %eq.30 = pred[2,2048,128256]{2,1,0} compare(%eq.29, %iota.26), direction=EQ, metadata={op_name="jit(train_step)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/src/MaxText/train.py" source_line=143 source_end_line=143 source_column=24 source_end_column=24}
  %convert_element_type.358 = f32[2,2048,128256]{2,1,0} convert(%eq.30), metadata={op_name="jit(train_step)/jvp(jit(_one_hot))/convert_element_type" source_file="/opt/maxtext/src/MaxText/train.py" source_line=143 source_end_line=143 source_column=24 source_end_column=24}
  %sub.115 = f32[2,2048,128256]{2,1,0} subtract(%div.293, %convert_element_type.358), metadata={op_name="jit(train_step)/transpose(jvp())/sub" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %mul.858 = f32[2,2048,128256]{2,1,0} multiply(%mul.849, %sub.115), metadata={op_name="jit(train_step)/transpose(jvp())/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=144 source_end_line=144 source_column=16 source_end_column=16}
  %convert_element_type.359 = bf16[2,2048,128256]{2,1,0} convert(%mul.858), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/decoders.py" source_line=674 source_end_line=674 source_column=15 source_end_column=15}
  %dot.36 = bf16[2,2048,4096]{2,1,0} dot(%convert_element_type.359, %all-gather.17), lhs_contracting_dims={2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/dot_general" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %transpose.147 = bf16[4096,2,2048]{0,2,1} transpose(%dot.36), dimensions={2,0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/i...k,...k->i...k/transpose" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %dot.37 = bf16[4096]{0} dot(%transpose.147, %convert_element_type.355), lhs_batch_dims={0}, lhs_contracting_dims={1,2}, rhs_batch_dims={2}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %all-reduce.10 = bf16[4096]{0} all-reduce(%dot.37), channel_id=32, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.9.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %convert_element_type.360 = f32[4096]{0} convert(%all-reduce.10), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=78 source_end_line=78 source_column=12 source_end_column=12}
  %mul.859 = f32[4096]{0} multiply(%convert_element_type.360, %convert_element_type.360), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.10 = f32[] reduce(%mul.859, %constant.608), dimensions={0}, to_apply=%region_8.9, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %dot_general.138 = bf16[4096,2,2048]{2,1,0} broadcast(%convert_element_type.343), dimensions={0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %dot_general.139 = bf16[4096,2,2048]{0,2,1} multiply(%transpose.147, %dot_general.138), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/i...k,...k->i...k/dot_general" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=81 source_end_line=81 source_column=11 source_end_column=11}
  %convert_element_type.361 = f32[4096,2,2048]{0,2,1} convert(%dot_general.139), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=8 source_end_column=8}
  %convert_element_type.362 = f32[2,2048,4096]{2,1,0} transpose(%convert_element_type.361), dimensions={1,2,0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=8 source_end_column=8}
  %mul.862 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.362, %mul.853), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %mul.863 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.354, %convert_element_type.362), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %reduce.11 = f32[2,2048]{1,0} reduce(%mul.863, %constant.608), dimensions={2}, to_apply=%region_9.10, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reduce_sum" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %reshape.331 = f32[2,2048,1]{2,1,0} reshape(%reduce.11), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/reshape" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=20 source_end_column=20}
  %div.294 = f32[2,2048,1]{2,1,0} divide(%rsqrt.20, %add.409), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/div" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=24 source_end_column=24}
  %constant.634 = f32[] constant(-0.5)
  %mul.864 = f32[2,2048,1]{2,1,0} broadcast(%constant.634), dimensions={}, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=24 source_end_column=24}
  %mul.865 = f32[2,2048,1]{2,1,0} multiply(%div.294, %mul.864), metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=24 source_end_column=24}
  %mul.866 = f32[2,2048,1]{2,1,0} multiply(%reshape.331, %mul.865), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=68 source_end_line=68 source_column=24 source_end_column=24}
  %constant.635 = f32[] constant(0.00048828125)
  %mul.867 = f32[2,2048,1]{2,1,0} broadcast(%constant.635), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %mul.868 = f32[2,2048,1]{2,1,0} multiply(%mul.866, %mul.867), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %mul.869 = f32[2,2048]{1,0} reshape(%mul.868), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %mul.870 = f32[2,2048,4096]{2,1,0} broadcast(%mul.869), dimensions={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %mul.871 = f32[2,2048,4096]{2,1,0} multiply(%convert_element_type.354, %mul.870), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/mul" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %add_any.51 = f32[2,2048,4096]{2,1,0} add(%mul.862, %mul.871), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/add_any" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=67 source_end_line=67 source_column=21 source_end_column=21}
  %convert_element_type.363 = bf16[2,2048,4096]{2,1,0} convert(%add_any.51), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/decoder_norm/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/normalizations.py" source_line=66 source_end_line=66 source_column=8 source_end_column=8}
  %broadcast.261 = f32[32,1024,14336]{2,1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="broadcast.83"}
  %broadcast_in_dim.162 = f32[32,14336,1024]{2,1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %broadcast.262 = f32[32,4096]{1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="broadcast.82"}
  %broadcast.263 = f32[32,1024,8,128]{3,2,1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="broadcast.81"}
  %broadcast_in_dim.163 = f32[32,32,128,1024]{3,2,1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %broadcast_in_dim.164 = f32[32,1024,32,128]{3,2,1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.320 = bf16[32,2,2048,4096]{3,2,1,0} get-tuple-element(%while.36), index=2, metadata={op_name="jit(train_step)/jvp(TransformerLinenPure.apply)/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %tuple.43 = (s32[], bf16[2,2048,4096]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,14336,1024]{2,1,0}, /*index=5*/f32[32,4096]{1,0}, f32[32,4096]{1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,32,128]{3,2,1,0}, /*index=10*/f32[32,1024,8,128]{3,2,1,0}, f32[32,4096]{0,1}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{0,1}, f32[32,1024,32,128]{3,2,0,1}, /*index=15*/s32[2,2048]{1,0}, f32[32,1024,8,128]{3,2,0,1}, f32[32,1024,8,128]{3,2,0,1}, f32[32,32,128,1024]{3,2,0,1}, f32[32,1024,14336]{2,0,1}, /*index=20*/f32[32,14336,1024]{2,0,1}, f32[32,1024,14336]{2,0,1}) tuple(%constant.593, %convert_element_type.363, %broadcast.261, %broadcast.261, %broadcast_in_dim.162, /*index=5*/%broadcast.262, %broadcast.262, %broadcast.263, %broadcast_in_dim.163, %broadcast_in_dim.164, /*index=10*/%broadcast.263, %transpose.138, %get-tuple-element.320, %transpose.139, %transpose.140, /*index=15*/%slice.22, %transpose.141, %transpose.142, %transpose.143, %transpose.144, /*index=20*/%transpose.146, %transpose.145), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %while.37 = (s32[], bf16[2,2048,4096]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,1024,14336]{2,1,0}, f32[32,14336,1024]{2,1,0}, /*index=5*/f32[32,4096]{1,0}, f32[32,4096]{1,0}, f32[32,1024,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[32,1024,32,128]{3,2,1,0}, /*index=10*/f32[32,1024,8,128]{3,2,1,0}, f32[32,4096]{0,1}, bf16[32,2,2048,4096]{3,2,1,0}, f32[32,4096]{0,1}, f32[32,1024,32,128]{3,2,0,1}, /*index=15*/s32[2,2048]{1,0}, f32[32,1024,8,128]{3,2,0,1}, f32[32,1024,8,128]{3,2,0,1}, f32[32,32,128,1024]{3,2,0,1}, f32[32,1024,14336]{2,0,1}, /*index=20*/f32[32,14336,1024]{2,0,1}, f32[32,1024,14336]{2,0,1}) while(%tuple.43), condition=%region_15.16_spmd, body=%region_10.15_spmd, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %get-tuple-element.321 = f32[32,1024,14336]{2,1,0} get-tuple-element(%while.37), index=2, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.148 = f32[1024,32,14336]{2,0,1} transpose(%get-tuple-element.321), dimensions={1,0,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.872 = f32[1024,32,14336]{2,0,1} multiply(%transpose.148, %transpose.148), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.12 = f32[] reduce(%mul.872, %constant.608), dimensions={0,1,2}, to_apply=%region_16.17, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.11 = f32[] all-reduce(%reduce.12), channel_id=33, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_16.17.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.413 = f32[] add(%reduce.10, %all-reduce.11), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.322 = f32[32,1024,14336]{2,1,0} get-tuple-element(%while.37), index=3, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.149 = f32[1024,32,14336]{2,0,1} transpose(%get-tuple-element.322), dimensions={1,0,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.873 = f32[1024,32,14336]{2,0,1} multiply(%transpose.149, %transpose.149), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.13 = f32[] reduce(%mul.873, %constant.608), dimensions={0,1,2}, to_apply=%region_17.18, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.12 = f32[] all-reduce(%reduce.13), channel_id=34, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_17.18.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.414 = f32[] add(%add.413, %all-reduce.12), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.323 = f32[32,14336,1024]{2,1,0} get-tuple-element(%while.37), index=4, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.150 = f32[14336,32,1024]{2,0,1} transpose(%get-tuple-element.323), dimensions={1,0,2}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.874 = f32[14336,32,1024]{2,0,1} multiply(%transpose.150, %transpose.150), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.14 = f32[] reduce(%mul.874, %constant.608), dimensions={0,1,2}, to_apply=%region_18.19, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.13 = f32[] all-reduce(%reduce.14), channel_id=35, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_18.19.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.415 = f32[] add(%add.414, %all-reduce.13), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.324 = f32[32,4096]{1,0} get-tuple-element(%while.37), index=5, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.151 = f32[4096,32]{0,1} transpose(%get-tuple-element.324), dimensions={1,0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.875 = f32[4096,32]{0,1} multiply(%transpose.151, %transpose.151), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.15 = f32[] reduce(%mul.875, %constant.608), dimensions={0,1}, to_apply=%region_19.20, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.416 = f32[] add(%add.415, %reduce.15), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.325 = f32[32,4096]{1,0} get-tuple-element(%while.37), index=6, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.152 = f32[4096,32]{0,1} transpose(%get-tuple-element.325), dimensions={1,0}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.876 = f32[4096,32]{0,1} multiply(%transpose.152, %transpose.152), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.16 = f32[] reduce(%mul.876, %constant.608), dimensions={0,1}, to_apply=%region_20.21, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.417 = f32[] add(%add.416, %reduce.16), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.326 = f32[32,1024,8,128]{3,2,1,0} get-tuple-element(%while.37), index=7, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.153 = f32[1024,32,8,128]{3,2,0,1} transpose(%get-tuple-element.326), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.877 = f32[1024,32,8,128]{3,2,0,1} multiply(%transpose.153, %transpose.153), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.17 = f32[] reduce(%mul.877, %constant.608), dimensions={0,1,2,3}, to_apply=%region_21.22, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.14 = f32[] all-reduce(%reduce.17), channel_id=36, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_21.22.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.418 = f32[] add(%add.417, %all-reduce.14), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.327 = f32[32,32,128,1024]{3,2,1,0} get-tuple-element(%while.37), index=8, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.154 = f32[32,32,128,1024]{3,2,0,1} transpose(%get-tuple-element.327), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.878 = f32[32,32,128,1024]{3,2,0,1} multiply(%transpose.154, %transpose.154), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.18 = f32[] reduce(%mul.878, %constant.608), dimensions={0,1,2,3}, to_apply=%region_22.23, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.15 = f32[] all-reduce(%reduce.18), channel_id=37, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_22.23.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.419 = f32[] add(%add.418, %all-reduce.15), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.328 = f32[32,1024,32,128]{3,2,1,0} get-tuple-element(%while.37), index=9, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.155 = f32[1024,32,32,128]{3,2,0,1} transpose(%get-tuple-element.328), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.879 = f32[1024,32,32,128]{3,2,0,1} multiply(%transpose.155, %transpose.155), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.19 = f32[] reduce(%mul.879, %constant.608), dimensions={0,1,2,3}, to_apply=%region_23.24, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.16 = f32[] all-reduce(%reduce.19), channel_id=38, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_23.24.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.420 = f32[] add(%add.419, %all-reduce.16), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.329 = f32[32,1024,8,128]{3,2,1,0} get-tuple-element(%while.37), index=10, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %transpose.156 = f32[1024,32,8,128]{3,2,0,1} transpose(%get-tuple-element.329), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=123 source_end_line=123 source_column=13 source_end_column=13}
  %mul.880 = f32[1024,32,8,128]{3,2,0,1} multiply(%transpose.156, %transpose.156), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.20 = f32[] reduce(%mul.880, %constant.608), dimensions={0,1,2,3}, to_apply=%region_24.25, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.17 = f32[] all-reduce(%reduce.20), channel_id=39, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_24.25.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.421 = f32[] add(%add.420, %all-reduce.17), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %dot.38 = bf16[4096,128256]{1,0} dot(%dot_general.137, %convert_element_type.359), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/transpose" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %all-reduce.18 = bf16[4096,128256]{1,0} all-reduce(%dot.38), channel_id=40, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.10.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/transpose" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %constant.638 = s32[4]{0} constant({0, 1024, 2048, 3072}), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/transpose" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dynamic-slice.51 = s32[1]{0} dynamic-slice(%constant.638, %partition-id.4), dynamic_slice_sizes={1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/transpose" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %reshape.332 = s32[] reshape(%dynamic-slice.51), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/transpose" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %dynamic-slice.52 = bf16[1024,128256]{1,0} dynamic-slice(%all-reduce.18, %reshape.332, %constant.593), dynamic_slice_sizes={1024,128256}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/transpose" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=99 source_end_line=99 source_column=9 source_end_column=9}
  %convert_element_type.364 = f32[1024,128256]{1,0} convert(%dynamic-slice.52), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_output_head/logits_dense/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/linears.py" source_line=227 source_end_line=227 source_column=15 source_end_column=15}
  %mul.881 = f32[1024,128256]{1,0} multiply(%convert_element_type.364, %convert_element_type.364), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.21 = f32[] reduce(%mul.881, %constant.608), dimensions={0,1}, to_apply=%region_25.26, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.19 = f32[] all-reduce(%reduce.21), channel_id=41, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_25.26.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.422 = f32[] add(%add.421, %all-reduce.19), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %get-tuple-element.330 = bf16[2,2048,4096]{2,1,0} get-tuple-element(%while.37), index=1, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=199 source_end_line=199 source_column=14 source_end_column=14}
  %dot.39 = bf16[128256,4096]{1,0} dot(%convert_element_type.344, %get-tuple-element.330), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/transpose" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=167 source_end_line=167 source_column=15 source_end_column=15}
  %all-reduce.20 = bf16[128256,4096]{1,0} all-reduce(%dot.39), channel_id=42, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.11.clone, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/transpose" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=167 source_end_line=167 source_column=15 source_end_column=15}
  %dynamic-slice.54 = bf16[128256,1024]{1,0} dynamic-slice(%all-reduce.20, %constant.593, %reshape.332), dynamic_slice_sizes={128256,1024}, metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/transpose" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=167 source_end_line=167 source_column=15 source_end_column=15}
  %convert_element_type.365 = f32[128256,1024]{1,0} convert(%dynamic-slice.54), metadata={op_name="jit(train_step)/transpose(jvp(TransformerLinenPure.apply))/TransformerLinenPure/decoder/decoder._apply_embedding/token_embedder/convert_element_type" source_file="/opt/maxtext/src/MaxText/layers/embeddings.py" source_line=150 source_end_line=150 source_column=16 source_end_column=16}
  %mul.882 = f32[128256,1024]{1,0} multiply(%convert_element_type.365, %convert_element_type.365), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45 source_end_line=45 source_column=10 source_end_column=10}
  %reduce.22 = f32[] reduce(%mul.882, %constant.608), dimensions={0,1}, to_apply=%region_26.27, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %all-reduce.21 = f32[] all-reduce(%reduce.22), channel_id=43, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_26.27.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=10 source_end_column=10}
  %add.423 = f32[] add(%add.422, %all-reduce.21), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38 source_end_line=38 source_column=6 source_end_column=6}
  %sqrt.32 = f32[] sqrt(%add.423), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=37 source_end_line=37 source_column=9 source_end_column=9}
  %lt.87 = pred[] compare(%sqrt.32, %constant.592), direction=LT, metadata={op_name="jit(train_step)/lt" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=98 source_end_line=98 source_column=26 source_end_column=26}
  %select_n.133 = pred[4096]{0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.295 = f32[4096]{0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.296 = f32[4096]{0} divide(%convert_element_type.360, %div.295), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.134 = f32[4096]{0} select(%select_n.133, %convert_element_type.360, %div.296), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %constant.644 = f32[] constant(0.1)
  %broadcast.264 = f32[4096]{0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.80"}
  %mul.883 = f32[4096]{0} multiply(%select_n.134, %broadcast.264), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.22 = f32[4096]{0} parameter(14), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'decoder_norm\'][\'scale\']"}
  %constant.645 = f32[] constant(0.9)
  %mul.884 = f32[4096]{0} broadcast(%constant.645), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %mul.885 = f32[4096]{0} multiply(%param.22, %mul.884), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.424 = f32[4096]{0} add(%mul.883, %mul.885), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.23 = s32[] parameter(13), sharding={replicated}, metadata={op_name="state.opt_state[0].count"}
  %constant.646 = s32[] constant(2147483647)
  %lt.88 = pred[] compare(%param.23, %constant.646), direction=LT, metadata={op_name="jit(train_step)/lt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142 source_end_line=142 source_column=19 source_end_column=19}
  %add.425 = s32[] add(%param.23, %constant.590), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142 source_end_line=142 source_column=38 source_end_column=38}
  %select_n.135 = s32[] select(%lt.88, %add.425, %constant.646), metadata={op_name="jit(train_step)/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142 source_end_line=142 source_column=9 source_end_column=9}
  %pow.22 = f32[] convert(%select_n.135), metadata={op_name="jit(train_step)/pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %pow.23 = f32[] power(%constant.645, %pow.22), metadata={op_name="jit(train_step)/pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %sub.116 = f32[] subtract(%constant.592, %pow.23), metadata={op_name="jit(train_step)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %div.297 = f32[4096]{0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.24 = f32[4096]{0} multiply(%select_n.134, %select_n.134), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %constant.647 = f32[] constant(0.05)
  %mul.886 = f32[4096]{0} broadcast(%constant.647), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %mul.887 = f32[4096]{0} multiply(%integer_pow.24, %mul.886), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.24 = f32[4096]{0} parameter(26), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'decoder_norm\'][\'scale\']"}
  %constant.648 = f32[] constant(0.95)
  %mul.888 = f32[4096]{0} broadcast(%constant.648), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %mul.889 = f32[4096]{0} multiply(%param.24, %mul.888), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.426 = f32[4096]{0} add(%mul.887, %mul.889), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %pow.25 = f32[] power(%constant.648, %pow.22), metadata={op_name="jit(train_step)/pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sub.117 = f32[] subtract(%constant.592, %pow.25), metadata={op_name="jit(train_step)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.298 = f32[4096]{0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.299 = f32[4096]{0} divide(%add.426, %div.298), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.33 = f32[4096]{0} sqrt(%div.299), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.427 = f32[4096]{0} broadcast(%constant.607), dimensions={}, metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.428 = f32[4096]{0} add(%sqrt.33, %add.427), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.91 = f32[4096]{0} multiply(%div.297, %add.428), metadata={op_name="multiply.32"}
  %div.300 = f32[4096]{0} divide(%add.424, %multiply.91), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.890 = f32[4096]{0} multiply(%param.5, %broadcast.264), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.429 = f32[4096]{0} add(%div.300, %mul.890), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.891 = f32[4096]{0} multiply(%mul.847, %add.429), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.430 = f32[4096]{0} add(%param.5, %mul.891), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %mul.892 = f32[1024,32,14336]{2,1,0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %select_n.136 = pred[1024,32,14336]{2,1,0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.301 = f32[1024,32,14336]{2,1,0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.302 = f32[1024,32,14336]{2,0,1} divide(%transpose.148, %div.301), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.137 = f32[1024,32,14336]{2,1,0} select(%select_n.136, %transpose.148, %div.302), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %broadcast.265 = f32[1024,32,14336]{2,1,0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.79"}
  %mul.893 = f32[1024,32,14336]{2,1,0} multiply(%select_n.137, %broadcast.265), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.25 = f32[1024,32,14336]{2,1,0} parameter(15), sharding={devices=[4,1,1]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_0\'][\'kernel\']"}
  %broadcast.266 = f32[1024,32,14336]{2,1,0} broadcast(%constant.645), dimensions={}, metadata={op_name="broadcast.78"}
  %mul.894 = f32[1024,32,14336]{2,1,0} multiply(%param.25, %broadcast.266), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.431 = f32[1024,32,14336]{2,1,0} add(%mul.893, %mul.894), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %div.303 = f32[1024,32,14336]{2,1,0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.25 = f32[1024,32,14336]{2,1,0} multiply(%select_n.137, %select_n.137), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %broadcast.267 = f32[1024,32,14336]{2,1,0} broadcast(%constant.647), dimensions={}, metadata={op_name="broadcast.68"}
  %mul.895 = f32[1024,32,14336]{2,1,0} multiply(%integer_pow.25, %broadcast.267), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.26 = f32[1024,32,14336]{2,1,0} parameter(27), sharding={devices=[4,1,1]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_0\'][\'kernel\']"}
  %broadcast.268 = f32[1024,32,14336]{2,1,0} broadcast(%constant.648), dimensions={}, metadata={op_name="broadcast.67"}
  %mul.896 = f32[1024,32,14336]{2,1,0} multiply(%param.26, %broadcast.268), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.432 = f32[1024,32,14336]{2,1,0} add(%mul.895, %mul.896), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.304 = f32[1024,32,14336]{2,1,0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.305 = f32[1024,32,14336]{2,1,0} divide(%add.432, %div.304), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.34 = f32[1024,32,14336]{2,1,0} sqrt(%div.305), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %broadcast.269 = f32[1024,32,14336]{2,1,0} broadcast(%constant.607), dimensions={}, metadata={op_name="broadcast.62"}
  %add.433 = f32[1024,32,14336]{2,1,0} add(%sqrt.34, %broadcast.269), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.92 = f32[1024,32,14336]{2,1,0} multiply(%div.303, %add.433), metadata={op_name="multiply.33"}
  %div.306 = f32[1024,32,14336]{2,1,0} divide(%add.431, %multiply.92), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.897 = f32[1024,32,14336]{2,1,0} multiply(%param.17, %broadcast.265), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.434 = f32[1024,32,14336]{2,1,0} add(%div.306, %mul.897), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.898 = f32[1024,32,14336]{2,1,0} multiply(%mul.892, %add.434), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.435 = f32[1024,32,14336]{2,1,0} add(%param.17, %mul.898), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %div.307 = f32[1024,32,14336]{2,0,1} divide(%transpose.149, %div.301), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.138 = f32[1024,32,14336]{2,1,0} select(%select_n.136, %transpose.149, %div.307), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %mul.899 = f32[1024,32,14336]{2,1,0} multiply(%select_n.138, %broadcast.265), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.27 = f32[1024,32,14336]{2,1,0} parameter(16), sharding={devices=[4,1,1]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_1\'][\'kernel\']"}
  %mul.900 = f32[1024,32,14336]{2,1,0} multiply(%param.27, %broadcast.266), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.436 = f32[1024,32,14336]{2,1,0} add(%mul.899, %mul.900), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %integer_pow.26 = f32[1024,32,14336]{2,1,0} multiply(%select_n.138, %select_n.138), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %mul.901 = f32[1024,32,14336]{2,1,0} multiply(%integer_pow.26, %broadcast.267), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.28 = f32[1024,32,14336]{2,1,0} parameter(28), sharding={devices=[4,1,1]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_1\'][\'kernel\']"}
  %mul.902 = f32[1024,32,14336]{2,1,0} multiply(%param.28, %broadcast.268), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.437 = f32[1024,32,14336]{2,1,0} add(%mul.901, %mul.902), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.308 = f32[1024,32,14336]{2,1,0} divide(%add.437, %div.304), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.35 = f32[1024,32,14336]{2,1,0} sqrt(%div.308), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.438 = f32[1024,32,14336]{2,1,0} add(%sqrt.35, %broadcast.269), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.93 = f32[1024,32,14336]{2,1,0} multiply(%div.303, %add.438), metadata={op_name="multiply.34"}
  %div.309 = f32[1024,32,14336]{2,1,0} divide(%add.436, %multiply.93), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.903 = f32[1024,32,14336]{2,1,0} multiply(%param.18, %broadcast.265), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.439 = f32[1024,32,14336]{2,1,0} add(%div.309, %mul.903), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.904 = f32[1024,32,14336]{2,1,0} multiply(%mul.892, %add.439), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.440 = f32[1024,32,14336]{2,1,0} add(%param.18, %mul.904), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %mul.905 = f32[14336,32,1024]{2,1,0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %select_n.139 = pred[14336,32,1024]{2,1,0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.310 = f32[14336,32,1024]{2,1,0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.311 = f32[14336,32,1024]{2,0,1} divide(%transpose.150, %div.310), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.140 = f32[14336,32,1024]{2,1,0} select(%select_n.139, %transpose.150, %div.311), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %broadcast.270 = f32[14336,32,1024]{2,1,0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.77"}
  %mul.906 = f32[14336,32,1024]{2,1,0} multiply(%select_n.140, %broadcast.270), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.29 = f32[14336,32,1024]{2,1,0} parameter(17), sharding={devices=[1,1,4]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wo\'][\'kernel\']"}
  %mul.907 = f32[14336,32,1024]{2,1,0} broadcast(%constant.645), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %mul.908 = f32[14336,32,1024]{2,1,0} multiply(%param.29, %mul.907), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.441 = f32[14336,32,1024]{2,1,0} add(%mul.906, %mul.908), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %div.312 = f32[14336,32,1024]{2,1,0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.27 = f32[14336,32,1024]{2,1,0} multiply(%select_n.140, %select_n.140), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %mul.909 = f32[14336,32,1024]{2,1,0} broadcast(%constant.647), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %mul.910 = f32[14336,32,1024]{2,1,0} multiply(%integer_pow.27, %mul.909), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.30 = f32[14336,32,1024]{2,1,0} parameter(29), sharding={devices=[1,1,4]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wo\'][\'kernel\']"}
  %mul.911 = f32[14336,32,1024]{2,1,0} broadcast(%constant.648), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %mul.912 = f32[14336,32,1024]{2,1,0} multiply(%param.30, %mul.911), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.442 = f32[14336,32,1024]{2,1,0} add(%mul.910, %mul.912), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.313 = f32[14336,32,1024]{2,1,0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.314 = f32[14336,32,1024]{2,1,0} divide(%add.442, %div.313), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.36 = f32[14336,32,1024]{2,1,0} sqrt(%div.314), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.443 = f32[14336,32,1024]{2,1,0} broadcast(%constant.607), dimensions={}, metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.444 = f32[14336,32,1024]{2,1,0} add(%sqrt.36, %add.443), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.94 = f32[14336,32,1024]{2,1,0} multiply(%div.312, %add.444), metadata={op_name="multiply.35"}
  %div.315 = f32[14336,32,1024]{2,1,0} divide(%add.441, %multiply.94), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.913 = f32[14336,32,1024]{2,1,0} multiply(%param.19, %broadcast.270), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.445 = f32[14336,32,1024]{2,1,0} add(%div.315, %mul.913), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.914 = f32[14336,32,1024]{2,1,0} multiply(%mul.905, %add.445), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.446 = f32[14336,32,1024]{2,1,0} add(%param.19, %mul.914), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %mul.915 = f32[4096,32]{1,0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %select_n.141 = pred[4096,32]{1,0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.316 = f32[4096,32]{1,0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.317 = f32[4096,32]{0,1} divide(%transpose.151, %div.316), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.142 = f32[4096,32]{1,0} select(%select_n.141, %transpose.151, %div.317), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %broadcast.271 = f32[4096,32]{1,0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.76"}
  %mul.916 = f32[4096,32]{1,0} multiply(%select_n.142, %broadcast.271), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.31 = f32[4096,32]{1,0} parameter(18), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'post_self_attention_layer_norm\'][\'scale\']"}
  %broadcast.272 = f32[4096,32]{1,0} broadcast(%constant.645), dimensions={}, metadata={op_name="broadcast.75"}
  %mul.917 = f32[4096,32]{1,0} multiply(%param.31, %broadcast.272), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.447 = f32[4096,32]{1,0} add(%mul.916, %mul.917), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %div.318 = f32[4096,32]{1,0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.28 = f32[4096,32]{1,0} multiply(%select_n.142, %select_n.142), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %broadcast.273 = f32[4096,32]{1,0} broadcast(%constant.647), dimensions={}, metadata={op_name="broadcast.66"}
  %mul.918 = f32[4096,32]{1,0} multiply(%integer_pow.28, %broadcast.273), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.32 = f32[4096,32]{1,0} parameter(30), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'post_self_attention_layer_norm\'][\'scale\']"}
  %broadcast.274 = f32[4096,32]{1,0} broadcast(%constant.648), dimensions={}, metadata={op_name="broadcast.65"}
  %mul.919 = f32[4096,32]{1,0} multiply(%param.32, %broadcast.274), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.448 = f32[4096,32]{1,0} add(%mul.918, %mul.919), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.319 = f32[4096,32]{1,0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.320 = f32[4096,32]{1,0} divide(%add.448, %div.319), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.37 = f32[4096,32]{1,0} sqrt(%div.320), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %broadcast.275 = f32[4096,32]{1,0} broadcast(%constant.607), dimensions={}, metadata={op_name="broadcast.61"}
  %add.449 = f32[4096,32]{1,0} add(%sqrt.37, %broadcast.275), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.95 = f32[4096,32]{1,0} multiply(%div.318, %add.449), metadata={op_name="multiply.36"}
  %div.321 = f32[4096,32]{1,0} divide(%add.447, %multiply.95), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.920 = f32[4096,32]{1,0} multiply(%param.10, %broadcast.271), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.450 = f32[4096,32]{1,0} add(%div.321, %mul.920), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.921 = f32[4096,32]{1,0} multiply(%mul.915, %add.450), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.451 = f32[4096,32]{1,0} add(%param.10, %mul.921), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %div.322 = f32[4096,32]{0,1} divide(%transpose.152, %div.316), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.143 = f32[4096,32]{1,0} select(%select_n.141, %transpose.152, %div.322), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %mul.922 = f32[4096,32]{1,0} multiply(%select_n.143, %broadcast.271), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.33 = f32[4096,32]{1,0} parameter(19), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'pre_self_attention_layer_norm\'][\'scale\']"}
  %mul.923 = f32[4096,32]{1,0} multiply(%param.33, %broadcast.272), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.452 = f32[4096,32]{1,0} add(%mul.922, %mul.923), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %integer_pow.29 = f32[4096,32]{1,0} multiply(%select_n.143, %select_n.143), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %mul.924 = f32[4096,32]{1,0} multiply(%integer_pow.29, %broadcast.273), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.34 = f32[4096,32]{1,0} parameter(31), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'pre_self_attention_layer_norm\'][\'scale\']"}
  %mul.925 = f32[4096,32]{1,0} multiply(%param.34, %broadcast.274), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.453 = f32[4096,32]{1,0} add(%mul.924, %mul.925), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.323 = f32[4096,32]{1,0} divide(%add.453, %div.319), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.38 = f32[4096,32]{1,0} sqrt(%div.323), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.454 = f32[4096,32]{1,0} add(%sqrt.38, %broadcast.275), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.96 = f32[4096,32]{1,0} multiply(%div.318, %add.454), metadata={op_name="multiply.37"}
  %div.324 = f32[4096,32]{1,0} divide(%add.452, %multiply.96), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.926 = f32[4096,32]{1,0} multiply(%param.11, %broadcast.271), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.455 = f32[4096,32]{1,0} add(%div.324, %mul.926), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.927 = f32[4096,32]{1,0} multiply(%mul.915, %add.455), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.456 = f32[4096,32]{1,0} add(%param.11, %mul.927), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %mul.928 = f32[1024,32,8,128]{3,2,1,0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %select_n.144 = pred[1024,32,8,128]{3,2,1,0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.325 = f32[1024,32,8,128]{3,2,1,0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.326 = f32[1024,32,8,128]{3,2,0,1} divide(%transpose.153, %div.325), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.145 = f32[1024,32,8,128]{3,2,1,0} select(%select_n.144, %transpose.153, %div.326), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %broadcast.276 = f32[1024,32,8,128]{3,2,1,0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.74"}
  %mul.929 = f32[1024,32,8,128]{3,2,1,0} multiply(%select_n.145, %broadcast.276), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.35 = f32[1024,32,8,128]{3,2,1,0} parameter(20), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'key\'][\'kernel\']"}
  %broadcast.277 = f32[1024,32,8,128]{3,2,1,0} broadcast(%constant.645), dimensions={}, metadata={op_name="broadcast.73"}
  %mul.930 = f32[1024,32,8,128]{3,2,1,0} multiply(%param.35, %broadcast.277), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.457 = f32[1024,32,8,128]{3,2,1,0} add(%mul.929, %mul.930), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %div.327 = f32[1024,32,8,128]{3,2,1,0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.30 = f32[1024,32,8,128]{3,2,1,0} multiply(%select_n.145, %select_n.145), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %broadcast.278 = f32[1024,32,8,128]{3,2,1,0} broadcast(%constant.647), dimensions={}, metadata={op_name="broadcast.64"}
  %mul.931 = f32[1024,32,8,128]{3,2,1,0} multiply(%integer_pow.30, %broadcast.278), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.36 = f32[1024,32,8,128]{3,2,1,0} parameter(32), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'key\'][\'kernel\']"}
  %broadcast.279 = f32[1024,32,8,128]{3,2,1,0} broadcast(%constant.648), dimensions={}, metadata={op_name="broadcast.63"}
  %mul.932 = f32[1024,32,8,128]{3,2,1,0} multiply(%param.36, %broadcast.279), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.458 = f32[1024,32,8,128]{3,2,1,0} add(%mul.931, %mul.932), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.328 = f32[1024,32,8,128]{3,2,1,0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.329 = f32[1024,32,8,128]{3,2,1,0} divide(%add.458, %div.328), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.39 = f32[1024,32,8,128]{3,2,1,0} sqrt(%div.329), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %broadcast.280 = f32[1024,32,8,128]{3,2,1,0} broadcast(%constant.607), dimensions={}, metadata={op_name="broadcast.60"}
  %add.459 = f32[1024,32,8,128]{3,2,1,0} add(%sqrt.39, %broadcast.280), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.97 = f32[1024,32,8,128]{3,2,1,0} multiply(%div.327, %add.459), metadata={op_name="multiply.38"}
  %div.330 = f32[1024,32,8,128]{3,2,1,0} divide(%add.457, %multiply.97), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.933 = f32[1024,32,8,128]{3,2,1,0} multiply(%param.14, %broadcast.276), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.460 = f32[1024,32,8,128]{3,2,1,0} add(%div.330, %mul.933), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.934 = f32[1024,32,8,128]{3,2,1,0} multiply(%mul.928, %add.460), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.461 = f32[1024,32,8,128]{3,2,1,0} add(%param.14, %mul.934), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %mul.935 = f32[32,32,128,1024]{3,2,1,0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %select_n.146 = pred[32,32,128,1024]{3,2,1,0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.331 = f32[32,32,128,1024]{3,2,1,0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.332 = f32[32,32,128,1024]{3,2,0,1} divide(%transpose.154, %div.331), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.147 = f32[32,32,128,1024]{3,2,1,0} select(%select_n.146, %transpose.154, %div.332), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %broadcast.281 = f32[32,32,128,1024]{3,2,1,0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.72"}
  %mul.936 = f32[32,32,128,1024]{3,2,1,0} multiply(%select_n.147, %broadcast.281), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.37 = f32[32,32,128,1024]{3,2,1,0} parameter(21), sharding={devices=[1,1,1,4]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'out\'][\'kernel\']"}
  %mul.937 = f32[32,32,128,1024]{3,2,1,0} broadcast(%constant.645), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %mul.938 = f32[32,32,128,1024]{3,2,1,0} multiply(%param.37, %mul.937), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.462 = f32[32,32,128,1024]{3,2,1,0} add(%mul.936, %mul.938), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %div.333 = f32[32,32,128,1024]{3,2,1,0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.31 = f32[32,32,128,1024]{3,2,1,0} multiply(%select_n.147, %select_n.147), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %mul.939 = f32[32,32,128,1024]{3,2,1,0} broadcast(%constant.647), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %mul.940 = f32[32,32,128,1024]{3,2,1,0} multiply(%integer_pow.31, %mul.939), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.38 = f32[32,32,128,1024]{3,2,1,0} parameter(33), sharding={devices=[1,1,1,4]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'out\'][\'kernel\']"}
  %mul.941 = f32[32,32,128,1024]{3,2,1,0} broadcast(%constant.648), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %mul.942 = f32[32,32,128,1024]{3,2,1,0} multiply(%param.38, %mul.941), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.463 = f32[32,32,128,1024]{3,2,1,0} add(%mul.940, %mul.942), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.334 = f32[32,32,128,1024]{3,2,1,0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.335 = f32[32,32,128,1024]{3,2,1,0} divide(%add.463, %div.334), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.40 = f32[32,32,128,1024]{3,2,1,0} sqrt(%div.335), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.464 = f32[32,32,128,1024]{3,2,1,0} broadcast(%constant.607), dimensions={}, metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.465 = f32[32,32,128,1024]{3,2,1,0} add(%sqrt.40, %add.464), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.98 = f32[32,32,128,1024]{3,2,1,0} multiply(%div.333, %add.465), metadata={op_name="multiply.39"}
  %div.336 = f32[32,32,128,1024]{3,2,1,0} divide(%add.462, %multiply.98), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.943 = f32[32,32,128,1024]{3,2,1,0} multiply(%param.16, %broadcast.281), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.466 = f32[32,32,128,1024]{3,2,1,0} add(%div.336, %mul.943), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.944 = f32[32,32,128,1024]{3,2,1,0} multiply(%mul.935, %add.466), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.467 = f32[32,32,128,1024]{3,2,1,0} add(%param.16, %mul.944), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %mul.945 = f32[1024,32,32,128]{3,2,1,0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %select_n.148 = pred[1024,32,32,128]{3,2,1,0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.337 = f32[1024,32,32,128]{3,2,1,0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.338 = f32[1024,32,32,128]{3,2,0,1} divide(%transpose.155, %div.337), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.149 = f32[1024,32,32,128]{3,2,1,0} select(%select_n.148, %transpose.155, %div.338), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %broadcast.282 = f32[1024,32,32,128]{3,2,1,0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.71"}
  %mul.946 = f32[1024,32,32,128]{3,2,1,0} multiply(%select_n.149, %broadcast.282), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.39 = f32[1024,32,32,128]{3,2,1,0} parameter(22), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'query\'][\'kernel\']"}
  %mul.947 = f32[1024,32,32,128]{3,2,1,0} broadcast(%constant.645), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %mul.948 = f32[1024,32,32,128]{3,2,1,0} multiply(%param.39, %mul.947), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.468 = f32[1024,32,32,128]{3,2,1,0} add(%mul.946, %mul.948), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %div.339 = f32[1024,32,32,128]{3,2,1,0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.32 = f32[1024,32,32,128]{3,2,1,0} multiply(%select_n.149, %select_n.149), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %mul.949 = f32[1024,32,32,128]{3,2,1,0} broadcast(%constant.647), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %mul.950 = f32[1024,32,32,128]{3,2,1,0} multiply(%integer_pow.32, %mul.949), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.40 = f32[1024,32,32,128]{3,2,1,0} parameter(34), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'query\'][\'kernel\']"}
  %mul.951 = f32[1024,32,32,128]{3,2,1,0} broadcast(%constant.648), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %mul.952 = f32[1024,32,32,128]{3,2,1,0} multiply(%param.40, %mul.951), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.469 = f32[1024,32,32,128]{3,2,1,0} add(%mul.950, %mul.952), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.340 = f32[1024,32,32,128]{3,2,1,0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.341 = f32[1024,32,32,128]{3,2,1,0} divide(%add.469, %div.340), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.41 = f32[1024,32,32,128]{3,2,1,0} sqrt(%div.341), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.470 = f32[1024,32,32,128]{3,2,1,0} broadcast(%constant.607), dimensions={}, metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.471 = f32[1024,32,32,128]{3,2,1,0} add(%sqrt.41, %add.470), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.99 = f32[1024,32,32,128]{3,2,1,0} multiply(%div.339, %add.471), metadata={op_name="multiply.40"}
  %div.342 = f32[1024,32,32,128]{3,2,1,0} divide(%add.468, %multiply.99), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.953 = f32[1024,32,32,128]{3,2,1,0} multiply(%param.12, %broadcast.282), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.472 = f32[1024,32,32,128]{3,2,1,0} add(%div.342, %mul.953), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.954 = f32[1024,32,32,128]{3,2,1,0} multiply(%mul.945, %add.472), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.473 = f32[1024,32,32,128]{3,2,1,0} add(%param.12, %mul.954), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %div.343 = f32[1024,32,8,128]{3,2,0,1} divide(%transpose.156, %div.325), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.150 = f32[1024,32,8,128]{3,2,1,0} select(%select_n.144, %transpose.156, %div.343), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %mul.955 = f32[1024,32,8,128]{3,2,1,0} multiply(%select_n.150, %broadcast.276), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.41 = f32[1024,32,8,128]{3,2,1,0} parameter(23), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'value\'][\'kernel\']"}
  %mul.956 = f32[1024,32,8,128]{3,2,1,0} multiply(%param.41, %broadcast.277), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.474 = f32[1024,32,8,128]{3,2,1,0} add(%mul.955, %mul.956), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %integer_pow.33 = f32[1024,32,8,128]{3,2,1,0} multiply(%select_n.150, %select_n.150), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %mul.957 = f32[1024,32,8,128]{3,2,1,0} multiply(%integer_pow.33, %broadcast.278), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.42 = f32[1024,32,8,128]{3,2,1,0} parameter(35), sharding={devices=[4,1,1,1]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'value\'][\'kernel\']"}
  %mul.958 = f32[1024,32,8,128]{3,2,1,0} multiply(%param.42, %broadcast.279), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.475 = f32[1024,32,8,128]{3,2,1,0} add(%mul.957, %mul.958), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.344 = f32[1024,32,8,128]{3,2,1,0} divide(%add.475, %div.328), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.42 = f32[1024,32,8,128]{3,2,1,0} sqrt(%div.344), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.476 = f32[1024,32,8,128]{3,2,1,0} add(%sqrt.42, %broadcast.280), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.100 = f32[1024,32,8,128]{3,2,1,0} multiply(%div.327, %add.476), metadata={op_name="multiply.41"}
  %div.345 = f32[1024,32,8,128]{3,2,1,0} divide(%add.474, %multiply.100), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.959 = f32[1024,32,8,128]{3,2,1,0} multiply(%param.15, %broadcast.276), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.477 = f32[1024,32,8,128]{3,2,1,0} add(%div.345, %mul.959), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.960 = f32[1024,32,8,128]{3,2,1,0} multiply(%mul.928, %add.477), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.478 = f32[1024,32,8,128]{3,2,1,0} add(%param.15, %mul.960), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %mul.961 = f32[1024,128256]{1,0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %select_n.151 = pred[1024,128256]{1,0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.346 = f32[1024,128256]{1,0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.347 = f32[1024,128256]{1,0} divide(%convert_element_type.364, %div.346), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.152 = f32[1024,128256]{1,0} select(%select_n.151, %convert_element_type.364, %div.347), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %broadcast.283 = f32[1024,128256]{1,0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.70"}
  %mul.962 = f32[1024,128256]{1,0} multiply(%select_n.152, %broadcast.283), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.43 = f32[1024,128256]{1,0} parameter(24), sharding={devices=[4,1]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'logits_dense\'][\'kernel\']"}
  %mul.963 = f32[1024,128256]{1,0} broadcast(%constant.645), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %mul.964 = f32[1024,128256]{1,0} multiply(%param.43, %mul.963), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.479 = f32[1024,128256]{1,0} add(%mul.962, %mul.964), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %div.348 = f32[1024,128256]{1,0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.34 = f32[1024,128256]{1,0} multiply(%select_n.152, %select_n.152), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %mul.965 = f32[1024,128256]{1,0} broadcast(%constant.647), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %mul.966 = f32[1024,128256]{1,0} multiply(%integer_pow.34, %mul.965), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.44 = f32[1024,128256]{1,0} parameter(36), sharding={devices=[4,1]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'logits_dense\'][\'kernel\']"}
  %mul.967 = f32[1024,128256]{1,0} broadcast(%constant.648), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %mul.968 = f32[1024,128256]{1,0} multiply(%param.44, %mul.967), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.480 = f32[1024,128256]{1,0} add(%mul.966, %mul.968), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.349 = f32[1024,128256]{1,0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.350 = f32[1024,128256]{1,0} divide(%add.480, %div.349), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.43 = f32[1024,128256]{1,0} sqrt(%div.350), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.481 = f32[1024,128256]{1,0} broadcast(%constant.607), dimensions={}, metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.482 = f32[1024,128256]{1,0} add(%sqrt.43, %add.481), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.101 = f32[1024,128256]{1,0} multiply(%div.348, %add.482), metadata={op_name="multiply.42"}
  %div.351 = f32[1024,128256]{1,0} divide(%add.479, %multiply.101), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.969 = f32[1024,128256]{1,0} multiply(%param.20, %broadcast.283), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.483 = f32[1024,128256]{1,0} add(%div.351, %mul.969), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.970 = f32[1024,128256]{1,0} multiply(%mul.961, %add.483), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.484 = f32[1024,128256]{1,0} add(%param.20, %mul.970), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %mul.971 = f32[128256,1024]{1,0} broadcast(%mul.846), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %select_n.153 = pred[128256,1024]{1,0} broadcast(%lt.87), dimensions={}, metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %div.352 = f32[128256,1024]{1,0} broadcast(%sqrt.32), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %div.353 = f32[128256,1024]{1,0} divide(%convert_element_type.365, %div.352), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=41 source_end_column=41}
  %select_n.154 = f32[128256,1024]{1,0} select(%select_n.153, %convert_element_type.365, %div.353), metadata={op_name="jit(train_step)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=102 source_end_line=102 source_column=13 source_end_column=13}
  %broadcast.284 = f32[128256,1024]{1,0} broadcast(%constant.644), dimensions={}, metadata={op_name="broadcast.69"}
  %mul.972 = f32[128256,1024]{1,0} multiply(%select_n.154, %broadcast.284), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %param.45 = f32[128256,1024]{1,0} parameter(25), sharding={devices=[1,4]<=[4]}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'token_embedder\'][\'embedding\']"}
  %mul.973 = f32[128256,1024]{1,0} broadcast(%constant.645), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %mul.974 = f32[128256,1024]{1,0} multiply(%param.45, %mul.973), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=37 source_end_column=37}
  %add.485 = f32[128256,1024]{1,0} add(%mul.972, %mul.974), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=357 source_end_line=357 source_column=10 source_end_column=10}
  %div.354 = f32[128256,1024]{1,0} broadcast(%sub.116), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294 source_end_line=294 source_column=15 source_end_column=15}
  %integer_pow.35 = f32[128256,1024]{1,0} multiply(%select_n.154, %select_n.154), metadata={op_name="jit(train_step)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=382 source_end_line=382 source_column=13 source_end_column=13}
  %mul.975 = f32[128256,1024]{1,0} broadcast(%constant.647), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %mul.976 = f32[128256,1024]{1,0} multiply(%integer_pow.35, %mul.975), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %param.46 = f32[128256,1024]{1,0} parameter(37), sharding={devices=[1,4]<=[4]}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'token_embedder\'][\'embedding\']"}
  %mul.977 = f32[128256,1024]{1,0} broadcast(%constant.648), dimensions={}, metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %mul.978 = f32[128256,1024]{1,0} multiply(%param.46, %mul.977), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=42 source_end_column=42}
  %add.486 = f32[128256,1024]{1,0} add(%mul.976, %mul.978), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=392 source_end_line=392 source_column=10 source_end_column=10}
  %div.355 = f32[128256,1024]{1,0} broadcast(%sub.117), dimensions={}, metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %div.356 = f32[128256,1024]{1,0} divide(%add.486, %div.355), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298 source_end_line=298 source_column=13 source_end_column=13}
  %sqrt.44 = f32[128256,1024]{1,0} sqrt(%div.356), metadata={op_name="jit(train_step)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.487 = f32[128256,1024]{1,0} broadcast(%constant.607), dimensions={}, metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %add.488 = f32[128256,1024]{1,0} add(%sqrt.44, %add.487), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=49 source_end_column=49}
  %multiply.102 = f32[128256,1024]{1,0} multiply(%div.354, %add.488), metadata={op_name="multiply.43"}
  %div.357 = f32[128256,1024]{1,0} divide(%add.485, %multiply.102), metadata={op_name="jit(train_step)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300 source_end_line=300 source_column=44 source_end_column=44}
  %mul.979 = f32[128256,1024]{1,0} multiply(%param.9, %broadcast.284), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=48 source_end_column=48}
  %add.489 = f32[128256,1024]{1,0} add(%div.357, %mul.979), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=65 source_end_line=65 source_column=44 source_end_column=44}
  %mul.980 = f32[128256,1024]{1,0} multiply(%mul.971, %add.489), metadata={op_name="jit(train_step)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=989 source_end_line=989 source_column=18 source_end_column=18}
  %add.490 = f32[128256,1024]{1,0} add(%param.9, %mul.980), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43 source_end_line=43 source_column=45 source_end_column=45}
  %lt.89 = pred[] compare(%param.6, %constant.646), direction=LT, metadata={op_name="jit(train_step)/lt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142 source_end_line=142 source_column=19 source_end_column=19}
  %add.491 = s32[] add(%param.6, %constant.590), metadata={op_name="jit(train_step)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142 source_end_line=142 source_column=38 source_end_column=38}
  %select_n.155 = s32[] select(%lt.89, %add.491, %constant.646), metadata={op_name="jit(train_step)/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142 source_end_line=142 source_column=9 source_end_column=9}
  %reduce.23 = f32[] reduce(%integer_pow.24, %constant.608), dimensions={0}, to_apply=%region_27.28, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %reduce.24 = f32[] reduce(%integer_pow.25, %constant.608), dimensions={0,1,2}, to_apply=%region_28.29, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.22 = f32[] all-reduce(%reduce.24), channel_id=44, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_28.29.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.492 = f32[] add(%reduce.23, %all-reduce.22), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.25 = f32[] reduce(%integer_pow.26, %constant.608), dimensions={0,1,2}, to_apply=%region_29.30, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.23 = f32[] all-reduce(%reduce.25), channel_id=45, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_29.30.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.493 = f32[] add(%add.492, %all-reduce.23), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.26 = f32[] reduce(%integer_pow.27, %constant.608), dimensions={0,1,2}, to_apply=%region_30.31, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.24 = f32[] all-reduce(%reduce.26), channel_id=46, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_30.31.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.494 = f32[] add(%add.493, %all-reduce.24), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.27 = f32[] reduce(%integer_pow.28, %constant.608), dimensions={0,1}, to_apply=%region_31.32, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.495 = f32[] add(%add.494, %reduce.27), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.28 = f32[] reduce(%integer_pow.29, %constant.608), dimensions={0,1}, to_apply=%region_32.33, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.496 = f32[] add(%add.495, %reduce.28), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.29 = f32[] reduce(%integer_pow.30, %constant.608), dimensions={0,1,2,3}, to_apply=%region_33.34, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.25 = f32[] all-reduce(%reduce.29), channel_id=47, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_33.34.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.497 = f32[] add(%add.496, %all-reduce.25), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.30 = f32[] reduce(%integer_pow.31, %constant.608), dimensions={0,1,2,3}, to_apply=%region_34.35, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.26 = f32[] all-reduce(%reduce.30), channel_id=48, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_34.35.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.498 = f32[] add(%add.497, %all-reduce.26), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.31 = f32[] reduce(%integer_pow.32, %constant.608), dimensions={0,1,2,3}, to_apply=%region_35.36, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.27 = f32[] all-reduce(%reduce.31), channel_id=49, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_35.36.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.499 = f32[] add(%add.498, %all-reduce.27), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.32 = f32[] reduce(%integer_pow.33, %constant.608), dimensions={0,1,2,3}, to_apply=%region_36.37, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.28 = f32[] all-reduce(%reduce.32), channel_id=50, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_36.37.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.500 = f32[] add(%add.499, %all-reduce.28), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.33 = f32[] reduce(%integer_pow.34, %constant.608), dimensions={0,1}, to_apply=%region_37.38, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.29 = f32[] all-reduce(%reduce.33), channel_id=51, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_37.38.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.501 = f32[] add(%add.500, %all-reduce.29), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %reduce.34 = f32[] reduce(%integer_pow.35, %constant.608), dimensions={0,1}, to_apply=%region_38.39, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.30 = f32[] all-reduce(%reduce.34), channel_id=52, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_38.39.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.502 = f32[] add(%add.501, %all-reduce.30), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %sqrt.45 = f32[] sqrt(%add.502), metadata={op_name="jit(train_step)/sqrt" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=9 source_end_column=9}
  %sub.118 = f32[2,2048,128256]{2,1,0} broadcast(%log.6), dimensions={0,1}, metadata={op_name="jit(train_step)/jvp()/sub" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=556 source_end_line=556 source_column=16 source_end_column=16}
  %sub.119 = f32[2,2048,128256]{2,1,0} subtract(%sub.114, %sub.118), metadata={op_name="jit(train_step)/jvp()/sub" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=556 source_end_line=556 source_column=16 source_end_column=16}
  %broadcast.285 = f32[2,2048,128256]{2,1,0} broadcast(%constant.608), dimensions={}, metadata={op_name="broadcast.95"}
  %mul.981 = f32[2,2048,128256]{2,1,0} select(%eq.30, %sub.119, %broadcast.285), metadata={op_name="jit(train_step)/jvp()/mul" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=557 source_end_line=557 source_column=18 source_end_column=18}
  %reduce.35 = f32[2,2048]{1,0} reduce(%mul.981, %constant.608), dimensions={2}, to_apply=%region_39.40, metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=557 source_end_line=557 source_column=10 source_end_column=10}
  %neg.44 = f32[2,2048]{1,0} negate(%reduce.35), metadata={op_name="jit(train_step)/jvp()/neg" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=557 source_end_line=557 source_column=9 source_end_column=9}
  %square.107 = f32[2,2048]{1,0} multiply(%add.410, %add.410), metadata={op_name="jit(train_step)/jvp()/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=560 source_end_line=560 source_column=26 source_end_column=26}
  %mul.982 = f32[2,2048]{1,0} multiply(%square.107, %broadcast.260), metadata={op_name="jit(train_step)/jvp()/mul" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=560 source_end_line=560 source_column=17 source_end_column=17}
  %add.503 = f32[2,2048]{1,0} add(%neg.44, %mul.982), metadata={op_name="jit(train_step)/jvp()/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=561 source_end_line=561 source_column=2 source_end_column=2}
  %all-gather.18 = f32[2,8192]{1,0} all-gather(%add.503), channel_id=53, replica_groups=[1,4]<=[4], dimensions={1}, use_global_device_ids=true, metadata={op_name="jit(train_step)/jvp()/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=561 source_end_line=561 source_column=2 source_end_column=2}
  %sharding_constraint.157 = f32[2,8192]{1,0} copy(%all-gather.18), metadata={op_name="jit(train_step)/jvp()/sharding_constraint" source_file="/opt/maxtext/src/MaxText/sharding.py" source_line=44 source_end_line=44 source_column=11 source_end_column=11}
  %dynamic-slice.56 = f32[2,2048]{1,0} dynamic-slice(%sharding_constraint.157, %constant.593, %reshape.327), dynamic_slice_sizes={2,2048}, metadata={op_name="jit(train_step)/jvp()/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=152 source_end_line=152 source_column=13 source_end_column=13}
  %mul.983 = f32[2,2048]{1,0} select(%ne.10, %dynamic-slice.56, %broadcast.260), metadata={op_name="jit(train_step)/jvp()/mul" source_file="/opt/maxtext/src/MaxText/train.py" source_line=152 source_end_line=152 source_column=13 source_end_column=13}
  %reduce.36 = f32[] reduce(%mul.983, %constant.608), dimensions={0,1}, to_apply=%region_40.41, metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/train.py" source_line=153 source_end_line=153 source_column=19 source_end_column=19}
  %all-reduce.31 = f32[] all-reduce(%reduce.36), channel_id=54, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_40.41.clone, metadata={op_name="jit(train_step)/jvp()/reduce_sum" source_file="/opt/maxtext/src/MaxText/train.py" source_line=153 source_end_line=153 source_column=19 source_end_column=19}
  %div.358 = f32[] divide(%all-reduce.31, %add.406), metadata={op_name="jit(train_step)/jvp()/div" source_file="/opt/maxtext/src/MaxText/train.py" source_line=186 source_end_line=186 source_column=11 source_end_column=11}
  %square.108 = f32[4096]{0} multiply(%add.430, %add.430), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.37 = f32[] reduce(%square.108, %constant.608), dimensions={0}, to_apply=%region_41.42, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %square.109 = f32[1024,32,14336]{2,1,0} multiply(%add.435, %add.435), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.38 = f32[] reduce(%square.109, %constant.608), dimensions={0,1,2}, to_apply=%region_42.43, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.32 = f32[] all-reduce(%reduce.38), channel_id=55, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_42.43.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.504 = f32[] add(%reduce.37, %all-reduce.32), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.110 = f32[1024,32,14336]{2,1,0} multiply(%add.440, %add.440), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.39 = f32[] reduce(%square.110, %constant.608), dimensions={0,1,2}, to_apply=%region_43.44, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.33 = f32[] all-reduce(%reduce.39), channel_id=56, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_43.44.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.505 = f32[] add(%add.504, %all-reduce.33), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.111 = f32[14336,32,1024]{2,1,0} multiply(%add.446, %add.446), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.40 = f32[] reduce(%square.111, %constant.608), dimensions={0,1,2}, to_apply=%region_44.45, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.34 = f32[] all-reduce(%reduce.40), channel_id=57, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_44.45.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.506 = f32[] add(%add.505, %all-reduce.34), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.112 = f32[4096,32]{1,0} multiply(%add.451, %add.451), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.41 = f32[] reduce(%square.112, %constant.608), dimensions={0,1}, to_apply=%region_45.46, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.507 = f32[] add(%add.506, %reduce.41), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.113 = f32[4096,32]{1,0} multiply(%add.456, %add.456), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.42 = f32[] reduce(%square.113, %constant.608), dimensions={0,1}, to_apply=%region_46.47, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.508 = f32[] add(%add.507, %reduce.42), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.114 = f32[1024,32,8,128]{3,2,1,0} multiply(%add.461, %add.461), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.43 = f32[] reduce(%square.114, %constant.608), dimensions={0,1,2,3}, to_apply=%region_47.48, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.35 = f32[] all-reduce(%reduce.43), channel_id=58, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_47.48.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.509 = f32[] add(%add.508, %all-reduce.35), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.115 = f32[32,32,128,1024]{3,2,1,0} multiply(%add.467, %add.467), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.44 = f32[] reduce(%square.115, %constant.608), dimensions={0,1,2,3}, to_apply=%region_48.49, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.36 = f32[] all-reduce(%reduce.44), channel_id=59, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_48.49.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.510 = f32[] add(%add.509, %all-reduce.36), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.116 = f32[1024,32,32,128]{3,2,1,0} multiply(%add.473, %add.473), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.45 = f32[] reduce(%square.116, %constant.608), dimensions={0,1,2,3}, to_apply=%region_49.50, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.37 = f32[] all-reduce(%reduce.45), channel_id=60, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_49.50.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.511 = f32[] add(%add.510, %all-reduce.37), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.117 = f32[1024,32,8,128]{3,2,1,0} multiply(%add.478, %add.478), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.46 = f32[] reduce(%square.117, %constant.608), dimensions={0,1,2,3}, to_apply=%region_50.51, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.38 = f32[] all-reduce(%reduce.46), channel_id=61, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_50.51.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.512 = f32[] add(%add.511, %all-reduce.38), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.118 = f32[1024,128256]{1,0} multiply(%add.484, %add.484), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.47 = f32[] reduce(%square.118, %constant.608), dimensions={0,1}, to_apply=%region_51.52, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.39 = f32[] all-reduce(%reduce.47), channel_id=62, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_51.52.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.513 = f32[] add(%add.512, %all-reduce.39), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %square.119 = f32[128256,1024]{1,0} multiply(%add.490, %add.490), metadata={op_name="jit(train_step)/square" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=69 source_end_column=69}
  %reduce.48 = f32[] reduce(%square.119, %constant.608), dimensions={0,1}, to_apply=%region_52.53, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %all-reduce.40 = f32[] all-reduce(%reduce.48), channel_id=63, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_52.53.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.514 = f32[] add(%add.513, %all-reduce.40), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %sqrt.46 = f32[] sqrt(%add.514), metadata={op_name="jit(train_step)/sqrt" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=9 source_end_column=9}
  %all-reduce.41 = f32[] all-reduce(%reduce.12), channel_id=64, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_54.55.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.515 = f32[] add(%reduce.10, %all-reduce.41), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %all-reduce.42 = f32[] all-reduce(%reduce.13), channel_id=65, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_55.56.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.516 = f32[] add(%add.515, %all-reduce.42), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %all-reduce.43 = f32[] all-reduce(%reduce.14), channel_id=66, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_56.57.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.517 = f32[] add(%add.516, %all-reduce.43), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %add.518 = f32[] add(%add.517, %reduce.15), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %add.519 = f32[] add(%add.518, %reduce.16), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %all-reduce.44 = f32[] all-reduce(%reduce.17), channel_id=67, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_59.60.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.520 = f32[] add(%add.519, %all-reduce.44), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %all-reduce.45 = f32[] all-reduce(%reduce.18), channel_id=68, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_60.61.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.521 = f32[] add(%add.520, %all-reduce.45), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %all-reduce.46 = f32[] all-reduce(%reduce.19), channel_id=69, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_61.62.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.522 = f32[] add(%add.521, %all-reduce.46), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %all-reduce.47 = f32[] all-reduce(%reduce.20), channel_id=70, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_62.63.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.523 = f32[] add(%add.522, %all-reduce.47), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %all-reduce.48 = f32[] all-reduce(%reduce.21), channel_id=71, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_63.64.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.524 = f32[] add(%add.523, %all-reduce.48), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %all-reduce.49 = f32[] all-reduce(%reduce.22), channel_id=72, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%region_64.65.clone, metadata={op_name="jit(train_step)/reduce_sum" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=61 source_end_column=61}
  %add.525 = f32[] add(%add.524, %all-reduce.49), metadata={op_name="jit(train_step)/add" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=57 source_end_column=57}
  %sqrt.47 = f32[] sqrt(%add.525), metadata={op_name="jit(train_step)/sqrt" source_file="/opt/maxtext/src/MaxText/max_utils.py" source_line=69 source_end_line=69 source_column=9 source_end_column=9}
  ROOT %tuple.44 = (s32[], f32[4096]{0}, f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, /*index=5*/f32[4096,32]{1,0}, f32[4096,32]{1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, /*index=10*/f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, f32[128256,1024]{1,0}, s32[], f32[4096]{0}, /*index=15*/f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, f32[4096,32]{1,0}, f32[4096,32]{1,0}, /*index=20*/f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, /*index=25*/f32[128256,1024]{1,0}, f32[4096]{0}, f32[1024,32,14336]{2,1,0}, f32[1024,32,14336]{2,1,0}, f32[14336,32,1024]{2,1,0}, /*index=30*/f32[4096,32]{1,0}, f32[4096,32]{1,0}, f32[1024,32,8,128]{3,2,1,0}, f32[32,32,128,1024]{3,2,1,0}, f32[1024,32,32,128]{3,2,1,0}, /*index=35*/f32[1024,32,8,128]{3,2,1,0}, f32[1024,128256]{1,0}, f32[128256,1024]{1,0}, s32[], f32[], /*index=40*/f32[], f32[], f32[], f32[], f32[], /*index=45*/s32[]) tuple(%add.402, %add.430, %add.435, %add.440, %add.446, /*index=5*/%add.451, %add.456, %add.461, %add.467, %add.473, /*index=10*/%add.478, %add.484, %add.490, %select_n.135, %add.424, /*index=15*/%add.431, %add.436, %add.441, %add.447, %add.452, /*index=20*/%add.457, %add.462, %add.468, %add.474, %add.479, /*index=25*/%add.485, %add.426, %add.432, %add.437, %add.442, /*index=30*/%add.448, %add.453, %add.458, %add.463, %add.469, /*index=35*/%add.475, %add.480, %add.486, %select_n.155, %sqrt.45, /*index=40*/%div.358, %constant.608, %constant.608, %sqrt.46, %sqrt.47, /*index=45*/%all-reduce.9)
}

