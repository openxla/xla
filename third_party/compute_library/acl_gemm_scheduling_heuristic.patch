# Copyright 2025 The OpenXLA Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
diff --git a/src/cpu/kernels/assembly/arm_gemm.hpp b/src/cpu/kernels/assembly/arm_gemm.hpp
index cbc8be416..e65bc00a0 100644
--- a/src/cpu/kernels/assembly/arm_gemm.hpp
+++ b/src/cpu/kernels/assembly/arm_gemm.hpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018-2022, 2024 Arm Limited.
+ * Copyright (c) 2018-2022, 2024-2025 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -44,9 +44,7 @@ enum class GemmMethod
     GEMM_NATIVE,
     GEMM_HYBRID,
     GEMM_INTERLEAVED,
-    GEMM_INTERLEAVED_2D,
     QUANTIZE_WRAPPER,
-    QUANTIZE_WRAPPER_2D,
     GEMM_HYBRID_QUANTIZED
 };
 
diff --git a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp
index fc106140f..ec2039207 100644
--- a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp
+++ b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018-2024 Arm Limited.
+ * Copyright (c) 2018-2025 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -137,34 +137,6 @@ Params extract_parameters(const ITensorInfo *a, const ITensorInfo *b, const ITen
     return p;
 }
 
-IScheduler::Hints scheduling_hint_heuristic(arm_gemm::GemmMethod method, DataType data_type)
-{
-    // Schedule assembly kernel
-    const int         granule_threshold = 200;
-    IScheduler::Hints scheduling_hint   = IScheduler::Hints(Window::DimX);
-    if (method == arm_gemm::GemmMethod::GEMM_INTERLEAVED && data_type == DataType::F32)
-    {
-        scheduling_hint = IScheduler::Hints(Window::DimX, IScheduler::StrategyHint::DYNAMIC, granule_threshold);
-    }
-    else if (method == arm_gemm::GemmMethod::GEMM_INTERLEAVED_2D &&
-             (data_type == DataType::F32 || data_type == DataType::F16 || data_type == DataType::U8 ||
-              data_type == DataType::S8))
-    {
-        //GEMM_INTERLEAVED supports 2D parallelism, IScheduler::split_dimensions_all signals to parallelise over all window dimensions
-        scheduling_hint =
-            IScheduler::Hints(IScheduler::split_dimensions_all, IScheduler::StrategyHint::STATIC, granule_threshold);
-    }
-    else if (method == arm_gemm::GemmMethod::QUANTIZE_WRAPPER_2D &&
-             (data_type == DataType::QASYMM8 || data_type == DataType::QASYMM8_SIGNED))
-    {
-        //special case for QASYMM8 to support 2D parallelism, scheduler here may be tweaked differently compared to FP32 case
-        scheduling_hint =
-            IScheduler::Hints(IScheduler::split_dimensions_all, IScheduler::StrategyHint::STATIC, granule_threshold);
-    }
-
-    return scheduling_hint;
-}
-
 /** Fallback in case ACL doesn't have a function */
 template <typename TypeInput, typename TypeWeight, typename TypeOutput, class OutputStage = arm_gemm::Nothing>
 class Fallback : public CpuGemmAssemblyDispatch::IFallback
@@ -300,8 +272,6 @@ private:
     bool _is_prepared{false};
     /** GEMM meta-data */
     AsmGemmInfo _gemm_info{};
-    /** GEMM kernel description */
-    arm_gemm::KernelDescription _kernel_info{};
     /** Per channel quantization shifts */
     std::vector<int32_t> _shifts{};
     std::vector<int32_t> right_shifts{};
@@ -760,7 +730,20 @@ void Fallback<TypeInput, TypeWeight, TypeOutput, OutputStage>::run(ITensorPack &
         }
     }
 
-    const auto scheduling_hint = scheduling_hint_heuristic(_kernel_info.method, d->info()->data_type());
+    // The scheduling_hint needs to be compatible with the window exposed by arm_gemm
+    // The default case is when we split among the X dimension
+    IScheduler::Hints scheduling_hint = IScheduler::Hints(Window::DimX);
+    // If arm_gemm exposes a 2D window, perform 2D scheduling
+    if (_optimised_kernel->window().num_iterations(Window::DimY) > 1 &&
+        _optimised_kernel->window().num_iterations(Window::DimX) > 1)
+    {
+        scheduling_hint = IScheduler::Hints(IScheduler::split_dimensions_all);
+    }
+    // Split among Y
+    else if (_optimised_kernel->window().num_iterations(Window::DimY) > 1)
+    {
+        scheduling_hint = IScheduler::Hints(Window::DimY);
+    }
 
     // Set workspace if needed and reset number of threads as buffer manager gets re-created with max_threads
     CpuAuxTensorHandler workspace(offset_int_vec(AsmGemmWorkspace), _workspace_info, tensors, false);
diff --git a/src/runtime/IScheduler.cpp b/src/runtime/IScheduler.cpp
index 2dd87310a..9fa815fbd 100644
--- a/src/runtime/IScheduler.cpp
+++ b/src/runtime/IScheduler.cpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2016-2024 Arm Limited.
+ * Copyright (c) 2016-2025 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -69,9 +69,20 @@ void IScheduler::schedule_common(ICPPKernel *kernel, const Hints &hints, const W
         const std::size_t m = max_window.num_iterations(Window::DimX);
         const std::size_t n = max_window.num_iterations(Window::DimY);
 
+        const unsigned int num_iterations = m * n;
+        const unsigned int num_threads    = std::min(num_iterations, this->num_threads());
+
         //in c++17 this can be swapped for   auto [ m_threads, n_threads ] = split_2d(...
         unsigned m_threads, n_threads;
-        std::tie(m_threads, n_threads) = scheduler_utils::split_2d(this->num_threads(), m, n);
+        std::tie(m_threads, n_threads) = scheduler_utils::split_2d(num_threads, m, n);
+
+        // Clamp m_threads and n_threads if not all threads have work to do
+        unsigned int max_parallelism = std::min<unsigned int>(m, m_threads) * std::min<unsigned int>(n, n_threads);
+        if (max_parallelism < num_threads)
+        {
+            m_threads = std::min<unsigned int>(m, m_threads);
+            n_threads = std::min<unsigned int>(n, n_threads);
+        }
 
         std::vector<IScheduler::Workload> workloads;
         for (unsigned int ni = 0; ni != n_threads; ++ni)
