diff --git a/docs/mpmd/mpmd_export_passes.md b/docs/mpmd/mpmd_export_passes.md
index afcea25..bbdaab0 100644
--- a/docs/mpmd/mpmd_export_passes.md
+++ b/docs/mpmd/mpmd_export_passes.md
@@ -96,12 +96,3 @@ Marks fragment args and results with attributes to identify which values
 live in host memory, so that this information can be used by XLA. Also marks
 the entrypoint func args and results so that Pathways can use this
 information.
-
-### `-mpmd-validate-no-reshards`
-
-_Validates that no reshard-only fragments exist._
-
-A reshard-only fragment is a fragment that contains only a `mpmd.return` of
-one of its arguments. These fragments usually indicate an unexpected reshard.
-
-This pass identifies such fragments and emits an error.
diff --git a/docs/sdy_export_passes.md b/docs/sdy_export_passes.md
index 5e9258c..0db6deb 100755
--- a/docs/sdy_export_passes.md
+++ b/docs/sdy_export_passes.md
@@ -16,16 +16,6 @@ constant sub-computation. If the constants have same shardings after
 propagation, this pass merges them to save compilation time. See
 -sdy-constant-or-scalar-splitter for more info.
 
-### `-sdy-convert-global-to-local`
-
-_Converts an SDY program from global shapes to local shapes._
-
-Converts an SDY program from global shapes to local shapes by partitioning
-logical dimensions based on sharding attributes.
-
-This pass leverages a type converter to map RankedTensorType from global
-logical shapes to device-local physical shapes.
-
 ### `-sdy-drop-sharding-rules`
 
 _Drops `OpShardingRuleAttr` from all registered ops._
diff --git a/shardy/dialect/mpmd/transforms/export/BUILD b/shardy/dialect/mpmd/transforms/export/BUILD
index 9e61aa0..86f3e7f 100644
--- a/shardy/dialect/mpmd/transforms/export/BUILD
+++ b/shardy/dialect/mpmd/transforms/export/BUILD
@@ -38,7 +38,6 @@ cc_library(
         "mark_input_output_with_layouts.cc",
         "mark_offloaded_input_output.cc",
         "reschedule_ops.cc",
-        "validate_no_reshards.cc",
     ],
     hdrs = [
         "passes.h",
diff --git a/shardy/dialect/mpmd/transforms/export/export_pipeline.cc b/shardy/dialect/mpmd/transforms/export/export_pipeline.cc
index 15dd169..3f9bf97 100644
--- a/shardy/dialect/mpmd/transforms/export/export_pipeline.cc
+++ b/shardy/dialect/mpmd/transforms/export/export_pipeline.cc
@@ -15,10 +15,8 @@ limitations under the License.
 
 #include <utility>
 
-#include "llvm/Support/CommandLine.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
 #include "mlir/Pass/PassManager.h"
-#include "mlir/Pass/PassOptions.h"
 #include "mlir/Pass/PassRegistry.h"
 #include "mlir/Transforms/GreedyPatternRewriteDriver.h"
 #include "mlir/Transforms/Passes.h"
@@ -123,10 +121,6 @@ void addExportPipeline(OpPassManager& pm, const ExportOptions& options) {
   // offloading and aliasing passes.
   pm.addNestedPass<FuncOp>(createMarkFragmentReservedMemoryPass());
 
-  if (options.failOnReshardOnlyFragments) {
-    pm.addNestedPass<FuncOp>(createValidateNoReshardsPass());
-  }
-
   // This pass should be applied after all passes that operate on fragment ops.
   LowerToFragmentCallsPassOptions lower_to_fragment_calls_options;
   lower_to_fragment_calls_options.groupAcrossMeshes =
@@ -136,28 +130,12 @@ void addExportPipeline(OpPassManager& pm, const ExportOptions& options) {
       std::move(lower_to_fragment_calls_options)));
 }
 
-namespace {
-
-struct ExportPipelineOptions
-    : public PassPipelineOptions<ExportPipelineOptions> {
-  Option<bool> failOnReshardOnlyFragments{
-      *this, "fail-on-reshard-only-fragments",
-      llvm::cl::desc(
-          "Whether to emit an error when a reshard-only fragment is detected."),
-      llvm::cl::init(false)};
-};
-
-}  // namespace
-
 void registerExportPipeline() {
-  PassPipelineRegistration<ExportPipelineOptions>(
+  PassPipelineRegistration<>(
       "mpmd-export-pipeline",
       "Run the standard set of passes to export an MPMD program.",
-      [](OpPassManager& pm, const ExportPipelineOptions& pipelineOptions) {
-        ExportOptions options;
-        options.failOnReshardOnlyFragments =
-            pipelineOptions.failOnReshardOnlyFragments;
-        addExportPipeline(pm, options);
+      [](OpPassManager& pm) {
+        addExportPipeline(pm);
       });
 }
 
diff --git a/shardy/dialect/mpmd/transforms/export/passes.h b/shardy/dialect/mpmd/transforms/export/passes.h
index 8f44012..52bdb39 100644
--- a/shardy/dialect/mpmd/transforms/export/passes.h
+++ b/shardy/dialect/mpmd/transforms/export/passes.h
@@ -45,7 +45,6 @@ struct ExportOptions {
   bool groupFragmentsAcrossMeshes = true;
   // Whether to apply the merge transfers optimization pass.
   bool applyMergeTransfers = false;
-  bool failOnReshardOnlyFragments = false;
   // Whether to enable verbose logging.
   bool verboseLogging = false;
 };
diff --git a/shardy/dialect/mpmd/transforms/export/passes.td b/shardy/dialect/mpmd/transforms/export/passes.td
index a160e9e..caad263 100644
--- a/shardy/dialect/mpmd/transforms/export/passes.td
+++ b/shardy/dialect/mpmd/transforms/export/passes.td
@@ -131,14 +131,3 @@ def DelayTransfersFromCpuPass :
     can be beneficial for HBM usage.
   }];
 }
-
-def ValidateNoReshardsPass :
-        PassBase<"mpmd-validate-no-reshards", "DistributedFunctionPass"> {
-  let summary = "Validates that no reshard-only fragments exist.";
-  let description = [{
-    A reshard-only fragment is a fragment that contains only a `mpmd.return` of
-    one of its arguments. These fragments usually indicate an unexpected reshard.
-
-    This pass identifies such fragments and emits an error.
-  }];
-}
diff --git a/shardy/dialect/mpmd/transforms/export/test/export_pipeline_failures.mlir b/shardy/dialect/mpmd/transforms/export/test/export_pipeline_failures.mlir
deleted file mode 100644
index f601e71..0000000
--- a/shardy/dialect/mpmd/transforms/export/test/export_pipeline_failures.mlir
+++ /dev/null
@@ -1,27 +0,0 @@
-// RUN: mpmd_opt %s -mpmd-export-pipeline='fail-on-reshard-only-fragments=true' -split-input-file -verify-diagnostics 2>&1
-
-!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
-!mesh_1_tensor_sharded_x = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@m1, [{"x"}, {?}]>>
-
-func.func @has_reshard_only_fragment(%arg0: !mesh_1_tensor) -> !mesh_1_tensor_sharded_x attributes {
-    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
-  // expected-error@+1 {{Detected reshard-only fragment. This usually indicates an unexpected reshard. Operands:}}
-  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<4x8xf32>) {
-    mpmd.return %arg1 : tensor<4x8xf32>
-  } : (!mesh_1_tensor) -> !mesh_1_tensor_sharded_x
-  func.return %0 : !mesh_1_tensor_sharded_x
-}
-
-// -----
-
-!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
-!mesh_1_tensor_sharded_x = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@m1, [{"x"}, {?}]>>
-
-func.func @has_non_reshard_only_fragment(%arg0: !mesh_1_tensor) -> !mesh_1_tensor_sharded_x attributes {
-    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
-  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<4x8xf32>) {
-    %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
-    mpmd.return %1 : tensor<4x8xf32>
-  } : (!mesh_1_tensor) -> !mesh_1_tensor_sharded_x
-  func.return %0 : !mesh_1_tensor_sharded_x
-}
diff --git a/shardy/dialect/mpmd/transforms/export/test/reshard_only_fragments_failure.mlir b/shardy/dialect/mpmd/transforms/export/test/reshard_only_fragments_failure.mlir
deleted file mode 100644
index 153b8b1..0000000
--- a/shardy/dialect/mpmd/transforms/export/test/reshard_only_fragments_failure.mlir
+++ /dev/null
@@ -1,27 +0,0 @@
-// RUN: mpmd_opt %s -mpmd-validate-no-reshards -split-input-file -verify-diagnostics 2>&1
-
-!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
-!mesh_1_tensor_sharded_x = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@m1, [{"x"}, {?}]>>
-
-func.func @has_reshard_only_fragment(%arg0: !mesh_1_tensor loc("x")) -> (!mesh_1_tensor_sharded_x {jax.result_info = "result"}) attributes {
-    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
-  // expected-error@+1 {{Detected reshard-only fragment. This usually indicates an unexpected reshard. Operands: Replicated loc("x"). Results: #sdy.sharding<@m1, [{"x"}, {?}]> loc("result")}}
-  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<4x8xf32>) {
-    mpmd.return %arg1 : tensor<4x8xf32>
-  } : (!mesh_1_tensor) -> !mesh_1_tensor_sharded_x
-  func.return %0 : !mesh_1_tensor_sharded_x
-}
-
-// -----
-
-!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
-!mesh_1_tensor_sharded_x = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@m1, [{"x"}, {?}]>>
-
-func.func @has_non_reshard_only_fragment(%arg0: !mesh_1_tensor) -> !mesh_1_tensor_sharded_x attributes {
-    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
-  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<4x8xf32>) {
-    %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
-    mpmd.return %1 : tensor<4x8xf32>
-  } : (!mesh_1_tensor) -> !mesh_1_tensor_sharded_x
-  func.return %0 : !mesh_1_tensor_sharded_x
-}
diff --git a/shardy/dialect/mpmd/transforms/export/validate_no_reshards.cc b/shardy/dialect/mpmd/transforms/export/validate_no_reshards.cc
deleted file mode 100644
index 9fc94df..0000000
--- a/shardy/dialect/mpmd/transforms/export/validate_no_reshards.cc
+++ /dev/null
@@ -1,98 +0,0 @@
-/* Copyright 2025 The MPMD Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include "llvm/ADT/STLExtras.h"
-#include "mlir/Dialect/Func/IR/FuncOps.h"
-#include "mlir/IR/Diagnostics.h"
-#include "mlir/IR/OpDefinition.h"
-#include "mlir/IR/Value.h"
-#include "mlir/Support/LLVM.h"
-#include "shardy/dialect/mpmd/ir/dialect.h"
-#include "shardy/dialect/mpmd/ir/utils.h"
-#include "shardy/dialect/mpmd/transforms/export/passes.h"  // IWYU pragma: keep
-
-namespace mlir::mpmd {
-
-#define GEN_PASS_DEF_VALIDATENORESHARDSPASS
-#include "shardy/dialect/mpmd/transforms/export/passes.h.inc"
-
-namespace {
-
-class ValidateNoReshardsPass
-    : public impl::ValidateNoReshardsPassBase<ValidateNoReshardsPass> {
-  using ValidateNoReshardsPassBase::ValidateNoReshardsPassBase;
-
- protected:
-  void runOnFunc(func::FuncOp func) override {
-    func.walk([&](FragmentOp fragmentOp) {
-      if (!isReshardOnly(fragmentOp)) {
-        return;
-      }
-
-      InFlightDiagnostic diag =
-          fragmentOp.emitError()
-          << "Detected reshard-only fragment. This usually indicates an "
-             "unexpected reshard. Operands: ";
-
-      auto printSharding = [](Type type, InFlightDiagnostic& diag) {
-        auto meshType = mlir::dyn_cast<MeshTensorType>(type);
-        if (auto sharding = meshType ? meshType.getSharding() : nullptr) {
-          diag << sharding;
-        } else {
-          diag << "Replicated";
-        }
-      };
-
-      llvm::interleaveComma(
-          llvm::zip(fragmentOp.getOperands(), fragmentOp.getOperandTypes()),
-          diag, [&](auto pair) {
-            auto [operand, type] = pair;
-            printSharding(type, diag);
-            diag << " " << operand.getLoc();
-          });
-      diag << ". Results: ";
-      bool first = true;
-      for (auto [result, type] :
-           llvm::zip(fragmentOp.getResults(), fragmentOp.getResultTypes())) {
-        if (!first) {
-          diag << ", ";
-        }
-        first = false;
-        printSharding(type, diag);
-        for (OpOperand& use : result.getUses()) {
-          auto returnOp = mlir::dyn_cast<func::ReturnOp>(use.getOwner());
-          if (!returnOp) {
-            continue;
-          }
-          if (auto loc = GetResultInfoLoc(func, use.getOperandNumber())) {
-            diag << " " << *loc;
-          }
-        }
-      }
-
-      signalPassFailure();
-    });
-  }
-
-  static bool isReshardOnly(FragmentOp fragmentOp) {
-    Block& body = fragmentOp.getRegion().front();
-    return llvm::hasSingleElement(body) &&
-           body.front().hasTrait<OpTrait::IsTerminator>();
-  }
-};
-
-}  // namespace
-
-}  // namespace mlir::mpmd
diff --git a/shardy/dialect/sdy/transforms/export/BUILD b/shardy/dialect/sdy/transforms/export/BUILD
index 82dd86e..5357a90 100644
--- a/shardy/dialect/sdy/transforms/export/BUILD
+++ b/shardy/dialect/sdy/transforms/export/BUILD
@@ -32,7 +32,6 @@ cc_library(
     srcs = [
         "close_shardings.cc",
         "constant_or_scalar_merger.cc",
-        "convert_global_to_local.cc",
         "drop_sharding_rules.cc",
         "export_pipeline.cc",
         "insert_explicit_reshards.cc",
@@ -63,7 +62,6 @@ cc_library(
         "//shardy/dialect/sdy/transforms/propagation/debugging:source_sharding",
         "@llvm-project//llvm:Support",
         "@llvm-project//mlir:FuncDialect",
-        "@llvm-project//mlir:FuncTransforms",
         "@llvm-project//mlir:IR",
         "@llvm-project//mlir:Pass",
         "@llvm-project//mlir:Rewrite",
diff --git a/shardy/dialect/sdy/transforms/export/convert_global_to_local.cc b/shardy/dialect/sdy/transforms/export/convert_global_to_local.cc
deleted file mode 100644
index 5709aa5..0000000
--- a/shardy/dialect/sdy/transforms/export/convert_global_to_local.cc
+++ /dev/null
@@ -1,300 +0,0 @@
-/* Copyright 2026 The Shardy Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include <iterator>
-#include <optional>
-#include <utility>
-
-#include "llvm/ADT/DenseSet.h"
-#include "llvm/ADT/STLExtras.h"
-#include "mlir/Dialect/Func/IR/FuncOps.h"
-#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
-#include "mlir/IR/Builders.h"
-#include "mlir/IR/BuiltinOps.h"
-#include "mlir/IR/BuiltinTypes.h"
-#include "mlir/IR/Location.h"
-#include "mlir/IR/SymbolTable.h"
-#include "mlir/IR/Value.h"
-#include "mlir/IR/ValueRange.h"
-#include "mlir/Pass/Pass.h"  // IWYU pragma: keep
-#include "mlir/Support/LLVM.h"
-#include "mlir/Transforms/DialectConversion.h"
-#include "shardy/common/logging.h"
-#include "shardy/dialect/sdy/ir/dialect.h"  // IWYU pragma: keep
-#include "shardy/dialect/sdy/ir/utils.h"
-#include "stablehlo/dialect/StablehloOps.h"
-
-namespace mlir {
-namespace sdy {
-
-#define GEN_PASS_DEF_CONVERTGLOBALTOLOCALPASS
-#include "shardy/dialect/sdy/transforms/export/passes.h.inc"
-
-namespace {
-
-// Computes the local type of a given type and sharding.
-//
-// If the sharding is not defined or the mesh is not defined, the original
-// type is returned. If the sharding is defined and the mesh is defined,
-// the local type is computed using the sharding. If the local type
-// computation fails, an error is reported.
-Type getLocalType(Type type, TensorShardingAttr sharding,
-                  const SymbolTable& symbolTable) {
-  if (!sharding) {
-    return type;
-  }
-  MeshAttr mesh = sharding.getMesh(symbolTable);
-  if (!mesh) {
-    return type;
-  }
-
-  Type localType =
-      sharding.getLocalType(type, mesh, /*allowNonDivisible=*/false);
-  SDY_CHECK(localType)
-      << "Failed to compute local type due to non-divisible sharding";
-  return localType;
-}
-
-struct ConversionState {
-  llvm::DenseSet<Operation*> convertedOps;
-
-  void addConvertedOp(Operation* op) { convertedOps.insert(op); }
-  bool isConverted(Operation* op) { return convertedOps.contains(op); }
-};
-
-class GlobalToLocalTypeConverter : public TypeConverter {
- public:
-  GlobalToLocalTypeConverter(SymbolTable& symbolTable)
-      : symbolTable(symbolTable) {
-    addConversion([](Type type) { return type; });
-
-    // Converts global RankedTensorType to local type.
-    addConversion([&](Value value) -> std::optional<Type> {
-      auto type = dyn_cast<RankedTensorType>(value.getType());
-      if (!type) {
-        // Pass through to other converters if it is not a RankedTensorType.
-        return std::nullopt;
-      }
-      TensorShardingAttr sharding;
-      if (auto blockArg = dyn_cast<BlockArgument>(value)) {
-        // For block arguments, we look up the pre-populated sharding. This is
-        // because the block is unlinked from its original function by the
-        // conversion framework when FuncOp signature is converted.
-        auto it = argShardings.find(blockArg);
-        if (it != argShardings.end()) {
-          sharding = it->second;
-        }
-      } else {
-        // For other values, it's safe to call getSharding as we keep the
-        // sharding attribute on the converted op.
-        sharding = getSharding(value);
-      }
-      return getLocalType(type, sharding, symbolTable);
-    });
-
-    // Materializations to resolve intermediate casts.
-    auto materialize = [](OpBuilder& b, Type t, ValueRange inputs,
-                          Location loc) -> Value {
-      return UnrealizedConversionCastOp::create(b, loc, t, inputs).getResult(0);
-    };
-    addSourceMaterialization(materialize);
-    addTargetMaterialization(materialize);
-  }
-
-  // Constructs a map from block argument to sharding attribute, as the block
-  // will be unlinked during the FuncOp signature conversion which prevents us
-  // from using the getSharding() utility function.
-  void populateArgShardings(func::FuncOp funcOp) {
-    for (BlockArgument arg : funcOp.getArguments()) {
-      if (auto sharding = funcOp.getArgAttrOfType<TensorShardingAttr>(
-              arg.getArgNumber(), "sdy.sharding")) {
-        argShardings[arg] = sharding;
-      }
-    }
-  }
-
-  const SymbolTable& getSymbolTable() const { return symbolTable; }
-
- private:
-  const SymbolTable& symbolTable;
-  llvm::DenseMap<Value, TensorShardingAttr> argShardings;
-};
-
-// Generic StableHLO op pattern.
-class StablehloOpPattern : public ConversionPattern {
- public:
-  StablehloOpPattern(TypeConverter& converter, MLIRContext* ctx,
-                     ConversionState& state)
-      : ConversionPattern(converter, MatchAnyOpTypeTag(), 1, ctx),
-        conversionState(state) {}
-
-  LogicalResult matchAndRewrite(
-      Operation* op, ArrayRef<Value> operands,
-      ConversionPatternRewriter& rewriter) const override {
-    // Skip non-StableHLO ops and ops with specific patterns.
-    if (op->getDialect()->getNamespace() != "stablehlo" ||
-        isa<stablehlo::ConstantOp>(op)) {
-      return failure();
-    }
-
-    // Compute local shapes for results.
-    SmallVector<Type> newResultTypes;
-    llvm::transform(
-        op->getResults(), std::back_inserter(newResultTypes),
-        [&](Value result) { return typeConverter->convertType(result); });
-    Operation* newOp =
-        rewriter.create(op->getLoc(), op->getName().getIdentifier(), operands,
-                        newResultTypes, op->getAttrs(), op->getSuccessors());
-
-    conversionState.addConvertedOp(newOp);
-    rewriter.replaceOp(op, newOp->getResults());
-    return success();
-  }
-
- private:
-  ConversionState& conversionState;
-};
-
-class ReturnOpPattern : public OpConversionPattern<func::ReturnOp> {
- public:
-  ReturnOpPattern(TypeConverter& converter, MLIRContext* ctx,
-                  ConversionState& state)
-      : OpConversionPattern<func::ReturnOp>(converter, ctx),
-        conversionState(state) {}
-
-  LogicalResult matchAndRewrite(
-      func::ReturnOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
-    conversionState.addConvertedOp(op);
-    rewriter.replaceOpWithNewOp<func::ReturnOp>(op, adaptor.getOperands());
-    return success();
-  }
-
- private:
-  ConversionState& conversionState;
-};
-
-class FuncOpSignaturePattern : public OpConversionPattern<func::FuncOp> {
- public:
-  FuncOpSignaturePattern(TypeConverter& converter, MLIRContext* ctx,
-                         ConversionState& state)
-      : OpConversionPattern<func::FuncOp>(converter, ctx),
-        conversionState(state) {}
-
-  LogicalResult matchAndRewrite(
-      func::FuncOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
-    auto* converter =
-        static_cast<const GlobalToLocalTypeConverter*>(getTypeConverter());
-    const SymbolTable& symbolTable = converter->getSymbolTable();
-
-    TypeConverter::SignatureConversion signature(op.getNumArguments());
-    for (const BlockArgument& arg : op.getArguments()) {
-      signature.addInputs(arg.getArgNumber(), converter->convertType(arg));
-    }
-    SmallVector<Type> newResultTypes;
-    for (int i = 0; i < op.getNumResults(); ++i) {
-      Type globalType = op.getResultTypes()[i];
-      if (auto rankedType = dyn_cast<RankedTensorType>(globalType)) {
-        auto sharding = getFuncResultSharding(op, i);
-        newResultTypes.push_back(
-            getLocalType(rankedType, sharding, symbolTable));
-      } else {
-        newResultTypes.push_back(globalType);
-      }
-    }
-    conversionState.addConvertedOp(op);
-    auto newFuncType =
-        rewriter.getFunctionType(signature.getConvertedTypes(), newResultTypes);
-    // Update the function type.
-    rewriter.modifyOpInPlace(op, [&] { op.setType(newFuncType); });
-
-    if (failed(rewriter.convertRegionTypes(&op.getBody(), *getTypeConverter(),
-                                           &signature))) {
-      return failure();
-    }
-
-    return success();
-  }
-
- private:
-  ConversionState& conversionState;
-};
-
-// This pass converts a Shardy module with consistent sharding notations and
-// global tensor types to a module with local tensor types.
-//
-// The conversion is based on TensorShardingAttr which is not part of a type
-// representation. For example, in order to convert the type of a value into a
-// local type, we use getSharding(value) to retreat the TensorShardingAttr from
-// its defining op or its owning op (for block arguments). The problem of not
-// having TensorShardingAttr as part of the type is that we can't just look at
-// the type and its associated TensorShardingAttr to tell whether the type for
-// a given value has already been converted and become "legal". To resolve this,
-// we keep track of the converted ops in the state of the converter and consider
-// only ops in the set as legal.
-//
-// When a FuncOp is converted, the function body is unlinked by the conversion
-// framework. After that, when an op of the function body is converted, we can
-// no longer use getSharding() to retrieve the sharding attribute for a function
-// argument. To resolve this, we collect a map from function arguments to
-// sharding attributes before we start to convert any ops.
-//
-struct ConvertGlobalToLocalPass
-    : public impl::ConvertGlobalToLocalPassBase<ConvertGlobalToLocalPass> {
- protected:
-  void runOnOperation() final {
-    ModuleOp module = getOperation();
-    SymbolTable symbolTable(module);
-    GlobalToLocalTypeConverter typeConverter(symbolTable);
-
-    module.walk([&](func::FuncOp funcOp) {
-      typeConverter.populateArgShardings(funcOp);
-    });
-
-    ConversionState conversionState;
-    RewritePatternSet patterns(&getContext());
-    populateFunctionOpInterfaceTypeConversionPattern<func::FuncOp>(
-        patterns, typeConverter);
-    populateCallOpTypeConversionPattern(patterns, typeConverter);
-    populateReturnOpTypeConversionPattern(patterns, typeConverter);
-
-    patterns.add<FuncOpSignaturePattern, ReturnOpPattern, StablehloOpPattern>(
-        typeConverter, &getContext(), conversionState);
-
-    ConversionTarget target(getContext());
-    target.addDynamicallyLegalOp<func::FuncOp>(
-        [&](func::FuncOp op) { return conversionState.isConverted(op); });
-    target.addDynamicallyLegalOp<func::ReturnOp>(
-        [&](func::ReturnOp op) { return conversionState.isConverted(op); });
-
-    target.addDynamicallyLegalDialect<stablehlo::StablehloDialect>(
-        [&](Operation* op) { return conversionState.isConverted(op); });
-
-    // Ensure all 'sdy' ops are gone, except for the mesh definition, which will
-    // be removed later.
-    target.addIllegalDialect<SdyDialect>();
-    target.addLegalOp<sdy::MeshOp>();
-
-    if (failed(applyPartialConversion(module, target, std::move(patterns)))) {
-      signalPassFailure();
-    }
-  }
-};
-
-}  // namespace
-
-}  // namespace sdy
-}  // namespace mlir
diff --git a/shardy/dialect/sdy/transforms/export/passes.td b/shardy/dialect/sdy/transforms/export/passes.td
index 14e68c3..4a893a3 100644
--- a/shardy/dialect/sdy/transforms/export/passes.td
+++ b/shardy/dialect/sdy/transforms/export/passes.td
@@ -218,15 +218,3 @@ def RemovePropagationDebugInfoPass : Pass<"sdy-remove-propagation-debug-info", "
   let summary = "Removes propagation debug info (propagation edges and origin shardings) during export.";
   let dependentDialects = ["mlir::sdy::SdyDialect"];
 }
-
-def ConvertGlobalToLocalPass : Pass<"sdy-convert-global-to-local", "ModuleOp"> {
-  let summary = "Converts an SDY program from global shapes to local shapes.";
-  let description = [{
-    Converts an SDY program from global shapes to local shapes by partitioning
-    logical dimensions based on sharding attributes.
-
-    This pass leverages a type converter to map RankedTensorType from global
-    logical shapes to device-local physical shapes.
-  }];
-  let dependentDialects = ["mlir::sdy::SdyDialect", "mlir::stablehlo::StablehloDialect"];
-}
diff --git a/shardy/dialect/sdy/transforms/export/test/convert_global_to_local.mlir b/shardy/dialect/sdy/transforms/export/test/convert_global_to_local.mlir
deleted file mode 100644
index 957dfa3..0000000
--- a/shardy/dialect/sdy/transforms/export/test/convert_global_to_local.mlir
+++ /dev/null
@@ -1,30 +0,0 @@
-// RUN: sdy_opt %s -sdy-convert-global-to-local | FileCheck %s
-
-// CHECK: sdy.mesh @mesh_2 = <["x"=2]>
-sdy.mesh @mesh_2 = <["x"=2]>
-// CHECK: sdy.mesh @mesh_2_4 = <["x"=2, "y"=4]>
-sdy.mesh @mesh_2_4 = <["x"=2, "y"=4]>
-
-// CHECK-LABEL: func.func @func_returning_sharded_arg
-// CHECK-SAME:    (%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh_2, [{"x"}]>}) -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh_2, [{"x"}]>})
-func.func @func_returning_sharded_arg(%arg0: tensor<16xf32> {sdy.sharding = #sdy.sharding<@mesh_2, [{"x"}]>}) -> (tensor<16xf32> {sdy.sharding = #sdy.sharding<@mesh_2, [{"x"}]>}) {
-  // CHECK-NEXT:  return %arg0 : tensor<8xf32>
-  return %arg0 : tensor<16xf32>
-}
-
-// CHECK-LABEL: func.func @func_with_dot_then_add
-// CHECK-SAME:    (%arg0: tensor<4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_2_4, [{"x"}, {}]>},
-// CHECK-SAME:    %arg1: tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh_2_4, [{}, {"y"}]>},
-// CHECK-SAME:    %arg2: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_2_4, [{"x"}, {"y"}]>})
-// CHECK-SAME:    -> (tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_2_4, [{"x"}, {"y"}]>}) {
-func.func @func_with_dot_then_add(%arg0: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_2_4, [{"x"}, {}]>},
-  %arg1: tensor<16x32xf32> {sdy.sharding = #sdy.sharding<@mesh_2_4, [{}, {"y"}]>},
-  %arg2: tensor<8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_2_4, [{"x"}, {"y"}]>})
-  -> (tensor<8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_2_4, [{"x"}, {"y"}]>}) {
-  // CHECK-NEXT:  %[[DOT:.*]] = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_2_4, [{"x"}, {"y"}]>]>} : (tensor<4x16xf32>, tensor<16x8xf32>) -> tensor<4x8xf32>
-  %0 = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_2_4, [{"x"}, {"y"}]>]>} : (tensor<8x16xf32>, tensor<16x32xf32>) -> tensor<8x32xf32>
-  // CHECK-NEXT:  %[[ADD:.*]] = stablehlo.add %[[DOT]], %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_2_4, [{"x"}, {"y"}]>]>} : tensor<4x8xf32>
-  %1 = stablehlo.add %0, %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_2_4, [{"x"}, {"y"}]>]>} : tensor<8x32xf32>
-  // CHECK-NEXT:  return %[[ADD]] : tensor<4x8xf32>
-  return %1 : tensor<8x32xf32>
-}
diff --git a/shardy/integrations/python/jax/mpmd/README.md b/shardy/integrations/python/jax/mpmd/README.md
index d2fb6d8..02f1996 100644
--- a/shardy/integrations/python/jax/mpmd/README.md
+++ b/shardy/integrations/python/jax/mpmd/README.md
@@ -22,7 +22,7 @@ To set up a development environment:
 
     ```bash
     git clone https://github.com/openxla/shardy.git
-    cd shardy/integrations/python/jax/mpmd
+    cd shardy/shardy/integrations/python/jax/mpmd
     ```
 
 2.  **Create and activate a virtual environment:**
@@ -41,7 +41,7 @@ To set up a development environment:
 4.  **Run unit tests:**
 
     ```bash
-    pytest -s jit_test.py
+    pytest jit_test.py
     ```
 
 ## Usage
diff --git a/shardy/integrations/python/jax/mpmd/jit_test.py b/shardy/integrations/python/jax/mpmd/jit_test.py
index 9574a45..c9bdae1 100644
--- a/shardy/integrations/python/jax/mpmd/jit_test.py
+++ b/shardy/integrations/python/jax/mpmd/jit_test.py
@@ -140,6 +140,18 @@ class SdyPropagationTest(parameterized.TestCase):
         lowered1.function_named_shardings.output_specs,
     )
 
+  def test_compile_fails_because_not_pathways_backend(self):
+
+    lowered: stages.MpmdLowered = mpmd.jit(
+        lambda x: x, get_2_stage_mpmd_config()
+    ).lower(np.ones((3, 5), dtype=jnp.float32))
+    with self.assertRaisesRegex(
+        NotImplementedError,
+        'MPMD functions can only be compiled through Pathways, but only the'
+        r" following backends are available: \('cpu',\)",
+    ):
+      lowered.compile()
+
   def test_mpmd_simple_spmd_sharding(self):
     topology = test_utils.get_two_mesh_topology()
     assignment = {'stage1': 'mesh1', 'stage2': 'mesh2'}
diff --git a/shardy/integrations/python/jax/mpmd/stages.py b/shardy/integrations/python/jax/mpmd/stages.py
index 22373bc..60f6a79 100644
--- a/shardy/integrations/python/jax/mpmd/stages.py
+++ b/shardy/integrations/python/jax/mpmd/stages.py
@@ -26,6 +26,7 @@ from jax._src import core
 from jax._src import stages
 from jax._src.interpreters import pxla
 from jax.experimental import layout
+import jax.extend.backend as jax_backend
 from jax.interpreters import mlir as jax_mlir
 from jaxlib import _sdy_mpmd as jaxlib_mpmd
 import jaxtyping
@@ -36,6 +37,8 @@ from shardy.integrations.python.jax.mpmd import utils
 
 PyTree = jaxtyping.PyTree
 FunctionNamedShardings = utils.FunctionNamedShardings
+# Will replace this once jaxlib landed.
+ifrt_mpmd_py = Any # pylint: disable=invalid-name
 
 
 @dataclasses.dataclass(frozen=True)
@@ -369,6 +372,7 @@ class MpmdLowered(stages.Lowered):
       stablehlo_mlir_module: the result of JAX lowering to StableHLO.
       partitioning_result: the result of partitioning.
       jax_fn_info: See `utils.JaxFunctionInfo`.
+      jax_fn_info: See `utils.JaxFunctionInfo`.
       args_info: A PyTree of `ArgInfo` corresponding to the input PyTree.
       lowering_metadata: lowering metrics and additional information.
       topology: the MPMD topology used.
@@ -435,62 +439,13 @@ class MpmdLowered(stages.Lowered):
 
     self.mpmd_module = partitioning_result.mpmd_module
 
-  def _get_compile_options(
+  def _compile_pathways(
       self,
       compiler_options: (
           stages.CompilerOptions | mpmd_types.MeshToCompileOptions | None
       ) = None,
-  ) -> dict[str, jax.stages.CompilerOptions]:
-    option_overrides = {}
-    if compiler_options:
-      if any(isinstance(v, dict) for v in compiler_options.values()):
-        if not all(isinstance(v, dict) for v in compiler_options.values()):
-          raise ValueError(
-              '`compiler_options` must either be a dict of CompilerOptions or a'
-              ' single CompilerOptions object.'
-          )
-        bad_mesh_names = set(compiler_options) - set(self.topology)
-        if bad_mesh_names:
-          raise ValueError(
-              f'Received compiler_options for mesh: {",".join(bad_mesh_names)},'
-              ' which is not defined by the topology.'
-          )
-      else:
-        compiler_options = {k: compiler_options for k in self.topology}
-      compiler_options = cast(
-          mpmd_types.MeshToCompileOptions, compiler_options
-      )  # for pytype.
-      for mesh_name, env_option_overrides in compiler_options.items():
-        option_overrides[mesh_name] = list(env_option_overrides.items())
-    return jaxlib_mpmd.get_compile_options(
-        self.partitioning_result.ifrt_ir_module, option_overrides
-    )
-
-  def compile(
-      self,
-      compiler_options: (
-          stages.CompilerOptions | mpmd_types.MeshToCompileOptions | None
-      ) = None,
-      device_assignment=None,
+      device_assignment: tuple[jax.Device, ...] | None = None,
   ) -> MpmdCompiled:
-    """See base class.
-
-    Args:
-      compiler_options: An optional set of compiler options. If a single
-        CompilerOptions object is provided, then we use that to override for
-        every mesh. If a dict is provided, then every mesh name must be part of
-        the mpmd topology and if the user doesn't set compiler options for a
-        given mesh, the compiler will use defaults.
-      device_assignment: An optional sequence of devices to use for compilation.
-        This argument can be used to compile a lowered MPMD computation for
-        different devices.
-
-    Returns:
-      A `jax.stages.Compiled` object.
-
-    Raises:
-      ValueError: if the function has already been compiled.
-    """
     in_avals = jax.tree.map(lambda x: x._aval, self.args_info)  # pylint: disable=protected-access
     flat_in_avals = jax.tree.leaves(in_avals)
     flat_out_avals = self.global_flat_output_abstract_values
@@ -530,17 +485,13 @@ class MpmdLowered(stages.Lowered):
       )
       flat_out_shardings = [
           jax.sharding.NamedSharding(
-              lowered_mesh_to_compiled_mesh[s.mesh],
-              s.spec,
-              memory_kind=s.memory_kind,
+              lowered_mesh_to_compiled_mesh[s.mesh], s.spec, s.memory_kind  # type: ignore
           )
           for s in flat_out_shardings
       ]
       in_shardings = jax.tree.map(
           lambda s: jax.sharding.NamedSharding(
-              lowered_mesh_to_compiled_mesh[s.mesh],
-              s.spec,
-              memory_kind=s.memory_kind,
+              lowered_mesh_to_compiled_mesh[s.mesh], s.spec, s.memory_kind  # type: ignore
           ),
           self.function_named_shardings.input_specs,
       )
@@ -551,14 +502,15 @@ class MpmdLowered(stages.Lowered):
     compiled_ifrt_module = jaxlib_mpmd.clone_mlir_module(
         self.partitioning_result.ifrt_ir_module
     )
+    backend_py = device_assignment[0].client
     program_executable = jaxlib_mpmd.compile_mpmd(
-        backend=device_assignment[0].client,
-        ifrt_mlir_module=compiled_ifrt_module,
-        devices=device_assignment,
-        out_avals=flat_out_avals,
-        out_shardings=flat_out_shardings,
-        xla_compile_options=self._get_compile_options(compiler_options),
-        loaded_executable_bindings={},
+        backend_py,
+        compiled_ifrt_module,
+        device_assignment,
+        flat_out_avals,
+        flat_out_shardings,
+        self._get_compile_options(compiler_options),
+        {},
     )
     executable = MpmdExecutable(
         program_executable,
@@ -579,6 +531,84 @@ class MpmdLowered(stages.Lowered):
         self.no_kwargs,
     )
 
+  def _get_compile_options(
+      self,
+      compiler_options: (
+          stages.CompilerOptions | mpmd_types.MeshToCompileOptions | None
+      ) = None,
+  ) -> dict[str, jax.stages.CompilerOptions]:
+    option_overrides = {}
+    if compiler_options:
+      if any(isinstance(v, dict) for v in compiler_options.values()):
+        if not all(isinstance(v, dict) for v in compiler_options.values()):
+          raise ValueError(
+              '`compiler_options` must either be a dict of CompilerOptions or a'
+              ' single CompilerOptions object.'
+          )
+        bad_mesh_names = set(compiler_options) - set(self.topology)
+        if bad_mesh_names:
+          raise ValueError(
+              f'Received compiler_options for mesh: {",".join(bad_mesh_names)},'
+              ' which is not defined by the topology.'
+          )
+      else:
+        compiler_options = {k: compiler_options for k in self.topology}
+      compiler_options = cast(
+          mpmd_types.MeshToCompileOptions, compiler_options
+      )  # for pytype.
+      for mesh_name, env_option_overrides in compiler_options.items():
+        option_overrides[mesh_name] = list(env_option_overrides.items())
+    return jaxlib_mpmd.get_compile_options(
+        self.partitioning_result.ifrt_ir_module, option_overrides
+    )
+
+  def compile(
+      self,
+      compiler_options: (
+          stages.CompilerOptions | mpmd_types.MeshToCompileOptions | None
+      ) = None,
+      device_assignment=None,
+  ) -> MpmdCompiled:
+    """See base class.
+
+    Args:
+      compiler_options: An optional set of compiler options. If a single
+        CompilerOptions object is provided, then we use that to override for
+        every mesh. If a dict is provided, then every mesh name must be part of
+        the mpmd topology and if the user doesn't set compiler options for a
+        given mesh, the compiler will use defaults.
+      device_assignment: An optional sequence of devices to use for compilation.
+        This argument can be used to compile a lowered MPMD computation for
+        different devices.
+
+    Returns:
+      A `jax.stages.Compiled` object.
+
+    Raises:
+      ValueError: if the function has already been compiled.
+      NotImplementedError: if the backend is not supported, or if the backend is
+        MLCR and Shardy partitioner is not enabled.
+    """
+    available_backends = tuple(jax_backend.backends().keys())
+    if 'pathways' in available_backends:
+      return self._compile_pathways(compiler_options, device_assignment)
+    # TODO(b/428206925): Remove sliceme once the backend is fully deprecated.
+    # See go/mlcr for more details.
+    if 'sliceme' in available_backends or 'mlcr' in available_backends:
+      if jax.config.jax_use_shardy_partitioner:
+        return self._compile_pathways(compiler_options, device_assignment)
+      else:
+        raise NotImplementedError(
+            'MLCR backends only supports Shardy partitioning.'
+            ' Please set `jax.config.jax_use_shardy_partitioner` to True.'
+        )
+    raise NotImplementedError(
+        'MPMD functions can only be compiled through Pathways, but only the'
+        f' following backends are available: {available_backends}.'
+        ' Make sure you have one of the following backends available:'
+        ' pathways, mlcr.'
+    )
+
   def as_text(
       self, dialect: str | None = None, *, debug_info: bool = False
   ) -> str:
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 509398d..df128d9 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1 +1,961 @@
 Auto generated patch. Do not edit or delete it, even if empty.
+diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaType.cpp b/clang/lib/Sema/SemaType.cpp
+--- a/clang/lib/Sema/SemaType.cpp
++++ b/clang/lib/Sema/SemaType.cpp
+@@ -9309,59 +9309,11 @@
+ 
+   // If this definition was instantiated from a template, map back to the
+   // pattern from which it was instantiated.
+-  if (isa<TagDecl>(D) && cast<TagDecl>(D)->isBeingDefined())
++  if (isa<TagDecl>(D) && cast<TagDecl>(D)->isBeingDefined()) {
+     // We're in the middle of defining it; this definition should be treated
+     // as visible.
+     return true;
+-
+-  auto DefinitionIsAcceptable = [&](NamedDecl *D) {
+-    // The (primary) definition might be in a visible module.
+-    if (isAcceptable(D, Kind))
+-      return true;
+-
+-    // A visible module might have a merged definition instead.
+-    if (D->isModulePrivate() ? hasMergedDefinitionInCurrentModule(D)
+-                             : hasVisibleMergedDefinition(D)) {
+-      if (CodeSynthesisContexts.empty() &&
+-          !getLangOpts().ModulesLocalVisibility) {
+-        // Cache the fact that this definition is implicitly visible because
+-        // there is a visible merged definition.
+-        D->setVisibleDespiteOwningModule();
+-      }
+-      return true;
+-    }
+-
+-    return false;
+-  };
+-  auto IsDefinition = [](NamedDecl *D) {
+-    if (auto *RD = dyn_cast<CXXRecordDecl>(D))
+-      return RD->isThisDeclarationADefinition();
+-    if (auto *ED = dyn_cast<EnumDecl>(D))
+-      return ED->isThisDeclarationADefinition();
+-    if (auto *FD = dyn_cast<FunctionDecl>(D))
+-      return FD->isThisDeclarationADefinition();
+-    if (auto *VD = dyn_cast<VarDecl>(D))
+-      return VD->isThisDeclarationADefinition() == VarDecl::Definition;
+-    llvm_unreachable("unexpected decl type");
+-  };
+-  auto FoundAcceptableDefinition = [&](NamedDecl *D) {
+-    if (!isa<CXXRecordDecl, FunctionDecl, EnumDecl, VarDecl>(D))
+-      return DefinitionIsAcceptable(D);
+-
+-    for (auto *RD : D->redecls()) {
+-      auto *ND = cast<NamedDecl>(RD);
+-      if (!IsDefinition(ND))
+-        continue;
+-      if (DefinitionIsAcceptable(ND)) {
+-        *Suggested = ND;
+-        return true;
+-      }
+-    }
+-
+-    return false;
+-  };
+-
+-  if (auto *RD = dyn_cast<CXXRecordDecl>(D)) {
++  } else if (auto *RD = dyn_cast<CXXRecordDecl>(D)) {
+     if (auto *Pattern = RD->getTemplateInstantiationPattern())
+       RD = Pattern;
+     D = RD->getDefinition();
+@@ -9400,14 +9352,34 @@
+ 
+   *Suggested = D;
+ 
+-  if (FoundAcceptableDefinition(D))
++  auto DefinitionIsAcceptable = [&] {
++    // The (primary) definition might be in a visible module.
++    if (isAcceptable(D, Kind))
++      return true;
++
++    // A visible module might have a merged definition instead.
++    if (D->isModulePrivate() ? hasMergedDefinitionInCurrentModule(D)
++                             : hasVisibleMergedDefinition(D)) {
++      if (CodeSynthesisContexts.empty() &&
++          !getLangOpts().ModulesLocalVisibility) {
++        // Cache the fact that this definition is implicitly visible because
++        // there is a visible merged definition.
++        D->setVisibleDespiteOwningModule();
++      }
++      return true;
++    }
++
++    return false;
++  };
++
++  if (DefinitionIsAcceptable())
+     return true;
+ 
+   // The external source may have additional definitions of this entity that are
+   // visible, so complete the redeclaration chain now and ask again.
+   if (auto *Source = Context.getExternalSource()) {
+     Source->CompleteRedeclChain(D);
+-    return FoundAcceptableDefinition(D);
++    return DefinitionIsAcceptable();
+   }
+ 
+   return false;
+diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTReaderDecl.cpp b/clang/lib/Serialization/ASTReaderDecl.cpp
+--- a/clang/lib/Serialization/ASTReaderDecl.cpp
++++ b/clang/lib/Serialization/ASTReaderDecl.cpp
+@@ -3642,9 +3642,23 @@
+ void ASTDeclReader::attachPreviousDeclImpl(ASTReader &Reader,
+                                            Redeclarable<VarDecl> *D,
+                                            Decl *Previous, Decl *Canon) {
++  auto *VD = static_cast<VarDecl *>(D);
+   auto *PrevVD = cast<VarDecl>(Previous);
+   D->RedeclLink.setPrevious(PrevVD);
+   D->First = PrevVD->First;
++
++  // We should keep at most one definition on the chain.
++  // FIXME: Cache the definition once we've found it. Building a chain with
++  // N definitions currently takes O(N^2) time here.
++  if (VD->isThisDeclarationADefinition() == VarDecl::Definition) {
++    for (VarDecl *CurD = PrevVD; CurD; CurD = CurD->getPreviousDecl()) {
++      if (CurD->isThisDeclarationADefinition() == VarDecl::Definition) {
++        Reader.mergeDefinitionVisibility(CurD, VD);
++        VD->demoteThisDefinitionToDeclaration();
++        break;
++      }
++    }
++  }
+ }
+ 
+ static bool isUndeducedReturnType(QualType T) {
+diff -ruN --strip-trailing-cr a/clang/test/Modules/demote-var-def.cpp b/clang/test/Modules/demote-var-def.cpp
+--- a/clang/test/Modules/demote-var-def.cpp
++++ b/clang/test/Modules/demote-var-def.cpp
+@@ -1,94 +0,0 @@
+-// RUN: rm -rf %t
+-// RUN: mkdir -p %t
+-// RUN: split-file %s %t
+-// RUN: cd %t
+-//
+-// DEFINE: %{common-flags}= -I %t -isystem %t -xc++ -std=c++20 -fmodules
+-//
+-// RUN: mkdir -p %t/b2
+-// RUN: mkdir -p %t/b1
+-// RUN: %clang_cc1 %{common-flags} -emit-module -fmodule-name=module_d \
+-// RUN:     d.cppmap -o d.pcm
+-// RUN: %clang_cc1 %{common-flags} -emit-module -fmodule-name=module_a \
+-// RUN:     -fmodule-file=d.pcm  a.cppmap -o a.pcm
+-// RUN: %clang_cc1 %{common-flags} -emit-module -fmodule-name=module_b2 \
+-// RUN:     -fmodule-file=a.pcm b2/b.cppmap -o b2/b.pcm
+-// RUN: %clang_cc1 %{common-flags} -emit-module -fmodule-name=module_b1 \
+-// RUN:     -fmodule-file=b2/b.pcm b1/b.cppmap -o b1/b.pcm
+-// RUN: %clang_cc1 %{common-flags} -emit-module -fmodule-name=module_f \
+-// RUN:     -fmodule-file=b1/b.pcm f.cppmap -o f.pcm
+-// RUN: %clang_cc1 %{common-flags} -emit-module -fmodule-name=module_c \
+-// RUN:     -fmodule-file=f.pcm c.cppmap -o c.pcm
+-// RUN: %clang_cc1 %{common-flags} -emit-module \
+-// RUN:     -fmodule-name=module_e e.cppmap -o e.pcm
+-//
+-// RUN: %clang_cc1 %{common-flags} \
+-// RUN:     -fmodule-file=c.pcm -fmodule-file=e.pcm \
+-// RUN:     src.cpp -o src.pic.o
+-
+-//--- invoke.h
+-#ifndef _LIBCPP___TYPE_TRAITS_IS_SAME_H
+-#define _LIBCPP___TYPE_TRAITS_IS_SAME_H
+-namespace std { inline namespace _LIBCPP_ABI_NAMESPACE {
+-template <class _Tp, class _Up>
+-constexpr bool is_same_v = __is_same(_Tp, _Up);
+-} }
+-#endif
+-
+-//--- memory
+-#include <invoke.h>
+-namespace std { inline namespace _LIBCPP_ABI_NAMESPACE {
+-template <class _Tp>
+-using __decay_t = __decay(_Tp);
+-template <class _Tp>
+-using decay_t = __decay_t<_Tp>;
+-} }
+-
+-//--- other.h
+-#include <invoke.h>
+-
+-//--- a.cppmap
+-module "module_a" {
+-}
+-
+-//--- b1/b.cppmap
+-module "module_b1" {
+-}
+-
+-//--- b2/b.cppmap
+-module "module_b2" {
+-}
+-
+-//--- c.cppmap
+-module "module_c" {
+-}
+-
+-//--- d.cppmap
+-module "module_d" {
+-    header "d.h"
+-}
+-
+-//--- d.h
+-#include <other.h>
+-
+-//--- e.cppmap
+-module "module_e" {
+-    header "e.h"
+-}
+-
+-//--- e.h
+-#include <memory>
+-
+-//--- f.cppmap
+-module "module_f" {
+-}
+-
+-//--- src.cpp
+-#include <d.h>
+-#include <memory>
+-template <typename T>
+-concept coroutine_result =
+-    std::is_same_v<std::decay_t<T>, T>;
+-template <coroutine_result R>
+-class Co;
+-using T = Co<void>;
+diff -ruN --strip-trailing-cr a/clang/test/Modules/pr149404-02.cppm b/clang/test/Modules/pr149404-02.cppm
+--- a/clang/test/Modules/pr149404-02.cppm
++++ b/clang/test/Modules/pr149404-02.cppm
+@@ -1,104 +0,0 @@
+-// RUN: rm -rf %t
+-// RUN: mkdir -p %t
+-// RUN: split-file %s %t
+-
+-// RUN: %clang_cc1 -std=c++20 -emit-module-interface -o %t/format.pcm %t/format.cppm
+-// RUN: %clang_cc1 -std=c++20  -emit-module-interface -o %t/includes_in_gmf.pcm %t/includes_in_gmf.cppm
+-// RUN: %clang_cc1 -std=c++20 -fprebuilt-module-path=%t %t/test.cpp -verify -fsyntax-only
+-
+-// RUN: %clang_cc1 -std=c++20 -emit-reduced-module-interface -o %t/format.pcm %t/format.cppm
+-// RUN: %clang_cc1 -std=c++20  -emit-reduced-module-interface -o %t/includes_in_gmf.pcm %t/includes_in_gmf.cppm
+-// RUN: %clang_cc1 -std=c++20 -fprebuilt-module-path=%t %t/test.cpp -verify -fsyntax-only
+-
+-//--- format.h
+-#pragma once
+-
+-namespace test {
+-
+-template <class _Tp>
+-struct type_identity {
+-    typedef _Tp type;
+-};
+-
+-template <class _Tp>
+-using type_identity_t = typename type_identity<_Tp>::type;
+-
+-
+-template <class _Tp, class _CharT>
+-struct formatter
+-{
+-    formatter() = delete;
+-};
+-
+-template <>
+-struct formatter<char, char>
+-{};
+-
+-template <class _CharT, class... _Args>
+-struct basic_format_string {
+-    static inline const int __handles_{ [] {
+-        formatter<char, _CharT> f;
+-        (void)f;
+-        return 0;
+-        }() };
+-    
+-    consteval basic_format_string(const _CharT*) {
+-        (void)__handles_;
+-    }
+-};
+-
+-template <class... _Args>
+-using wformat_string = basic_format_string<wchar_t, type_identity_t<_Args>...>;
+-
+-template <class... _Args>
+-using format_string = basic_format_string<char, type_identity_t<_Args>...>;
+-
+-template <class... _Args>
+-void format(format_string<_Args...> __fmt, _Args&&... __args) {}
+-
+-template <class... _Args>
+-void format(wformat_string<_Args...> __fmt, _Args&&... __args) {}
+-
+-}
+-
+-//--- format.cppm
+-module;
+-#include "format.h"
+-export module format;
+-
+-export namespace test {
+-	using test::format;
+-	using test::formatter;
+-	using test::format_string;
+-}
+-
+-auto something() -> void
+-{
+-	auto a = 'a';
+-	test::format("{}", a);
+-}
+-
+-//--- includes_in_gmf.cppm
+-module;
+-#include "format.h"
+-export module includes_in_gmf;
+-
+-namespace test {
+-	using test::format;
+-	using test::formatter;
+-	using test::format_string;
+-}
+-
+-//--- test.cpp
+-// expected-no-diagnostics
+-import format;
+-import includes_in_gmf;
+-
+-auto what() -> void
+-{
+-    auto a = 'a';
+-    test::format("{}", a);
+-
+-    constexpr auto fs = "{}"; // test::format_string<char>{ "{}" }; // <- same result even passing exact param type
+-    test::format(fs, 'r');
+-}
+diff -ruN --strip-trailing-cr a/clang/test/Modules/pr172241.cppm b/clang/test/Modules/pr172241.cppm
+--- a/clang/test/Modules/pr172241.cppm
++++ b/clang/test/Modules/pr172241.cppm
+@@ -1,47 +0,0 @@
+-// RUN: rm -rf %t
+-// RUN: mkdir -p %t
+-// RUN: split-file %s %t
+-//
+-// RUN: %clang_cc1 -std=c++20 -triple %itanium_abi_triple %t/m.cppm -emit-module-interface -o %t/m.pcm
+-// RUN: %clang_cc1 -std=c++20 -triple %itanium_abi_triple %t/use.cpp -fmodule-file=m=%t/m.pcm -emit-llvm -o - | FileCheck %t/use.cpp
+-//
+-// RUN: %clang_cc1 -std=c++20 -triple %itanium_abi_triple %t/m.cppm -emit-reduced-module-interface -o %t/m.pcm
+-// RUN: %clang_cc1 -std=c++20 -triple %itanium_abi_triple %t/use.cpp -fmodule-file=m=%t/m.pcm -emit-llvm -o - | FileCheck %t/use.cpp
+-
+-//--- header.h
+-#pragma once
+-
+-template <unsigned T>
+-class Templ {
+-public:
+-    void lock() { __set_locked_bit(); }
+-
+-private:
+-    static constexpr auto __set_locked_bit = [](){};
+-};
+-
+-class JT {
+-public:
+-    ~JT() {
+-        Templ<4> state;
+-        state.lock();
+-    }
+-};
+-
+-//--- m.cppm
+-module;
+-#include "header.h"
+-export module m;
+-export struct M {
+-    JT jt;
+-};
+-//--- use.cpp
+-#include "header.h"
+-import m;
+-
+-int main() {
+-    M m;
+-    return 0;
+-}
+-
+-// CHECK: @_ZN5TemplILj4EE16__set_locked_bitE = {{.*}}linkonce_odr
+diff -ruN --strip-trailing-cr a/clang/test/Modules/var-inst-def.cppm b/clang/test/Modules/var-inst-def.cppm
+--- a/clang/test/Modules/var-inst-def.cppm
++++ b/clang/test/Modules/var-inst-def.cppm
+@@ -1,110 +0,0 @@
+-// RUN: rm -rf %t
+-// RUN: mkdir -p %t
+-// RUN: split-file %s %t
+-// RUN: cd %t
+-//
+-// RUN: %clang_cc1 -fmodule-name=A -xc++ -emit-module -fmodules \
+-// RUN:   -fno-cxx-modules -fno-implicit-modules \
+-// RUN:   -fmodule-map-file-home-is-cwd -std=c++20 -I. a.modulemap -o a.pcm
+-//
+-// RUN: %clang_cc1 -fmodule-name=B -xc++ -emit-module -fmodules \
+-// RUN:   -fno-cxx-modules -fno-implicit-modules \
+-// RUN:   -fmodule-map-file-home-is-cwd -std=c++20 -I. b.modulemap -o b.pcm
+-//
+-// RUN: %clang_cc1 -fmodule-name=C -xc++ -emit-module -fmodules \
+-// RUN:   -fno-cxx-modules -fno-implicit-modules \
+-// RUN:   -fmodule-map-file-home-is-cwd -std=c++20 -I. c.modulemap -o c.pcm
+-//
+-// RUN: %clang_cc1 -fno-cxx-modules -fmodules -fno-implicit-modules \
+-// RUN:   -fmodule-map-file-home-is-cwd \
+-// RUN:   -fmodule-file=a.pcm -fmodule-file=b.pcm -fmodule-file=c.pcm \
+-// RUN:   -std=c++20 -I. main.cpp -o /dev/null
+-
+-//--- a.modulemap
+-module "A" { header "a.h" }
+-//--- b.modulemap
+-module "B" { header "b.h" }
+-//--- c.modulemap
+-module "C" { header "c.h" }
+-
+-//--- common.h
+-#pragma once
+-#include "stl.h"
+-
+-//--- a.h
+-#pragma once
+-#include "common.h"
+-#include "repro.h"
+-
+-//--- b.h
+-#pragma once
+-#include "common.h"
+-#include "repro.h"
+-
+-//--- c.h
+-#pragma once
+-#include "common.h"
+-#include "repro.h"
+-
+-//--- repro.h
+-#pragma once
+-#include "stl.h"
+-
+-namespace k {
+-template <template <typename> class , typename >
+-struct is_instantiation : std::integral_constant<bool, false> {};
+-template <template <typename> class C, typename T>
+-constexpr bool is_instantiation_v = is_instantiation<C, T>::value;
+-}  
+-
+-struct ThreadState;
+-
+-namespace cc::subtle {
+-template <typename T>
+-class U;
+-}  
+-namespace cc {
+-template <typename T> class Co;
+-namespace internal {
+-template <typename T>
+-class Promise {
+-  static_assert(!k::is_instantiation_v<subtle::U, T>);
+-};
+-}  
+-}
+-
+-//--- stl.h
+-#pragma once
+-namespace std {
+-inline namespace abi {
+-template <class _Tp, _Tp __v>
+-struct integral_constant {
+-  static const _Tp value = __v;
+-};
+-template <class _Tp, class _Up>
+-constexpr bool is_same_v = __is_same(_Tp, _Up);
+-template <class _Tp>
+-using decay_t = __decay(_Tp);
+-
+-template <class>
+-struct __invoke_result_impl ;
+-template <class... _Args>
+-using invoke_result_t = __invoke_result_impl<_Args...>;
+-}
+-}
+-
+-//--- main.cpp
+-#include "stl.h"
+-#include "a.h"
+-
+-namespace cc {
+-template <typename F>
+-  requires k::is_instantiation_v<Co, std::invoke_result_t<F>>
+-using result_type =
+-    std::invoke_result_t<F>;
+-}  
+-namespace cc::internal {
+-class final {
+- Promise<ThreadState> outgoing_work_;
+-};
+-}
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
+--- a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
++++ b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
+@@ -9223,7 +9223,23 @@
+         if (auto *VPI = dyn_cast<VPInstruction>(PhiR->getStartValue())) {
+           assert(VPI->getOpcode() == VPInstruction::ReductionStartVector &&
+                  "unexpected start value");
+-          VPI->setOperand(0, StartVal);
++          // Partial sub-reductions always start at 0 and account for the
++          // reduction start value in a final subtraction. Update it to use the
++          // resume value from the main vector loop.
++          if (PhiR->getVFScaleFactor() > 1 &&
++              PhiR->getRecurrenceKind() == RecurKind::Sub) {
++            auto *Sub = cast<VPInstruction>(RdxResult->getSingleUser());
++            assert(Sub->getOpcode() == Instruction::Sub && "Unexpected opcode");
++            assert(isa<VPIRValue>(Sub->getOperand(0)) &&
++                   "Expected operand to match the original start value of the "
++                   "reduction");
++            assert(VPlanPatternMatch::match(VPI->getOperand(0),
++                                            VPlanPatternMatch::m_ZeroInt()) &&
++                   "Expected start value for partial sub-reduction to start at "
++                   "zero");
++            Sub->setOperand(0, StartVal);
++          } else
++            VPI->setOperand(0, StartVal);
+           continue;
+         }
+       }
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/AArch64/partial-reduce-sub-epilogue-vec.ll b/llvm/test/Transforms/LoopVectorize/AArch64/partial-reduce-sub-epilogue-vec.ll
+--- a/llvm/test/Transforms/LoopVectorize/AArch64/partial-reduce-sub-epilogue-vec.ll
++++ b/llvm/test/Transforms/LoopVectorize/AArch64/partial-reduce-sub-epilogue-vec.ll
+@@ -0,0 +1,188 @@
++; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --check-globals none --version 6
++; RUN: opt -passes=loop-vectorize -force-vector-interleave=1 -enable-epilogue-vectorization=true -epilogue-vectorization-force-VF=4 -vectorizer-maximize-bandwidth -S < %s | FileCheck %s --check-prefix=CHECK-EPI
++; RUN: opt -passes=loop-vectorize -force-vector-interleave=1 -enable-epilogue-vectorization=true -epilogue-vectorization-force-VF=8 -vectorizer-maximize-bandwidth -S < %s | FileCheck %s --check-prefix=CHECK-PARTIAL-RED-EPI
++
++target triple = "aarch64"
++
++define i32 @sub_reduction(i32 %startval, ptr %src1, ptr %src2) #0 {
++; CHECK-EPI-LABEL: define i32 @sub_reduction(
++; CHECK-EPI-SAME: i32 [[STARTVAL:%.*]], ptr [[SRC1:%.*]], ptr [[SRC2:%.*]]) #[[ATTR0:[0-9]+]] {
++; CHECK-EPI-NEXT:  [[ITER_CHECK:.*]]:
++; CHECK-EPI-NEXT:    br i1 false, label %[[VEC_EPILOG_SCALAR_PH:.*]], label %[[VECTOR_MAIN_LOOP_ITER_CHECK:.*]]
++; CHECK-EPI:       [[VECTOR_MAIN_LOOP_ITER_CHECK]]:
++; CHECK-EPI-NEXT:    [[TMP0:%.*]] = call i32 @llvm.vscale.i32()
++; CHECK-EPI-NEXT:    [[TMP1:%.*]] = shl nuw i32 [[TMP0]], 4
++; CHECK-EPI-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i32 39, [[TMP1]]
++; CHECK-EPI-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[VEC_EPILOG_PH:.*]], label %[[VECTOR_PH:.*]]
++; CHECK-EPI:       [[VECTOR_PH]]:
++; CHECK-EPI-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vscale.i32()
++; CHECK-EPI-NEXT:    [[TMP3:%.*]] = shl nuw i32 [[TMP2]], 4
++; CHECK-EPI-NEXT:    [[N_MOD_VF:%.*]] = urem i32 39, [[TMP3]]
++; CHECK-EPI-NEXT:    [[N_VEC:%.*]] = sub i32 39, [[N_MOD_VF]]
++; CHECK-EPI-NEXT:    br label %[[VECTOR_BODY:.*]]
++; CHECK-EPI:       [[VECTOR_BODY]]:
++; CHECK-EPI-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
++; CHECK-EPI-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[PARTIAL_REDUCE:%.*]], %[[VECTOR_BODY]] ]
++; CHECK-EPI-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[SRC1]], i32 [[INDEX]]
++; CHECK-EPI-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 16 x i8>, ptr [[TMP4]], align 4
++; CHECK-EPI-NEXT:    [[WIDE_LOAD1:%.*]] = load <vscale x 16 x i8>, ptr [[TMP4]], align 4
++; CHECK-EPI-NEXT:    [[TMP5:%.*]] = sext <vscale x 16 x i8> [[WIDE_LOAD]] to <vscale x 16 x i32>
++; CHECK-EPI-NEXT:    [[TMP6:%.*]] = sext <vscale x 16 x i8> [[WIDE_LOAD1]] to <vscale x 16 x i32>
++; CHECK-EPI-NEXT:    [[TMP7:%.*]] = mul <vscale x 16 x i32> [[TMP5]], [[TMP6]]
++; CHECK-EPI-NEXT:    [[PARTIAL_REDUCE]] = call <vscale x 4 x i32> @llvm.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[VEC_PHI]], <vscale x 16 x i32> [[TMP7]])
++; CHECK-EPI-NEXT:    [[INDEX_NEXT]] = add nuw i32 [[INDEX]], [[TMP3]]
++; CHECK-EPI-NEXT:    [[TMP8:%.*]] = icmp eq i32 [[INDEX_NEXT]], [[N_VEC]]
++; CHECK-EPI-NEXT:    br i1 [[TMP8]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
++; CHECK-EPI:       [[MIDDLE_BLOCK]]:
++; CHECK-EPI-NEXT:    [[TMP9:%.*]] = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[PARTIAL_REDUCE]])
++; CHECK-EPI-NEXT:    [[TMP10:%.*]] = sub i32 [[STARTVAL]], [[TMP9]]
++; CHECK-EPI-NEXT:    [[CMP_N:%.*]] = icmp eq i32 39, [[N_VEC]]
++; CHECK-EPI-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[VEC_EPILOG_ITER_CHECK:.*]]
++; CHECK-EPI:       [[VEC_EPILOG_ITER_CHECK]]:
++; CHECK-EPI-NEXT:    [[MIN_EPILOG_ITERS_CHECK:%.*]] = icmp ult i32 [[N_MOD_VF]], 4
++; CHECK-EPI-NEXT:    br i1 [[MIN_EPILOG_ITERS_CHECK]], label %[[VEC_EPILOG_SCALAR_PH]], label %[[VEC_EPILOG_PH]], !prof [[PROF3:![0-9]+]]
++; CHECK-EPI:       [[VEC_EPILOG_PH]]:
++; CHECK-EPI-NEXT:    [[VEC_EPILOG_RESUME_VAL:%.*]] = phi i32 [ [[N_VEC]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 0, %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
++; CHECK-EPI-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ [[TMP10]], %[[VEC_EPILOG_ITER_CHECK]] ], [ [[STARTVAL]], %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
++; CHECK-EPI-NEXT:    [[TMP11:%.*]] = insertelement <4 x i32> zeroinitializer, i32 [[BC_MERGE_RDX]], i32 0
++; CHECK-EPI-NEXT:    br label %[[VEC_EPILOG_VECTOR_BODY:.*]]
++; CHECK-EPI:       [[VEC_EPILOG_VECTOR_BODY]]:
++; CHECK-EPI-NEXT:    [[INDEX2:%.*]] = phi i32 [ [[VEC_EPILOG_RESUME_VAL]], %[[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT6:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
++; CHECK-EPI-NEXT:    [[VEC_PHI3:%.*]] = phi <4 x i32> [ [[TMP11]], %[[VEC_EPILOG_PH]] ], [ [[TMP16:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
++; CHECK-EPI-NEXT:    [[TMP12:%.*]] = getelementptr i8, ptr [[SRC1]], i32 [[INDEX2]]
++; CHECK-EPI-NEXT:    [[WIDE_LOAD4:%.*]] = load <4 x i8>, ptr [[TMP12]], align 4
++; CHECK-EPI-NEXT:    [[TMP13:%.*]] = sext <4 x i8> [[WIDE_LOAD4]] to <4 x i32>
++; CHECK-EPI-NEXT:    [[WIDE_LOAD5:%.*]] = load <4 x i8>, ptr [[TMP12]], align 4
++; CHECK-EPI-NEXT:    [[TMP14:%.*]] = sext <4 x i8> [[WIDE_LOAD5]] to <4 x i32>
++; CHECK-EPI-NEXT:    [[TMP15:%.*]] = mul <4 x i32> [[TMP13]], [[TMP14]]
++; CHECK-EPI-NEXT:    [[TMP16]] = sub <4 x i32> [[VEC_PHI3]], [[TMP15]]
++; CHECK-EPI-NEXT:    [[INDEX_NEXT6]] = add nuw i32 [[INDEX2]], 4
++; CHECK-EPI-NEXT:    [[TMP17:%.*]] = icmp eq i32 [[INDEX_NEXT6]], 36
++; CHECK-EPI-NEXT:    br i1 [[TMP17]], label %[[VEC_EPILOG_MIDDLE_BLOCK:.*]], label %[[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
++; CHECK-EPI:       [[VEC_EPILOG_MIDDLE_BLOCK]]:
++; CHECK-EPI-NEXT:    [[TMP18:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[TMP16]])
++; CHECK-EPI-NEXT:    br i1 false, label %[[EXIT]], label %[[VEC_EPILOG_SCALAR_PH]]
++; CHECK-EPI:       [[VEC_EPILOG_SCALAR_PH]]:
++; CHECK-EPI-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i32 [ 36, %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ [[N_VEC]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 0, %[[ITER_CHECK]] ]
++; CHECK-EPI-NEXT:    [[BC_MERGE_RDX7:%.*]] = phi i32 [ [[TMP18]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ [[TMP10]], %[[VEC_EPILOG_ITER_CHECK]] ], [ [[STARTVAL]], %[[ITER_CHECK]] ]
++; CHECK-EPI-NEXT:    br label %[[LOOP:.*]]
++; CHECK-EPI:       [[LOOP]]:
++; CHECK-EPI-NEXT:    [[IV:%.*]] = phi i32 [ [[BC_RESUME_VAL]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
++; CHECK-EPI-NEXT:    [[ACCUM:%.*]] = phi i32 [ [[BC_MERGE_RDX7]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[SUB:%.*]], %[[LOOP]] ]
++; CHECK-EPI-NEXT:    [[SRC1_GEP:%.*]] = getelementptr i8, ptr [[SRC1]], i32 [[IV]]
++; CHECK-EPI-NEXT:    [[SRC1_LOAD:%.*]] = load i8, ptr [[SRC1_GEP]], align 4
++; CHECK-EPI-NEXT:    [[SRC1_LOAD_EXT:%.*]] = sext i8 [[SRC1_LOAD]] to i32
++; CHECK-EPI-NEXT:    [[SRC2_GEP:%.*]] = getelementptr i8, ptr [[SRC1]], i32 [[IV]]
++; CHECK-EPI-NEXT:    [[SRC2_LOAD:%.*]] = load i8, ptr [[SRC2_GEP]], align 4
++; CHECK-EPI-NEXT:    [[SRC2_LOAD_EXT:%.*]] = sext i8 [[SRC2_LOAD]] to i32
++; CHECK-EPI-NEXT:    [[MUL:%.*]] = mul i32 [[SRC1_LOAD_EXT]], [[SRC2_LOAD_EXT]]
++; CHECK-EPI-NEXT:    [[SUB]] = sub i32 [[ACCUM]], [[MUL]]
++; CHECK-EPI-NEXT:    [[IV_NEXT]] = add i32 [[IV]], 1
++; CHECK-EPI-NEXT:    [[EXITCOND_NOT:%.*]] = icmp eq i32 [[IV]], 38
++; CHECK-EPI-NEXT:    br i1 [[EXITCOND_NOT]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP5:![0-9]+]]
++; CHECK-EPI:       [[EXIT]]:
++; CHECK-EPI-NEXT:    [[SUB_LCSSA:%.*]] = phi i32 [ [[SUB]], %[[LOOP]] ], [ [[TMP10]], %[[MIDDLE_BLOCK]] ], [ [[TMP18]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ]
++; CHECK-EPI-NEXT:    ret i32 [[SUB_LCSSA]]
++;
++; CHECK-PARTIAL-RED-EPI-LABEL: define i32 @sub_reduction(
++; CHECK-PARTIAL-RED-EPI-SAME: i32 [[STARTVAL:%.*]], ptr [[SRC1:%.*]], ptr [[SRC2:%.*]]) #[[ATTR0:[0-9]+]] {
++; CHECK-PARTIAL-RED-EPI-NEXT:  [[ITER_CHECK:.*]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    br i1 false, label %[[VEC_EPILOG_SCALAR_PH:.*]], label %[[VECTOR_MAIN_LOOP_ITER_CHECK:.*]]
++; CHECK-PARTIAL-RED-EPI:       [[VECTOR_MAIN_LOOP_ITER_CHECK]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP0:%.*]] = call i32 @llvm.vscale.i32()
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP1:%.*]] = shl nuw i32 [[TMP0]], 4
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i32 39, [[TMP1]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[VEC_EPILOG_PH:.*]], label %[[VECTOR_PH:.*]]
++; CHECK-PARTIAL-RED-EPI:       [[VECTOR_PH]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vscale.i32()
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP3:%.*]] = shl nuw i32 [[TMP2]], 4
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[N_MOD_VF:%.*]] = urem i32 39, [[TMP3]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[N_VEC:%.*]] = sub i32 39, [[N_MOD_VF]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    br label %[[VECTOR_BODY:.*]]
++; CHECK-PARTIAL-RED-EPI:       [[VECTOR_BODY]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[PARTIAL_REDUCE:%.*]], %[[VECTOR_BODY]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[SRC1]], i32 [[INDEX]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 16 x i8>, ptr [[TMP4]], align 4
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[WIDE_LOAD1:%.*]] = load <vscale x 16 x i8>, ptr [[TMP4]], align 4
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP5:%.*]] = sext <vscale x 16 x i8> [[WIDE_LOAD]] to <vscale x 16 x i32>
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP6:%.*]] = sext <vscale x 16 x i8> [[WIDE_LOAD1]] to <vscale x 16 x i32>
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP7:%.*]] = mul <vscale x 16 x i32> [[TMP5]], [[TMP6]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[PARTIAL_REDUCE]] = call <vscale x 4 x i32> @llvm.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[VEC_PHI]], <vscale x 16 x i32> [[TMP7]])
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[INDEX_NEXT]] = add nuw i32 [[INDEX]], [[TMP3]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP8:%.*]] = icmp eq i32 [[INDEX_NEXT]], [[N_VEC]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    br i1 [[TMP8]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
++; CHECK-PARTIAL-RED-EPI:       [[MIDDLE_BLOCK]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP9:%.*]] = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[PARTIAL_REDUCE]])
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP10:%.*]] = sub i32 [[STARTVAL]], [[TMP9]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[CMP_N:%.*]] = icmp eq i32 39, [[N_VEC]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[VEC_EPILOG_ITER_CHECK:.*]]
++; CHECK-PARTIAL-RED-EPI:       [[VEC_EPILOG_ITER_CHECK]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[MIN_EPILOG_ITERS_CHECK:%.*]] = icmp ult i32 [[N_MOD_VF]], 8
++; CHECK-PARTIAL-RED-EPI-NEXT:    br i1 [[MIN_EPILOG_ITERS_CHECK]], label %[[VEC_EPILOG_SCALAR_PH]], label %[[VEC_EPILOG_PH]], !prof [[PROF3:![0-9]+]]
++; CHECK-PARTIAL-RED-EPI:       [[VEC_EPILOG_PH]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[VEC_EPILOG_RESUME_VAL:%.*]] = phi i32 [ [[N_VEC]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 0, %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ [[TMP10]], %[[VEC_EPILOG_ITER_CHECK]] ], [ [[STARTVAL]], %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    br label %[[VEC_EPILOG_VECTOR_BODY:.*]]
++; CHECK-PARTIAL-RED-EPI:       [[VEC_EPILOG_VECTOR_BODY]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[INDEX2:%.*]] = phi i32 [ [[VEC_EPILOG_RESUME_VAL]], %[[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT7:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[VEC_PHI3:%.*]] = phi <2 x i32> [ zeroinitializer, %[[VEC_EPILOG_PH]] ], [ [[PARTIAL_REDUCE6:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP12:%.*]] = getelementptr i8, ptr [[SRC1]], i32 [[INDEX2]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[WIDE_LOAD4:%.*]] = load <8 x i8>, ptr [[TMP12]], align 4
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[WIDE_LOAD5:%.*]] = load <8 x i8>, ptr [[TMP12]], align 4
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP13:%.*]] = sext <8 x i8> [[WIDE_LOAD4]] to <8 x i32>
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP14:%.*]] = sext <8 x i8> [[WIDE_LOAD5]] to <8 x i32>
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP15:%.*]] = mul <8 x i32> [[TMP13]], [[TMP14]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[PARTIAL_REDUCE6]] = call <2 x i32> @llvm.vector.partial.reduce.add.v2i32.v8i32(<2 x i32> [[VEC_PHI3]], <8 x i32> [[TMP15]])
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[INDEX_NEXT7]] = add nuw i32 [[INDEX2]], 8
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP16:%.*]] = icmp eq i32 [[INDEX_NEXT7]], 32
++; CHECK-PARTIAL-RED-EPI-NEXT:    br i1 [[TMP16]], label %[[VEC_EPILOG_MIDDLE_BLOCK:.*]], label %[[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
++; CHECK-PARTIAL-RED-EPI:       [[VEC_EPILOG_MIDDLE_BLOCK]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP17:%.*]] = call i32 @llvm.vector.reduce.add.v2i32(<2 x i32> [[PARTIAL_REDUCE6]])
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[TMP18:%.*]] = sub i32 [[BC_MERGE_RDX]], [[TMP17]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    br i1 false, label %[[EXIT]], label %[[VEC_EPILOG_SCALAR_PH]]
++; CHECK-PARTIAL-RED-EPI:       [[VEC_EPILOG_SCALAR_PH]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i32 [ 32, %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ [[N_VEC]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 0, %[[ITER_CHECK]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[BC_MERGE_RDX8:%.*]] = phi i32 [ [[TMP18]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ [[STARTVAL]], %[[VEC_EPILOG_ITER_CHECK]] ], [ [[STARTVAL]], %[[ITER_CHECK]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    br label %[[LOOP:.*]]
++; CHECK-PARTIAL-RED-EPI:       [[LOOP]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[IV:%.*]] = phi i32 [ [[BC_RESUME_VAL]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[ACCUM:%.*]] = phi i32 [ [[BC_MERGE_RDX8]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[SUB:%.*]], %[[LOOP]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[SRC1_GEP:%.*]] = getelementptr i8, ptr [[SRC1]], i32 [[IV]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[SRC1_LOAD:%.*]] = load i8, ptr [[SRC1_GEP]], align 4
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[SRC1_LOAD_EXT:%.*]] = sext i8 [[SRC1_LOAD]] to i32
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[SRC2_GEP:%.*]] = getelementptr i8, ptr [[SRC1]], i32 [[IV]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[SRC2_LOAD:%.*]] = load i8, ptr [[SRC2_GEP]], align 4
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[SRC2_LOAD_EXT:%.*]] = sext i8 [[SRC2_LOAD]] to i32
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[MUL:%.*]] = mul i32 [[SRC1_LOAD_EXT]], [[SRC2_LOAD_EXT]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[SUB]] = sub i32 [[ACCUM]], [[MUL]]
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[IV_NEXT]] = add i32 [[IV]], 1
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[EXITCOND_NOT:%.*]] = icmp eq i32 [[IV]], 38
++; CHECK-PARTIAL-RED-EPI-NEXT:    br i1 [[EXITCOND_NOT]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP5:![0-9]+]]
++; CHECK-PARTIAL-RED-EPI:       [[EXIT]]:
++; CHECK-PARTIAL-RED-EPI-NEXT:    [[SUB_LCSSA:%.*]] = phi i32 [ [[SUB]], %[[LOOP]] ], [ [[TMP10]], %[[MIDDLE_BLOCK]] ], [ [[TMP18]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ]
++; CHECK-PARTIAL-RED-EPI-NEXT:    ret i32 [[SUB_LCSSA]]
++;
++entry:
++  br label %loop
++
++loop:
++  %iv = phi i32 [ 0, %entry ], [ %iv.next, %loop ]
++  %accum = phi i32 [ %startval, %entry ], [ %sub, %loop ]
++  %src1.gep = getelementptr i8, ptr %src1, i32 %iv
++  %src1.load = load i8, ptr %src1.gep, align 4
++  %src1.load.ext = sext i8 %src1.load to i32
++  %src2.gep = getelementptr i8, ptr %src1, i32 %iv
++  %src2.load = load i8, ptr %src2.gep, align 4
++  %src2.load.ext = sext i8 %src2.load to i32
++  %mul = mul i32 %src1.load.ext, %src2.load.ext
++  %sub = sub i32 %accum, %mul
++  %iv.next = add i32 %iv, 1
++  %exitcond.not = icmp eq i32 %iv, 38
++  br i1 %exitcond.not, label %exit, label %loop
++
++exit:
++  ret i32 %sub
++}
++
++attributes #0 = { vscale_range(1,16) "target-features"="+sve" }
+diff -ruN --strip-trailing-cr a/mlir/include/mlir/Bytecode/BytecodeImplementation.h b/mlir/include/mlir/Bytecode/BytecodeImplementation.h
+--- a/mlir/include/mlir/Bytecode/BytecodeImplementation.h
++++ b/mlir/include/mlir/Bytecode/BytecodeImplementation.h
+@@ -398,6 +398,10 @@
+   /// written as-is, with no additional compression or compaction.
+   virtual void writeOwnedBlob(ArrayRef<char> blob) = 0;
+ 
++  /// Write a blob to the bytecode, which is not owned by the caller. The blob
++  /// is copied into the bytecode, and need not strictly outlive the call.
++  virtual void writeUnownedBlob(ArrayRef<char> blob) = 0;
++
+   /// Write a bool to the output stream.
+   virtual void writeOwnedBool(bool value) = 0;
+ 
+diff -ruN --strip-trailing-cr a/mlir/include/mlir/IR/BuiltinDialectBytecode.td b/mlir/include/mlir/IR/BuiltinDialectBytecode.td
+--- a/mlir/include/mlir/IR/BuiltinDialectBytecode.td
++++ b/mlir/include/mlir/IR/BuiltinDialectBytecode.td
+@@ -175,10 +175,11 @@
+ def DenseElementsAttr : WithType<"DenseElementsAttr", Attribute>;
+ def DenseIntOrFPElementsAttr : DialectAttribute<(attr
+   ShapedType:$type,
+-  Blob:$rawData
+-)> {
+-  let cBuilder = "cast<$_resultType>($_resultType::getFromRawBuffer($_args))";
+-}
++  WithBuilder<"$_args",
++    WithType<"SmallVector<char>",
++    WithParser<"succeeded(readDenseIntOrFPElementsAttr($_reader, type, $_var))",
++    WithPrinter<"writeDenseIntOrFPElementsAttr($_writer, $_name)">>>>:$rawData
++)>;
+ 
+ def DenseStringElementsAttr : DialectAttribute<(attr
+   ShapedType:$type,
+diff -ruN --strip-trailing-cr a/mlir/lib/Bytecode/Writer/BytecodeWriter.cpp b/mlir/lib/Bytecode/Writer/BytecodeWriter.cpp
+--- a/mlir/lib/Bytecode/Writer/BytecodeWriter.cpp
++++ b/mlir/lib/Bytecode/Writer/BytecodeWriter.cpp
+@@ -465,6 +465,14 @@
+         "dialect blob");
+   }
+ 
++  void writeUnownedBlob(ArrayRef<char> blob) override {
++    emitter.emitVarInt(blob.size(), "dialect blob");
++    emitter.emitBytes(
++        ArrayRef<uint8_t>(reinterpret_cast<const uint8_t *>(blob.data()),
++                          blob.size()),
++        "dialect blob");
++  }
++
+   void writeOwnedBool(bool value) override {
+     emitter.emitByte(value, "dialect bool");
+   }
+diff -ruN --strip-trailing-cr a/mlir/lib/Bytecode/Writer/IRNumbering.cpp b/mlir/lib/Bytecode/Writer/IRNumbering.cpp
+--- a/mlir/lib/Bytecode/Writer/IRNumbering.cpp
++++ b/mlir/lib/Bytecode/Writer/IRNumbering.cpp
+@@ -50,6 +50,7 @@
+   }
+   void writeOwnedBlob(ArrayRef<char> blob) override {}
+   void writeOwnedBool(bool value) override {}
++  void writeUnownedBlob(ArrayRef<char> blob) override {}
+ 
+   int64_t getBytecodeVersion() const override {
+     return state.getDesiredBytecodeVersion();
+diff -ruN --strip-trailing-cr a/mlir/lib/IR/BuiltinDialectBytecode.cpp b/mlir/lib/IR/BuiltinDialectBytecode.cpp
+--- a/mlir/lib/IR/BuiltinDialectBytecode.cpp
++++ b/mlir/lib/IR/BuiltinDialectBytecode.cpp
+@@ -148,6 +148,68 @@
+   writer.writeVarInt(range.getEndColumn());
+ }
+ 
++static LogicalResult
++readDenseIntOrFPElementsAttr(DialectBytecodeReader &reader, ShapedType type,
++                             SmallVectorImpl<char> &rawData) {
++  ArrayRef<char> blob;
++  if (failed(reader.readBlob(blob)))
++    return failure();
++
++  // If the type is not i1, just copy the blob.
++  if (!type.getElementType().isInteger(1)) {
++    rawData.append(blob.begin(), blob.end());
++    return success();
++  }
++
++  // Check to see if this is using the packed format.
++  // Note: this could be asserted instead as this should be the case. But we
++  // did have period where the unpacked was being serialized, this enables
++  // consuming those still and the check for which case we are in is pretty
++  // cheap.
++  size_t numElements = type.getNumElements();
++  size_t packedSize = llvm::divideCeil(numElements, 8);
++  if (blob.size() == packedSize && blob.size() != numElements &&
++      blob.size() != 1) {
++    // Unpack the blob.
++    rawData.resize(numElements);
++    for (size_t i = 0; i < numElements; ++i)
++      rawData[i] = (blob[i / 8] & (1 << (i % 8))) ? 0xFF : 0x00;
++    return success();
++  }
++  // Otherwise, fallback to the default behavior.
++  rawData.append(blob.begin(), blob.end());
++  return success();
++}
++
++static void writeDenseIntOrFPElementsAttr(DialectBytecodeWriter &writer,
++                                          DenseIntOrFPElementsAttr attr) {
++  // Check to see if this is an i1 dense attribute.
++  if (attr.getElementType().isInteger(1)) {
++    // Pack the data.
++    SmallVector<char> data;
++    ArrayRef<char> rawData = attr.getRawData();
++
++    // If the attribute is a splat, we can just splat the value directly.
++    if (attr.isSplat()) {
++      data.resize(1);
++      data[0] = rawData[0] ? 0xFF : 0x00;
++      writer.writeUnownedBlob(data);
++      return;
++    }
++
++    size_t numElements = attr.getNumElements();
++    data.resize(llvm::divideCeil(numElements, 8));
++    // Otherwise, pack the data manually.
++    for (size_t i = 0; i < numElements; ++i)
++      if (rawData[i])
++        data[i / 8] |= (1 << (i % 8));
++    writer.writeUnownedBlob(data);
++    return;
++  }
++
++  writer.writeOwnedBlob(attr.getRawData());
++}
++
+ #include "mlir/IR/BuiltinDialectBytecode.cpp.inc"
+ 
+ /// This class implements the bytecode interface for the builtin dialect.
+diff -ruN --strip-trailing-cr a/mlir/test/Bytecode/bytecode_callback_write_unowned_blob.mlir b/mlir/test/Bytecode/bytecode_callback_write_unowned_blob.mlir
+--- a/mlir/test/Bytecode/bytecode_callback_write_unowned_blob.mlir
++++ b/mlir/test/Bytecode/bytecode_callback_write_unowned_blob.mlir
+@@ -0,0 +1,9 @@
++// RUN: mlir-opt %s -split-input-file --test-bytecode-roundtrip="test-kind=7" | FileCheck %s
++
++func.func @base_test(%arg0: !test.i32) {
++  return
++}
++
++// CHECK: Writing unowned blob...
++// CHECK: Successfully read the unowned blob.
++// CHECK: func.func @base_test([[ARG0:%.+]]: !test.i32) {
+diff -ruN --strip-trailing-cr a/mlir/test/Bytecode/i1_splat_roundtrip.mlir b/mlir/test/Bytecode/i1_splat_roundtrip.mlir
+--- a/mlir/test/Bytecode/i1_splat_roundtrip.mlir
++++ b/mlir/test/Bytecode/i1_splat_roundtrip.mlir
+@@ -0,0 +1,17 @@
++// RUN: mlir-opt %s -emit-bytecode | mlir-opt | FileCheck %s
++
++func.func @test_i1_splat_true() -> tensor<100xi1> {
++  %0 = arith.constant dense<true> : tensor<100xi1>
++  return %0 : tensor<100xi1>
++}
++
++// CHECK-LABEL: func.func @test_i1_splat_true
++// CHECK: arith.constant dense<true> : tensor<100xi1>
++
++func.func @test_i1_splat_false() -> tensor<100xi1> {
++  %0 = arith.constant dense<false> : tensor<100xi1>
++  return %0 : tensor<100xi1>
++}
++
++// CHECK-LABEL: func.func @test_i1_splat_false
++// CHECK: arith.constant dense<false> : tensor<100xi1>
+diff -ruN --strip-trailing-cr a/mlir/test/lib/IR/TestBytecodeRoundtrip.cpp b/mlir/test/lib/IR/TestBytecodeRoundtrip.cpp
+--- a/mlir/test/lib/IR/TestBytecodeRoundtrip.cpp
++++ b/mlir/test/lib/IR/TestBytecodeRoundtrip.cpp
+@@ -84,6 +84,8 @@
+       // test-kind 6 is a plain roundtrip with downgrade/upgrade to/from
+       // `targetVersion`.
+       return runTest6(getOperation());
++    case (7):
++      return runTest7(getOperation());
+     default:
+       llvm_unreachable("unhandled test kind for TestBytecodeCallbacks pass");
+     }
+@@ -412,6 +414,59 @@
+     doRoundtripWithConfigs(op, writeConfig, parseConfig);
+   }
+ 
++  // Test7: When writing bytecode, we override the encoding of TestI32Type with
++  // the encoding of builtin IntegerType, but we also write an unowned blob.
++  // We can natively parse this without the use of a callback, relying on the
++  // existing builtin reader mechanism.
++  void runTest7(Operation *op) {
++    auto *builtin = op->getContext()->getLoadedDialect<mlir::BuiltinDialect>();
++    BytecodeDialectInterface *iface =
++        builtin->getRegisteredInterface<BytecodeDialectInterface>();
++    BytecodeWriterConfig writeConfig;
++    writeConfig.attachTypeCallback(
++        [&](Type entryValue, std::optional<StringRef> &dialectGroupName,
++            DialectBytecodeWriter &writer) -> LogicalResult {
++          // Emit TestIntegerType using the builtin dialect encoding.
++          if (llvm::isa<test::TestI32Type>(entryValue)) {
++            auto builtinI32Type =
++                IntegerType::get(op->getContext(), 32,
++                                 IntegerType::SignednessSemantics::Signless);
++            // Specify that this type will need to be written as part of the
++            // builtin group. This will override the default dialect group of
++            // the attribute (test).
++            dialectGroupName = StringLiteral("builtin");
++            if (succeeded(iface->writeType(builtinI32Type, writer))) {
++              char dummyBlob[] = "test_blob";
++              llvm::outs() << "Writing unowned blob...\n";
++              writer.writeUnownedBlob(ArrayRef<char>(dummyBlob, 9));
++              return success();
++            }
++          }
++          return failure();
++        });
++    ParserConfig parseConfig(op->getContext(), /*verifyAfterParse=*/true);
++    parseConfig.getBytecodeReaderConfig().attachTypeCallback(
++        [&](DialectBytecodeReader &reader, StringRef dialectName,
++            Type &entry) -> LogicalResult {
++          if (dialectName != StringLiteral("builtin"))
++            return failure();
++          Type builtinAttr = iface->readType(reader);
++          if (auto integerType =
++                  llvm::dyn_cast_or_null<IntegerType>(builtinAttr)) {
++            if (integerType.getWidth() == 32 && integerType.isSignless()) {
++              ArrayRef<char> blob;
++              if (succeeded(reader.readBlob(blob)) &&
++                  blob == ArrayRef<char>("test_blob", 9)) {
++                llvm::outs() << "Successfully read the unowned blob.\n";
++                entry = test::TestI32Type::get(reader.getContext());
++              }
++            }
++          }
++          return success();
++        });
++    doRoundtripWithConfigs(op, writeConfig, parseConfig);
++  }
++
+   test::TestDialect *testDialect;
+ };
+ } // namespace
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index e99c82e..c7bd02f 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "c6bb1afbc5086c3f483921c575262bd95c346e5b"
-    LLVM_SHA256 = "e5f3f1d2f125921d4fe384ebc1c28d68abe396d9d20180f690cbe29e9a5f7277"
+    LLVM_COMMIT = "e003440b8e2cf0fe01515a9b957fada77cabcdf5"
+    LLVM_SHA256 = "306710f6c3b54cf9970c7f0f26efe6a13cbc5dba44d1ae747938adc6bfb476d8"
 
     tf_http_archive(
         name = name,
diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch
index 9d4e735..da31781 100755
--- a/third_party/stablehlo/temporary.patch
+++ b/third_party/stablehlo/temporary.patch
@@ -1,32 +1,304 @@
+diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel
+--- stablehlo/BUILD.bazel
++++ stablehlo/BUILD.bazel
+@@ -2000,6 +2000,7 @@
+     deps = [
+         ":attr_type_builder_util",
+         ":mlir_builder",
++        ":stablehlo_broadcast_lowering",
+         ":stablehlo_builder_inc",
+         ":stablehlo_ops",
+         ":stablehlo_type_inference",
 diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.cpp b/stablehlo/stablehlo/dialect/ChloOps.cpp
 --- stablehlo/stablehlo/dialect/ChloOps.cpp
 +++ stablehlo/stablehlo/dialect/ChloOps.cpp
-@@ -866,10 +866,8 @@
+@@ -781,6 +781,218 @@
+   return success();
  }
  
- LogicalResult ScanOp::verify() {
--  if (getInits().size() != getCarries().size()) {
--    return emitOpError() << "requires the number of inits ("
--                         << getInits().size() << ") and carries ("
--                         << getCarries().size() << ") to be equal";
++//===----------------------------------------------------------------------===//
++// ScanOp
++//===----------------------------------------------------------------------===//
++
++LogicalResult ScanOp::inferReturnTypeComponents(
++    MLIRContext*, std::optional<Location> location, ValueShapeRange operands,
++    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,
++    SmallVectorImpl<ShapedTypeComponents>& inferredReturnShapes) {
++  ScanOp::Adaptor adaptor(operands, attributes, properties, regions);
++  if (regions.empty() || regions.front()->empty()) {
++    return emitOptionalError(location, "ScanOp region is empty");
++  }
++  auto* terminator = regions.front()->front().getTerminator();
++  size_t numCarries = adaptor.getInits().size();
++  if (terminator->getNumOperands() < numCarries) {
++    return emitOptionalError(location, "ScanOp body must return at least ",
++                             numCarries, " values (carries)");
++  }
++  size_t numOutputs = terminator->getNumOperands() - numCarries;
++
++  // Find the scan dimension size. If the scan dimension is dynamic, the
++  // output dimension size will be dynamic as well.
++  int64_t dim = adaptor.getDimension();
++  int64_t dimSize = ShapedType::kDynamic;
++  for (auto [i, type] : llvm::enumerate(adaptor.getInputs().getTypes())) {
++    auto inputType = dyn_cast<RankedTensorType>(type);
++    if (!inputType) {
++      return emitOptionalError(location, "operand ", i, " is not ranked");
++    }
++    if (dim >= inputType.getRank()) {
++      return emitOptionalError(location, "scan dimension of operand ", i,
++                               " is out of bounds");
++    }
++    dimSize = inputType.getDimSize(dim);
++    if (dimSize == ShapedType::kDynamic) {
++      break;
++    }
++  }
++
++  for (auto [i, type] : llvm::enumerate(terminator->getOperands().getTypes())) {
++    auto resultType = dyn_cast<RankedTensorType>(type);
++    if (!resultType) {
++      return emitOptionalError(location, "terminator operand ", i,
++                               " is not ranked");
++    }
++    SmallVector<int64_t> shape(resultType.getShape().begin(),
++                               resultType.getShape().end());
++    if (i < numOutputs) {
++      shape.insert(std::next(shape.begin(), dim), dimSize);
++    }
++    inferredReturnShapes.emplace_back(shape, resultType.getElementType());
++  }
++
++  return success();
++}
++
++LogicalResult ScanOp::reifyReturnTypeShapes(
++    OpBuilder& builder, ValueRange operands,
++    SmallVectorImpl<Value>& reifiedReturnShapes) {
++  ScanOp::Adaptor adaptor(operands, getOperation()->getAttrDictionary(),
++                          getOperation()->getPropertiesStorage());
++  auto inputs = adaptor.getInputs();
++  size_t k = adaptor.getInits().size();
++  size_t numResults = getOperation()->getNumResults();
++  size_t numOutputs = numResults - k;
++
++  for (size_t i = 0; i < numOutputs; ++i) {
++    size_t inputIdx = i;
++    Value inputVal = (inputIdx < inputs.size()) ? inputs[inputIdx] : inputs[0];
++    if (failed(hlo::deriveShapeFromOperand(&builder, getOperation(), inputVal,
++                                           &reifiedReturnShapes))) {
++      return failure();
++    }
++  }
++
++  for (auto init : adaptor.getInits()) {
++    if (failed(hlo::deriveShapeFromOperand(&builder, getOperation(), init,
++                                           &reifiedReturnShapes))) {
++      return failure();
++    }
++  }
++  return success();
++}
++
++LogicalResult ScanOp::verify() {
 +  if (getInputs().empty() && getOutputs().empty()) {
 +    return emitOpError() << "at least one of inputs or outputs must be present";
-   }
++  }
++
++  // Check that the scan dimension is in bounds for all operands. Also check
++  // that all operands have the same scan dimension size.
++  int64_t dim = getDimension();
++  int64_t dimSize = ShapedType::kDynamic;
++  for (auto [i, type] : llvm::enumerate(getInputs().getTypes())) {
++    auto inputType = dyn_cast<RankedTensorType>(type);
++    if (!inputType) {
++      return emitOpError() << "operand " << i << " is not ranked";
++    }
++    if (dim >= inputType.getRank()) {
++      return emitOpError() << "scan dimension of operand " << i
++                           << " is out of bounds";
++    }
++    if (inputType.isDynamicDim(dim)) {
++      continue;
++    }
++    dimSize = inputType.getDimSize(dim);
++    if (dimSize != ShapedType::kDynamic &&
++        dimSize != inputType.getDimSize(dim)) {
++      return emitOpError() << "scan dimension size of operand " << i
++                           << " does not match previous operands";
++    }
++    dimSize = inputType.getDimSize(dim);
++  }
++
++  Block& bodyBlock = getBody().front();
++  if (bodyBlock.getNumArguments() != getNumOperands()) {
++    return emitOpError() << "expects " << getNumOperands()
++                         << " arguments in the body, but got "
++                         << bodyBlock.getNumArguments();
++  }
++
++  // Check that the operand types are compatible with the body arguments.
++  for (auto [i, type] : llvm::enumerate(getOperands().getTypes())) {
++    auto argType = dyn_cast<RankedTensorType>(type);
++    if (!argType) {
++      return emitOpError() << "operand " << i << " is not ranked";
++    }
++    if (i < getInputs().size()) {
++      auto argShape = llvm::to_vector(argType.getShape());
++      argShape.erase(std::next(argShape.begin(), dim));
++      argType = argType.clone(argShape);
++    }
++    if (!hlo::isCompatibleForHloTypeInference(
++            argType, bodyBlock.getArgument(i).getType())) {
++      return emitOpError() << "operand and body argument " << i
++                           << " are incompatible";
++    }
++  }
++
++  // Compatibility of terminator operands and result types is checked by
++  // InferTensorType trait.
++
++  return success();
++}
++
++ParseResult ScanOp::parse(OpAsmParser& parser, OperationState& result) {
++  SmallVector<OpAsmParser::UnresolvedOperand, 4> inputs, inits;
++  int64_t dimension = 0;
++  Region* body = result.addRegion();
++  FunctionType funcType;
++
++  if (parser.parseOperandList(inputs, OpAsmParser::Delimiter::Paren) ||
++      parser.parseKeyword("inits") ||
++      parser.parseOperandList(inits, OpAsmParser::Delimiter::Paren) ||
++      parser.parseKeyword("dimension") || parser.parseEqual() ||
++      parser.parseInteger(dimension) || parser.parseRegion(*body) ||
++      parser.parseOptionalAttrDict(result.attributes) ||
++      parser.parseColonType(funcType)) {
++    return failure();
++  }
++
++  size_t numInputs = inputs.size();
++  size_t numCarries = inits.size();
++  if (funcType.getInputs().size() != numInputs + numCarries) {
++    return parser.emitError(
++        parser.getNameLoc(),
++        "operand types must match the number of inputs and inits");
++  }
++  if (funcType.getResults().size() < numCarries) {
++    return parser.emitError(
++        parser.getNameLoc(),
++        "not enough result types to cover the required carries");
++  }
++
++  auto inputTypes = funcType.getInputs().take_front(numInputs);
++  auto initTypes = funcType.getInputs().take_back(numCarries);
++  if (parser.resolveOperands(inputs, inputTypes, parser.getNameLoc(),
++                             result.operands) ||
++      parser.resolveOperands(inits, initTypes, parser.getNameLoc(),
++                             result.operands)) {
++    return failure();
++  }
++  result.addTypes(funcType.getResults());
++
++  Builder& builder = parser.getBuilder();
++  result.addAttribute(ScanOp::getDimensionAttrName(result.name),
++                      builder.getI64IntegerAttr(dimension));
++  result.addAttribute(
++      ScanOp::getOperandSegmentSizeAttr(),
++      builder.getDenseI32ArrayAttr({(int32_t)numInputs, (int32_t)numCarries}));
++  size_t numOutputs = funcType.getNumResults() - numCarries;
++  result.addAttribute(
++      ScanOp::getResultSegmentSizeAttr(),
++      builder.getDenseI32ArrayAttr({(int32_t)numOutputs, (int32_t)numCarries}));
++
++  return success();
++}
++
++void ScanOp::print(OpAsmPrinter& p) {
++  p << "(";
++  p.printOperands(getInputs());
++  p << ") inits (";
++  p.printOperands(getInits());
++  p << ") dimension=" << getDimension() << " ";
++  p.printRegion(getBody(), /*printEntryBlockArgs=*/true);
++  p.printOptionalAttrDict(getOperation()->getAttrs(),
++                          /*elidedAttrs=*/{"dimension", "operandSegmentSizes",
++                                           "resultSegmentSizes"});
++  p << " : ";
++  p.printFunctionalType(*this);
++}
++
+ }  // namespace chlo
+ }  // namespace mlir
  
-   // Check that the scan dimension is in bounds for all operands. Also check
 diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.td b/stablehlo/stablehlo/dialect/ChloOps.td
 --- stablehlo/stablehlo/dialect/ChloOps.td
 +++ stablehlo/stablehlo/dialect/ChloOps.td
-@@ -978,6 +978,9 @@
-         setNameFn(region.getArgument(i), i < getInputs().size() ? "input" : "carry");
-       }
-     }
+@@ -31,6 +31,7 @@
+ #define STABLEHLO_DIALECT_CHLO_OPS
+ 
+ include "mlir/IR/BuiltinAttributeInterfaces.td"
++include "mlir/IR/OpAsmInterface.td"
+ include "mlir/IR/OpBase.td"
+ include "mlir/Interfaces/ControlFlowInterfaces.td"
+ include "mlir/Interfaces/InferTypeOpInterface.td"
+@@ -934,4 +935,56 @@
+   }];
+ }
+ 
++def CHLO_ScanOp : CHLO_Op<"scan", [
++      AttrSizedOperandSegments,
++      AttrSizedResultSegments,
++      InferTensorTypeWithReify,
++      IsolatedFromAbove,
++      OpAsmOpInterface,
++      RecursiveMemoryEffects,
++    ]> {
++  string summary = "Scan operation";
++
++  string description = [{
++    Applies a reduction function `body` to `inputs` and `inits` along the
++    `dimension` and produces `results` (comprising `outputs` and `carries`).
++
++    If `is_reverse` is true, the scan is performed in reverse order.
++    `is_associative` indicates whether the reduction function is associative.
++
++    See: https://www.tensorflow.org/xla/operation_semantics#scan
++
++    ScanOp currently does not have a decomposition to StableHLO.
++  }];
++
++  let arguments = (ins
++    Variadic<HLO_AnyTensor>:$inputs,
++    Variadic<HLO_AnyTensor>:$inits,
++    ConfinedAttr<I64Attr, [IntNonNegative]>:$dimension,
++    DefaultValuedOptionalAttr<BoolAttr, "false">:$is_reverse,
++    OptionalAttr<BoolAttr>:$is_associative
++  );
++
++  let results = (outs
++    Variadic<HLO_AnyTensor>:$outputs,
++    Variadic<HLO_AnyTensor>:$carries
++  );
++
++  let regions = (region SizedRegion<1>:$body);
++
++  let extraClassDeclaration = [{
++    void getAsmBlockArgumentNames(Region &region, OpAsmSetValueNameFn setNameFn) {
++      for (size_t i = 0; i < region.getNumArguments(); ++i) {
++        setNameFn(region.getArgument(i), i < getInputs().size() ? "input" : "carry");
++      }
++    }
 +    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
 +      return succeeded(mlir::verifyCompatibleShapes(l, r));
 +    }
-   }];
- 
-   let hasCustomAssemblyFormat = 1;
++  }];
++
++  let hasCustomAssemblyFormat = 1;
++  let hasVerifier = 1;
++}
++
+ #endif  // STABLEHLO_DIALECT_CHLO_OPS
 diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
 --- stablehlo/stablehlo/dialect/StablehloOps.cpp
 +++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -242,6 +514,174 @@ diff --ruN a/stablehlo/stablehlo/dialect/TypeInference.cpp b/stablehlo/stablehlo
    return success();
  }
  
+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
+--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
++++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
+@@ -18,6 +18,7 @@
+ #include <cstdint>
+ #include <optional>
+ 
++#include "llvm/ADT/SmallVectorExtras.h"
+ #include "llvm/Support/ErrorHandling.h"
+ #include "mlir/IR/Attributes.h"
+ #include "mlir/IR/BuiltinAttributes.h"
+@@ -30,6 +31,7 @@
+ #include "stablehlo/dialect/TypeInference.h"
+ #include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
+ #include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
++#include "stablehlo/transforms/StablehloBroadcastLowering.h"
+ 
+ namespace mlir {
+ namespace stablehlo {
+@@ -94,6 +96,49 @@
+       value));
+ }
+ 
++
++MlirOp IotaLike(MlirOp input, int64_t dim, Type elementType) {
++  auto inputType = mlir::cast<RankedTensorType>(input.getType());
++  if (inputType.getRank() == 0) {
++    // Need to construct 1-D iota and reshape to 0-D.
++    auto iota = stablehlo::Iota(input.getBuilder(),
++                                inputType.clone({1}, elementType), dim);
++    return stablehlo::Reshape(iota, {});
++  }
++  if (inputType.hasStaticShape()) {
++    return stablehlo::Iota(input.getBuilder(), inputType.clone(elementType),
++                           dim);
++  }
++
++  // Use input's static shape and slice to the dynamic shape.
++  auto dims = mlir::stablehlo::getDimensions(input.getValue());
++  if (mlir::failed(dims)) llvm::report_fatal_error(
++      "failed to create dynamically shaped iota op, with MLIR error: ");
++
++  mlir::SmallVector<int64_t> iotaShape = llvm::map_to_vector(
++      *dims,
++      [&](mlir::stablehlo::DimensionInfo dim_size) { return dim_size.size; });
++  auto iotaType =
++      mlir::makeTensorType(input.getContext(), iotaShape, elementType);
++  mlir::MlirOp iota = mlir::stablehlo::Iota(input.getBuilder(), iotaType, dim);
++
++  // Slice bounded dimensions to the dynamic shape.
++  for (const mlir::stablehlo::DimensionInfo& dim : *dims) {
++    if (!dim.boundOp.has_value()) continue;
++
++    auto runtime_dim_size =
++        mlir::stablehlo::GetDimensionSize(input, dim.boundOpDim);
++    iota = mlir::stablehlo::SetDimensionSize(iota, runtime_dim_size,
++                                             dim.boundOpDim);
++  }
++  return iota;
++}
++
++MlirOp IotaLike(MlirOp input, int64_t dim, ElementType elementType) {
++  auto resultElementType = getElementType(input.getContext(), elementType);
++  return IotaLike(input, dim, resultElementType);
++}
++
+ namespace {
+ 
+ // Use preferred element type, if not use LHS element type.
+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
+--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
++++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
+@@ -58,6 +58,12 @@
+ MlirOp Constant(MlirBuilder& builder, int64_t value);
+ MlirOp Constant(MlirBuilder& builder, std::vector<int64_t> value);
+ 
++// IotaLike is a sugar API for iota that accounts for bounded dynamism in the
++// input tensor. Eventually this should be a chlo.iota_like op with a StableHLO
++// decomposition, but for now it will be housed as a builder API.
++MlirOp IotaLike(MlirOp input, int64_t dim, ElementType elementType);
++MlirOp IotaLike(MlirOp input, int64_t dim, Type elementType);
++
+ // Better Dot / DotGeneral builders.
+ // These ops don't support full type inference because the result element type
+ // cannot be inferred from operands, however the result shape can be.
+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
+--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
++++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
+@@ -199,6 +199,79 @@
+         &ctx, {Precision::HIGHEST, Precision::HIGHEST});
+     auto dot = stablehlo::DotGeneral(arg0, arg1, dotDimsAttr, precision);
+     func::Return(fb, dot);
++  }
++
++  OwningOpRef<ModuleOp> module = mb->build();
++  EXPECT_TRUE(succeeded(mlir::verify(*module)));
++  EXPECT_EQ(expected, debugString(*module));
++}
++
++TEST(MlirBuilderTest, IotaLikeStatic) {
++  std::string expected = R"mlir(module {
++  func.func @main(%arg0: tensor<2x3xi64>) -> tensor<2x3xi64> {
++    %0 = stablehlo.iota dim = 1 : tensor<2x3xi64>
++    return %0 : tensor<2x3xi64>
++  }
++})mlir";
++  StablehloModuleBuilder mb;
++  {  // Build Main Func
++    func::FunctionBuilder fb(mb.get(), "main");
++    auto& ctx = fb.getContext();
++    auto type2x3xi64 = makeTensorType(ctx, {2, 3}, ElementType::I64);
++    auto arg0 = func::Argument(fb, type2x3xi64);
++    auto iota = stablehlo::IotaLike(arg0, 1, type2x3xi64.getElementType());
++    func::Return(fb, iota);
++  }
++
++  OwningOpRef<ModuleOp> module = mb->build();
++  EXPECT_TRUE(succeeded(mlir::verify(*module)));
++  EXPECT_EQ(expected, debugString(*module));
++}
++
++TEST(MlirBuilderTest, IotaLikeScalar) {
++  std::string expected = R"mlir(module {
++  func.func @main(%arg0: tensor<i64>) -> tensor<i64> {
++    %0 = stablehlo.iota dim = 0 : tensor<1xi64>
++    %1 = stablehlo.reshape %0 : (tensor<1xi64>) -> tensor<i64>
++    return %1 : tensor<i64>
++  }
++})mlir";
++  StablehloModuleBuilder mb;
++  {  // Build Main Func
++    func::FunctionBuilder fb(mb.get(), "main");
++    auto& ctx = fb.getContext();
++    auto typei64 = makeTensorType(ctx, {}, ElementType::I64);
++    auto arg0 = func::Argument(fb, typei64);
++    auto iota = stablehlo::IotaLike(arg0, 0, typei64.getElementType());
++    func::Return(fb, iota);
++  }
++
++  OwningOpRef<ModuleOp> module = mb->build();
++  EXPECT_TRUE(succeeded(mlir::verify(*module)));
++  EXPECT_EQ(expected, debugString(*module));
++}
++
++TEST(MlirBuilderTest, IotaLikeDynamic) {
++  std::string expected = R"mlir(module {
++  func.func @main(%arg0: tensor<2x3xi64>, %arg1: tensor<i32>) -> tensor<?x3xi64, #stablehlo.bounds<2, ?>> {
++    %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<2x3xi64>, tensor<i32>) -> tensor<?x3xi64, #stablehlo.bounds<2, ?>>
++    %1 = stablehlo.iota dim = 1 : tensor<2x3xi64>
++    %2 = stablehlo.get_dimension_size %0, dim = 0 : (tensor<?x3xi64, #stablehlo.bounds<2, ?>>) -> tensor<i32>
++    %3 = stablehlo.set_dimension_size %1, %2, dim = 0 : (tensor<2x3xi64>, tensor<i32>) -> tensor<?x3xi64, #stablehlo.bounds<2, ?>>
++    return %3 : tensor<?x3xi64, #stablehlo.bounds<2, ?>>
++  }
++})mlir";
++  StablehloModuleBuilder mb;
++  {  // Build Main Func
++    func::FunctionBuilder fb(mb.get(), "main");
++    auto& ctx = fb.getContext();
++    auto type2x3xi64 = makeTensorType(ctx, {2, 3}, ElementType::I64);
++    auto typei32 = makeTensorType(ctx, {}, ElementType::I32);
++    auto arg0 = func::Argument(fb, type2x3xi64);
++    auto arg1 = func::Argument(fb, typei32);
++    auto sds = stablehlo::SetDimensionSize(arg0, arg1, 0);
++    auto iota = stablehlo::IotaLike(sds, 1, type2x3xi64.getElementType());
++    func::Return(fb, iota);
+   }
+ 
+   OwningOpRef<ModuleOp> module = mb->build();
 diff --ruN a/stablehlo/stablehlo/integrations/python/ChloModule.cpp b/stablehlo/stablehlo/integrations/python/ChloModule.cpp
 --- stablehlo/stablehlo/integrations/python/ChloModule.cpp
 +++ stablehlo/stablehlo/integrations/python/ChloModule.cpp
@@ -448,6 +888,160 @@ diff --ruN a/stablehlo/stablehlo/tests/infer_stablehlo.mlir b/stablehlo/stablehl
 +  func.return %1 : tensor<3xindex>
  }
  
+ // -----
+diff --ruN a/stablehlo/stablehlo/tests/ops_chlo.mlir b/stablehlo/stablehlo/tests/ops_chlo.mlir
+--- stablehlo/stablehlo/tests/ops_chlo.mlir
++++ stablehlo/stablehlo/tests/ops_chlo.mlir
+@@ -417,3 +417,90 @@
+   %0 = chlo.erf_inv %arg0 : tensor<16x16xf32> -> tensor<16x16xf32>
+   return
+ }
++
++// -----
++
++// CHECK-LABEL: func @scan
++func.func @scan(%arg0: tensor<2x3xf32>, %arg1: tensor<3xf32>) -> tensor<2x3xf32> {
++  // CHECK: chlo.scan
++  %0, %1 = chlo.scan (%arg0) inits (%arg1) dimension = 0 {
++  ^bb0(%input0: tensor<3xf32>, %carry0: tensor<3xf32>):
++    %2 = stablehlo.add %input0, %carry0 : tensor<3xf32>
++    stablehlo.return %2, %2 : tensor<3xf32>, tensor<3xf32>
++  } : (tensor<2x3xf32>, tensor<3xf32>) -> (tensor<2x3xf32>, tensor<3xf32>)
++  func.return %0 : tensor<2x3xf32>
++}
++
++// -----
++
++// CHECK-LABEL: func @scan_variadic
++func.func @scan_variadic(%arg0: tensor<2x3xf32>, %arg1: tensor<3xf32>, %arg2: tensor<2x3xi32>, %arg3: tensor<3xi32>) -> (tensor<2x3xf32>, tensor<2x3xi32>) {
++  // CHECK: chlo.scan
++  %0:4 = chlo.scan(%arg0, %arg2) inits (%arg1, %arg3) dimension = 0 {
++  ^bb0(%arg4: tensor<3xf32>, %arg5: tensor<3xi32>, %arg6: tensor<3xf32>, %arg7: tensor<3xi32>):
++    %1 = stablehlo.add %arg4, %arg6 : tensor<3xf32>
++    %2 = stablehlo.add %arg5, %arg7 : tensor<3xi32>
++    stablehlo.return %1, %2, %1, %2 : tensor<3xf32>, tensor<3xi32>, tensor<3xf32>, tensor<3xi32>
++  } : (tensor<2x3xf32>, tensor<2x3xi32>, tensor<3xf32>, tensor<3xi32>) -> (tensor<2x3xf32>, tensor<2x3xi32>, tensor<3xf32>, tensor<3xi32>)
++  func.return %0#0, %0#1 : tensor<2x3xf32>, tensor<2x3xi32>
++}
++
++// -----
++
++func.func @scan_size_mismatch(%arg0: tensor<2x3xf32>, %arg1: tensor<3xf32>) -> tensor<2x3xf32> {
++  // expected-error @+1 {{'chlo.scan' op expects 1 arguments in the body, but got 2}}
++  %0 = chlo.scan(%arg0) inits () dimension = 0 {
++  ^bb0(%arg2: tensor<3xf32>, %arg3: tensor<3xf32>):
++    %1 = stablehlo.add %arg2, %arg3 : tensor<3xf32>
++    stablehlo.return %1 : tensor<3xf32>
++  } : (tensor<2x3xf32>) -> tensor<2x3xf32>
++  func.return %0 : tensor<2x3xf32>
++}
++
++// -----
++
++func.func @scan_element_type_mismatch(%arg0: tensor<2x3xf32>, %arg1: tensor<3xi32>) -> tensor<2x3xf32> {
++  // expected-error @+1 {{'chlo.scan' op operand and body argument 1 are incompatible}}
++  %0:2 = chlo.scan(%arg0) inits (%arg1) dimension = 0 {
++  ^bb0(%arg2: tensor<3xf32>, %arg3: tensor<3xf32>):
++    // This body is invalid given the types but checking the verifier first.
++    stablehlo.return %arg2, %arg2 : tensor<3xf32>, tensor<3xf32>
++  } : (tensor<2x3xf32>, tensor<3xi32>) -> (tensor<2x3xf32>, tensor<3xf32>)
++  func.return %0#0 : tensor<2x3xf32>
++}
++
++// -----
++
++func.func @scan_dim_out_of_bounds(%arg0: tensor<2x3xf32>, %arg1: tensor<3xf32>) -> tensor<2x3xf32> {
++  // expected-error @+1 {{'chlo.scan' op scan dimension of operand 0 is out of bounds}}
++  %0:2 = chlo.scan(%arg0) inits (%arg1) dimension = 2 {
++  ^bb0(%arg2: tensor<3xf32>, %arg3: tensor<3xf32>):
++    %1 = stablehlo.add %arg2, %arg3 : tensor<3xf32>
++    stablehlo.return %1, %1 : tensor<3xf32>, tensor<3xf32>
++  } : (tensor<2x3xf32>, tensor<3xf32>) -> (tensor<2x3xf32>, tensor<3xf32>)
++  func.return %0#0 : tensor<2x3xf32>
++}
++
++// -----
++
++func.func @scan_init_rank_mismatch(%arg0: tensor<2x3xf32>, %arg1: tensor<2x3xf32>) -> tensor<2x3xf32> {
++  // expected-error @+1 {{'chlo.scan' op operand and body argument 1 are incompatible}}
++  %0:2 = chlo.scan(%arg0) inits (%arg1) dimension = 0 {
++  ^bb0(%arg2: tensor<3xf32>, %arg3: tensor<3xf32>):
++    %1 = stablehlo.add %arg2, %arg3 : tensor<3xf32>
++    stablehlo.return %1, %1 : tensor<3xf32>, tensor<3xf32>
++  } : (tensor<2x3xf32>, tensor<2x3xf32>) -> (tensor<2x3xf32>, tensor<2x3xf32>)
++  func.return %0#0 : tensor<2x3xf32>
++}
++
++// -----
++
++func.func @scan_init_shape_mismatch(%arg0: tensor<2x3xf32>, %arg1: tensor<2xf32>) -> tensor<2x3xf32> {
++  // expected-error @+1 {{'chlo.scan' op operand and body argument 1 are incompatible}}
++  %0:2 = chlo.scan(%arg0) inits (%arg1) dimension = 0 {
++  ^bb0(%arg2: tensor<3xf32>, %arg3: tensor<3xf32>):
++    %1 = stablehlo.add %arg2, %arg3 : tensor<3xf32>
++    stablehlo.return %1, %1 : tensor<3xf32>, tensor<3xf32>
++  } : (tensor<2x3xf32>, tensor<2xf32>) -> (tensor<2x3xf32>, tensor<2xf32>)
++  func.return %0#0 : tensor<2x3xf32>
++}
+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
++++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+@@ -584,6 +584,56 @@
+   %1 = stablehlo.divide %cst_1, %cst_1 : tensor<ui32>
+   %2 = stablehlo.divide %cst_2, %cst_2 : tensor<f32>
+   return %0, %1, %2 : tensor<i32>, tensor<ui32>, tensor<f32>
++}
++
++// CHECK-LABEL: @div_fold_cst_zero_nan
++func.func @div_fold_cst_zero_nan() -> (tensor<f32>) {
++  %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>
++  %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
++  %0 = stablehlo.divide %cst, %cst_0 : tensor<f32>
++  // CHECK: stablehlo.constant dense<0x7F800000> : tensor<f32>
++  // CHECK-NOT: stablehlo.divide
++  return %0 : tensor<f32>
++}
++
++// CHECK-LABEL: @div_fold_cst_nan
++func.func @div_fold_cst_nan() -> (tensor<f32>) {
++  %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>
++  %cst_0 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
++  %0 = stablehlo.divide %cst, %cst_0 : tensor<f32>
++  // CHECK: stablehlo.constant dense<0x7FC00000> : tensor<f32>
++  // CHECK-NOT: stablehlo.divide
++  return %0 : tensor<f32>
++}
++
++// -----
++
++////////
++// MaximumOp
++
++// CHECK-LABEL: @max_fold_cst_nan
++func.func @max_fold_cst_nan() -> (tensor<f32>) {
++  %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>
++  %cst_0 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
++  %0 = stablehlo.maximum %cst, %cst_0 : tensor<f32>
++  // CHECK: stablehlo.constant dense<0x7FC00000> : tensor<f32>
++  // CHECK-NOT: stablehlo.maximum
++  return %0 : tensor<f32>
++}
++
++// -----
++
++////////
++// MinimumOp
++
++// CHECK-LABEL: @min_fold_cst_nan
++func.func @min_fold_cst_nan() -> (tensor<f32>) {
++  %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>
++  %cst_0 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
++  %0 = stablehlo.minimum %cst, %cst_0 : tensor<f32>
++  // CHECK: stablehlo.constant dense<0x7FC00000> : tensor<f32>
++  // CHECK-NOT: stablehlo.minimum
++  return %0 : tensor<f32>
+ }
+ 
  // -----
 diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
 --- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
@@ -796,4 +1390,54 @@ diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stable
  }
  }  // namespace
  
+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+@@ -1058,14 +1058,14 @@
+   std::function<APInt(APInt, APInt)> foldIntFn;
+ 
+   APFloat operator()(APFloat lhs, APFloat rhs) {
+-    return lhs >= rhs ? lhs : rhs;
++    return llvm::maximum(lhs, rhs);
+   }
+   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
+   static APInt foldUint(APInt lhs, APInt rhs) {
+-    return lhs.uge(rhs) ? lhs : rhs;
++    return llvm::APIntOps::umax(lhs, rhs);
+   }
+   static APInt foldSint(APInt lhs, APInt rhs) {
+-    return lhs.sge(rhs) ? lhs : rhs;
++    return llvm::APIntOps::smax(lhs, rhs);
+   }
+ };
+ 
+@@ -1075,14 +1075,14 @@
+   std::function<APInt(APInt, APInt)> foldIntFn;
+ 
+   APFloat operator()(APFloat lhs, APFloat rhs) {
+-    return lhs <= rhs ? lhs : rhs;
++    return llvm::minimum(lhs, rhs);
+   }
+   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
+   static APInt foldUint(APInt lhs, APInt rhs) {
+-    return lhs.ule(rhs) ? lhs : rhs;
++    return llvm::APIntOps::umin(lhs, rhs);
+   }
+   static APInt foldSint(APInt lhs, APInt rhs) {
+-    return lhs.sle(rhs) ? lhs : rhs;
++    return llvm::APIntOps::smin(lhs, rhs);
+   }
+ };
+ 
+@@ -1533,7 +1533,9 @@
+   using FoldUnaryOpPattern::FoldUnaryOpPattern;
+ 
+   static std::optional<APInt> EvaluateOp(APInt operand) { return -operand; }
+-  static std::optional<APFloat> EvaluateOp(APFloat operand) { return -operand; }
++  static std::optional<APFloat> EvaluateOp(APFloat operand) {
++    return llvm::neg(operand);
++  }
+ };
+ 
+ struct FoldNotOpPattern : public FoldUnaryOpPattern<FoldNotOpPattern, NotOp> {
 
diff --git a/third_party/stablehlo/workspace.bzl b/third_party/stablehlo/workspace.bzl
index 419248c..8f1a6cc 100644
--- a/third_party/stablehlo/workspace.bzl
+++ b/third_party/stablehlo/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive", "tf_mirror_urls")
 
 def repo():
     #
-    STABLEHLO_COMMIT = "bdbe31e8a1a2f4884c29c1c685de36e74ba6a68d"
-    STABLEHLO_SHA256 = "98b8f76f62e072226003880dda644a07d2224264fc8480686233683c9620ee2e"
+    STABLEHLO_COMMIT = "127d2f238010589ac96f2f402a27afc9dccbb7ab"
+    STABLEHLO_SHA256 = "3780e503599ebcc1acc4c96ea5f23c724dae2a4d746ed7d9a01d3f14d879265a"
     #
 
     tf_http_archive(
