diff --git a/docs/mpmd/mpmd_dialect.md b/docs/mpmd/mpmd_dialect.md
new file mode 100755
index 0000000..8afb23c
--- /dev/null
+++ b/docs/mpmd/mpmd_dialect.md
@@ -0,0 +1,3 @@
+<!-- Autogenerated by mlir-tblgen; don't manually edit -->
+
+# 'mpmd' Dialect
diff --git a/shardy/dialect/mpmd/README b/shardy/dialect/mpmd/README
new file mode 100644
index 0000000..6dbeec8
--- /dev/null
+++ b/shardy/dialect/mpmd/README
@@ -0,0 +1 @@
+This is the WIP package for the MPMD dialect.
diff --git a/shardy/dialect/mpmd/ir/BUILD b/shardy/dialect/mpmd/ir/BUILD
new file mode 100644
index 0000000..639fd0f
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/BUILD
@@ -0,0 +1,200 @@
+# The MPMD MLIR dialect.
+
+# load("@rules_cc//cc:cc_library.bzl", "cc_library")
+# load("@rules_cc//cc:cc_test.bzl", "cc_test")
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library", "gentbl_filegroup", "td_library")
+
+package(default_visibility = ["//visibility:public"])
+
+td_library(
+    name = "mpmd_td_files",
+    srcs = [
+        "attrs.td",
+        "dialect.td",
+        "enums.td",
+        "ops.td",
+        "types.td",
+    ],
+    deps = [
+        "//shardy/dialect/sdy/ir:sdy_td_files",
+        "@llvm-project//mlir:CallInterfacesTdFiles",
+        "@llvm-project//mlir:FunctionInterfacesTdFiles",
+        "@llvm-project//mlir:InferTypeOpInterfaceTdFiles",
+        "@llvm-project//mlir:LoopLikeInterfaceTdFiles",
+        "@llvm-project//mlir:OpBaseTdFiles",
+        "@llvm-project//mlir:SideEffectInterfacesTdFiles",
+        "@stablehlo//:base_td_files",
+    ],
+)
+
+gentbl_cc_library(
+    name = "dialect_inc",
+    tbl_outs = {
+        "dialect.h.inc": ["-gen-dialect-decls"],
+        "dialect.cc.inc": ["-gen-dialect-defs"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "dialect.td",
+    deps = [
+        ":mpmd_td_files",
+    ],
+)
+
+gentbl_cc_library(
+    name = "canonicalization_inc",
+    tbl_outs = {"canonicalization.cc.inc": ["-gen-rewriters"]},
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "canonicalization.td",
+    deps = [":mpmd_td_files"],
+)
+
+gentbl_cc_library(
+    name = "ops_inc",
+    tbl_outs = {
+        "ops.h.inc": ["-gen-op-decls"],
+        "ops.cc.inc": ["-gen-op-defs"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "ops.td",
+    deps = [":mpmd_td_files"],
+)
+
+gentbl_cc_library(
+    name = "types_inc",
+    tbl_outs = {
+        "types.h.inc": ["-gen-typedef-decls"],
+        "types.cc.inc": ["-gen-typedef-defs"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "types.td",
+    deps = [
+        ":mpmd_td_files",
+    ],
+)
+
+gentbl_cc_library(
+    name = "attrs_inc",
+    tbl_outs = [
+        (
+            ["-gen-attrdef-decls"],
+            "attrs.h.inc",
+        ),
+        (
+            ["-gen-attrdef-defs"],
+            "attrs.cc.inc",
+        ),
+    ],
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "attrs.td",
+    deps = [":mpmd_td_files"],
+)
+
+gentbl_cc_library(
+    name = "enums_inc",
+    tbl_outs = {
+        "enums.h.inc": ["-gen-enum-decls"],
+        "enums.cc.inc": ["-gen-enum-defs"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "enums.td",
+    deps = [":mpmd_td_files"],
+)
+
+gentbl_filegroup(
+    name = "dialect_doc_gen",
+    tbl_outs = [
+        (
+            [
+                "-gen-dialect-doc",
+                "-dialect=mpmd",
+            ],
+            "g3doc/mpmd_dialect.md",
+        ),
+    ],
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "ops.td",
+    deps = [":mpmd_td_files"],
+)
+
+cc_library(
+    name = "dialect",
+    srcs = [
+        "dialect.cc",
+        "utils.cc",
+    ],
+    hdrs = [
+        "dialect.h",
+        "utils.h",
+    ],
+    deps = [
+        ":attrs_inc",
+        ":canonicalization_inc",
+        ":dialect_inc",
+        ":enums_inc",
+        ":fragment_arg_res_attrs",
+        ":ops_inc",
+        ":types_inc",
+        "//shardy/common:logging",
+        "//shardy/dialect/sdy/ir:dialect",
+        "//shardy/dialect/sdy/transforms/propagation:op_sharding_rule_builder",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:BytecodeWriter",
+        "@llvm-project//mlir:CallOpInterfaces",
+        "@llvm-project//mlir:DataLayoutInterfaces",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:FuncExtensions",
+        "@llvm-project//mlir:FunctionInterfaces",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:InferTypeOpInterface",
+        "@llvm-project//mlir:InliningUtils",
+        "@llvm-project//mlir:LoopLikeInterface",
+        "@llvm-project//mlir:QuantOps",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@stablehlo//:stablehlo_ops",
+    ],
+)
+
+cc_library(
+    name = "register",
+    srcs = ["register.cc"],
+    hdrs = ["register.h"],
+    deps = [
+        ":dialect",
+        "//shardy/dialect/sdy/ir:dialect",
+        "//shardy/dialect/sdy/ir:register",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:FuncExtensions",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:TensorDialect",
+        "@stablehlo//:stablehlo_ops",
+    ],
+)
+
+cc_library(
+    name = "fragment_arg_res_attrs",
+    hdrs = ["fragment_arg_res_attrs.h"],
+    deps = [
+        "//shardy/common:logging",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_test(
+    name = "utils_test",
+    srcs = ["utils_test.cc"],
+    deps = [
+        ":dialect",
+        ":register",
+        "//shardy/common:logging",
+        "//shardy/dialect/sdy/ir:dialect",
+        "@com_google_googletest//:gtest_main",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Parser",
+        "@llvm-project//mlir:Support",
+    ],
+)
diff --git a/shardy/dialect/mpmd/ir/attrs.td b/shardy/dialect/mpmd/ir/attrs.td
new file mode 100644
index 0000000..2e80efa
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/attrs.td
@@ -0,0 +1,121 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef MPMD_ATTRS
+#define MPMD_ATTRS
+
+include "mlir/IR/AttrTypeBase.td"
+include "mlir/IR/OpBase.td"
+include "mlir/IR/DialectBase.td"
+include "shardy/dialect/mpmd/ir/dialect.td"
+include "shardy/dialect/mpmd/ir/enums.td"
+
+def Mpmd_NamedMesh : AttrDef<Mpmd_Dialect, "NamedMesh"> {
+  let mnemonic = "named_mesh";
+  let summary = "A pair with a name and a Mesh.";
+  let parameters = (ins
+      StringRefParameter<"name">:$name,
+      AttrParameter<"sdy::MeshAttr", "mesh">:$mesh
+  );
+  let assemblyFormat = "`<` $name `:` $mesh `>`";
+}
+
+// A topology of SPMD meshes. This is used for MPMD partition where a program
+// is composed of many programs, each running in a different mesh.
+def Mpmd_Topology : AttrDef<Mpmd_Dialect, "Topology"> {
+  let mnemonic = "topology";
+  let summary = "Topology of named meshes.";
+  let parameters = (ins
+    ArrayRefParameter<"NamedMeshAttr", "topology meshes">:$meshes
+  );
+  // TODO(b/425894364): consider combining the assemblyFormat with custom
+  // parse/print methods to remove unnecessary whitespaces.
+  let assemblyFormat = "`<` $meshes `>`";
+  let genVerifyDecl = 1;
+}
+
+// Attribute specifying the origin of named computations.
+def Mpmd_UserOrigin : AttrDef<Mpmd_Dialect, "UserOrigin"> {
+  let mnemonic = "user_origin";
+  let summary = "Origin of user-specified computation.";
+  let parameters = (ins "::mlir::StringAttr":$userName,
+                        "int64_t":$transposeCount);
+  let assemblyFormat = "`<` $userName `` custom<OptionalTransposeCount>($transposeCount) `>`";
+
+  let extraClassDeclaration = [{
+    void printShort(llvm::raw_ostream& os);
+    void printShort(AsmPrinter& printer);
+    static mlir::Attribute parseShort(AsmParser& parser);
+  }];
+}
+
+// TODO: b/415235792 - Properly implement this and use this for the Origins in
+// the AssignOp and UnassignOp.
+//
+// The plan is to store the debugging information in this Attr.
+def Mpmd_Origin : AttrDef<Mpmd_Dialect, "Origin"> {
+  let mnemonic = "origin";
+  let summary = "Origin of mesh assignment.";
+  let description = [{
+    The origin of a mesh assignment.
+
+    `origin_label` is a human-readable label for the origin.
+    It is intended to be used for debugging purposes.
+  }];
+
+  let parameters = (ins
+    StringRefParameter<"origin_label">:$origin_label
+  );
+
+  let assemblyFormat = "`` $origin_label";
+}
+
+def Mpmd_MeshWithOrigins : AttrDef<Mpmd_Dialect, "MeshWithOrigins"> {
+  let mnemonic = "mesh_with_origins";
+  let summary = "Mesh with its origins.";
+
+  let parameters = (ins
+    StringRefParameter<"mesh_name">:$mesh_name,
+    OptionalArrayRefParameter<"OriginAttr", "origins">:$origins
+  );
+
+  let assemblyFormat = "`` $mesh_name (```[``` $origins^ `]`)?";
+
+  let builders = [
+    AttrBuilder<(ins
+      "::mlir::StringRef":$mesh_name
+    ), [{
+      return $_get($_ctxt, mesh_name,
+        /*origin=*/(mlir::ArrayRef<OriginAttr>){});
+    }]>,
+  ];
+}
+
+def Mpmd_MeshesWithOrigins : ArrayOfAttr<Mpmd_Dialect,
+                                 "MeshesWithOrigins", "meshes_with_origins",
+                                 "MeshWithOriginsAttr"> {
+  let summary = "A list of meshes with their origins.";
+  let assemblyFormat = "`<` (`>`) : ($value^ `>`)?";
+}
+
+def Mpmd_Reduction : AttrDef<Mpmd_Dialect, "Reduction"> {
+  let mnemonic = "reduction";
+  let summary = "Denotes a reduction.";
+  let parameters = (ins EnumParameter<Mpmd_ReductionType>:$reduction_type);
+  let constBuilderCall = "ReductionAttr::get($_builder.getContext(), $0)";
+  let assemblyFormat = "`<` $reduction_type `>`";
+}
+
+#endif  // MPMD_ATTRS
diff --git a/shardy/dialect/mpmd/ir/canonicalization.td b/shardy/dialect/mpmd/ir/canonicalization.td
new file mode 100644
index 0000000..240b711
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/canonicalization.td
@@ -0,0 +1,63 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef MPMD_CANONICALIZATION
+#define MPMD_CANONICALIZATION
+
+include "shardy/dialect/mpmd/ir/ops.td"
+include "shardy/dialect/mpmd/ir/types.td"
+include "mlir/IR/PatternBase.td"
+
+def HaveSameType : Constraint<CPred<"$0.getType() == $1.getType()">, "have same type">;
+
+def HaveSameMeshName : Constraint<
+    CPred<[{mlir::cast<mlir::mpmd::MeshTensorType>($0.getType()).getMeshName() ==
+            mlir::cast<mlir::mpmd::MeshTensorType>($1.getType()).getMeshName()}]>,
+    "have same mesh name">;
+
+def IdentityTransferPattern : Pat<(TransferOp:$result $tensor),
+                                (replaceWithValue $tensor),
+                                [(HaveSameType $result, $tensor)]>;
+
+// Replaces a chain of intra-mesh transfers with a single transfer.
+//
+// In symbols:
+//
+//  y = transfer x : (mesh_tensor<M, <D1>>) -> mesh_tensor<M, <D2>>
+//  z = transfer y : (mesh_tensor<M, <D2>>) -> mesh_tensor<M, <D3>>
+//  ~>
+//  z = transfer x : (mesh_tensor<M, <D1>>) -> mesh_tensor<M, <D3>>
+//
+//  where y has only one use, M is a specific mesh name, and each di denotes a
+//  different distributed type.
+// TODO(jupvfranco): Consider removing. These transfers could have been created
+// by the user.
+def IntraMeshTransferOfTransferPattern : Pat<(TransferOp:$outer_res (TransferOp:$inner_res $tensor)),
+                                             (TransferOp $tensor),
+                                             [(HaveSameMeshName $tensor, $inner_res),
+                                              (HaveSameMeshName $inner_res, $outer_res)]>;
+
+// Replaces a chain of broadcasts with a single broadcast.
+//
+// In symbols:
+//
+//  y = broadcast x
+//  z = broadcast y
+//  ~>
+//  z = broadcast x
+def BroadcastOfBroadcastPattern : Pat<(BroadcastOp (BroadcastOp AnyTensor: $operand)),
+                                        (BroadcastOp $operand)>;
+
+#endif
diff --git a/shardy/dialect/mpmd/ir/dialect.cc b/shardy/dialect/mpmd/ir/dialect.cc
new file mode 100644
index 0000000..97647db
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/dialect.cc
@@ -0,0 +1,1513 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+#include <cstdint>
+#include <functional>
+#include <optional>
+#include <string>
+#include <vector>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/STLFunctionalExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/ADT/Twine.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Quant/IR/Quant.h"
+#include "mlir/Dialect/Quant/IR/QuantTypes.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Dialect.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/SymbolTable.h"
+#include "mlir/IR/TypeRange.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Interfaces/CallInterfaces.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Support/TypeID.h"
+#include "mlir/Transforms/InliningUtils.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/parsers.h"
+#include "shardy/dialect/sdy/ir/printers.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+#include "shardy/dialect/sdy/transforms/propagation/op_sharding_rule_builder.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir::mpmd {
+
+using ::mlir::func::FuncOp;
+using ::mlir::sdy::TensorShardingAttr;
+
+namespace {
+
+struct MpmdDialectInlinerInterface : public DialectInlinerInterface {
+  using DialectInlinerInterface::DialectInlinerInterface;
+
+  // ATM we have two types of calls: FragmentCalls and CallOps. It is never
+  // legal to inline the former. It is legal to inline the latter (if late
+  // enough in the compiler pipeline), but when we do so, we want to make sure
+  // we copy the call_counter attributes. So we will use our own inline pass.
+  bool isLegalToInline(Operation* call, Operation* callable,
+                       bool wouldBeCloned) const final {
+    return false;
+  }
+  // MPMD region-based ops include fragments and control-flow loops, and they
+  // can always be the destination of an inlined call.
+  bool isLegalToInline(Region* dest, Region* src, bool wouldBeCloned,
+                       IRMapping& valueMapping) const final {
+    return true;
+  }
+  // Operations in mpmd dialect are legal to inline since they are pure.
+  bool isLegalToInline(Operation*, Region*, bool, IRMapping&) const final {
+    return true;
+  }
+};
+
+}  // namespace
+
+void MpmdDialect::initialize() {
+  addInterface<MpmdDialectInlinerInterface>();
+  addAttributes<
+#define GET_ATTRDEF_LIST
+#include "shardy/dialect/mpmd/ir/attrs.cc.inc"
+      >();
+  addTypes<
+#define GET_TYPEDEF_LIST
+#include "shardy/dialect/mpmd/ir/types.cc.inc"
+      >();
+  addOperations<
+#define GET_OP_LIST
+#include "shardy/dialect/mpmd/ir/ops.cc.inc"
+      >();
+}
+
+//===----------------------------------------------------------------------===//
+// TopologyAttr
+//===----------------------------------------------------------------------===//
+
+LogicalResult TopologyAttr::verify(
+    llvm::function_ref<mlir::InFlightDiagnostic()> emitError,
+    ArrayRef<NamedMeshAttr> meshes) {
+  if (meshes.empty()) {
+    return emitError() << "TopologyAttr must have at least one mesh.";
+  }
+
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// MeshTensorType
+//===----------------------------------------------------------------------===//
+
+namespace {
+
+StringAttr ParseMemoryKind(AsmParser& parser) {
+  StringAttr memory_kind;
+  // We always parse > since the memory)_kind is always the last attribute of
+  // the type.
+  if (parser.parseKeyword("memory_kind") || parser.parseEqual() ||
+      parser.parseAttribute(memory_kind) || parser.parseGreater()) {
+    return {};
+  }
+  return memory_kind;
+}
+
+}  // namespace
+
+Type MeshTensorType::parse(AsmParser& parser) {
+  StringAttr mesh_name_attr;
+  RankedTensorType ranked_tensor_type;
+  if (parser.parseLess() || parser.parseAttribute(mesh_name_attr) ||
+      parser.parseComma() || parser.parseType(ranked_tensor_type)) {
+    return {};
+  }
+
+  if (!parser.parseOptionalGreater()) {
+    return MeshTensorType::get(parser.getContext(), mesh_name_attr.getValue(),
+                               ranked_tensor_type);
+  }
+
+  if (parser.parseComma()) {
+    return {};
+  }
+
+  sdy::TensorShardingAttr sharding;
+
+  if (!parser.parseOptionalKeyword("sharding")) {
+    // Must have a sharding.
+    if (parser.parseEqual() ||
+        parser.parseCustomAttributeWithFallback(sharding)) {
+      return {};
+    }
+  } else {
+    // If there's no sharding, we must have a memory kind.
+    if (StringAttr memory_kind = ParseMemoryKind(parser)) {
+      return MeshTensorType::get(parser.getContext(), mesh_name_attr.getValue(),
+                                 ranked_tensor_type, sharding, memory_kind);
+    }
+    return {};
+  }
+
+  if (!parser.parseOptionalComma()) {
+    // If there's another comma, we must have a memory kind.
+    if (StringAttr memory_kind = ParseMemoryKind(parser)) {
+      return MeshTensorType::get(parser.getContext(), mesh_name_attr.getValue(),
+                                 ranked_tensor_type, sharding, memory_kind);
+    }
+    return {};
+  }
+
+  if (!parser.parseGreater()) {
+    return MeshTensorType::get(parser.getContext(), mesh_name_attr.getValue(),
+                               ranked_tensor_type, sharding);
+  }
+
+  return {};
+}
+
+void MeshTensorType::print(AsmPrinter& printer) const {
+  printer << "<\"" << getMeshName() << "\", ";
+  printer.printType(getRankedTensorType());
+  if (getSharding()) {
+    printer << ", sharding=";
+    printer.printStrippedAttrOrType(getSharding());
+  }
+  if (getMemoryKind()) {
+    printer << ", memory_kind=" << getMemoryKind();
+  }
+  printer << ">";
+}
+
+LogicalResult MeshTensorType::verifyForTopology(Operation* op) {
+  FailureOr<sdy::MeshAttr> mesh_attr = GetMeshAttr(op, getMeshName());
+  return succeeded(mesh_attr) ? verifyForMesh(*mesh_attr, op) : failure();
+}
+
+LogicalResult MeshTensorType::verifyForMesh(sdy::MeshAttr mesh_attr,
+                                            Operation* op) {
+  sdy::TensorShardingAttr sharding = getSharding();
+  if (sharding) {
+    if (sharding
+            .verifyForType(
+                getRankedTensorType(), mesh_attr,
+                /*emitError=*/
+                [op](StringRef msg) { return op->emitOpError(msg); },
+                /*checkDivisibility=*/true)
+            .failed()) {
+      return failure();
+    }
+  }
+  return success();
+}
+
+MeshTensorType MeshTensorType::getFullyReplicated(MLIRContext* ctx,
+                                                  StringRef mesh_name,
+                                                  sdy::MeshAttr mesh,
+                                                  RankedTensorType local_type) {
+  return MeshTensorType::get(ctx, mesh_name, local_type);
+}
+
+MeshTensorType MeshTensorType::replaceSharding(
+    sdy::TensorShardingAttr sharding) {
+  return MeshTensorType::get(getContext(), getMeshName(), getRankedTensorType(),
+                             sharding, getMemoryKind());
+}
+
+// Gets the local tensor type of the MeshTensorType wrt the given mesh and
+// sharding. Assumes that the sharding is valid wrt the mesh and tensor
+// type.
+RankedTensorType MeshTensorType::getLocalTensorType(sdy::MeshAttr sdy_mesh) {
+  sdy::TensorShardingAttr sharding = getSharding();
+  if (!sharding) {
+    return getGlobalTensorType();
+  }
+  return sharding.getLocalTensorType(getGlobalTensorType(), sdy_mesh);
+}
+
+RankedTensorType MeshTensorType::getLocalTensorType(Operation* op) {
+  // Assumes that the topology is homogeneous so we can just get the first mesh.
+  return MeshTensorType::getLocalTensorType(
+      GetTopologyMeshes(sdy::getEnclosingOfType<FuncOp>(op)).front().getMesh());
+}
+
+// Functions for the ShapedTypeInterface.
+Type MeshTensorType::getElementType() const {
+  return getRankedTensorType().getElementType();
+}
+
+bool MeshTensorType::hasRank() const { return true; }
+
+ArrayRef<int64_t> MeshTensorType::getShape() const {
+  return getRankedTensorType().getShape();
+}
+
+// If a new shape is provided, the user intended to change the shape and we
+// create a new type without sharding. Otherwise, we create a new type with the
+// current shape and sharding.
+ShapedType MeshTensorType::cloneWith(std::optional<ArrayRef<int64_t>> shape,
+                                     Type elementType) const {
+  return MeshTensorType::get(
+      getContext(), getMeshName(),
+      RankedTensorType::get(shape.value_or(getShape()), elementType),
+      shape.has_value() ? nullptr : getSharding(), getMemoryKind());
+}
+
+bool MeshTensorType::isOnHost() {
+  return getMemoryKind() &&
+         getMemoryKind().getValue() == kMemoryKindPinnedHost;
+}
+
+//===----------------------------------------------------------------------===//
+// UserOriginAttr
+//===----------------------------------------------------------------------===//
+
+namespace {
+
+void printOptionalTransposeCount(llvm::raw_ostream& os,
+                                 int64_t transpose_count) {
+  if (transpose_count > 0) {
+    os << "(" << transpose_count << ")";
+  }
+}
+
+}  // namespace
+
+void printOptionalTransposeCount(AsmPrinter& printer, int64_t transpose_count) {
+  printOptionalTransposeCount(printer.getStream(), transpose_count);
+}
+
+void UserOriginAttr::printShort(llvm::raw_ostream& os) {
+  os << "\"" << getUserName().strref() << "\"";
+  printOptionalTransposeCount(os, getTransposeCount());
+}
+
+void UserOriginAttr::printShort(AsmPrinter& printer) {
+  printer.printAttribute(getUserName());
+  printOptionalTransposeCount(printer, getTransposeCount());
+}
+
+ParseResult parseOptionalTransposeCount(AsmParser& parser,
+                                        int64_t& transpose_count) {
+  transpose_count = 0;
+  if (!parser.parseOptionalLParen()) {
+    if (!parser.parseInteger(transpose_count) && !parser.parseRParen()) {
+      return success();
+    }
+    return parser.emitError(parser.getCurrentLocation())
+           << "could not parse transpose count value.";
+  }
+  return success();
+}
+
+Attribute UserOriginAttr::parseShort(AsmParser& parser) {
+  std::string name;
+  int64_t transpose_count = 0;
+  if (parser.parseString(&name) ||
+      parseOptionalTransposeCount(parser, transpose_count)) {
+    parser.emitError(parser.getCurrentLocation())
+        << "could not parse user origin.";
+    return UserOriginAttr();
+  }
+  return UserOriginAttr::get(parser.getContext(),
+                             StringAttr::get(parser.getContext(), name),
+                             transpose_count);
+}
+
+namespace {
+
+ParseResult parseFunctionalTypeAndResolveOperands(
+    OpAsmParser& parser, OperationState& result,
+    llvm::ArrayRef<OpAsmParser::UnresolvedOperand> operands) {
+  SmallVector<Type> operand_types;
+  SmallVector<Type> result_types;
+  if (parser.parseColon() || parser.parseLParen() ||
+      (parser.parseOptionalRParen() &&
+       (parser.parseTypeList(operand_types) || parser.parseRParen())) ||
+      parser.parseArrowTypeList(result_types)) {
+    return failure();
+  }
+
+  SmallVector<Value> operand_values;
+  if (parser.resolveOperands(operands, operand_types,
+                             parser.getCurrentLocation(), operand_values)) {
+    return failure();
+  }
+
+  result.addOperands(operand_values);
+  result.addTypes(result_types);
+
+  return success();
+}
+
+// Parses an op with a single region with single block and
+// verifies the terminator exists. It adds the parsed attrs, operands and result
+// types to the parsing result. This is an example of what it parses:
+//
+// (%opArg1, ..., %opArgN) { optional-attr-dict } (%blockArg1, ..., %blockArgM)
+// {
+//   // ops in the block
+// } : (inputType1, ..., inputTypeN) -> (resultType1, ..., resultTypeK)
+ParseResult parseSingleBlockRegionOp(OpAsmParser& parser,
+                                     OperationState& result,
+                                     NamedAttrList attrs) {
+  llvm::SmallVector<OpAsmParser::UnresolvedOperand> operands;
+  if (parser.parseOperandList(operands, OpAsmParser::Delimiter::Paren)) {
+    return failure();
+  }
+
+  // Parse optional attributes.
+  if (parser.parseOptionalAttrDict(attrs)) {
+    return failure();
+  }
+
+  if (sdy::parseSingleBlockRegionNoBlockId(parser, *result.addRegion())) {
+    return failure();
+  }
+
+  if (parseFunctionalTypeAndResolveOperands(parser, result, operands)) {
+    return failure();
+  }
+
+  result.addAttributes(attrs.getAttrs());
+
+  return success();
+}
+
+// Prints a single region op with a single block without the block id, for
+// example:
+//
+// (%opArg1, ..., %opArgN) { optional-attr-dict } (%blockArg1, ..., %blockArgM)
+// {
+//   // ops in the block
+// } : (inputType1, ..., inputTypeN) -> (resultType1, ..., resultTypeK)
+//
+// The `excluded_attr_names` are omitted from the optional-attr-dict and are
+// expected to be handled elsewhere.
+void printSingleBlockRegionOp(OpAsmPrinter& p, Operation* op,
+                              ArrayRef<StringRef> excluded_attr_names = {}) {
+  p << " (";
+  p.printOperands(op->getOperands());
+  p << ")";
+
+  // Print optional attributes.
+  p.printOptionalAttrDict(op->getAttrs(),
+                          /*elidedAttrs=*/excluded_attr_names);
+
+  p << " ";
+  sdy::printSingleBlockRegionNoBlockId(p, op, op->getRegion(0));
+
+  p << " : ";
+  p.printFunctionalType(op);
+}
+
+LogicalResult AllInnerAndOuterTypesMatchInNamedComputation(
+    NamedComputationOp op, TypeRange inner_types, TypeRange outer_types,
+    StringRef inner_name, StringRef outer_name) {
+  if (inner_types.size() != outer_types.size()) {
+    return op.emitError("number of ")
+           << inner_name << "s must match the number of " << outer_name
+           << "s respectively: " << inner_types.size()
+           << " != " << outer_types.size();
+  }
+
+  for (auto [inner_type, outer_type] : llvm::zip(inner_types, outer_types)) {
+    if (inner_type != outer_type) {
+      return op.emitError("expected the type of the ")
+             << inner_name << " to match the type of " << outer_name << ": "
+             << inner_type << ", got: " << outer_type;
+    }
+  }
+
+  return success();
+}
+
+}  // namespace
+
+//===----------------------------------------------------------------------===//
+// NamedComputationOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult NamedComputationOp::verify() {
+  if (failed(AllInnerAndOuterTypesMatchInNamedComputation(
+          *this, getBody()->getArgumentTypes(), getOperandTypes(),
+          "block argument", "operand")) ||
+      failed(AllInnerAndOuterTypesMatchInNamedComputation(
+          *this, getBody()->getTerminator()->getOperandTypes(),
+          getResultTypes(), "returned value", "result"))) {
+    return failure();
+  }
+
+  return success();
+}
+
+// mpmd.named_computation<"name"> (%op1,..,%opN)
+//   (%arg1, ..., %argN) {
+//  ...
+// } : type
+void NamedComputationOp::print(OpAsmPrinter& p) {
+  p << "<";
+  getOrigin().printShort(p);
+  p << ">";
+
+  printSingleBlockRegionOp(p, *this,
+                           /*excluded_attr_names=*/{"name", "origin"});
+}
+
+ParseResult NamedComputationOp::parse(OpAsmParser& parser,
+                                      OperationState& result) {
+  NamedAttrList attrs;
+
+  if (parser.parseLess()) {
+    return failure();
+  }
+  auto origin = UserOriginAttr::parseShort(parser);
+  if (!origin) {
+    return failure();
+  }
+  if (parser.parseGreater()) {
+    return failure();
+  }
+  attrs.set("origin", origin);
+  if (parseSingleBlockRegionOp(parser, result, attrs)) {
+    return failure();
+  }
+
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// FragmentOp
+//===----------------------------------------------------------------------===//
+
+namespace {
+
+// Checks that the `inner_type` and the `outer_type` of a FragmentOp or
+// FragmentCallOp match with each other. When we are verifying the inputs of the
+// fragment, `inner_type` is the type of a BlockArgument and `outer_type` is the
+// type of a fragment's operator. When we are verifying the outputs of the
+// fragment, `inner_type` is the type of an operand of the fragment's return and
+// `outer_type` is the type of a fragment's result.
+//
+// The `outer_type` is expected to be a a valid MeshTensorType w.r.t. the mesh,
+// such that its global tensor type must be identical to `inner_type`, which is
+// expected to be a RankedTensorType.
+template <class FragmentOpTy>
+LogicalResult InnerAndOuterTypesMatchInFragment(FragmentOpTy op,
+                                                Type inner_type,
+                                                Type outer_type,
+                                                StringRef inner_name,
+                                                StringRef outer_name) {
+  StringRef mesh_name = op.getMeshName();
+  FailureOr<sdy::MeshAttr> mesh_attr = GetMeshAttr(op, mesh_name);
+  if (failed(mesh_attr)) {
+    return failure();
+  }
+  auto mesh_type = cast<MeshTensorType>(outer_type);
+  if (mesh_type.getMeshName() != mesh_name) {
+    return op.emitError("expected the mesh name of the ")
+           << outer_name
+           << " to match that of the fragment op: " << mesh_type.getMeshName()
+           << " != " << mesh_name;
+  }
+  if (mesh_type.verifyForMesh(*mesh_attr, op).failed()) {
+    return failure();
+  }
+
+  if (inner_type != mesh_type.getGlobalTensorType()) {
+    return op.emitError("expected the type of the ")
+           << inner_name << " to be global tensor type " << outer_name << ": "
+           << mesh_type.getGlobalTensorType() << ", got: " << inner_type;
+  }
+
+  return success();
+}
+
+template <class FragmentOpTy>
+LogicalResult AllInnerAndOuterTypesMatchInFragment(FragmentOpTy op,
+                                                   TypeRange inner_types,
+                                                   TypeRange outer_types,
+                                                   StringRef inner_name,
+                                                   StringRef outer_name) {
+  if (inner_types.size() != outer_types.size()) {
+    return op.emitError("number of ")
+           << inner_name << "s must match the number of " << outer_name
+           << "s respectively: " << inner_types.size()
+           << " != " << outer_types.size();
+  }
+
+  for (auto [inner_type, outer_type] : llvm::zip(inner_types, outer_types)) {
+    if (failed(InnerAndOuterTypesMatchInFragment(op, inner_type, outer_type,
+                                                 inner_name, outer_name))) {
+      return failure();
+    }
+  }
+
+  return success();
+}
+
+}  // namespace
+
+LogicalResult FragmentOp::verify() {
+  ReturnOp return_op = cast<ReturnOp>(getBody()->getTerminator());
+  return success(AllInnerAndOuterTypesMatchInFragment(
+                     *this, getBody()->getArgumentTypes(), getOperandTypes(),
+                     "block argument", "operand")
+                     .succeeded() &&
+                 AllInnerAndOuterTypesMatchInFragment(
+                     *this, return_op.getOperandTypes(), getResultTypes(),
+                     "returned value", "result")
+                     .succeeded());
+}
+
+template <typename OpT>
+void printStage(OpT op, llvm::raw_ostream& os) {
+  if (op.getStageIdAttr()) {
+    os << ", stage=";
+    os << op.getStageIdAttr().getInt();
+  }
+}
+
+template <typename OpT>
+void printShardings(OpT op, OpAsmPrinter& p,
+                    std::optional<sdy::TensorShardingPerValueAttr> shardings,
+                    StringRef sharding_attr_name) {
+  if (!shardings) {
+    return;
+  }
+  p << ", " << sharding_attr_name << "=";
+  sdy::printStrippedTensorShardingPerValueAttr(p, op, *shardings);
+}
+
+template <typename OpT>
+void printMeshAndOrigin(OpT op, llvm::raw_ostream& os) {
+  os << "mesh=\"" << op.getMeshName() << "\"";
+  os << ", origin=[";
+  llvm::interleaveComma(op.getOriginAttr(), os, [&](Attribute attr) {
+    cast<UserOriginAttr>(attr).printShort(os);
+  });
+  os << "]";
+}
+
+void FragmentOp::printFragmentMetadata(llvm::raw_ostream& os) {
+  printMeshAndOrigin<FragmentOp>(*this, os);
+  printStage<FragmentOp>(*this, os);
+}
+
+// mpmd.fragment<mesh="mesh_name", origin=[... origin ...]>
+//  (%op1,..,%opN) dict-attrs
+// (%arg1, ..., %argN) {
+void FragmentOp::print(OpAsmPrinter& p) {
+  p << "<";
+  printFragmentMetadata(p.getStream());
+
+  printShardings(*this, p, getInShardings(), "in_shardings");
+  printShardings(*this, p, getOutShardings(), "out_shardings");
+  p << ">";
+  printSingleBlockRegionOp(
+      p, *this,
+      /*excluded_attr_names=*/
+      {"mesh_name", "origin", "stage_id", "in_shardings", "out_shardings"});
+}
+
+namespace {
+
+ParseResult parseMeshAndOrigin(OpAsmParser& parser, NamedAttrList& attrs) {
+  StringAttr mesh_name_attr;
+  std::vector<Attribute> user_origins;
+
+  if (parser.parseKeyword("mesh") || parser.parseEqual() ||
+      parser.parseAttribute(mesh_name_attr, "mesh_name", attrs))
+    return failure();
+
+  auto parse_user_origin_fn = [&]() -> ParseResult {
+    Attribute attr = UserOriginAttr::parseShort(parser);
+    if (!attr) return failure();
+    user_origins.push_back(attr);
+    return success();
+  };
+
+  if (parser.parseComma() || parser.parseKeyword("origin") ||
+      parser.parseEqual() ||
+      parser.parseCommaSeparatedList(AsmParser::Delimiter::Square,
+                                     parse_user_origin_fn)) {
+    return failure();
+  }
+  attrs.set("origin", ArrayAttr::get(parser.getContext(), user_origins));
+  return success();
+}
+
+ParseResult parseOptionalStage(OpAsmParser& parser, NamedAttrList& attrs,
+                               bool& parsed_comma_only) {
+  IntegerAttr stage_attr = IntegerAttr();
+  if (parser.parseOptionalComma()) {
+    return success();
+  }
+  if (parser.parseOptionalKeyword("stage")) {
+    parsed_comma_only = true;
+    return success();
+  }
+  // The "stage" keyword is present, meaning the stage attribute must be
+  // defined.
+  if (parser.parseEqual() ||
+      parser.parseAttribute(stage_attr, "stage_id", attrs)) {
+    return failure();
+  }
+  return success();
+}
+
+// Assumes the sharding attribute and keyword are both sharding_attr_name.
+ParseResult parseOptionalShardings(OpAsmParser& parser, NamedAttrList& attrs,
+                                   bool& parsed_comma_only,
+                                   StringRef sharding_attr_name) {
+  // Only parse the optional comma if we haven't parsed it.
+  if (!parsed_comma_only && parser.parseOptionalComma()) {
+    return success();
+  }
+
+  if (parser.parseOptionalKeyword(sharding_attr_name)) {
+    parsed_comma_only = true;
+    return success();
+  }
+  parsed_comma_only = false;
+
+  // This means that the sharding_attr_name attribute must be defined.
+  if (parser.parseEqual()) {
+    return failure();
+  }
+  sdy::TensorShardingPerValueAttr shardingPerValue;
+  if (sdy::parseStrippedTensorShardingPerValueAttr(parser, shardingPerValue)) {
+    return failure();
+  } else {
+    attrs.set(sharding_attr_name, shardingPerValue);
+  }
+  return success();
+}
+
+}  // namespace
+
+ParseResult FragmentOp::parse(OpAsmParser& parser, OperationState& result) {
+  NamedAttrList attrs;
+  bool parsed_comma_only = false;
+  // TODO: b/360076171 - Consider parsing a dictionary of optional attributes.
+  if (parser.parseLess() || parseMeshAndOrigin(parser, attrs) ||
+      parseOptionalStage(parser, attrs, parsed_comma_only) ||
+      parseOptionalShardings(parser, attrs, parsed_comma_only,
+                             "in_shardings") ||
+      parseOptionalShardings(parser, attrs, parsed_comma_only,
+                             "out_shardings") ||
+      parser.parseGreater() ||
+      parseSingleBlockRegionOp(parser, result, attrs)) {
+    return failure();
+  }
+  return success();
+}
+
+// Overrides of `Sdy_ShardableDataFlowOpInterface` functions.
+SmallVector<sdy::TensorShardingAttr>
+FragmentOp::getBlockArgumentEdgeOwnerShardings() {
+  if (std::optional<sdy::TensorShardingPerValueAttr> in_shardings =
+          getInShardings()) {
+    return llvm::to_vector(in_shardings->getShardings());
+  }
+  return {};
+}
+
+SmallVector<sdy::TensorShardingAttr>
+FragmentOp::getOpResultEdgeOwnerShardings() {
+  if (std::optional<sdy::TensorShardingPerValueAttr> out_shardings =
+          getOutShardings()) {
+    return llvm::to_vector(out_shardings->getShardings());
+  }
+  return {};
+}
+
+void FragmentOp::setBlockArgumentEdgeOwnerShardings(
+    ArrayRef<sdy::TensorShardingAttr> shardings) {
+  setInShardingsAttr(
+      sdy::TensorShardingPerValueAttr::get(getContext(), shardings));
+}
+
+void FragmentOp::setOpResultEdgeOwnerShardings(
+    ArrayRef<sdy::TensorShardingAttr> shardings) {
+  setOutShardingsAttr(
+      sdy::TensorShardingPerValueAttr::get(getContext(), shardings));
+}
+
+ArrayRef<BlockArgument> FragmentOp::getBlockArgumentEdgeOwners() {
+  return getBody()->getArguments();
+}
+
+ResultRange FragmentOp::getOpResultEdgeOwners() { return getResults(); }
+
+// Sets the sharding of a specific result of the fragment.
+void FragmentOp::setUserSpecifiedResultSharding(
+    unsigned result_index, sdy::TensorShardingAttr new_sharding) {
+  if (!new_sharding) {
+    return;
+  }
+  std::optional<sdy::TensorShardingPerValueAttr> current_result_sharding =
+      getOutShardings();
+
+  // If none of the results have a sharding, create a new one. Otherwise,
+  // replace the sharding of the result at the specified index.
+  if (!current_result_sharding) {
+    setOutShardingsAttr(
+        sdy::TensorShardingPerValueAttr::getOpenWithShardingAtIndex(
+            getContext(), getResultTypes(), result_index, new_sharding));
+  } else {
+    setOutShardingsAttr(current_result_sharding->replaceValueSharding(
+        result_index, new_sharding));
+  }
+}
+
+void FragmentOp::setInputSharding(
+    unsigned input_index, sdy::TensorShardingAttr sharding) {
+  if (!sharding) {
+    return;
+  }
+  std::optional<sdy::TensorShardingPerValueAttr> current_shardings =
+      getInShardings();
+
+  // If none of the results have a sharding, create a new one. Otherwise,
+  // replace the sharding of the result at the specified index.
+  if (!current_shardings) {
+    setInShardingsAttr(
+        sdy::TensorShardingPerValueAttr::getOpenWithShardingAtIndex(
+            getContext(), getResultTypes(), input_index, sharding));
+  } else {
+    setInShardingsAttr(
+        current_shardings->replaceValueSharding(input_index, sharding));
+  }
+}
+
+// Gets the sources given a target value which need not be an edge owner. Note
+// that the return values is a vector, for fragments there can only be one
+// value but sdy's interface expects a vector. For example, given the
+// following fragment,
+// ```
+// %r = fragment (%arg0) (%operand0) {
+//   %a = some_op
+//   return %a
+// }
+// ```
+// If the target is a block argument (e.g., `%operand0`), return `%arg0`.
+// If the target is a result (e.g., `%r`), return `%a`.
+SmallVector<OpOperand*> FragmentOp::getEdgeSources(Value owner) {
+  SDY_CHECK_EQ(sdy::getOwningOp(owner), getOperation());
+  if (auto op_result = dyn_cast<OpResult>(owner)) {
+    return {
+        &getBody()->getTerminator()->getOpOperand(op_result.getResultNumber())};
+  }
+  return {
+      &getOperation()->getOpOperand(cast<BlockArgument>(owner).getArgNumber())};
+}
+
+// Returns the edge owner value given a `target`. For fragments, there is only
+// one target per data flow edge which is also the edge owner.
+Value FragmentOp::getEdgeOwnerFromTarget(Value target) {
+  // Check the target is owned by the fragment.
+  SDY_CHECK_EQ(sdy::getOwningOp(target), getOperation());
+  return target;
+}
+
+// Returns the edge owner given a `source`.
+// Given the following fragment
+// ```
+// %r = fragment (%arg0) (%operand0) {
+//  %a = some_op
+//  return %a
+// }
+// ```
+//
+// If the `source` is an operand of a return op, return the corresponding
+// result. Otherwise it should be an operand of the fragment, return the block
+// argument with the same index.
+Value FragmentOp::getEdgeOwnerFromSource(OpOperand& source) {
+  Operation* source_owner = source.getOwner();
+  if (source_owner->hasTrait<OpTrait::IsTerminator>()) {
+    SDY_CHECK_EQ(source_owner->getParentOp(), getOperation());
+    return getResult(source.getOperandNumber());
+  } else {
+    SDY_CHECK_EQ(source_owner, getOperation());
+    return getBody()->getArgument(source.getOperandNumber());
+  }
+}
+
+bool FragmentOp::shouldKeepEdgeOwnerShardingsDivisible() { return true; }
+
+namespace {
+FragmentOp CreateMeshFragmentWithBody(
+    Location loc, ArrayRef<Attribute> user_origin, StringRef mesh_name,
+    ValueRange tensors, TypeRange result_types, OpBuilder& builder,
+    FragmentOpBodyPopulator body_populator,
+    std::function<Type(Value, sdy::MeshAttr)> get_arg_type) {
+  auto origin_attr = ArrayAttr::get(builder.getContext(), user_origin);
+  // Only user defined fragments can be assigned to a stage and any fragment
+  // created by the compiler is considered to be an inferred fragment.
+  // Therefore, the created fragment isn't assigned to a stage.
+  FragmentOp fragment_op = builder.create<FragmentOp>(
+      loc, result_types, tensors, origin_attr, mesh_name,
+      /*stage_id=*/IntegerAttr());
+  Block& fragment_block = fragment_op.getRegion().emplaceBlock();
+  sdy::MeshAttr mesh_attr = GetMeshOrFail(fragment_op, mesh_name);
+
+  for (Value operand : tensors) {
+    fragment_block.addArgument(get_arg_type(operand, mesh_attr),
+                               operand.getLoc());
+  }
+  ArrayRef<Value> arguments(fragment_block.args_begin(),
+                            fragment_block.args_end());
+
+  OpBuilder block_builder = OpBuilder::atBlockBegin(&fragment_block);
+  block_builder.create<ReturnOp>(loc, body_populator(arguments, block_builder));
+  return fragment_op;
+}
+}  // namespace
+
+FragmentOp FragmentOp::createMeshFragmentWithGlobalBody(
+    Location loc, ArrayRef<Attribute> user_origin, StringRef mesh_name,
+    ValueRange tensors, TypeRange result_types, OpBuilder& builder,
+    FragmentOpBodyPopulator body_populator) {
+  return CreateMeshFragmentWithBody(loc, user_origin, mesh_name, tensors,
+                                    result_types, builder, body_populator,
+                                    GetGlobalTensorTypeFromMeshType);
+}
+
+//===----------------------------------------------------------------------===//
+// FragmentCallOp
+//===----------------------------------------------------------------------===//
+
+void FragmentCallOp::setCalleeFromCallable(CallInterfaceCallable callee) {
+  // Direct call.
+  if (FlatSymbolRefAttr calleeAttr = getCalleeAttr()) {
+    auto symRef = callee.get<SymbolRefAttr>();
+    return setCalleeAttr(cast<FlatSymbolRefAttr>(symRef));
+  }
+  // Indirect call, callee Value is the first operand.
+  return setOperand(0, callee.get<Value>());
+}
+
+LogicalResult FragmentCallOp::verifySymbolUses(
+    SymbolTableCollection& symbolTable) {
+  // Check that the callee references a valid function.
+  auto func_op =
+      symbolTable.lookupNearestSymbolFrom<FuncOp>(*this, getCalleeAttr());
+  if (!func_op) {
+    return emitError("'") << getCallee()
+                          << "' does not reference a valid function";
+  }
+
+  FunctionType func_type = func_op.getFunctionType();
+  if (failed(AllInnerAndOuterTypesMatchInFragment(
+          *this, func_type.getInputs(), getOperandTypes(), "block argument",
+          "operand")) ||
+      failed(AllInnerAndOuterTypesMatchInFragment(
+          *this, func_type.getResults(), getResultTypes(), "returned value",
+          "result"))) {
+    return failure();
+  }
+
+  // We already verified the call mesh is valid above when verifying types.
+  sdy::MeshAttr call_mesh = GetMeshOrFail(*this, getMeshName());
+  FailureOr<sdy::MeshAttr> func_mesh = GetMeshAttr(func_op);
+  if (failed(func_mesh)) {
+    return failure();
+  } else if (call_mesh != *func_mesh) {
+    return emitError(
+               "Expected mesh of fragment call and callee function to "
+               "match: ")
+           << call_mesh << " vs " << *func_mesh;
+  } else {
+    return success();
+  }
+}
+
+// mpmd.fragment_call<mesh="mesh_name", origin=[... origin ...]>
+//   @callee(%op1,..,%opN) dict-attrs : functional-type
+void FragmentCallOp::print(OpAsmPrinter& p) {
+  p << "<";
+  printMeshAndOrigin(*this, p.getStream());
+  p << ">";
+  p << " ";
+  p.printSymbolName(getCalleeAttr().getValue());
+  p << "(";
+  p.printOperands(getOperands());
+  p << ")";
+  // Print optional attributes.
+  p.printOptionalAttrDict(
+      getOperation()->getAttrs(),
+      /*excluded_attr_names=*/{"mesh_name", "origin", "callee"});
+  p << " : ";
+  p.printFunctionalType(getOperation());
+}
+
+ParseResult FragmentCallOp::parse(OpAsmParser& parser, OperationState& result) {
+  NamedAttrList attrs;
+  llvm::SmallVector<OpAsmParser::UnresolvedOperand> operands;
+  SymbolRefAttr callee;
+  if (parser.parseLess() || parseMeshAndOrigin(parser, attrs) ||
+      parser.parseGreater() || parser.parseAttribute(callee, "callee", attrs) ||
+      parser.parseOperandList(operands, OpAsmParser::Delimiter::Paren) ||
+      parser.parseOptionalAttrDict(attrs) ||
+      parseFunctionalTypeAndResolveOperands(parser, result, operands)) {
+    return failure();
+  }
+  result.addAttributes(attrs.getAttrs());
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// TransferOp
+//===----------------------------------------------------------------------===//
+
+bool TransferOp::isIntraMesh() {
+  return getTensor().getType().getMeshName() ==
+             getResult().getType().getMeshName() &&
+         getTensor().getType().getMemoryKind() ==
+             getResult().getType().getMemoryKind();
+}
+
+namespace {
+
+StringAttr FindMemoryKindInAttributes(Value value, FuncOp func) {
+  if (auto block_arg = dyn_cast<BlockArgument>(value)) {
+    return func.getArgAttrOfType<StringAttr>(block_arg.getArgNumber(),
+                                             kMemoryKindAttr);
+  }
+
+  if (auto transfer_producer = dyn_cast<TransferOp>(value.getDefiningOp())) {
+    return transfer_producer->getAttrOfType<StringAttr>(kMemoryKindAttr);
+  }
+
+  // Operations with multiple results.
+  if (isa<FragmentOp, CallOp, FragmentCallOp, ForOp>(value.getDefiningOp())) {
+    return dyn_cast_if_present<StringAttr>(
+        GetResAttr(value.getDefiningOp(),
+                   cast<OpResult>(value).getResultNumber(), kMemoryKindAttr));
+  }
+  return nullptr;
+}
+
+}  // namespace
+
+LogicalResult TransferOp::verify() {
+  auto mesh_type_in = cast<MeshTensorType>(getTensor().getType());
+  auto mesh_type_out = cast<MeshTensorType>(getResult().getType());
+
+  if (mesh_type_in.verifyForTopology(getOperation()).failed() ||
+      mesh_type_out.verifyForTopology(getOperation()).failed()) {
+    return failure();
+  }
+  if (mesh_type_in.getRankedTensorType() !=
+      mesh_type_out.getRankedTensorType()) {
+    return emitError("cannot perform transfer between given types ")
+           << mesh_type_in << " and " << mesh_type_out
+           << ": global tensor types are different";
+  }
+
+  if (StringAttr in_memory_kind = mesh_type_in.getMemoryKind()) {
+    if (in_memory_kind.getValue() != kMemoryKindPinnedHost &&
+        in_memory_kind.getValue() != kMemoryKindDevice) {
+      return emitError("memory kind must be either '")
+             << kMemoryKindPinnedHost << "' or '" << kMemoryKindDevice << "'. Found '"
+             << in_memory_kind.getValue() << "'.";
+    }
+  }
+
+  if (StringAttr out_memory_kind = mesh_type_out.getMemoryKind()) {
+    if (out_memory_kind.getValue() != kMemoryKindPinnedHost &&
+        out_memory_kind.getValue() != kMemoryKindDevice) {
+      return emitError("memory kind must be either '")
+             << kMemoryKindPinnedHost << "' or '" << kMemoryKindDevice << "'. Found '"
+             << out_memory_kind.getValue() << "'.";
+    }
+  }
+
+  // TODO: b/399865449 - We should not rely on attributes for host offloading.
+  // Instead, we should use the memory kind in the type.
+  StringAttr in_memory_kind = FindMemoryKindInAttributes(
+      getTensor(), getOperation()->getParentOfType<FuncOp>());
+  if (in_memory_kind && in_memory_kind.getValue() == kMemoryKindPinnedHost) {
+    return emitError(
+        "Transfers from host with attributes are not supported. Memory kinds "
+        "must be expressed in the type.");
+  }
+  StringAttr out_memory_kind = FindMemoryKindInAttributes(
+      getResult(), getOperation()->getParentOfType<FuncOp>());
+  if (out_memory_kind && out_memory_kind.getValue() == kMemoryKindPinnedHost) {
+    return emitError(
+        "Transfers to host with attributes are not supported. Memory kinds "
+        "must expressed be in the type.");
+  }
+
+  return success();
+}
+
+sdy::OpShardingRuleAttr TransferOp::getShardingRule() {
+  return sdy::OpShardingRuleBuilder::buildPointwise(*this);
+}
+
+bool TransferOp::shouldKeepOutputShardingsDivisible() { return true; }
+
+namespace {
+
+// Verifies all of the following:
+// 1. The given `mesh_type` is valid.
+// 2. `mesh_type` is fully replicated.
+// 3. The given `local_type` matches the distributed type of `mesh_type`.
+LogicalResult VerifyAssignmentOpTypes(Operation* op,
+                                      RankedTensorType local_type,
+                                      MeshTensorType mesh_type) {
+  if (mesh_type.verifyForTopology(op).failed()) {
+    return failure();
+  }
+
+  // We expect the distributed types to be fully replicated because the
+  // assign/unassign ops are only present before import, when the program is
+  // still unpartitioned.
+  if (!mesh_type.isFullyReplicated()) {
+    return op->emitError() << "MeshTensorType should be fully replicated: "
+                           << mesh_type;
+  }
+
+  if (mesh_type.getRankedTensorType() != local_type) {
+    return op->emitError("mismatch between the given local type ")
+           << local_type << " and mesh_type ranked tensor type: "
+           << mesh_type.getRankedTensorType();
+  }
+
+  return success();
+}
+
+}  // namespace
+
+//===----------------------------------------------------------------------===//
+// AssignOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult AssignOp::verify() {
+  auto local_type_in = cast<RankedTensorType>(getTensor().getType());
+  auto mesh_type_out = cast<MeshTensorType>(getResult().getType());
+
+  return VerifyAssignmentOpTypes(getOperation(), local_type_in, mesh_type_out);
+}
+
+//===----------------------------------------------------------------------===//
+// UnassignOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult UnassignOp::verify() {
+  auto mesh_type_in = cast<MeshTensorType>(getTensor().getType());
+  auto local_type_out = cast<RankedTensorType>(getResult().getType());
+
+  return VerifyAssignmentOpTypes(getOperation(), local_type_out, mesh_type_in);
+}
+
+LogicalResult UnassignOp::inferReturnTypeComponents(
+    MLIRContext*, std::optional<Location> location, ValueShapeRange operands,
+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,
+    llvm::SmallVectorImpl<ShapedTypeComponents>& inferredReturnShapes) {
+  UnassignOp::Adaptor adaptor(operands, attributes, properties, regions);
+  inferredReturnShapes.emplace_back(
+      cast<ShapedType>(cast<MeshTensorType>(adaptor.getTensor().getType())
+                           .getRankedTensorType()));
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// CallOp
+//===----------------------------------------------------------------------===//
+
+void CallOp::setCalleeFromCallable(CallInterfaceCallable callee) {
+  // Direct call.
+  if (FlatSymbolRefAttr calleeAttr = getCalleeAttr()) {
+    auto symRef = callee.get<SymbolRefAttr>();
+    setCalleeAttr(cast<FlatSymbolRefAttr>(symRef));
+  }
+  // Indirect call, callee Value is the first operand.
+  setOperand(0, callee.get<Value>());
+}
+
+LogicalResult CallOp::verifySymbolUses(SymbolTableCollection& symbolTable) {
+  // Check that the callee references a valid function.
+  auto func_op =
+      symbolTable.lookupNearestSymbolFrom<FuncOp>(*this, getCalleeAttr());
+  if (!func_op) {
+    return emitError("No function was found for function ref '")
+           << getCallee() << "'";
+  }
+
+  FunctionType func_type = func_op.getFunctionType();
+  TypeRange operand_types = getOperandTypes();
+  for (auto [function_input_type, operand_type] :
+       llvm::zip(func_type.getInputs(), operand_types)) {
+    if (function_input_type != operand_type) {
+      return emitError("Type mismatch. Expected call operand to have type ")
+             << function_input_type << " but got " << operand_type;
+    }
+  }
+
+  TypeRange result_types = getResultTypes();
+  for (auto [function_result_type, call_result_type] :
+       llvm::zip(func_type.getResults(), result_types)) {
+    if (function_result_type != call_result_type) {
+      return emitError("Type mismatch. Expected call result to have type ")
+             << function_result_type << " but got " << call_result_type;
+    }
+  }
+
+  if (func_op->hasAttr(kTopologyAttr) && !func_op.isPrivate()) {
+    return emitError(
+        "MPMD CallOp callee with topology must also have visibility set to "
+        "private, as public functions with topologies are assumed to be "
+        "entry "
+        "point functions.");
+  }
+
+  return success();
+}
+
+LogicalResult CallOp::verify() {
+  if (auto count = (*this)->getAttrOfType<IntegerAttr>(kCallCounterAttrName)) {
+    if (!count.getType().isUnsignedInteger(32)) {
+      return emitError() << "call_counter must be an uint32, got "
+                         << count.getType();
+    }
+  }
+
+  Operation* parent_op = (*this)->getParentOp();
+  FuncOp parent_func = (*this)->getParentOfType<FuncOp>();
+
+  if (!isa<FuncOp, ForOp>(parent_op)) {
+    return emitError() << "Mpmd CallOp on \"" << (*this).getCallee()
+                       << "\" can only be used in a function or for_op block "
+                          "but was called from inside op "
+                       << parent_op->getName().getStringRef();
+  }
+
+  if (IsMpmdFunction(parent_func) && !IsEntryPointFunction(parent_func)) {
+    return emitError()
+           << "Mpmd CallOp on \"" << (*this).getCallee()
+           << "\" in an Mpmd function can only be used directly by "
+              "the entrypoint function, i.e. the main function, but "
+              "was called from \""
+           << parent_func.getSymName() << "\".";
+  }
+
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// ForOp
+//===----------------------------------------------------------------------===//
+
+ParseResult ForOp::parse(OpAsmParser& parser, OperationState& result) {
+  auto loc = parser.getCurrentLocation();
+
+  SmallVector<OpAsmParser::UnresolvedOperand> operands;
+  if (parser.parseOperandList(operands, OpAsmParser::Delimiter::Paren)) {
+    return failure();
+  }
+
+  NamedAttrList attrs;
+  if (parser.parseOptionalAttrDict(attrs)) {
+    return failure();
+  }
+  result.addAttributes(attrs.getAttrs());
+
+  if (sdy::parseSingleBlockRegionNoBlockId(parser, *result.addRegion())) {
+    return failure();
+  }
+
+  SmallVector<Type> result_types;
+  if (parser.parseColon() || parser.parseTypeList(result_types)) {
+    return failure();
+  }
+  result.addTypes(result_types);
+
+  SmallVector<Value> operand_values;
+  if (parser.resolveOperands(operands, result_types, loc, operand_values)) {
+    return failure();
+  }
+  result.addOperands(operand_values);
+
+  return success();
+}
+
+void ForOp::print(OpAsmPrinter& p) {
+  p << " (";
+  p.printOperands(getOperands());
+  p << ")";
+
+  p.printOptionalAttrDict((*this)->getAttrs());
+
+  p << " ";
+  sdy::printSingleBlockRegionNoBlockId(p, *this, getRegion());
+
+  p << " : ";
+  for (const auto& result_type : llvm::enumerate(getResultTypes())) {
+    p.printType(result_type.value());
+    if (result_type.index() != getNumResults() - 1) {
+      p << ", ";
+    }
+  }
+}
+
+LogicalResult ForOp::verify() {
+  if (getRegion().getNumArguments() != getNumOperands() + 1) {
+    return emitError("wrong number of arguments for region");
+  }
+
+  // Below we will perform type checking.
+  // Note: no need to explicitly check that operands and ForOp result types
+  // match because this is implied by HLO_PairwiseSameOperandAndResultType.
+  // Therefore we only check that the operands match the block arguments (except
+  // for the last, which is the loop index), and the results match the
+  // terminator operand types.
+  for (int i = 0; i < getNumOperands(); ++i) {
+    if (getOperand(i).getType() != getRegion().getArgument(i).getType()) {
+      return emitError("wrong argument type at argument no. ") << i;
+    }
+  }
+  // Permit other types for the index argument so that we can use MeshTensor.
+  // TODO(petebu): Break circular dependency between dialects and add check for
+  // MeshTensor.
+  if (auto index_type = dyn_cast<RankedTensorType>(getIndexArg().getType())) {
+    if (!index_type || index_type.getRank() != 0 ||
+        !index_type.getElementType().isInteger(32)) {
+      return emitError("index must have a 32-bit integer rank-0 tensor type");
+    }
+  }
+  Operation* terminator = getBody()->getTerminator();
+  if (terminator->getOperandTypes() != getResultTypes()) {
+    return emitError("type mismatch for result of region");
+  }
+
+  // Add a strict check that all types come from the same class. We may consider
+  // lifting this if we support e.g. tensors and tokens in the future.
+  if (getNumOperands() > 0) {
+    TypeID type_id = getOperand(0).getType().getTypeID();
+    if (!llvm::all_of(getOperandTypes(), [&type_id](Type operand_type) {
+          return operand_type.getTypeID() == type_id;
+        })) {
+      return emitError("type ids in operands/results are not identical");
+    }
+  }
+
+  if (getIterations() <= 0) {
+    return emitError("number of iterations must be greater than zero");
+  }
+
+  int factor = getUnrollFactor().value_or(1);
+  if (factor == 0 || getIterations() % factor != 0) {
+    return emitError("number of iterations ")
+           << getIterations() << " isn't divisible by unroll factor " << factor;
+  }
+
+  return success();
+}
+
+ForOp ForOp::create(Location loc, ValueRange tensors, uint32_t iterations,
+                    OpBuilder& builder, ForOpBodyPopulator body_populator,
+                    uint32_t unroll_factor) {
+  TypeRange result_types = tensors.getTypes();
+  auto op = builder.create<ForOp>(
+      loc, result_types, tensors, iterations,
+      unroll_factor == 1 ? nullptr : builder.getUI32IntegerAttr(unroll_factor));
+
+  Block& block = op.getRegion().emplaceBlock();
+  for (Value operand : tensors) {
+    block.addArgument(operand.getType(), operand.getLoc());
+  }
+  block.addArgument(
+      RankedTensorType::get({}, builder.getIntegerType(32, /*isSigned=*/false)),
+      loc);
+
+  ArrayRef<Value> args(block.args_begin(), block.args_end());
+
+  OpBuilder block_builder = OpBuilder::atBlockBegin(&block);
+  block_builder.create<ReturnOp>(
+      loc,
+      body_populator(args.drop_back(), /*index=*/args.back(), block_builder));
+  return op;
+}
+
+//===------------------------------------------------------------------===//
+// ShardableDataFlowOpInterface methods
+//===------------------------------------------------------------------===//
+ResultRange ForOp::getOpResultEdgeOwners() { return getResults(); }
+
+// Returns a list of sources given an edge `owner`.
+SmallVector<OpOperand*> ForOp::getEdgeSources(Value owner) {
+  auto op_result = cast<OpResult>(owner);
+  SDY_CHECK(op_result.getOwner() == getOperation());
+  unsigned res_num = op_result.getResultNumber();
+  return {&getOperation()->getOpOperand(res_num),
+          &getYieldedValuesMutable().value()[res_num]};
+}
+
+SmallVector<Value> ForOp::getNonEdgeOwnerTargets(Value owner) {
+  auto op_result = cast<OpResult>(owner);
+  SDY_CHECK(op_result.getOwner() == getOperation());
+  return {getRegionIterArgs()[op_result.getResultNumber()]};
+}
+
+// Returns the edge own given a `target`. `target` may not be an edge owner.
+Value ForOp::getEdgeOwnerFromTarget(Value target) {
+  SDY_CHECK(sdy::getOwningOp(target) == getOperation());
+  if (auto op_result = dyn_cast<OpResult>(target)) {
+    return op_result;
+  }
+  SDY_CHECK(isa<BlockArgument>(target));
+  return getResult(cast<BlockArgument>(target).getArgNumber());
+}
+
+// Returns the edge owner given a `source` of the data flow edge.
+Value ForOp::getEdgeOwnerFromSource(OpOperand& source) {
+  Operation* source_owner = source.getOwner();
+  if (source_owner->hasTrait<OpTrait::IsTerminator>()) {
+    SDY_CHECK_EQ(source_owner->getParentOp(), getOperation());
+  } else {
+    SDY_CHECK_EQ(source_owner, getOperation());
+  }
+  return getResult(source.getOperandNumber());
+}
+
+//===----------------------------------------------------------------------===//
+// ReduceOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult ReduceOp::verify() {
+  if (getReductionType() == ReductionType::kNone && getNumOperands() != 1) {
+    return emitError(
+        "ReduceOp must have exactly one operand if the reduction type is none");
+  }
+  return success();
+}
+
+// Replaces a chain of reduces with a single reduce if the outer reduce type
+// is none or the reduction types match.
+//
+// In symbols:
+//
+//  y = reduce<K> x
+//  z = reduce<none> y
+//  ~>
+//  z = reduce<K> x
+// and
+//  y = reduce<K> x1, x2
+//  z = reduce<K> y
+//  ~>
+//  z = reduce<K> x1, x2
+LogicalResult ReduceOp::canonicalize(ReduceOp op, PatternRewriter& rewriter) {
+  Operation* defining_op = op.getOperands().front().getDefiningOp();
+  if (!defining_op) {
+    return failure();
+  }
+  ReductionType outer_reduction_type = op.getReductionType();
+  if (ReduceOp inner_reduce = llvm::dyn_cast<ReduceOp>(defining_op)) {
+    if (inner_reduce.getReductionType() == outer_reduction_type ||
+        outer_reduction_type == ReductionType::kNone) {
+      rewriter.replaceAllOpUsesWith(op, inner_reduce);
+      return success();
+    }
+  }
+  return failure();
+}
+
+// Avoids passing values through the fragment just to be used by other
+// fragments or transfers. Instead, we want to use those values directly.
+// NOTE: this may have benefits from memory usage.
+LogicalResult FragmentOp::canonicalize(FragmentOp op,
+                                       PatternRewriter& rewriter) {
+  Block& block = op.getRegion().front();
+  Operation* return_op = block.getTerminator();
+  bool result_replaced = false;
+  for (BlockArgument arg : block.getArguments()) {
+    auto uses = arg.getUses();
+    auto it = llvm::find_if(uses, [&return_op](OpOperand& use) {
+      return use.getOwner() == return_op;
+    });
+    if (it != uses.end()) {
+      Value fragment_result = op.getResult(it->getOperandNumber());
+      if (fragment_result.use_empty()) {
+        continue;
+      }
+      if (op.getOperand(arg.getArgNumber()).getType() !=
+          fragment_result.getType()) {
+        continue;
+      }
+      rewriter.replaceAllUsesWith(fragment_result,
+                                  op.getOperand(arg.getArgNumber()));
+      result_replaced = true;
+    }
+  }
+  return success(result_replaced);
+}
+
+}  // namespace mlir::mpmd
+
+using ::mlir::stablehlo::TokenType;  // NOLINT
+
+#include "shardy/dialect/mpmd/ir/dialect.cc.inc"
+#include "shardy/dialect/mpmd/ir/enums.cc.inc"
+#define GET_ATTRDEF_CLASSES
+#include "shardy/dialect/mpmd/ir/attrs.cc.inc"
+#define GET_TYPEDEF_CLASSES
+#include "shardy/dialect/mpmd/ir/types.cc.inc"
+#define GET_OP_CLASSES
+#include "shardy/dialect/mpmd/ir/ops.cc.inc"
+
+namespace {
+
+#include "shardy/dialect/mpmd/ir/canonicalization.cc.inc"
+
+}  // namespace
+
+namespace mlir::mpmd {
+
+void TransferOp::getCanonicalizationPatterns(RewritePatternSet& results,
+                                             MLIRContext* context) {
+  results.add<IdentityTransferPattern, IntraMeshTransferOfTransferPattern>(
+      context);
+}
+
+void BroadcastOp::getCanonicalizationPatterns(RewritePatternSet& results,
+                                              MLIRContext* context) {
+  results.add<BroadcastOfBroadcastPattern>(context);
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/ir/dialect.h b/shardy/dialect/mpmd/ir/dialect.h
new file mode 100644
index 0000000..51bdff0
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/dialect.h
@@ -0,0 +1,85 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_IR_DIALECT_H_
+#define SHARDY_DIALECT_MPMD_IR_DIALECT_H_
+
+// IWYU pragma: begin_keep
+
+#include <cstdint>
+#include <functional>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Dialect.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Interfaces/LoopLikeInterface.h"
+#include "mlir/Interfaces/SideEffectInterfaces.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+// IWYU pragma: end_keep
+
+namespace mlir::mpmd {
+
+// A function that adds operations to the body of a FragmentOp. The operations
+// must be non-mpmd operations (e.g. StableHLO operations). The function is
+// returning the final values that callers of this method should add as
+// terminator operands of the FragmentOp they are building.
+using FragmentOpBodyPopulator = std::function<SmallVector<Value>(
+    ArrayRef<Value> args, OpBuilder& block_builder)>;
+
+// A function that adds operations to the body of a ForOp using the provided
+// block arguments and returns the values that should be returned by the body's
+// terminator (which will be added after calling this function).
+using ForOpBodyPopulator = std::function<SmallVector<Value>(
+    ArrayRef<Value> args, Value index, OpBuilder& block_builder)>;
+
+// Parses an optional transpose count.
+mlir::ParseResult parseOptionalTransposeCount(AsmParser& parser,
+                                              int64_t& transpose_count);
+
+// Prints an optional transpose count.
+void printOptionalTransposeCount(AsmPrinter& printer, int64_t transpose_count);
+
+}  // namespace mlir::mpmd
+
+// IWYU pragma: begin_exports
+
+// Dialect main class is defined in ODS, we include it here.
+#include "shardy/dialect/mpmd/ir/dialect.h.inc"
+
+// Include the auto-generated header file containing type declarations. (This
+// has to go before including `ops.h.inc` since the MPMD operation definitions
+// require types and attrs to be defined.)
+
+// ODS-generated enum classes.
+#include "shardy/dialect/mpmd/ir/enums.h.inc"
+#define GET_ATTRDEF_CLASSES
+#include "shardy/dialect/mpmd/ir/attrs.h.inc"
+#define GET_TYPEDEF_CLASSES
+#include "shardy/dialect/mpmd/ir/types.h.inc"
+
+// Include the auto-generated header file containing op declarations.
+#define GET_OP_CLASSES
+#include "shardy/dialect/mpmd/ir/ops.h.inc"
+
+// IWYU pragma: end_exports
+
+#endif  // SHARDY_DIALECT_MPMD_IR_DIALECT_H_
diff --git a/shardy/dialect/mpmd/ir/dialect.td b/shardy/dialect/mpmd/ir/dialect.td
new file mode 100644
index 0000000..cbc8a08
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/dialect.td
@@ -0,0 +1,39 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef MPMD_DIALECT
+#define MPMD_DIALECT
+
+include "mlir/IR/OpBase.td"
+
+// Introduce the MPMD dialect to ODS framework.
+// The MPMD dialect allows partitioning a single MLIR program into multiple
+// fragments, each represents a sub-computation of an SPMD program on a mesh
+// within an MPMD topology of meshes.
+def Mpmd_Dialect : Dialect {
+  let name = "mpmd";
+  let cppNamespace = "::mlir::mpmd";
+
+  let dependentDialects = [
+      // We need to load the SDY dialect as it contains the sharding attributes
+      // and interfaces.
+      "sdy::SdyDialect"
+  ];
+
+  let useDefaultAttributePrinterParser = 1;
+  let useDefaultTypePrinterParser = 1;
+}
+
+#endif
diff --git a/shardy/dialect/mpmd/ir/enums.td b/shardy/dialect/mpmd/ir/enums.td
new file mode 100644
index 0000000..8950421
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/enums.td
@@ -0,0 +1,40 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef MPMD_ENUMS
+#define MPMD_ENUMS
+
+include "mlir/IR/AttrTypeBase.td"
+include "shardy/dialect/mpmd/ir/dialect.td"
+include "mlir/IR/EnumAttr.td"
+include "mlir/IR/OpBase.td"
+
+
+def Mpmd_ReductionType : I32EnumAttr<"ReductionType", "reduction type attribute", [
+                           // The `none` reduction type is a special case where
+                           // all tensors to reduce are identical and the
+                           // reduction acts as an identity function.
+                           I32EnumAttrCase<"kNone", 0, "none">,
+                           I32EnumAttrCase<"kAdd", 1, "add">,
+                           I32EnumAttrCase<"kMax", 2, "max">,
+                           I32EnumAttrCase<"kMin", 3, "min">,
+                           I32EnumAttrCase<"kMul", 4, "mul">,
+                           I32EnumAttrCase<"kOr",  5, "or">,
+                           I32EnumAttrCase<"kAnd", 6, "and">]> {
+  let genSpecializedAttr = 0;
+  let cppNamespace = Mpmd_Dialect.cppNamespace;
+}
+
+#endif  // MPMD_ENUMS
diff --git a/shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h b/shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h
new file mode 100644
index 0000000..8becc02
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h
@@ -0,0 +1,106 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_IR_FRAGMENT_ARG_RES_ATTRS_H_
+#define SHARDY_DIALECT_MPMD_IR_FRAGMENT_ARG_RES_ATTRS_H_
+
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+
+namespace mlir::mpmd {
+
+// Attribute to store arg attrs for ops without this built in.
+constexpr StringRef kArgAttrName = "arg_attrs";
+// Attribute to store res attrs for ops without this built in.
+constexpr StringRef kResAttrName = "res_attrs";
+
+// Get the existing arg attr if it exists, or create a new one of the length of
+// the operands.
+inline SmallVector<Attribute> GetArgAttrsOrCreateDefault(Operation* op) {
+  return op->hasAttr(kArgAttrName)
+             ? llvm::to_vector(
+                   op->getAttrOfType<ArrayAttr>(kArgAttrName).getValue())
+             : SmallVector<Attribute>(
+                   op->getNumOperands(),
+                   DictionaryAttr::get(op->getContext(), {}));
+}
+
+// Retrieve the arg attr at `index`.
+inline Attribute GetArgAttr(Operation* op, int index, StringRef name) {
+  if (auto attr = op->getAttrOfType<ArrayAttr>(kArgAttrName)) {
+    return cast<DictionaryAttr>(attr[index]).get(name);
+  }
+  return nullptr;
+}
+
+// Update the arg attrs Attribute.
+inline void SetArgAttrs(Operation* op, ArrayRef<Attribute> arg_attrs) {
+  op->setAttr(kArgAttrName, ArrayAttr::get(op->getContext(), arg_attrs));
+}
+
+// Get the existing res attr if it exists, or create a new one of the length of
+// the results.
+inline SmallVector<Attribute> GetResAttrsOrCreateDefault(Operation* op) {
+  return op->hasAttr(kResAttrName)
+             ? llvm::to_vector(
+                   op->getAttrOfType<ArrayAttr>(kResAttrName).getValue())
+             : SmallVector<Attribute>(
+                   op->getNumResults(),
+                   DictionaryAttr::get(op->getContext(), {}));
+}
+
+// Retrieve the res attr at `index`.
+inline Attribute GetResAttr(Operation* op, int index, StringRef name) {
+  if (auto attr = op->getAttrOfType<ArrayAttr>(kResAttrName)) {
+    return cast<DictionaryAttr>(attr[index]).get(name);
+  }
+  return nullptr;
+}
+
+// Update the res attrs Attribute.
+inline void SetResAttrs(Operation* op, ArrayRef<Attribute> res_attrs) {
+  op->setAttr(kResAttrName, ArrayAttr::get(op->getContext(), res_attrs));
+}
+
+// Updates `base_dict_attr` with the new value in place. Crashes if the value
+// already exists and `insert_if_not_present = false`.
+inline void InsertAttr(Attribute& base_dict_attr, StringRef name,
+                       Attribute value, bool insert_if_not_present = false) {
+  NamedAttrList attributes(cast<DictionaryAttr>(base_dict_attr));
+  Attribute old_value = attributes.set(name, value);
+  if (!insert_if_not_present) {
+    SDY_CHECK(!old_value);
+  }
+  base_dict_attr = attributes.getDictionary(base_dict_attr.getContext());
+}
+
+// Removes in-place the attribute with the given name from the base dictionary.
+// Returns true if the attribute was removed.
+inline bool RemoveAttr(Attribute& base_dict_attr, StringRef name) {
+  NamedAttrList attributes(cast<DictionaryAttr>(base_dict_attr));
+  bool removed = attributes.erase(name) != nullptr;
+  base_dict_attr = attributes.getDictionary(base_dict_attr.getContext());
+  return removed;
+}
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_IR_FRAGMENT_ARG_RES_ATTRS_H_
diff --git a/shardy/dialect/mpmd/ir/ops.td b/shardy/dialect/mpmd/ir/ops.td
new file mode 100644
index 0000000..b5c49cc
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/ops.td
@@ -0,0 +1,632 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef MPMD_OPS
+#define MPMD_OPS
+
+include "mlir/IR/OpBase.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/CallInterfaces.td"
+include "mlir/Interfaces/FunctionInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/LoopLikeInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "shardy/dialect/mpmd/ir/attrs.td"
+include "shardy/dialect/mpmd/ir/dialect.td"
+include "shardy/dialect/mpmd/ir/enums.td"
+include "shardy/dialect/mpmd/ir/types.td"
+include "shardy/dialect/sdy/ir/op_interface.td"
+include "shardy/dialect/sdy/ir/attrs.td"
+include "stablehlo/dialect/Base.td"
+
+// Base class for MPMD operations.
+class Mpmd_Op<string mnemonic, list<Trait> traits = []> :
+    Op<Mpmd_Dialect, mnemonic, traits>;
+
+// NOTE: any MPMD op is expected to live at the top-level of a function.
+// If this changes in the future, then we need to revisit certain assumptions in
+// the code base (e.g., in merge passes).
+
+//===----------------------------------------------------------------------===//
+// ReturnOp
+//===----------------------------------------------------------------------===//
+
+def ReturnOp : Mpmd_Op<"return", [Pure, Terminator]> {
+  let summary = [{
+    The `mpmd.return` operation terminates the regions attached to mpmd
+    region-based ops. It is variadic: it takes as arguments a list of values
+    whose types can be any (but of the same kind, e.g. `AnyTensor`) and
+    therefore can be reused at various levels of the MPMD IR stack.
+  }];
+
+  let arguments = (ins Variadic<AnyType>:$results);
+  let assemblyFormat = "attr-dict $results (`:` type($results)^)?";
+
+  let builders = [
+    OpBuilder<(ins), [{ }]>,  // needed for ensureTerminator() during parsing.
+  ];
+}
+
+//===----------------------------------------------------------------------===//
+// NamedComputationOp
+//===----------------------------------------------------------------------===//
+
+def NamedComputationOp : Mpmd_Op<"named_computation", [
+       SingleBlockImplicitTerminator<"ReturnOp">,
+       RecursiveMemoryEffects, RecursivelySpeculatable, IsolatedFromAbove]> {
+  let summary = "named scope operation";
+  let description = [{
+    Groups a computation, i.e. a block of operations, and gives it a name and
+    a transpose count via the UserOrigin attribute. This NamedComputation can be
+    used to assign a mesh to the computation in MPMD or for optimizations.
+
+    The transpose count (default=0) denotes whether the named computation has
+    been produced by a certain number of JAX AD transpose transformations.
+
+    The op's region shouldn't have any free variables, and the type of
+    each block arguments and returned values in the region must be the same as
+    the type of the inputs and the return type of the op.
+  }];
+
+  let arguments = (ins
+    Variadic<HLO_TensorOrToken>:$tensors,
+    Mpmd_UserOrigin:$origin
+  );
+
+  let results = (outs Variadic<HLO_TensorOrToken>:$results);
+  let regions = (region SizedRegion<1>:$region);
+
+  let hasCustomAssemblyFormat = 1;
+  let hasVerifier = 1;
+
+  let extraClassDeclaration = [{
+    StringRef getName() { return getOrigin().getUserName(); }
+    int64_t getTransposeCount() { return getOrigin().getTransposeCount(); }
+  }];
+
+}
+
+//===----------------------------------------------------------------------===//
+// NamedTensorOp
+//===----------------------------------------------------------------------===//
+
+def NamedTensorOp : Mpmd_Op<"named_tensor", [Pure, SameOperandsAndResultType]> {
+  let summary = "Assign a tensor to a mesh";
+  let description = [{
+    An identity op that associates the result of the tensor with a given name.
+    This NamedTensor can be used to assign a mesh to the tensor in MPMD.
+
+    NOTE: this is different than TagOp in that TagOp is used for naming a tensor
+    and can be used to partition that tensor. NamedTensorOp is for MPMD programs
+    for tensors that may be explicitly assigned to meshes.
+  }];
+
+  let arguments = (ins
+    AnyTensor:$tensor,
+    StrAttr:$name
+  );
+  let results = (outs AnyTensor:$result);
+  let assemblyFormat = "$tensor `name````=```$name attr-dict `:` type($result)";
+}
+
+//===----------------------------------------------------------------------===//
+// FragmentOp
+//===----------------------------------------------------------------------===//
+
+def FragmentOp : Mpmd_Op<"fragment", [
+       SingleBlockImplicitTerminator<"ReturnOp">,
+       RecursiveMemoryEffects, RecursivelySpeculatable, IsolatedFromAbove,
+       ParentOneOf<["::mlir::func::FuncOp", "ForOp"]>,
+       DeclareOpInterfaceMethods<Sdy_ShardableDataFlowOpInterface,
+       /*methodOverrides=*/["setBlockArgumentEdgeOwnerSharding", "getBlockArgumentEdgeOwnerShardings",
+       "setBlockArgumentEdgeOwnerShardings", "setOpResultEdgeOwnerShardings",
+       "getBlockArgumentEdgeOwners", "getOpResultEdgeOwnerShardings",
+       "getOpResultEdgeOwners", "getEdgeSources", "getEdgeOwnerFromTarget",
+       "getEdgeOwnerFromSource", "shouldKeepEdgeOwnerShardingsDivisible"]>]> {
+  let summary = "fragment operation";
+  let description = [{
+    Assigns a computation, i.e. a block of operations, to a specific mesh in an
+    MPMD topology, that is intended to be executed as an individual SPMD program
+    fragment.
+
+    The fragment takes and returns only mesh tensors that are assigned to the
+    same mesh as the fragment.
+
+    The mesh name of the fragment should correspond to a mesh in the topology.
+
+    The fragment includes a list of origins, i.e., metadata with information re
+    the original named_computations that formed this fragment, and a staged_id
+    defined _iff_ it is a user defined fragment, i.e., it has a non-empty list
+    of origins. The optional in_shardings specifies the sharding of the
+    block arguments of a fragment, which correspond to the operands.
+    The optional out_shardings specifies the shardings of the results.
+
+    The fragment's region shouldn't have any free variables, and the type of
+    each block arguments and returned values in the region is the global tensor
+    type of the corresponding mesh tensor.
+  }];
+
+  let arguments = (ins
+    Variadic<Mpmd_MeshTensorType>:$inputs,
+    TypedArrayAttrBase<Mpmd_UserOrigin, "array of origin infos">:$origin,
+    StrAttr:$mesh_name,
+    OptionalAttr<I64Attr>:$stage_id,
+    OptionalAttr<Sdy_TensorShardingPerValue>:$in_shardings,
+    OptionalAttr<Sdy_TensorShardingPerValue>:$out_shardings
+  );
+  let results = (outs Variadic<Mpmd_MeshTensorType>:$results);
+  let regions = (region SizedRegion<1>:$region);
+
+  let hasCustomAssemblyFormat = 1;
+  let hasVerifier = 1;
+  let hasCanonicalizeMethod = 1;
+
+
+  let builders = [
+    OpBuilder<(ins
+      "::mlir::TypeRange":$results,
+      "::mlir::ValueRange":$inputs,
+      "::mlir::ArrayAttr":$origin,
+      "::mlir::StringRef":$mesh_name,
+      "::mlir::IntegerAttr":$stage_id),
+      [{ build($_builder, $_state,
+             results, inputs, origin, mesh_name,
+             stage_id, /*in_shardings=*/nullptr, /*out_shardings=*/nullptr); }]>
+  ];
+
+  let extraClassDeclaration = [{
+
+    // Returns true if the fragment originates (e.g. is a merge of) some user
+    // named computation.
+    bool isUserFragment() { return !getOrigin().empty(); }
+
+    // Prints the metadata (e.g., origin, mesh name, stage) of the fragment.
+    void printFragmentMetadata(llvm::raw_ostream& os);
+
+    // Sets the sharding of an input of the fragment.
+    void setInputSharding
+    (unsigned input_index, sdy::TensorShardingAttr sharding);
+
+    // Sets the sharding of a result of the fragment specified by the user.
+    void setUserSpecifiedResultSharding
+    (unsigned result_index, sdy::TensorShardingAttr sharding);
+
+    // Creates a new FragmentOp with `tensors` as inputs (that must have
+    // mesh types), returning `result_types` (that must be mesh types), and
+    // a body populator `body_populator` that will populate the global region
+    // of the fragment.
+    static FragmentOp createMeshFragmentWithGlobalBody(
+      Location loc,
+      llvm::ArrayRef<Attribute> user_origin,
+      llvm::StringRef mesh_name,
+      ValueRange tensors,
+      TypeRange result_types,
+      OpBuilder& builder,
+      FragmentOpBodyPopulator body_populator);
+
+  }];
+}
+
+//===----------------------------------------------------------------------===//
+// FragmentCallOp
+//===----------------------------------------------------------------------===//
+
+def FragmentCallOp : Mpmd_Op<"fragment_call",
+    [HasParent<"::mlir::func::FuncOp">, CallOpInterface, MemRefsNormalizable,
+     DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
+  let summary = "fragment call operation";
+  let description = [{
+    Represents a call to a function that holds an MPMD fragment body, i.e. a
+    computation assigned to a specific mesh in an MPMD topology, that is
+    intended to be executed as an individual SPMD program fragment.
+
+    The mesh name of the fragment should correspond to a mesh in the topology of
+    the enclosing function, and that mesh shape should match that of the callee.
+
+    The origin specifies the user named computations that contributed to this
+    fragment call e.g. through merging.
+
+    The function input and result types of the callee must be the local tensor
+    types of the corresponding mesh tensors of this op's operands and results
+    respectively.
+
+    Example:
+
+    ```mlir
+    %2 = mpmd.fragment_call<mesh="m1", origin=[]> @my_fragment(%0, %1) :
+      (mesh_tensor<...>, mesh_tensor<...>) -> mesh_tensor<...>
+    ```
+  }];
+
+  let arguments = (ins
+    Variadic<Mpmd_MeshTensorType>:$tensors,
+    TypedArrayAttrBase<Mpmd_UserOrigin, "array of origin infos">:$origin,
+    StrAttr:$mesh_name,
+    FlatSymbolRefAttr:$callee);
+  let results = (outs Variadic<Mpmd_MeshTensorType>);
+
+  let extraClassDeclaration = [{
+    // Get the argument operands to the called function.
+    operand_range getArgOperands() {
+      return {arg_operand_begin(), arg_operand_end()};
+    }
+    MutableOperandRange getArgOperandsMutable() {
+      return getTensorsMutable();
+    }
+
+    operand_iterator arg_operand_begin() { return operand_begin(); }
+    operand_iterator arg_operand_end() { return operand_end(); }
+
+    // Return the callee of this operation.
+    CallInterfaceCallable getCallableForCallee() {
+      return getCalleeAttr();
+    }
+    // Set the callee from the callable.
+    void setCalleeFromCallable(CallInterfaceCallable callee);
+
+    Attribute removeArgAttrsAttr() { return nullptr; }
+    Attribute removeResAttrsAttr() { return nullptr; }
+    ArrayAttr getArgAttrsAttr() { return nullptr; }
+    ArrayAttr getResAttrsAttr() { return nullptr; }
+    void setArgAttrsAttr(ArrayAttr) { return; }
+    void setResAttrsAttr(ArrayAttr) { return; }
+  }];
+
+  let hasCustomAssemblyFormat = 1;
+}
+
+//===----------------------------------------------------------------------===//
+// TransferOp
+//===----------------------------------------------------------------------===//
+
+def TransferOp : Mpmd_Op<"transfer", [Pure, HasParent<"::mlir::func::FuncOp">, DeclareOpInterfaceMethods<Sdy_ShardingRuleOpInterface,
+       /*methodOverrides=*/["shouldKeepOutputShardingsDivisible"]>]> {
+  let summary = "transfer operation";
+  let description = [{
+    Transfers a distributed tensor from one mesh to another.
+
+    The mesh names of the operand and result types should correspond to meshes
+    in the topology, and their global types should be identical.
+  }];
+
+  let arguments = (ins Mpmd_MeshTensorType:$tensor);
+  let results = (outs Mpmd_MeshTensorType:$result);
+
+  let assemblyFormat = "attr-dict $tensor `:` functional-type(operands, results)";
+  let hasVerifier = 1;
+  let hasCanonicalizer = 1;
+
+  let extraClassDeclaration = [{
+    // Returns true if this transfer is between two tensors of the same mesh.
+    bool isIntraMesh();
+
+    // Returns true if this transfer is between two tensors of a different mesh.
+    bool isInterMesh() {
+      return !isIntraMesh();
+    }
+  }];
+}
+
+//===----------------------------------------------------------------------===//
+// AssignOp
+//===----------------------------------------------------------------------===//
+
+def AssignOp : Mpmd_Op<"assign",
+    [Pure, HasParent<"::mlir::func::FuncOp, ForOp">]> {
+  let summary = "assign operation";
+  let description = [{
+    Assigns a local tensor to a mesh as fully replicated within that mesh.
+
+    This is a temporary op that is introduced when lowering jax ops, to move
+    from local types to mesh types. These ops will be eliminated during import,
+    when the inputs and results of the func op become mesh tensors.
+
+    The mesh name of the result type should correspond to a mesh in the
+    topology, and its global type should be identical to the operand type.
+
+    The origin of the assign op is the origin of mesh, e.g. named_computation,
+    mesh inference, etc.
+  }];
+
+  let arguments = (
+    ins AnyTensor:$tensor,
+    OptionalAttr<StrAttr>:$origin  // TODO: b/396601755 - Make required.
+  );
+  let results = (outs Mpmd_MeshTensorType:$result);
+
+  let assemblyFormat = "attr-dict $tensor `:` functional-type(operands, results)";
+  let hasVerifier = 1;
+
+  let builders = [
+    // Standard builders but with optional origin, or origin as StringRef.
+    OpBuilder<(ins "::mlir::Type":$result_type, "::mlir::Value":$tensor),
+    [{ build($_builder, $_state, result_type, tensor,
+             /*origin=*/(::mlir::StringAttr)nullptr); }]>,
+    OpBuilder<(ins "::mlir::Type":$result_type, "::mlir::Value":$tensor,
+               "::mlir::StringRef":$origin),
+    [{ build($_builder, $_state, result_type, tensor,
+             ::mlir::StringAttr::get($_builder.getContext(), origin)); }]>,
+
+    // Builds an AssignOp whose result type is a MeshTensorType with a fully
+    // replicated distributed tensor type.
+    OpBuilder<(ins "::mlir::Value":$tensor, "::mlir::StringRef":$mesh_name,
+                   "sdy::MeshAttr":$mesh),
+    [{ build($_builder, $_state,
+             MeshTensorType::getFullyReplicated($_builder.getContext(),
+                mesh_name, mesh, cast<RankedTensorType>(tensor.getType())),
+             tensor); }]>,
+    OpBuilder<(ins "::mlir::Value":$tensor, "::mlir::StringRef":$mesh_name,
+                   "sdy::MeshAttr":$mesh, "::mlir::StringRef":$origin),
+    [{ build($_builder, $_state,
+             MeshTensorType::getFullyReplicated($_builder.getContext(),
+                mesh_name, mesh, cast<RankedTensorType>(tensor.getType())),
+             tensor, origin); }]>,
+  ];
+
+  let extraClassDeclaration = [{
+    StringRef getDestinationMeshName() {
+      return getType().getMeshName();
+    }
+    MeshWithOriginsAttr getMeshWithOrigin() {
+      return MeshWithOriginsAttr::get(getContext(),
+              getDestinationMeshName(),
+              getOriginAttr() ?
+                OriginAttr::get(getContext(), getOriginAttr()) :
+                ArrayRef<OriginAttr>());
+    }
+  }];
+}
+
+//===----------------------------------------------------------------------===//
+// UnassignOp
+//===----------------------------------------------------------------------===//
+
+def UnassignOp : Mpmd_Op<"unassign",
+    [Pure, HasParent<"::mlir::func::FuncOp, ForOp">, InferTensorType]> {
+  let summary = "unassign operation";
+  let description = [{
+    Unassigns a fully replicated tensor from a mesh.
+
+    This is a temporary op that is introduced when lowering jax ops, to move
+    from local types to mesh types. These ops will be eliminated during import,
+    when the inputs and results of the func op become mesh tensors.
+
+    The mesh name of the operand type should correspond to a mesh in the
+    topology, and its global type should be identical to the result type.
+  }];
+
+  let arguments = (
+    ins Mpmd_MeshTensorType:$tensor,
+    OptionalAttr<StrAttr>:$origin
+  );
+  let results = (outs AnyTensor:$result);
+
+  let assemblyFormat = "attr-dict $tensor `:` functional-type(operands, results)";
+  let hasVerifier = 1;
+
+  let builders = [
+    // Standard builders but with optional origin, or origin as StringRef.
+    OpBuilder<(ins "::mlir::Value":$tensor),
+    [{ build($_builder, $_state, tensor,
+             /*origin=*/(::mlir::StringAttr)nullptr); }]>,
+    OpBuilder<(ins "::mlir::Value":$tensor, "::mlir::StringRef":$origin),
+    [{ build($_builder, $_state, tensor,
+            ::mlir::StringAttr::get($_builder.getContext(), origin)); }]>,
+  ];
+
+  let extraClassDeclaration = [{
+    StringRef getSourceMeshName() {
+      return getTensor().getType().getMeshName();
+    }
+    MeshWithOriginsAttr getMeshWithOrigin() {
+      return MeshWithOriginsAttr::get(getContext(), getSourceMeshName(),
+              getOriginAttr() ?
+                OriginAttr::get(getContext(), getOriginAttr()) :
+                ArrayRef<OriginAttr>());
+    }
+  }];
+}
+
+//===----------------------------------------------------------------------===//
+// CallOp
+//===----------------------------------------------------------------------===//
+
+def CallOp : Mpmd_Op<"call",
+    [DeclareOpInterfaceMethods<SymbolUserOpInterface>, CallOpInterface]> {
+  let summary = "MPMD specific call function";
+  let description = [{
+    A function call operation. Useful to wrap the body of loops in function
+    declarations to reduce code size, for example.
+  }];
+
+  let arguments = (ins
+    Variadic<LocalOrMeshTensor>:$tensors,
+    FlatSymbolRefAttr:$callee);
+  let results = (outs Variadic<LocalOrMeshTensor>);
+
+  let extraClassDeclaration = [{
+    // Get the argument operands to the called function.
+    operand_range getArgOperands() {
+      return {arg_operand_begin(), arg_operand_end()};
+    }
+    MutableOperandRange getArgOperandsMutable() {
+      return getTensorsMutable();
+    }
+
+    operand_iterator arg_operand_begin() { return operand_begin(); }
+    operand_iterator arg_operand_end() { return operand_end(); }
+
+    // Return the callee of this operation.
+    CallInterfaceCallable getCallableForCallee() {
+      return getCalleeAttr();
+    }
+    // Set the callee from the callable.
+    void setCalleeFromCallable(CallInterfaceCallable callee);
+
+    Attribute removeArgAttrsAttr() { return nullptr; }
+    Attribute removeResAttrsAttr() { return nullptr; }
+    ArrayAttr getArgAttrsAttr() { return nullptr; }
+    ArrayAttr getResAttrsAttr() { return nullptr; }
+    void setArgAttrsAttr(ArrayAttr) { return; }
+    void setResAttrsAttr(ArrayAttr) { return; }
+  }];
+
+  let assemblyFormat = [{
+    $callee `(` $tensors `)` attr-dict `:` functional-type(operands, results)
+  }];
+  let hasVerifier = 1;
+}
+
+//===----------------------------------------------------------------------===//
+// ForOp
+//===----------------------------------------------------------------------===//
+
+def ForOp : Mpmd_Op<"for",
+    [HLO_PairwiseSameOperandAndResultType,
+     RecursiveMemoryEffects, RecursivelySpeculatable,
+     SingleBlockImplicitTerminator<"ReturnOp">,
+     OpAsmOpInterface,
+     LoopLikeOpInterface,
+     DeclareOpInterfaceMethods<Sdy_ShardableDataFlowOpInterface,
+     /*methodOverrides=*/ ["getNonEdgeOwnerTargets"]>]> {
+  let summary = "For operator";
+  let description = [{
+    Returns the result of executing a body function for a fixed number of
+    iterations, with the iteration index available in the body.
+
+    An optional unroll factor, that must divide the number of iterations,
+    can be specified to unroll the body of the op by that factor, i.e. for
+    unroll factor N, the body is replicated to create N copies and the number of
+    iterations is reduced by a factor of 1/N. Each copy except the first uses
+    the results of the previous copy instead of the block arguments, and the
+    iteration index is multiplied by the unroll factor and incremented after
+    every copy.
+
+    A for operator can accept and return any types, but the TypeID of these
+    must be the same -- e.g. all tensor types or all MPMD mesh types etc. This
+    allows us to use the op at various levels, sharing implementation and
+    transformations.
+  }];
+
+  let arguments = (ins
+    Variadic<AnyType>:$tensors,
+    UI32Attr:$iterations,
+    OptionalAttr<UI32Attr>:$unroll_factor
+  );
+  let results = (outs Variadic<AnyType>:$results);
+  let regions = (region SizedRegion<1>:$region);
+
+  let hasCustomAssemblyFormat = 1;
+  let hasVerifier = 1;
+  let extraClassDeclaration = [{
+    void getAsmBlockArgumentNames(Region &region,
+                                  OpAsmSetValueNameFn setNameFn) {
+      setNameFn(getIndexArg(), "index");
+    }
+
+    // Returns the index block argument, assuming it was already added.
+    BlockArgument getIndexArg() {
+      return *getRegion().args_rbegin();
+    }
+
+    // Creates a new ForOp with a block argument for each result type and the
+    // iteration index as the last block argument, populates the block using the
+    // given `body_populator` and adds a ReturnOp at the end of the block with
+    // the values returned from `body_populator`.
+    static ForOp create(Location loc,
+                        ValueRange tensors,
+                        uint32_t iterations,
+                        OpBuilder& builder,
+                        ForOpBodyPopulator body_populator,
+                        uint32_t unroll_factor = 1);
+
+    //===------------------------------------------------------------------===//
+    // LoopLikeOpInterface methods
+    //===------------------------------------------------------------------===//
+    SmallVector<Region*> getLoopRegions() {
+      return {&getRegion()};
+    }
+    std::optional<::llvm::SmallVector<Value>> getLoopInductionVars() {
+      return llvm::SmallVector<Value>{getIndexArg()};
+    }
+    MutableArrayRef<OpOperand> getInitsMutable() {
+      return getOperation()->getOpOperands();
+    }
+    Block::BlockArgListType getRegionIterArgs() {
+      return getRegion().getArguments().drop_back();
+    }
+    std::optional<::llvm::MutableArrayRef<OpOperand>>
+      getYieldedValuesMutable() {
+      return getRegion().front().getTerminator()->getOpOperands();
+    }
+   std::optional<ResultRange> getLoopResults() {
+      return getResults();
+    }
+  }];
+
+}
+
+//===----------------------------------------------------------------------===//
+// BroadcastOp
+//===----------------------------------------------------------------------===//
+
+def BroadcastOp : Mpmd_Op<"broadcast",
+    [SameOperandsAndResultType, Pure]> {
+  let summary = "broadcast operation";
+  let description = [{
+    Allows for a tensor to be transferred (or replicated) in any mesh where it's
+    used. Whenever transferred, the origin of the transfer is the current
+    location of the operand.
+  }];
+
+  let arguments = (ins AnyTensor:$tensor);
+  let results = (outs AnyTensor:$result);
+
+  let assemblyFormat = "attr-dict $tensor `:` type($tensor)";
+  let hasCanonicalizer = 1;
+}
+
+//===----------------------------------------------------------------------===//
+// ReduceOp
+//===----------------------------------------------------------------------===//
+
+def ReduceOp : Mpmd_Op<"reduce",
+    [Pure, SameOperandsAndResultType]> {
+  let summary = "cross-mesh reduce operation";
+  let description = [{
+    Allows for a tensor to be reduced across different meshes, and then
+    broadcast to wherever it needs to be used.
+  }];
+
+  let arguments = (ins
+    Variadic<AnyTensor>:$tensors,
+    DefaultValuedAttr<
+      Mpmd_Reduction, "ReductionType::kNone">:$reduction
+  );
+  let results = (outs AnyTensor:$result);
+  let hasCustomAssemblyFormat = 1;
+  let extraClassDeclaration = [{
+    ReductionType getReductionType() {
+      return getReduction().getReductionType();
+    }
+  }];
+  let hasVerifier = 1;
+  let hasCanonicalizeMethod = 1;
+
+  let assemblyFormat = "`` $reduction attr-dict $tensors `:` functional-type(operands, results)";
+}
+
+#endif
diff --git a/shardy/dialect/mpmd/ir/register.cc b/shardy/dialect/mpmd/ir/register.cc
new file mode 100644
index 0000000..81c3003
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/register.cc
@@ -0,0 +1,46 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/ir/register.h"
+
+#include "mlir/Dialect/Func/Extensions/AllExtensions.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/IR/DialectRegistry.h"
+#include "mlir/IR/MLIRContext.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/register.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir {
+namespace mpmd {
+
+void registerAllDialects(DialectRegistry& registry) {
+  registry.insert<MpmdDialect, sdy::SdyDialect, func::FuncDialect,
+                  tensor::TensorDialect, stablehlo::StablehloDialect>();
+}
+
+void loadAllRequiredDialects(MLIRContext* context) {
+  DialectRegistry registry;
+  func::registerAllExtensions(registry);
+  sdy::registerAllDialects(registry);
+  registerAllDialects(registry);
+  context->appendDialectRegistry(registry);
+  context->loadAllAvailableDialects();
+}
+
+}  // namespace mpmd
+}  // namespace mlir
diff --git a/shardy/dialect/mpmd/ir/register.h b/shardy/dialect/mpmd/ir/register.h
new file mode 100644
index 0000000..2c7a795
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/register.h
@@ -0,0 +1,34 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_IR_REGISTER_H_
+#define SHARDY_DIALECT_MPMD_IR_REGISTER_H_
+
+#include "mlir/IR/DialectRegistry.h"
+#include "mlir/IR/MLIRContext.h"
+
+namespace mlir {
+namespace mpmd {
+
+// Add all required dialects to the provided registry.
+void registerAllDialects(DialectRegistry& registry);
+
+// Loads all required dialects to the provided context.
+void loadAllRequiredDialects(MLIRContext* context);
+
+}  // namespace mpmd
+}  // namespace mlir
+
+#endif  // SHARDY_DIALECT_MPMD_IR_REGISTER_H_
diff --git a/shardy/dialect/mpmd/ir/test/BUILD b/shardy/dialect/mpmd/ir/test/BUILD
new file mode 100644
index 0000000..b3266dd
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/BUILD
@@ -0,0 +1,21 @@
+# Lit tests for the MPMD dialect.
+
+load("//shardy:lit.bzl", "glob_lit_tests")
+
+package(default_visibility = ["//visibility:public"])
+
+filegroup(
+    name = "test_data",
+    testonly = True,
+    data = [
+        "//shardy/tools:mpmd_opt",
+        "@llvm-project//llvm:FileCheck",
+    ],
+)
+
+glob_lit_tests(
+    name = "all_tests",
+    data = [":test_data"],
+    driver = "@llvm-project//mlir:run_lit.sh",
+    test_file_exts = ["mlir"],
+)
diff --git a/shardy/dialect/mpmd/ir/test/broadcast_canonicalization.mlir b/shardy/dialect/mpmd/ir/test/broadcast_canonicalization.mlir
new file mode 100644
index 0000000..8b8f0af
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/broadcast_canonicalization.mlir
@@ -0,0 +1,15 @@
+// RUN: mpmd_opt %s -canonicalize 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @broadcast_of_broadcast
+// CHECK-NEXT: %[[BROADCAST:.*]] = mpmd.broadcast %arg0 : tensor<4x8xf32>
+// CHECK-NEXT: return %[[BROADCAST]] : tensor<4x8xf32>
+func.func @broadcast_of_broadcast(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+
+  %0 = mpmd.broadcast %arg0 : tensor<4x8xf32>
+  %1 = mpmd.broadcast %0 : tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/ir/test/broadcast_parse_and_print.mlir b/shardy/dialect/mpmd/ir/test/broadcast_parse_and_print.mlir
new file mode 100644
index 0000000..b560f07
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/broadcast_parse_and_print.mlir
@@ -0,0 +1,9 @@
+// RUN: mpmd_opt %s 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=4]>} {
+  // CHECK-NEXT: mpmd.broadcast %arg0 : tensor<4x8xf32>
+  %0 = mpmd.broadcast %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/ir/test/call_err.mlir b/shardy/dialect/mpmd/ir/test/call_err.mlir
new file mode 100644
index 0000000..c8e62e5
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/call_err.mlir
@@ -0,0 +1,43 @@
+// RUN: mpmd_opt %s 2>&1 -split-input-file -verify-diagnostics
+
+
+func.func public @main(%arg0: tensor<i32>) -> (tensor<i32>) {
+  // The function expects an integer, not a float.
+  %0 = stablehlo.constant dense<0.0> : tensor<f32>
+  // expected-error@+1 {{Type mismatch. Expected call operand to have type 'tensor<i32>' but got 'tensor<f32>'}}
+  %1 = mpmd.call @fn(%0, %arg0) : (tensor<f32>, tensor<i32>) -> tensor<i32>
+  return %1 : tensor<i32>
+}
+
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
+
+// -----
+
+func.func public @main(%arg0: tensor<i32>) -> (tensor<f32>) {
+  %0 = stablehlo.constant dense<0> : tensor<i32>
+  // expected-error@+1 {{Type mismatch. Expected call result to have type 'tensor<i32>' but got 'tensor<f32>'}}
+  %1 = mpmd.call @fn(%0, %arg0) : (tensor<i32>, tensor<i32>) -> tensor<f32>
+  return %1 : tensor<f32>
+}
+
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
+
+// -----
+
+func.func public @main(%arg0: tensor<i32>) -> (tensor<f32>) {
+  %0 = stablehlo.constant dense<0> : tensor<i32>
+  // expected-error@+1 {{call_counter must be an uint32, got 'i64'}}
+  %1 = mpmd.call @fn(%0, %arg0) {call_counter = 1} : (tensor<i32>, tensor<i32>) -> tensor<f32>
+  return %1 : tensor<f32>
+}
+
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
diff --git a/shardy/dialect/mpmd/ir/test/call_op_validation.mlir b/shardy/dialect/mpmd/ir/test/call_op_validation.mlir
new file mode 100644
index 0000000..5835a62
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/call_op_validation.mlir
@@ -0,0 +1,75 @@
+// RUN: mpmd_opt %s -split-input-file -verify-diagnostics
+
+
+#topology = #mpmd.topology<<"m1" : <["x"=1]>>>
+module {
+
+  func.func public @call_not_in_function_not_ok(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+    attributes {topology = #topology}
+  {
+    %c = stablehlo.constant dense<0> : tensor<i32>
+    %0:2 = stablehlo.while(%iterArg = %c, %iterArg_0 = %arg0) : tensor<i32>, tensor<3x5xf32>
+     cond {
+      %1 = stablehlo.compare  LT, %iterArg, %iterArg,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>
+      stablehlo.return %1 : tensor<i1>
+    } do {
+    // expected-error @+1 {{Mpmd CallOp on "f" can only be used in a function or for_op block but was called from inside op stablehlo.while}}
+      %1 = mpmd.call @f(%iterArg_0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+      stablehlo.return %iterArg, %1 : tensor<i32>, tensor<3x5xf32>
+    }
+    return %0#1 : tensor<3x5xf32>
+  }
+
+  func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+    attributes {topology = #topology} {
+    return %arg0 : tensor<3x5xf32>
+  }
+}
+
+// -----
+
+#topology = #mpmd.topology<<"m1" : <["x"=1]>>>
+module {
+
+  func.func public @nested_mpmd_call_not_ok(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+    attributes {topology = #topology}
+  {
+    %1 = mpmd.call @f(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    return %1 : tensor<3x5xf32>
+  }
+
+  func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+    attributes {topology = #topology} {
+    // expected-error @+1 {{Mpmd CallOp on "g" in an Mpmd function can only be used directly by the entrypoint function, i.e. the main function, but was called from "f"}}
+    %1 = mpmd.call @g(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    return %1 : tensor<3x5xf32>
+  }
+
+  func.func private @g(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+    attributes {topology = #topology} {
+    return %arg0 : tensor<3x5xf32>
+  }
+}
+
+// -----
+
+#topology = #mpmd.topology<<"m1" : <["x"=1]>>>
+module {
+
+  func.func public @mpmd_call_in_nested_in_func_call_is_ok(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+    attributes {topology = #topology}
+  {
+    %1 = func.call @f(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    return %1 : tensor<3x5xf32>
+  }
+
+  func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32> {
+    %1 = mpmd.call @g(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    return %1 : tensor<3x5xf32>
+  }
+
+  func.func private @g(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+    attributes {topology = #topology} {
+    return %arg0 : tensor<3x5xf32>
+  }
+}
diff --git a/shardy/dialect/mpmd/ir/test/call_print_and_parse.mlir b/shardy/dialect/mpmd/ir/test/call_print_and_parse.mlir
new file mode 100644
index 0000000..dcc48cb
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/call_print_and_parse.mlir
@@ -0,0 +1,69 @@
+// RUN: mpmd_opt %s 2>&1 -split-input-file | FileCheck %s
+
+// CHECK-LABEL: module
+// CHECK: func.func public @main
+func.func public @main(%arg0: tensor<3xi32>) -> (tensor<i32>) {
+  %0 = stablehlo.slice %arg0 [0:1] : (tensor<3xi32>) -> tensor<1xi32>
+  %1 = stablehlo.reshape %0 : (tensor<1xi32>) -> tensor<i32>
+  %2 = stablehlo.slice %arg0 [1:2] : (tensor<3xi32>) -> tensor<1xi32>
+  %3 = stablehlo.reshape %2 : (tensor<1xi32>) -> tensor<i32>
+  %4 = stablehlo.slice %arg0 [2:3] : (tensor<3xi32>) -> tensor<1xi32>
+  %5 = stablehlo.reshape %4 : (tensor<1xi32>) -> tensor<i32>
+  %6 = stablehlo.constant dense<0> : tensor<i32>
+  // CHECK: %[[i7:.+]] = mpmd.call @fn(%[[i6:.+]], %[[i1:.+]]) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  %7 = mpmd.call @fn(%6, %1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  // CHECK-NEXT: %[[i8:.+]] = mpmd.call @fn(%[[i7]], %[[i3:.+]]) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  %8 = mpmd.call @fn(%7, %3) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  // CHECK-NEXT: %{{.*}} = mpmd.call @fn(%[[i8]], %[[i5:.+]]) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  %9 = mpmd.call @fn(%8, %5) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  return %9 : tensor<i32>
+}
+
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
+
+// -----
+
+!m3elements = !mpmd.mesh_tensor<"mesh1", tensor<3xi32>>
+!m_scalar = !mpmd.mesh_tensor<"mesh1", tensor<i32>>
+
+// CHECK-LABEL: module
+// CHECK: func.func public @main
+func.func public @main(%arg0: !m3elements) -> (!m_scalar) attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK: %[[SPLIT_INPUTS:.*]]:3 = mpmd.fragment<mesh="mesh1", origin=["split"]> (%arg0)
+  %split_inputs:3 = mpmd.fragment<mesh="mesh1", origin=["split"]> (%arg0) (%arg1: tensor<3xi32>) {
+    %0 = stablehlo.slice %arg1 [0:1] : (tensor<3xi32>) -> tensor<1xi32>
+    %1 = stablehlo.reshape %0 : (tensor<1xi32>) -> tensor<i32>
+    %2 = stablehlo.slice %arg1 [1:2] : (tensor<3xi32>) -> tensor<1xi32>
+    %3 = stablehlo.reshape %2 : (tensor<1xi32>) -> tensor<i32>
+    %4 = stablehlo.slice %arg1 [2:3] : (tensor<3xi32>) -> tensor<1xi32>
+    %5 = stablehlo.reshape %4 : (tensor<1xi32>) -> tensor<i32>
+    mpmd.return %1, %3, %5 : tensor<i32>, tensor<i32>, tensor<i32>
+  } : (!m3elements) -> (!m_scalar, !m_scalar, !m_scalar)
+  // CHECK-DAG: %[[INIT_COUNTER:.*]] = mpmd.fragment<mesh="mesh1", origin=["split"]> ()
+  %init_counter = mpmd.fragment<mesh="mesh1", origin=["split"]> () () {
+    %6 = stablehlo.constant dense<0> : tensor<i32>
+    mpmd.return %6 : tensor<i32>
+  } : () -> !m_scalar
+  // CHECK-DAG: %[[I1:.*]] = mpmd.call @fn(%[[INIT_COUNTER]], %[[SPLIT_INPUTS]]#0)
+  %7 = mpmd.call @fn(%init_counter, %split_inputs#0) : (!m_scalar, !m_scalar) -> !m_scalar
+  // CHECK-NEXT: %[[I2:.*]] = mpmd.call @fn(%[[I1]], %[[SPLIT_INPUTS]]#1)
+  %8 = mpmd.call @fn(%7, %split_inputs#1) : (!m_scalar, !m_scalar) -> !m_scalar
+  // CHECK-NEXT: mpmd.call @fn(%[[I2]], %[[SPLIT_INPUTS]]#2)
+  %9 = mpmd.call @fn(%8, %split_inputs#2) : (!m_scalar, !m_scalar) -> !m_scalar
+  return %9 : !m_scalar
+}
+
+func.func private @fn(%arg0: !m_scalar, %arg1: !m_scalar) -> !m_scalar attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  %add = mpmd.fragment<mesh="mesh1", origin=["add"]> (%arg0, %arg1) (%arg2: tensor<i32>, %arg3: tensor<i32>) {
+    %0 = stablehlo.add %arg2, %arg3 : tensor<i32>
+    mpmd.return %0 : tensor<i32>
+  } : (!m_scalar, !m_scalar) -> !m_scalar
+  return %add : !m_scalar
+}
diff --git a/shardy/dialect/mpmd/ir/test/cannot_inline_calls.mlir b/shardy/dialect/mpmd/ir/test/cannot_inline_calls.mlir
new file mode 100644
index 0000000..d620338
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/cannot_inline_calls.mlir
@@ -0,0 +1,91 @@
+// RUN: mpmd_opt %s -inline -split-input-file 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: module
+// CHECK: func.func @test_inline_func_call
+func.func @test_inline_func_call(%arg0 : !mesh_1_tensor) -> !mesh_1_tensor attributes {
+  topology=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>>}
+{
+  // A call from the func dialect can still be inlined, even if the function
+  // contains fragment_calls. I.e., we cannot inline a fragment call, but we can
+  // inline a call of a function that includes a fragment_call (or any MPMD op).
+  // CHECK-NEXT: mpmd.fragment_call<mesh="m1", origin=[]> @f(%arg0)
+  // CHECK-NOT: call @func_to_inline(%arg0)
+  %0 = call @func_to_inline(%arg0) : (!mesh_1_tensor) -> !mesh_1_tensor
+  func.return %0 : !mesh_1_tensor
+}
+
+func.func @func_to_inline(%arg0 : !mesh_1_tensor) -> !mesh_1_tensor attributes {
+  topology=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>>}
+{
+  %0 = mpmd.fragment_call<mesh="m1", origin=[]> @f(%arg0) : (!mesh_1_tensor) -> !mesh_1_tensor
+  func.return %0 : !mesh_1_tensor
+}
+
+func.func @f(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=4]>} {
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: module
+// CHECK: func.func @test_no_inline_of_fragment_calls
+func.func @test_no_inline_of_fragment_calls(%arg0 : !mesh_tensor) -> !mesh_tensor attributes {
+  topology=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  // fragment_call ops cannot be inlined.
+  // CHECK-NEXT: mpmd.fragment_call<mesh="m1", origin=[]> @fragment(%arg0)
+  %0 = mpmd.fragment_call<mesh="m1", origin=[]> @fragment(%arg0) : (!mesh_tensor) -> !mesh_tensor
+  func.return %0 : !mesh_tensor
+}
+
+func.func @fragment(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["x"=2]>} {
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// -----
+
+// CHECK-LABEL: module
+// CHECK: func.func @test_no_inline_of_mpmd_calls
+func.func @test_no_inline_of_mpmd_calls(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>, <"mesh2" : <["y"=1]>>>}
+{
+  // mpmd.call ops cannot be inlined.
+  // CHECK-NEXT: %[[C1:.*]] = mpmd.call @f(%arg0) {call_counter = 0 : ui32}
+  // CHECK-NEXT: %[[C2:.*]] = mpmd.call @f(%[[C1]]) {call_counter = 1 : ui32}
+  // CHECK-NEXT: return %[[C2]]
+  %0 = mpmd.call @f(%arg0) {call_counter = 0 : ui32} : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  %1 = mpmd.call @f(%0) {call_counter = 1 : ui32} : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %1 : tensor<3x5xf32>
+}
+
+func.func @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32> {
+  return %arg0 : tensor<3x5xf32>
+}
+
+// -----
+
+// CHECK-LABEL: module
+// CHECK: func.func @test_assign_op_is_inlined
+func.func @test_assign_op_is_inlined(%arg0: tensor<4x8xf32>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>>
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>, <"mesh2" : <["y"=1]>>>}
+{
+  // An AssignOp can always be inlined.
+  // CHECK-NEXT: mpmd.assign %arg
+  // CHECK-NOT: call @assign_fn
+  // CHECK-NEXT: return
+  %0 = call @assign_fn(%arg0) : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>>
+  return %0 : !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>>
+}
+
+func.func @assign_fn(%arg0: tensor<4x8xf32>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>>
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>, <"mesh2" : <["y"=1]>>>}
+{
+  %0 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>>
+  return %0 : !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>>
+}
diff --git a/shardy/dialect/mpmd/ir/test/fragment_call_parse_and_print.mlir b/shardy/dialect/mpmd/ir/test/fragment_call_parse_and_print.mlir
new file mode 100644
index 0000000..127617f
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/fragment_call_parse_and_print.mlir
@@ -0,0 +1,36 @@
+// RUN: mpmd_opt %s 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0 : !mesh_1_tensor) -> !mesh_2_tensor attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2, "y"=4]>>,
+      <"m2": <["z"=3]>>
+    >} {
+  // CHECK-NEXT: %[[FRAGMENT_CALL_0:.*]] = mpmd.fragment_call<mesh="m1", origin=["f"(1), "g"]> @fragment1(%arg0)
+  // CHECK-NEXT: %[[FRAGMENT_CALL_1:.*]] = mpmd.fragment_call<mesh="m1", origin=[]> @fragment1(%[[FRAGMENT_CALL_0]])
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %[[FRAGMENT_CALL_1]]
+  // CHECK-NEXT: %[[FRAGMENT_CALL_2:.*]] = mpmd.fragment_call<mesh="m2", origin=[]> @fragment2(%[[TRANSFER]])
+  // CHECK-NEXT: return %[[FRAGMENT_CALL_2]]
+  %0 = mpmd.fragment_call<mesh="m1", origin=["f"(1), "g"]> @fragment1(%arg0) : (!mesh_1_tensor) -> !mesh_1_tensor
+  %1 = mpmd.fragment_call<mesh="m1", origin=[]> @fragment1(%0) : (!mesh_1_tensor) -> !mesh_1_tensor
+  %2 = mpmd.transfer %1 : (!mesh_1_tensor) -> !mesh_2_tensor
+  %3 = mpmd.fragment_call<mesh="m2", origin=[]> @fragment2(%2) : (!mesh_2_tensor) -> !mesh_2_tensor
+  func.return %3 : !mesh_2_tensor
+}
+
+// CHECK-LABEL: func @fragment1
+func.func @fragment1(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=4]>} {
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @fragment2
+func.func @fragment2(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["z"=3]>} {
+  %0 = stablehlo.multiply %arg0, %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/ir/test/fragment_canonicalization.mlir b/shardy/dialect/mpmd/ir/test/fragment_canonicalization.mlir
new file mode 100644
index 0000000..03eb71b
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/fragment_canonicalization.mlir
@@ -0,0 +1,57 @@
+// RUN: mpmd_opt %s -canonicalize 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_1_sharded_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+
+module {
+
+// CHECK-LABEL: func @one_used_one_unused_pass_through
+func.func @one_used_one_unused_pass_through(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor)
+  attributes { "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>> } {
+
+  // CHECK-NEXT: %[[F:.*]]:3 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+  // CHECK-NEXT:   mpmd.return %[[ADD]], %arg2, %arg3 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[F]]#0, %arg0
+
+  %0:3 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+      %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+      mpmd.return %1, %arg2, %arg3 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+  func.return %0#0, %0#1 : !mesh_1_tensor, !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @canonicalize_is_noop
+func.func @canonicalize_is_noop(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor) -> !mesh_1_tensor
+  attributes { "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>> } {
+
+  // CHECK-NEXT: %[[F:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+  // CHECK-NEXT:   mpmd.return %[[ADD]] : tensor<4x8xf32>
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[F]]
+
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+      %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+      mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+  func.return %0 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @canonicalize_is_noop_because_of_different_types
+func.func @canonicalize_is_noop_because_of_different_types(%arg0: !mesh_1_tensor) -> !mesh_1_sharded_tensor
+  attributes { "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>> } {
+  // CHECK-NEXT: %[[F:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+  // CHECK-NEXT:   mpmd.return %arg1 : tensor<4x8xf32>
+  // CHECK-NEXT: } : ({{.*}}tensor<4x8xf32>>) -> {{.*}}tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+  // CHECK-NEXT: return %[[F]]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_sharded_tensor
+  func.return %0 : !mesh_1_sharded_tensor
+}
+
+}
diff --git a/shardy/dialect/mpmd/ir/test/fragment_parse_and_print.mlir b/shardy/dialect/mpmd/ir/test/fragment_parse_and_print.mlir
new file mode 100644
index 0000000..0531dfd
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/fragment_parse_and_print.mlir
@@ -0,0 +1,91 @@
+// RUN: mpmd_opt %s 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @fragment_with_stage
+func.func @fragment_with_stage(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>>>}
+{
+  // CHECK: mpmd.fragment<mesh="mesh1", origin=["f1"], stage=123> (%arg0) (%arg1: tensor<12x16xf32>)
+  %0 = mpmd.fragment<mesh="mesh1", origin=["f1"], stage=123> (%arg0) (%arg1: tensor<12x16xf32>) {
+    mpmd.return %arg1 : tensor<12x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+   -> (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+  func.return %0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+}
+
+// CHECK-LABEL: func @fragment_with_stage_and_in_shardings
+func.func @fragment_with_stage_and_in_shardings(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>>>}
+{
+  // CHECK: mpmd.fragment<mesh="mesh1", origin=["f1"], stage=123, in_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>)
+  %0 = mpmd.fragment<mesh="mesh1", origin=["f1"], stage=123, in_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>) {
+    mpmd.return %arg1 : tensor<12x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+   -> (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+  func.return %0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+}
+
+// CHECK-LABEL: func @fragment_with_stage_and_out_shardings
+func.func @fragment_with_stage_and_out_shardings(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>>>}
+{
+  // CHECK: mpmd.fragment<mesh="mesh1", origin=["f1"], stage=123, out_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>)
+  %0 = mpmd.fragment<mesh="mesh1", origin=["f1"], stage=123, out_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>) {
+    mpmd.return %arg1 : tensor<12x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+   -> (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+  func.return %0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+}
+
+// CHECK-LABEL: func @fragment_with_in_shardings_without_stage
+func.func @fragment_with_in_shardings_without_stage(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>>>}
+{
+  // CHECK: mpmd.fragment<mesh="mesh1", origin=["f1"], in_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>)
+  %0 = mpmd.fragment<mesh="mesh1", origin=["f1"], in_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>) {
+    mpmd.return %arg1 : tensor<12x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+   -> (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+  func.return %0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+}
+
+// CHECK-LABEL: func @fragment_with_out_shardings_without_stage
+func.func @fragment_with_out_shardings_without_stage(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>>>}
+{
+  // CHECK: mpmd.fragment<mesh="mesh1", origin=["f1"], out_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>)
+  %0 = mpmd.fragment<mesh="mesh1", origin=["f1"], out_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>) {
+    mpmd.return %arg1 : tensor<12x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+   -> (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+  func.return %0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+}
+
+// CHECK-LABEL: func @fragment_with_stage_and_in_shardings_and_out_shardings
+func.func @fragment_with_stage_and_in_shardings_and_out_shardings(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>>>}
+{
+  // CHECK: mpmd.fragment<mesh="mesh1", origin=["f1"], stage=123, in_shardings=[<@mesh, [{"x"}, {?}]>], out_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>)
+  %0 = mpmd.fragment<mesh="mesh1", origin=["f1"], stage=123, in_shardings=[<@mesh, [{"x"}, {?}]>], out_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>) {
+    mpmd.return %arg1 : tensor<12x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+   -> (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+  func.return %0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+}
+
+// CHECK-LABEL: func @fragment_without_stage_with_in_shardings_and_out_shardings
+func.func @fragment_without_stage_with_in_shardings_and_out_shardings(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>>>}
+{
+  %0 = mpmd.fragment<mesh="mesh1", origin=["f1"], in_shardings=[<@mesh, [{"x"}, {?}]>], out_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<12x16xf32>) {
+    mpmd.return %arg1 : tensor<12x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+   -> (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+  func.return %0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+}
diff --git a/shardy/dialect/mpmd/ir/test/memory_kind_parse_and_print.mlir b/shardy/dialect/mpmd/ir/test/memory_kind_parse_and_print.mlir
new file mode 100644
index 0000000..d860116
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/memory_kind_parse_and_print.mlir
@@ -0,0 +1,112 @@
+// RUN: mpmd_opt %s -verify-diagnostics -split-input-file 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @no_sharding_nor_memory_kind
+// CHECK-SAME: !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>>
+func.func @no_sharding_nor_memory_kind(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>>)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// CHECK-LABEL: func @sharding_but_no_memory_kind
+// CHECK-SAME: !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+func.func @sharding_but_no_memory_kind(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// CHECK-LABEL: func @no_sharding_but_memory_kind
+// CHECK-SAME
+func.func @no_sharding_but_memory_kind(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, memory_kind="device">)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// CHECK-LABEL: func @sharding_and_memory_kind
+// CHECK-SAME: !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>, memory_kind="pinned_host">
+func.func @sharding_and_memory_kind(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>, memory_kind="pinned_host">)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// expected-error @+1 {{expected '>'}}
+func.func @missing_comma(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]> memory_kind="pinned_host">)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// expected-error @+1 {{unbalanced '<' character in pretty dialect name}}
+func.func @last_gt_missing(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// expected-error @+1 {{expected '>'}}
+func.func @too_many_commas(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>, memory_kind="pinned_host",>)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// expected-error @+1 {{expected '>'}}
+func.func @too_many_commas(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, memory_kind="pinned_host",>)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// expected-error @+1 {{expected 'memory_kind'}}
+func.func @too_many_commas(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>, >)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// expected-error @+1 {{expected '<'}}
+func.func @invalid_sharding_without_memory_kind(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding="not_a_sharding">)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+// -----
+
+// expected-error @+1 {{expected '<'}}
+func.func @invalid_sharding_without_comma_before_memory_kind(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding="not_a_sharding" memory_kind="pinned_host">)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
+
+
+// -----
+
+// expected-error @+1 {{expected '<'}}
+func.func @invalid_sharding(%arg0 : !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding="not_a_sharding", memory_kind="pinned_host">)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  func.return
+}
diff --git a/shardy/dialect/mpmd/ir/test/origin_attrs_parse_and_print.mlir b/shardy/dialect/mpmd/ir/test/origin_attrs_parse_and_print.mlir
new file mode 100644
index 0000000..9925a02
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/origin_attrs_parse_and_print.mlir
@@ -0,0 +1,12 @@
+// RUN: mpmd_opt %s -- 2>&1 | FileCheck %s
+
+// Just verifying no errors in parsing and printing, and giving examples of
+// how the attributes are printed.
+
+// CHECK-LABEL: func @simple_example(%arg0
+func.func @simple_example(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {
+  %0 = stablehlo.add %arg0, %arg0  {mpmd.use_set = #mpmd.meshes_with_origins<"m3">}: tensor<4x8xf32>
+  %1 = stablehlo.add %arg0, %arg0  {mpmd.use_set = #mpmd.meshes_with_origins<"m3"["origin1","origin2"]>}: tensor<4x8xf32>
+  %2 = stablehlo.add %arg0, %arg0  {mpmd.use_set = #mpmd.meshes_with_origins<>}: tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/ir/test/reduce_canonicalization.mlir b/shardy/dialect/mpmd/ir/test/reduce_canonicalization.mlir
new file mode 100644
index 0000000..0855dc2
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/reduce_canonicalization.mlir
@@ -0,0 +1,49 @@
+// RUN: mpmd_opt %s -canonicalize 2>&1 | FileCheck %s
+
+#topology =#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["y"=2]>>>
+
+// CHECK-LABEL: func @reduce_of_reduce_chain_of_none_type
+func.func @reduce_of_reduce_chain_of_none_type(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {"topology"=#topology} {
+  // CHECK-NEXT: %[[REDUCE:.*]] = mpmd.reduce<none> %arg0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  // CHECK-NEXT: return %[[REDUCE]] : tensor<4x8xf32>
+  %0 = mpmd.reduce<none> %arg0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  %1 = mpmd.reduce<none> %0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  %2 = mpmd.reduce<none> %1 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %2 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @flatten_reduce_chain_if_types_match
+func.func @flatten_reduce_chain_if_types_match(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {"topology"=#topology} {
+  // CHECK-NEXT: %[[REDUCE:.*]] = mpmd.reduce<add> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  // CHECK-NEXT: return %[[REDUCE]] : tensor<4x8xf32>
+  %0 = mpmd.reduce<add> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  %1 = mpmd.reduce<add> %0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @flatten_reduce_chain_if_outer_reduce_is_none
+func.func @flatten_reduce_chain_if_outer_reduce_is_none(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {"topology"=#topology} {
+  // CHECK-NEXT: %[[REDUCE:.*]] = mpmd.reduce<add> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  // CHECK-NEXT: return %[[REDUCE]] : tensor<4x8xf32>
+  %0 = mpmd.reduce<add> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  %1 = mpmd.reduce<none> %0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @do_not_flatten_reduce_chain_if_types_dont_match
+func.func @do_not_flatten_reduce_chain_if_types_dont_match(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {"topology"=#topology} {
+  // CHECK-NEXT: %[[REDUCE_MUL:.*]] = mpmd.reduce<mul> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  // CHECK-NEXT: %[[REDUCE_ADD:.*]] = mpmd.reduce<add> %[[REDUCE_MUL]] : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  // CHECK-NEXT: return %[[REDUCE_ADD]] : tensor<4x8xf32>
+  %0 = mpmd.reduce<mul> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  %1 = mpmd.reduce<add> %0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @single_reduce_with_block_arg
+func.func @single_reduce_with_block_arg(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {"topology"=#topology} {
+  // CHECK-NEXT: %[[REDUCE:.*]] = mpmd.reduce<none> %arg0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  // CHECK-NEXT: return %[[REDUCE]] : tensor<4x8xf32>
+  %0 = mpmd.reduce<none> %arg0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/ir/test/reduce_parse_and_print.mlir b/shardy/dialect/mpmd/ir/test/reduce_parse_and_print.mlir
new file mode 100644
index 0000000..764eab9
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/reduce_parse_and_print.mlir
@@ -0,0 +1,17 @@
+// RUN: mpmd_opt %s 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @reduce_with_none_reduction_type
+func.func @reduce_with_none_reduction_type(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=4]>} {
+  // CHECK-NEXT: mpmd.reduce<none> %arg0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  %0 = mpmd.reduce<none> %arg0 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @reduce_with_add_reduction_type
+func.func @reduce_with_add_reduction_type(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=4]>} {
+  // CHECK-NEXT: mpmd.reduce<add> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  %0 = mpmd.reduce<add> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/ir/test/reshard_fragment.mlir b/shardy/dialect/mpmd/ir/test/reshard_fragment.mlir
new file mode 100644
index 0000000..698e014
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/reshard_fragment.mlir
@@ -0,0 +1,22 @@
+// RUN: mpmd_opt %s 2>&1 | FileCheck %s
+
+// The purpose of this test is to demonstrate that it's possible to reshard a
+// tensor with a fragment, when in global_view.
+
+module {
+
+func.func @reshard(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>>>}
+{
+  // The fragment reshards a value (from sharded to replicated). This is only
+  // possible when the module is in global view.
+  // CHECK: (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>) -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>>
+  %0 = mpmd.fragment<mesh="mesh1", origin=[]> (%arg0) (%arg1: tensor<12x16xf32>) {
+    mpmd.return %arg1 : tensor<12x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+   -> !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>>
+  func.return %0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>>
+}
+
+}
diff --git a/shardy/dialect/mpmd/ir/test/transfer_canonicalization.mlir b/shardy/dialect/mpmd/ir/test/transfer_canonicalization.mlir
new file mode 100644
index 0000000..31ded53
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/transfer_canonicalization.mlir
@@ -0,0 +1,62 @@
+// RUN: mpmd_opt %s -canonicalize 2>&1 | FileCheck %s
+
+!mesh_1_replicated = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_1_distributed_0 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+!mesh_1_distributed_1 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{?}, {"x"}]>>
+
+!mesh_2_replicated = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!mesh_2_distributed = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"y"}, {?}]>>
+
+// CHECK-LABEL: func @identity_transfer
+func.func @identity_transfer(%arg0: !mesh_1_replicated) -> !mesh_1_distributed_0 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[TRANSFER:.*]] = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+// CHECK-NEXT:  return %[[TRANSFER]]
+  %0 = mpmd.transfer %arg0 : (!mesh_1_replicated) -> !mesh_1_replicated
+  %1= mpmd.transfer %0 : (!mesh_1_replicated) -> !mesh_1_distributed_0
+  func.return %1 : !mesh_1_distributed_0
+}
+
+// CHECK-LABEL: func @intra_mesh_transfer_of_transfer_one_use
+func.func @intra_mesh_transfer_of_transfer_one_use(%arg0: !mesh_1_replicated) -> !mesh_1_distributed_1 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[TRANSFER:.*]] = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{?}, {"x"}]>>
+// CHECK-NEXT:  return %[[TRANSFER]]
+  %0 = mpmd.transfer %arg0 : (!mesh_1_replicated) -> !mesh_1_distributed_0
+  %1= mpmd.transfer %0 : (!mesh_1_distributed_0) -> !mesh_1_distributed_1
+  func.return %1 : !mesh_1_distributed_1
+}
+
+// CHECK-LABEL: func @intra_mesh_transfer_of_transfer_multi_use
+func.func @intra_mesh_transfer_of_transfer_multi_use(%arg0: !mesh_1_replicated) -> (!mesh_1_distributed_0, !mesh_1_distributed_1) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[TRANSFER_0:.*]] = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+// CHECK-NEXT:  %[[TRANSFER_1:.*]] = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{?}, {"x"}]>>
+// CHECK-NEXT:  return %[[TRANSFER_0]], %[[TRANSFER_1]]
+  %0 = mpmd.transfer %arg0 : (!mesh_1_replicated) -> !mesh_1_distributed_0
+  %1= mpmd.transfer %0 : (!mesh_1_distributed_0) -> !mesh_1_distributed_1
+  func.return %0, %1 : !mesh_1_distributed_0, !mesh_1_distributed_1
+}
+
+// CHECK-LABEL: func @inter_mesh_transfer_of_transfer
+func.func @inter_mesh_transfer_of_transfer(%arg0: !mesh_1_replicated) -> !mesh_2_distributed attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[TRANSFER_0:.*]] = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+// CHECK-NEXT:  %[[TRANSFER_1:.*]] = mpmd.transfer %[[TRANSFER_0]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"y"}, {?}]>>
+// CHECK-NEXT:  return %[[TRANSFER_1]]
+  %0 = mpmd.transfer %arg0 : (!mesh_1_replicated) -> !mesh_1_distributed_0
+  %1= mpmd.transfer %0 : (!mesh_1_distributed_0) -> !mesh_2_distributed
+  func.return %1 : !mesh_2_distributed
+}
diff --git a/shardy/dialect/mpmd/ir/test/transfer_parse_and_print.mlir b/shardy/dialect/mpmd/ir/test/transfer_parse_and_print.mlir
new file mode 100644
index 0000000..d9439d0
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/transfer_parse_and_print.mlir
@@ -0,0 +1,14 @@
+// RUN: mpmd_opt %s 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @transfer
+func.func @transfer(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+    -> !mpmd.mesh_tensor<"mesh2", tensor<12x16xf32>, sharding=<@mesh, [{"z"}, {?}]>> attributes {
+    "topology"=#mpmd.topology<
+      <"mesh1": <["x"=2, "y"=4]>>,
+      <"mesh2": <["z"=3]>>
+    >} {
+  // CHECK:      mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+  // CHECK-SAME:   -> !mpmd.mesh_tensor<"mesh2", tensor<12x16xf32>, sharding=<@mesh, [{"z"}, {?}]>>
+  %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>) -> !mpmd.mesh_tensor<"mesh2", tensor<12x16xf32>, sharding=<@mesh, [{"z"}, {?}]>>
+  func.return %0 : !mpmd.mesh_tensor<"mesh2", tensor<12x16xf32>, sharding=<@mesh, [{"z"}, {?}]>>
+}
diff --git a/shardy/dialect/mpmd/ir/test/transfer_w_memory_kind_verify.mlir b/shardy/dialect/mpmd/ir/test/transfer_w_memory_kind_verify.mlir
new file mode 100644
index 0000000..6a9680d
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/transfer_w_memory_kind_verify.mlir
@@ -0,0 +1,75 @@
+// RUN: mpmd_opt %s -verify-diagnostics -split-input-file
+
+!m_undefined = !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+!m_device = !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>, memory_kind = "device">
+!m_host = !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>, memory_kind = "device">
+!m_invalid = !mpmd.mesh_tensor<"mesh", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>, memory_kind = "qwerty">
+
+func.func @f(%arg0 : !m_device)
+    attributes {"topology"=#mpmd.topology<<"mesh": <["x"=2, "y"=4]>>>}
+{
+  %t1 = mpmd.transfer %arg0 : (!m_device) -> !m_undefined  // No error.
+  %t2 = mpmd.transfer %t1 : (!m_undefined) -> !m_host      // No error.
+  // expected-error@+1 {{memory kind must be either 'pinned_host' or 'device'. Found 'qwerty'.}}
+  %t3 = mpmd.transfer %t2 : (!m_host) -> !m_invalid
+  func.return
+}
+
+// -----
+
+!m1_type = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!m2_type = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+func.func private @transfer_on_device_is_allowed(%arg0 : !m1_type {mhlo.memory_kind = "device"}) -> !m2_type
+  attributes {topology=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>}
+{
+  %t = mpmd.transfer %arg0 : (!m1_type) -> !m2_type
+  func.return %t : !m2_type
+}
+
+// -----
+
+!m1_type = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!m2_type = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+func.func private @arg_cannot_be_pinned_to_host_if_transferred(%arg0 : !m1_type {mhlo.memory_kind = "pinned_host"}) -> !m2_type
+  attributes {topology=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>}
+{
+// expected-error@+1 {{Transfers from host with attributes are not supported. Memory kinds must be expressed in the type.}}
+  %t = mpmd.transfer %arg0 : (!m1_type) -> !m2_type
+  func.return %t : !m2_type
+}
+
+// -----
+
+!m1_type = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!m2_type = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+
+func.func private @fragment_result_cannot_be_pinned_to_host_if_transferred(%arg0 : !m1_type {mhlo.memory_kind = "pinned_host"}) -> !m2_type
+  attributes {topology=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>}
+{
+  %f:2 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    {arg_attrs = [{mhlo.memory_kind = "pinned_host"}],
+     res_attrs = [{mhlo.memory_kind = "pinned_host"}, {mhlo.memory_kind = "device"}]}
+  (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1, %arg1 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!m1_type) -> (!m1_type, !m1_type)
+
+// expected-error@+1 {{Transfers from host with attributes are not supported. Memory kinds must be expressed in the type.}}
+  %t = mpmd.transfer %f#0 : (!m1_type) -> !m2_type
+  func.return %t : !m2_type
+}
+
+// -----
+
+!m1_type = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!m2_type = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+func.func private @transfer_to_host_with_attributes_is_not_allowed(%arg0 : !m1_type) -> !m2_type
+  attributes {topology=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>}
+{
+// expected-error@+1 {{Transfers to host with attributes are not supported. Memory kinds must expressed be in the type.}}
+  %t = mpmd.transfer {mhlo.memory_kind = "pinned_host"} %arg0 : (!m1_type) -> !m2_type
+  func.return %t : !m2_type
+}
diff --git a/shardy/dialect/mpmd/ir/test/verify_mesh_tensor_type.mlir b/shardy/dialect/mpmd/ir/test/verify_mesh_tensor_type.mlir
new file mode 100644
index 0000000..cc5ddd5
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/verify_mesh_tensor_type.mlir
@@ -0,0 +1,70 @@
+// RUN: mpmd_opt %s -verify-diagnostics -split-input-file
+
+
+func.func @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+      -> (!mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=4]>>
+    >} {
+    %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+        -> !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+    func.return %0 : !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+}
+
+// -----
+
+func.func @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<1x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+      -> (!mpmd.mesh_tensor<"m2", tensor<1x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=4]>>
+    >} {
+    // expected-error @+1 {{'mpmd.transfer' op dim 0 with size 1 is not divisible by its sharded size 4}}
+    %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<1x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+        -> !mpmd.mesh_tensor<"m2", tensor<1x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+    func.return %0 : !mpmd.mesh_tensor<"m2", tensor<1x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+}
+
+// -----
+
+func.func @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"y"}, {?}]>>)
+      -> (!mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=4]>>
+    >} {
+    // expected-error @+1 {{'mpmd.transfer' op unknown axis name: "y"}}
+    %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"y"}, {?}]>>)
+        -> !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+    func.return %0 : !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+}
+
+// -----
+
+func.func @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<32xf32>, sharding=<@mesh, [{"x"}, {"x"}]>>)
+      -> (!mpmd.mesh_tensor<"m2", tensor<32xf32>, sharding=<@mesh, [{"x"}, {"x"}]>>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=4]>>
+    >} {
+    // expected-error @+1 {{'mpmd.transfer' op sharding doesn't match tensor rank: 2 != 1}}
+    %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<32xf32>, sharding=<@mesh, [{"x"}, {"x"}]>>)
+        -> !mpmd.mesh_tensor<"m2", tensor<32xf32>, sharding=<@mesh, [{"x"}, {"x"}]>>
+    func.return %0 : !mpmd.mesh_tensor<"m2", tensor<32xf32>, sharding=<@mesh, [{"x"}, {"x"}]>>
+}
+
+
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+func.func @subaxes_are_allowed_when_they_divide_sharding_dim(
+   %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x":(1)4}, {}]>>)
+  -> !mesh_2_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=16]>>,
+      <"m2": <["x"=16]>>
+    >} {
+  %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x":(1)4}, {}]>>)
+        -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  func.return %0 : !mesh_2_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/ir/test/verify_reduce_op.mlir b/shardy/dialect/mpmd/ir/test/verify_reduce_op.mlir
new file mode 100644
index 0000000..5bc65db
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/test/verify_reduce_op.mlir
@@ -0,0 +1,8 @@
+// RUN: mpmd_opt %s -verify-diagnostics
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=4]>} {
+  %0 = mpmd.reduce<none> %arg0, %arg0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32> // expected-error {{ReduceOp must have exactly one operand if the reduction type is none}}
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/ir/types.td b/shardy/dialect/mpmd/ir/types.td
new file mode 100644
index 0000000..c95c87a
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/types.td
@@ -0,0 +1,119 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef MPMD_TYPES
+#define MPMD_TYPES
+
+include "mlir/IR/AttrTypeBase.td"
+include "mlir/IR/OpBase.td"
+include "mlir/IR/BuiltinTypeInterfaces.td"
+include "shardy/dialect/mpmd/ir/dialect.td"
+
+// Base class for MPMD types.
+class Mpmd_Type<string name, string typeMnemonic, list<Trait> traits = []>
+    : TypeDef<Mpmd_Dialect, name, traits> {
+  let mnemonic = typeMnemonic;
+}
+
+def Mpmd_MeshTensorType : Mpmd_Type<"MeshTensor", "mesh_tensor",
+  [DeclareTypeInterfaceMethods<ShapedTypeInterface>]> {
+  let summary = "mesh tensor type";
+  let description = [{
+    Assigns a RankedTensorType to a specific SPMD mesh in the program's
+    MPMD topology of meshes.
+    The type holds an optional sharding that specifies how the tensor is
+    sharded w.r.t to the SPMD mesh.
+    If the sharding is not present the tensor is fully replicated.
+  }];
+
+  let parameters = (ins
+    StringRefParameter<"mesh name">:$mesh_name,
+    TypeParameter<"::mlir::RankedTensorType", "ranked tensor type">:$ranked_tensor_type,
+    OptionalParameter<"::mlir::sdy::TensorShardingAttr">:$sharding,
+    OptionalParameter<"::mlir::StringAttr">:$memory_kind
+  );
+
+  let hasCustomAssemblyFormat = 1;
+
+  let builders = [
+    TypeBuilder<(ins
+      "::llvm::StringRef":$mesh_name,
+      "::mlir::RankedTensorType":$ranked_tensor_type),
+    [{
+      return $_get($_ctxt, mesh_name, ranked_tensor_type,
+                   /*sharding=*/nullptr, /*memory_kind=*/nullptr);
+    }]>,
+
+    TypeBuilder<(ins
+      "::llvm::StringRef":$mesh_name,
+      "::mlir::RankedTensorType":$ranked_tensor_type,
+      "::mlir::sdy::TensorShardingAttr":$sharding),
+    [{
+      return $_get($_ctxt, mesh_name, ranked_tensor_type,
+                   /*sharding=*/sharding, /*memory_kind=*/nullptr);
+    }]>,
+
+    TypeBuilder<(ins
+      "::llvm::StringRef":$mesh_name,
+      "::mlir::RankedTensorType":$ranked_tensor_type,
+      "::mlir::StringAttr":$memory_kind),
+    [{
+      return $_get($_ctxt, mesh_name, ranked_tensor_type,
+                   /*sharding=*/nullptr, /*memory_kind=*/memory_kind);
+    }]>
+  ];
+
+  let extraClassDeclaration = [{
+    // Verifies the MeshTensorType in the context of a topology, by locating the
+    // TopologyAttr in the main FuncOp via `op`.
+    LogicalResult verifyForTopology(Operation* op);
+    // Verifies the MeshTensorType in the context of the given `mesh`.
+    LogicalResult verifyForMesh(sdy::MeshAttr mesh, Operation* op);
+
+    // Returns the local tensor type of the MeshTensorType wrt the given `mesh`.
+    RankedTensorType getLocalTensorType(sdy::MeshAttr sdy_mesh);
+
+    // Returns the local tensor type of the MeshTensorType wrt
+    // the mesh (of the op) and sharding specified.
+    // Assumes that the sharding is valid wrt the mesh and tensor type.
+    RankedTensorType getLocalTensorType(Operation* op);
+
+    RankedTensorType getGlobalTensorType() {
+      return getRankedTensorType();
+    }
+
+    // Builds a MeshTensorType with a fully replicated sharding,
+    // whose local type is `local_type`.
+    static MeshTensorType getFullyReplicated(
+        MLIRContext* ctx, StringRef mesh_name, sdy::MeshAttr mesh,
+        RankedTensorType local_type);
+
+    // Returns a MeshTensorType with only the sharding replaced.
+    MeshTensorType replaceSharding(sdy::TensorShardingAttr sharding);
+
+    // Returns true if the MeshTensorType is fully replicated.
+    bool isFullyReplicated() {
+      sdy::TensorShardingAttr sharding = getSharding();
+      return !sharding || sharding.isFullyReplicated();
+    }
+
+    // Returns true if the MeshTensorType is on the host.
+    bool isOnHost();
+  }];
+}
+
+def LocalOrMeshTensor : AnyTypeOf<[AnyTensor, Mpmd_MeshTensorType]>;
+
+#endif
diff --git a/shardy/dialect/mpmd/ir/utils.cc b/shardy/dialect/mpmd/ir/utils.cc
new file mode 100644
index 0000000..19116ed
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/utils.cc
@@ -0,0 +1,737 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/ir/utils.h"
+
+#include <algorithm>
+#include <cstddef>
+#include <cstdint>
+#include <functional>
+#include <optional>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Bytecode/BytecodeWriter.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/IRMapping.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/RegionUtils.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir::mpmd {
+
+namespace {
+
+using ::mlir::func::FuncOp;
+
+inline constexpr StringRef kJaxResultInfoAttr = "jax.result_info";
+
+SpmdTensorShardingSpec ExtractTensorShardingSpec(MeshTensorType type,
+                                                 sdy::MeshAttr mesh_attr) {
+  if (!type.getSharding()) {
+    return {};
+  }
+  SpmdTensorShardingSpec spec;
+  spec.reserve(type.getSharding().getDimShardings().size());
+  llvm::SmallDenseMap<StringRef, int> axis_name_to_index;
+  for (auto [index, axis] : llvm::enumerate(mesh_attr.getAxes())) {
+    axis_name_to_index[axis.getName()] = index;
+  }
+
+  for (sdy::DimensionShardingAttr dim_sharding :
+       type.getSharding().getDimShardings()) {
+    std::vector<int> indices;
+    indices.reserve(dim_sharding.getAxes().size());
+    // SDY sharding axes are in major-to-minor, but `SpmdTensorShardingSpec` is
+    // expected to be minor-to-major. So we reverse the order.
+    for (sdy::AxisRefAttr axis : llvm::reverse(dim_sharding.getAxes())) {
+      indices.push_back(axis_name_to_index[axis.getName()]);
+    }
+    spec.emplace_back(indices);
+  }
+  return spec;
+}
+
+NamedSpmdShardingSpec GetNamedShardingSpec(MeshTensorType mesh_tensor,
+                                           sdy::MeshAttr mesh_attr) {
+  SpmdTensorShardingSpec spec =
+      ExtractTensorShardingSpec(mesh_tensor, mesh_attr);
+  std::optional<std::string> memory_kind;
+  if (mesh_tensor.getMemoryKind()) {
+    memory_kind = mesh_tensor.getMemoryKind().getValue().str();
+  }
+  return NamedSpmdShardingSpec{mesh_tensor.getMeshName().str(), spec,
+                               memory_kind};
+}
+
+std::vector<FragmentOrigin> GetFragmentOrigins(FragmentOp op) {
+  std::vector<FragmentOrigin> origins;
+  for (Attribute origin_attr : op.getOrigin()) {
+    auto user_origin = cast<UserOriginAttr>(origin_attr);
+    origins.push_back(
+        FragmentOrigin{/*computation_name=*/user_origin.getUserName().str(),
+                       /*transpose_count=*/user_origin.getTransposeCount()});
+  }
+  llvm::sort(origins);
+  return origins;
+}
+
+}  // namespace
+
+FragmentInfo GetFragmentInfo(FragmentOp fragment) {
+  std::optional<uint64_t> stage_id;
+  if (fragment.getStageIdAttr()) {
+    stage_id = fragment.getStageIdAttr().getInt();
+  }
+  std::optional<int64_t> call_counter = TryToFindCallCounter(fragment);
+  std::vector<FragmentOrigin> origins = GetFragmentOrigins(fragment);
+
+  return FragmentInfo{origins, stage_id, call_counter};
+}
+
+void SetFragmentInfo(FragmentOp fragment, const FragmentInfo& metadata,
+                     RewriterBase& rewriter) {
+  std::vector<Attribute> new_origins;
+  for (const auto& origin : metadata.origins) {
+    Attribute attr = UserOriginAttr::get(
+        rewriter.getContext(),
+        StringAttr::get(rewriter.getContext(), origin.computation_name),
+        origin.transpose_count);
+    new_origins.push_back(attr);
+  }
+
+  fragment.setOriginAttr(ArrayAttr::get(rewriter.getContext(), new_origins));
+
+  fragment.setStageId(metadata.stage_id);
+
+  if (metadata.call_counter.has_value()) {
+    fragment->setAttr(kCallCounterAttrName,
+                      rewriter.getUI32IntegerAttr(*metadata.call_counter));
+  } else {
+    fragment->removeAttr(kCallCounterAttrName);
+  }
+}
+
+FunctionIOShardingSpecsAndMeshes ExtractFunctionIOShardingSpecsAndMeshes(
+    FuncOp func_op) {
+  FunctionIOShardingSpecsAndMeshes io_sharding_and_mesh;
+  io_sharding_and_mesh.input_specs.reserve(func_op.getNumArguments());
+  for (BlockArgument arg : func_op.getArguments()) {
+    MeshTensorType arg_type = cast<MeshTensorType>(arg.getType());
+    sdy::MeshAttr mesh_attr = GetMeshOrFail(func_op, arg_type.getMeshName());
+    NamedSpmdShardingSpec mesh_and_spec =
+        GetNamedShardingSpec(arg_type, mesh_attr);
+    if (auto memory_kind = func_op.getArgAttrOfType<StringAttr>(
+            arg.getArgNumber(), kMemoryKindAttr)) {
+      if (!mesh_and_spec.memory_kind.has_value()) {
+        mesh_and_spec.memory_kind = memory_kind.getValue().str();
+      }
+      // TODO: b/374994155 - We should drop the memory kind from the
+      // attributes and move it to types only. If for some reason that's not
+      // possible we should at least be consistent and use the memory kind from
+      // the type if then we need at least some verification of their
+      // consistency.
+    }
+    io_sharding_and_mesh.input_specs.push_back(mesh_and_spec);
+  }
+
+  for (auto [index, type] : llvm::enumerate(func_op.getResultTypes())) {
+    MeshTensorType result_type = cast<MeshTensorType>(type);
+    sdy::MeshAttr mesh_attr = GetMeshOrFail(func_op, result_type.getMeshName());
+    NamedSpmdShardingSpec mesh_and_spec =
+        GetNamedShardingSpec(result_type, mesh_attr);
+    if (auto memory_kind =
+            func_op.getResultAttrOfType<StringAttr>(index, kMemoryKindAttr)) {
+      if (!mesh_and_spec.memory_kind.has_value()) {
+        mesh_and_spec.memory_kind = memory_kind.getValue().str();
+      }
+      // TODO: b/374994155 - We should drop the memory kind from the
+      // attributes and move it to types only. If for some reason that's not
+      // possible we should at least be consistent and use the memory kind from
+      // the type if then we need at least some verification of their
+      // consistency.
+    }
+    io_sharding_and_mesh.output_specs.push_back(mesh_and_spec);
+  }
+
+  return io_sharding_and_mesh;
+}
+
+FuncOp GetMainFunction(ModuleOp module) {
+  FuncOp func = dyn_cast_or_null<FuncOp>(module.lookupSymbol("main"));
+  SDY_CHECK(func);
+  return func;
+}
+
+bool IsMpmdModule(ModuleOp module) { return !GetMpmdFunctions(module).empty(); }
+
+bool IsMpmdFunction(FuncOp func_op) { return func_op->hasAttr(kTopologyAttr); }
+
+bool IsSpmdFunction(FuncOp func_op) { return func_op->hasAttr(kMeshShapeAttr); }
+
+bool IsDistributedFunction(FuncOp func_op) {
+  return IsSpmdFunction(func_op) || IsMpmdFunction(func_op);
+}
+
+bool IsEntryPointFunction(FuncOp func_op) {
+  return IsDistributedFunction(func_op) && func_op.isPublic();
+}
+
+TopologyAttr GetTopology(FuncOp func_op) {
+  SDY_CHECK(IsMpmdFunction(func_op));
+  return cast<TopologyAttr>(func_op->getAttr(kTopologyAttr));
+}
+
+TopologyAttr GetTopology(ModuleOp module_op) {
+  SmallVector<FuncOp> mpmd_funcs = GetMpmdFunctions(module_op);
+  SDY_CHECK(!mpmd_funcs.empty());
+  return GetTopology(mpmd_funcs.front());
+}
+
+namespace {
+
+void SetTopologyImpl(ArrayRef<NamedMeshAttr> meshes, FuncOp func) {
+  // Make sure the MPMD dialect is loaded before we insert an MPMD attribute.
+  func.getContext()->loadDialect<MpmdDialect>();
+  func->setAttr(kTopologyAttr, TopologyAttr::get(func.getContext(), meshes));
+}
+
+}  // namespace
+
+void SetTopology(
+    const std::vector<std::pair<std::string, FlatMesh>>& topology_shape,
+    Operation* op) {
+  MLIRContext* context = op->getContext();
+
+  // Make sure the MPMD dialect is loaded before we create an MPMD attribute.
+  FuncOp func = sdy::getEnclosingOfType<FuncOp>(op);
+  func.getContext()->loadDialect<MpmdDialect>();
+
+  auto make_mesh_axis_attr = [&](const std::pair<std::string, int> p) {
+    return sdy::MeshAxisAttr::get(context, p.first, p.second);
+  };
+  auto make_mesh_attr = [&](const FlatMesh& flat_mesh) {
+    SmallVector<sdy::MeshAxisAttr> attr_axes(flat_mesh.size());
+    llvm::transform(flat_mesh, attr_axes.begin(), make_mesh_axis_attr);
+    return sdy::MeshAttr::get(context, attr_axes);
+  };
+
+  SmallVector<NamedMeshAttr> named_meshes(topology_shape.size());
+  llvm::transform(topology_shape, named_meshes.begin(),
+                  [&](const std::pair<std::string, FlatMesh> name_mesh_pair) {
+                    return NamedMeshAttr::get(
+                        context, name_mesh_pair.first,
+                        make_mesh_attr(name_mesh_pair.second));
+                  });
+  SetTopologyImpl(named_meshes, func);
+}
+
+ArrayRef<NamedMeshAttr> GetTopologyMeshes(FuncOp func_op) {
+  SDY_CHECK(IsMpmdFunction(func_op));
+  ArrayRef<NamedMeshAttr> meshes = GetTopology(func_op).getMeshes();
+  return meshes;
+}
+
+llvm::DenseMap<StringRef, sdy::MeshAttr> GetMeshesByName(
+    ArrayRef<NamedMeshAttr> meshes) {
+  llvm::DenseMap<StringRef, sdy::MeshAttr> meshes_by_name;
+  meshes_by_name.reserve(meshes.size());
+  for (NamedMeshAttr mesh : meshes) {
+    meshes_by_name[mesh.getName()] = mesh.getMesh();
+  }
+  return meshes_by_name;
+}
+
+Type GetLocalTensorTypeFromMeshType(Value value, sdy::MeshAttr mesh_attr) {
+  return cast<MeshTensorType>(value.getType()).getLocalTensorType(mesh_attr);
+}
+
+Type GetGlobalTensorTypeFromMeshType(Value value, sdy::MeshAttr) {
+  return cast<MeshTensorType>(value.getType()).getGlobalTensorType();
+}
+
+TransferOp DynCastIntraMeshTransfer(Operation* op) {
+  if (auto transfer_op = dyn_cast_or_null<TransferOp>(op);
+      transfer_op && transfer_op.isIntraMesh()) {
+    return transfer_op;
+  }
+  return nullptr;
+}
+
+TransferOp DynCastInterMeshTransfer(Operation* op) {
+  if (auto transfer_op = dyn_cast_or_null<TransferOp>(op);
+      transfer_op && transfer_op.isInterMesh()) {
+    return transfer_op;
+  }
+  return nullptr;
+}
+
+bool IsInterMeshTransfer(Operation* op) { return DynCastInterMeshTransfer(op); }
+
+namespace {
+
+// Creates M multiple-source single-target edges, where M is the number of
+// inputs of func_op and of operands of all CallOps in `call_ops`.
+void CreateEdgesForFuncArguments(FuncOp func_op, ArrayRef<CallOp> call_ops,
+                                 SmallVector<MpmdDataflowEdge>& all_edges) {
+  for (BlockArgument arg : func_op.getArguments()) {
+    SmallVector<OpOperand*> sources;
+    sources.reserve(call_ops.size());
+    for (CallOp call_op : call_ops) {
+      OpOperand& operand = call_op->getOpOperand(arg.getArgNumber());
+      sources.push_back(&operand);
+    }
+    all_edges.push_back(
+        MpmdDataflowEdge{std::move(sources), /*targets=*/{arg}});
+  }
+}
+
+// Creates N single-source multiple-target edges, where N is the number of
+// outputs of all CallOps in `call_ops`.
+void CreateEdgesForFuncResults(FuncOp func_op, ArrayRef<CallOp> call_ops,
+                               SmallVector<MpmdDataflowEdge>& all_edges) {
+  auto return_op =
+      cast<func::ReturnOp>(func_op.getBody().front().getTerminator());
+  for (OpOperand& operand : return_op->getOpOperands()) {
+    SmallVector<Value> targets;
+    targets.reserve(call_ops.size());
+    for (CallOp call_op : call_ops) {
+      targets.push_back(call_op.getResult(operand.getOperandNumber()));
+    }
+    all_edges.push_back(MpmdDataflowEdge{/*sources=*/{&operand},
+                                         /*targets=*/std::move(targets)});
+  }
+}
+
+}  // namespace
+
+SmallVector<FuncOp> GetMpmdFunctions(ModuleOp module_op) {
+  SmallVector<FuncOp> funs;
+  for (auto func_op : module_op.getOps<FuncOp>()) {
+    if (IsMpmdFunction(func_op)) {
+      funs.push_back(func_op);
+    }
+  }
+  return funs;
+}
+
+SmallVector<CallOp> GetCallOps(FuncOp func_op) {
+  SmallVector<CallOp> call_ops;
+  StringRef func_name = func_op.getSymName();
+
+  // We walk through the ops before their regions, so we can skip over regions
+  // which aren't relevant. The only regions which are relevant are mpmd funcs
+  // and module ops (because they could contain mpmd funcs).
+  func_op->getParentOfType<ModuleOp>()->walk<WalkOrder::PreOrder>(
+      [&call_ops, func_name](Operation* op) {
+        if (auto call_op = dyn_cast<CallOp>(op);
+            call_op && call_op.getCallee() == func_name &&
+            !call_op->use_empty()) {
+          call_ops.push_back(call_op);
+        } else if (isa<ModuleOp>(op)) {
+          return WalkResult::advance();
+        } else if (auto func = dyn_cast<FuncOp>(op);
+                   func && IsMpmdFunction(func)) {
+          return WalkResult::advance();
+        } else if (dyn_cast<ForOp>(op)) {
+          return WalkResult::advance();
+        }
+        return WalkResult::skip();
+      });
+  return call_ops;
+}
+
+SmallVector<MpmdDataflowEdge> GetMpmdDataflowEdgesForFuncArgs(FuncOp func_op) {
+  // Find all CallOps that refer to this function.
+  SmallVector<CallOp> call_ops = GetCallOps(func_op);
+
+  if (call_ops.empty()) {
+    return {};
+  }
+
+  SmallVector<MpmdDataflowEdge> all_edges;
+  all_edges.reserve(func_op.getNumArguments());
+  CreateEdgesForFuncArguments(func_op, call_ops, all_edges);
+  return all_edges;
+}
+
+SmallVector<MpmdDataflowEdge> GetMpmdDataflowEdgesForFuncResults(
+    FuncOp func_op) {
+  // Find all CallOps that refer to this function.
+  SmallVector<CallOp> call_ops = GetCallOps(func_op);
+
+  if (call_ops.empty()) {
+    return {};
+  }
+
+  SmallVector<MpmdDataflowEdge> all_edges;
+  all_edges.reserve(func_op.getNumResults());
+  CreateEdgesForFuncResults(func_op, call_ops, all_edges);
+  return all_edges;
+}
+
+SmallVector<MpmdDataflowEdge> GetMpmdDataflowEdges(FuncOp func_op) {
+  // Find all CallOps that refer to this function.
+  SmallVector<CallOp> call_ops = GetCallOps(func_op);
+
+  if (call_ops.empty()) {
+    return {};
+  }
+
+  SmallVector<MpmdDataflowEdge> all_edges;
+  all_edges.reserve(func_op.getNumResults() + func_op.getNumArguments());
+  CreateEdgesForFuncResults(func_op, call_ops, all_edges);
+  CreateEdgesForFuncArguments(func_op, call_ops, all_edges);
+  return all_edges;
+}
+
+FragmentOp WrapOpWithFragment(
+    Operation* op, StringRef mesh_name, RewriterBase& rewriter,
+    std::function<bool(OpOperand&)> should_replace_use) {
+  // We set the insertion point right before `op` so assigns of operands will be
+  // in the right place regardless of previous insertion point.
+  rewriter.setInsertionPoint(op);
+
+  llvm::SetVector<Value> operands_and_free_vars(op->operand_begin(),
+                                                op->operand_end());
+  getUsedValuesDefinedAbove(op->getRegions(), operands_and_free_vars);
+
+  MLIRContext* ctx = rewriter.getContext();
+  Location loc = op->getLoc();
+
+  sdy::MeshAttr mesh_attr = GetMeshOrFail(op, mesh_name);
+
+  // Assign all operands and free tensor vars to fully replicated mesh tensors
+  // of the same mesh as `other_mesh_tensor`, which would become the operands to
+  // the fragment op.
+  SmallVector<Value> fragment_operands;
+  fragment_operands.reserve(operands_and_free_vars.size());
+  for (Value value : operands_and_free_vars) {
+    fragment_operands.push_back(
+        rewriter.create<AssignOp>(loc, value, mesh_name, mesh_attr));
+  }
+
+  // The fragment result types are fully replicated mesh tensors of the same
+  // mesh as `other_mesh_tensor`, with a local type corresponding to the
+  // respective result type of `op`.
+  SmallVector<Type> fragment_result_types;
+  fragment_result_types.reserve(op->getNumResults());
+  for (Type result_type : op->getResultTypes()) {
+    auto local_type = cast<RankedTensorType>(result_type);
+    fragment_result_types.push_back(MeshTensorType::getFullyReplicated(
+        ctx, mesh_name, mesh_attr, local_type));
+  }
+
+  FragmentOp fragment_op = FragmentOp::createMeshFragmentWithGlobalBody(
+      loc, /*user_origin=*/{}, mesh_name, fragment_operands,
+      fragment_result_types, rewriter,
+      [&](ArrayRef<Value> args,
+          OpBuilder& block_builder) -> SmallVector<Value> {
+        // Map the original operand or free tensor var to the corresponding
+        // argument.
+        IRMapping mapping;
+        mapping.map(operands_and_free_vars, args);
+        // Clone the original `op` inside the fragment's block using the
+        // populated IRMapping, and return the results of the cloned op.
+        return block_builder.clone(*op, mapping)->getResults();
+      });
+
+  // Unassign all fragment results and replace all uses of `op` with the
+  // corresponding unassign op for which `should_replace_use` returns true.
+  for (auto [original_result, fragment_result] :
+       llvm::zip(op->getResults(), fragment_op.getResults())) {
+    auto unassign_op = rewriter.create<UnassignOp>(loc, fragment_result);
+    rewriter.replaceUsesWithIf(original_result, unassign_op,
+                               should_replace_use);
+  }
+  return fragment_op;
+}
+
+std::vector<int64_t> GetTransposeCounts(FragmentOp fragment) {
+  ArrayAttr origin = fragment.getOrigin();
+  std::vector<int64_t> transpose_counts;
+  for (Attribute origin_attr : origin) {
+    auto user_origin = cast<UserOriginAttr>(origin_attr);
+    transpose_counts.push_back(user_origin.getTransposeCount());
+  }
+  return transpose_counts;
+}
+
+std::optional<int64_t> TryToFindSingleTransposeCount(FragmentOp fragment) {
+  std::vector<int64_t> transpose_counts = GetTransposeCounts(fragment);
+  if (!transpose_counts.empty() &&
+      std::adjacent_find(transpose_counts.begin(), transpose_counts.end(),
+                         std::not_equal_to<>()) == transpose_counts.end()) {
+    return transpose_counts.front();
+  }
+  return std::nullopt;
+}
+
+std::optional<uint32_t> TryToFindCallCounter(FragmentOp fragment) {
+  if (auto count = fragment->getAttrOfType<IntegerAttr>(kCallCounterAttrName)) {
+    SDY_CHECK(count.getType().isUnsignedInteger(32));
+    return count.getUInt();
+  }
+  return std::nullopt;
+}
+
+bool IsExecutedImmediatelyAfter(FragmentOp fragment1, FragmentOp fragment2) {
+  Operation* current = fragment1->getNextNode();
+  while (current) {
+    if (auto fragment = dyn_cast<FragmentOp>(current);
+        fragment && fragment.getMeshName() == fragment1.getMeshName()) {
+      return fragment2 == fragment;
+    }
+    current = current->getNextNode();
+  }
+  return false;
+}
+
+bool HasHomogeneousTopology(FuncOp func) {
+  ArrayRef<NamedMeshAttr> named_meshes = GetTopologyMeshes(func);
+  DenseSet<sdy::MeshAttr> meshes;
+  for (NamedMeshAttr named_mesh : named_meshes) {
+    meshes.insert(named_mesh.getMesh());
+  }
+  return meshes.size() == 1;
+}
+
+ArrayAttr GetFragmentOriginUnion(FragmentOp fragment1, FragmentOp fragment2,
+                                 RewriterBase& rewriter) {
+  std::vector<Attribute> merged_origin(fragment1.getOrigin().begin(),
+                                       fragment1.getOrigin().end());
+  for (auto attr : fragment2.getOrigin()) {
+    if (!llvm::is_contained(fragment1.getOrigin().getValue(), attr)) {
+      merged_origin.push_back(attr);
+    }
+  }
+  return rewriter.getArrayAttr(merged_origin);
+}
+
+bool IsLoweredWithSdy(ModuleOp module) {
+  return module->hasAttr(kIsSdyLowered);
+}
+
+bool IsRemat(mlir::Operation* op) { return op->hasAttr(kRematAttributeName); }
+
+void MarkAsRemat(mlir::Operation* op, RewriterBase& rewriter) {
+  op->setAttr(kRematAttributeName, rewriter.getUnitAttr());
+}
+
+std::pair<StringRef, std::optional<StringRef>>
+TryToExtractMemoryKindFromMeshName(StringRef mesh_name) {
+  std::pair<StringRef, StringRef> mesh_and_memory_kind = mesh_name.split('#');
+  if (mesh_and_memory_kind.second.empty()) {
+    return {mesh_name, std::nullopt};
+  }
+  return mesh_and_memory_kind;
+}
+
+void UpdateValueTypeWithSharding(Value value,
+                                 sdy::TensorShardingAttr sharding) {
+  if (!sharding) {
+    return;
+  }
+  value.setType(
+      cast<MeshTensorType>(value.getType()).replaceSharding(sharding));
+}
+
+std::optional<Location> GetResultInfoLoc(FuncOp func, int64_t result_index) {
+  auto result_info =
+      func.getResultAttrOfType<StringAttr>(result_index, kJaxResultInfoAttr);
+  if (!result_info) {
+    return std::nullopt;
+  }
+  return NameLoc::get(result_info);
+}
+
+FailureOr<sdy::MeshAttr> GetMeshAttr(Operation* op) {
+  if (auto fragmentOp = sdy::getEnclosingOfType<FragmentOp>(op)) {
+    return GetMeshAttr(op, fragmentOp.getMeshName());
+  }
+
+  FuncOp func = sdy::getEnclosingOfType<FuncOp>(op);
+  auto meshAttr = func->getAttrOfType<sdy::MeshAttr>(kMeshShapeAttr);
+  if (!meshAttr) {
+    return op->emitError("Function does not have a ")
+           << kMeshShapeAttr << " attribute: " << func.getSymName();
+  }
+  return meshAttr;
+}
+
+FailureOr<sdy::MeshAttr> GetMeshAttr(Operation* op, StringRef mesh_name) {
+  FuncOp func = sdy::getEnclosingOfType<FuncOp>(op);
+  if (!func->hasAttr(kTopologyAttr)) {
+    return op->emitError("Function does not have a ")
+           << kTopologyAttr << " attribute: " << func.getSymName();
+  }
+  for (NamedMeshAttr named_mesh_attr :
+       cast<TopologyAttr>(func->getAttr(kTopologyAttr)).getMeshes()) {
+    if (named_mesh_attr.getName() == mesh_name) {
+      return named_mesh_attr.getMesh();
+    }
+  }
+  return op->emitError("Topology doesn't have a mesh with name: ") << mesh_name;
+}
+
+sdy::MeshAttr GetMeshOrFail(Operation* op) {
+  FailureOr<sdy::MeshAttr> mesh_attr = GetMeshAttr(op);
+  SDY_CHECK(succeeded(mesh_attr));
+  return *mesh_attr;
+}
+
+sdy::MeshAttr GetMeshOrFail(Operation* op, StringRef mesh_name) {
+  FailureOr<sdy::MeshAttr> mesh_attr = GetMeshAttr(op, mesh_name);
+  SDY_CHECK(succeeded(mesh_attr));
+  return *mesh_attr;
+}
+
+Operation* FindAnnotatedOperation(ModuleOp module, StringRef annotation) {
+  Operation* result = nullptr;
+  module->walk([&](Operation* op) {
+    if (op->hasAttr(annotation)) {
+      result = op;
+    }
+  });
+  return result;
+}
+
+std::optional<ReductionType> GetReductionOpType(Operation* op) {
+  if (!op) {
+    return std::nullopt;
+  }
+  if (isa<stablehlo::AddOp>(op)) {
+    return ReductionType::kAdd;
+  }
+  if (isa<stablehlo::MaxOp>(op)) {
+    return ReductionType::kMax;
+  }
+  if (isa<stablehlo::MinOp>(op)) {
+    return ReductionType::kMin;
+  }
+  if (isa<stablehlo::MulOp>(op)) {
+    return ReductionType::kMul;
+  }
+  if (isa<stablehlo::OrOp>(op)) {
+    return ReductionType::kOr;
+  }
+  if (isa<stablehlo::AndOp>(op)) {
+    return ReductionType::kAnd;
+  }
+  return std::nullopt;
+}
+
+Operation* CreateStablehloReduceOp(ReductionType reduction_type,
+                                   ValueRange values, Location loc,
+                                   OpBuilder& builder) {
+  switch (reduction_type) {
+    case ReductionType::kAdd:
+      return builder.create<stablehlo::AddOp>(loc, values);
+    case ReductionType::kMul:
+      return builder.create<stablehlo::MulOp>(loc, values);
+    case ReductionType::kMax:
+      return builder.create<stablehlo::MaxOp>(loc, values);
+    case ReductionType::kMin:
+      return builder.create<stablehlo::MinOp>(loc, values);
+    case ReductionType::kOr:
+      return builder.create<stablehlo::OrOp>(loc, values);
+    case ReductionType::kAnd:
+      return builder.create<stablehlo::AndOp>(loc, values);
+    case ReductionType::kNone:
+      return nullptr;
+  }
+  llvm_unreachable("unknown ReductionType");
+}
+
+std::optional<ReductionType> ComputeReductionType(Block& block) {
+  Operation* ret = block.getTerminator();
+  if (!ret || block.getNumArguments() != (ret->getNumOperands() * 2)) {
+    return std::nullopt;
+  }
+
+  std::optional<ReductionType> reduction_type;
+  for (OpOperand& ret_operand : ret->getOpOperands()) {
+    Operation* reduction_op = ret_operand.get().getDefiningOp();
+    std::optional<ReductionType> inner_type = GetReductionOpType(reduction_op);
+    if (!inner_type.has_value() ||
+        (reduction_type.has_value() && reduction_type.value() != inner_type)) {
+      return std::nullopt;
+    }
+    reduction_type = inner_type;
+
+    int result_idx = ret_operand.getOperandNumber();
+    BlockArgument arg1 = block.getArgument(result_idx);
+    BlockArgument arg2 = block.getArgument(result_idx + ret->getNumOperands());
+    if (!((reduction_op->getOperand(0) == arg1 &&
+           reduction_op->getOperand(1) == arg2) ||
+          (reduction_op->getOperand(0) == arg2 &&
+           reduction_op->getOperand(1) == arg1))) {
+      return std::nullopt;
+    }
+  }
+
+  return reduction_type;
+}
+
+}  // namespace mlir::mpmd
+
+namespace llvm::cl {
+
+using ::mlir::mpmd::FragmentMergeRule;
+
+template class basic_parser<FragmentMergeRule>;
+
+bool parser<FragmentMergeRule>::parse(Option& opt, StringRef, StringRef arg,
+                                      FragmentMergeRule& value) {
+  // TODO(petebu): implement and align FragmentMergeRule::operator<<.
+  return opt.error("unimplemented parser for FragmentMergeRule");
+}
+
+void parser<FragmentMergeRule>::printOptionDiff(const Option& opt,
+                                                const FragmentMergeRule& value,
+                                                const OptVal& defaultValue,
+                                                size_t globalWidth) const {
+  printOptionName(opt, globalWidth);
+  outs() << "= " << value << "\n";
+}
+
+void parser<FragmentMergeRule>::anchor() {}
+
+}  // namespace llvm::cl
diff --git a/shardy/dialect/mpmd/ir/utils.h b/shardy/dialect/mpmd/ir/utils.h
new file mode 100644
index 0000000..a5702a1
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/utils.h
@@ -0,0 +1,502 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_IR_UTILS_H_
+#define SHARDY_DIALECT_MPMD_IR_UTILS_H_
+
+#include <cstddef>
+#include <cstdint>
+#include <functional>
+#include <optional>
+#include <string>
+#include <tuple>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/DenseMapInfo.h"
+#include "llvm/ADT/Hashing.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Interfaces/CallInterfaces.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+// Globsl sdy mesh name.
+constexpr StringRef kGlobalMeshName = "mesh";
+
+// The function attribute that holds the SPMD mesh.
+constexpr StringRef kMeshShapeAttr = "mesh_shape";
+// The function attribute that holds the MPMD topology.
+constexpr StringRef kTopologyAttr = "topology";
+
+// TODO(b/428336749): Remove these attributes once gspmd path is gone.
+
+// When set in the module, it means that the module fragments have been SDY
+// sharded by Shardy.
+inline constexpr StringRef kIsSdyPartitioned = "mpmd.is_sdy_partitioned";
+
+// When set in the module, it means that the module fragments have been SPMD
+// sharded by GSPMD.
+// NOTE: once set, this attribute needs to be preserved throughout our lowering
+// pipelines, so that it survives lowering to IFRT-IR and can be used to build
+// compiling options.
+inline constexpr StringRef kIsGspmdPartitioned = "mpmd.is_gspmd_partitioned";
+
+// The suffix of the mesh name for a CPU mesh.
+constexpr StringRef kCpuMeshSuffix = "/cpu";
+
+// Memory kind attributes.
+// Attr on func args and results to indicate whether the value lives on host or
+// device. If not present, it means it lives on device.
+inline constexpr StringRef kMemoryKindAttr = "mhlo.memory_kind";
+// Attr value to indicate whether the value is on the host.
+inline constexpr StringRef kMemoryKindPinnedHost = "pinned_host";
+// Attr value to indicate whether the value is on the device.
+inline constexpr StringRef kMemoryKindDevice = "device";
+
+// Layout-related attributes.
+inline constexpr StringRef kLayoutModeAttr = "mhlo.layout_mode";
+// Attr value to use the default compact layout.
+inline constexpr StringRef kLayoutModeDefault = "default";
+// Attr value to let compiler choose layout.
+inline constexpr StringRef kLayoutModeAuto = "auto";
+
+// A module attribute to indicate the module has been lowered with JAX using sdy
+// config. The dialect name prefix is needed.
+inline constexpr StringRef kIsSdyLowered = "mpmd.sdy_lowered";
+
+inline constexpr StringRef kRematAttributeName = "remat";
+
+template <typename... Args>
+std::string StrCat(Args&&... args) {
+  std::string result;
+  llvm::raw_string_ostream os(result);
+
+  // C++17 fold expression
+  (os << ... << std::forward<Args>(args));
+
+  return result;
+}
+
+// TODO(dvytin): define a call count attribute name to be used in fragments.
+
+// Specifies for a distributed tensor how each of its dimensions are sharded.
+//
+// Note that SDY sharding axes are in major-to-minor, but
+// `SpmdTensorShardingSpec` is expected to be minor-to-major.
+//
+// For example, given
+//   sdy.mesh @mesh = <["x"=2, "y"=2, "z"=2]>
+//   sdy.sharding = <@mesh, [{}, {"x", "z"}]>
+// the corresponding `SpmdTensorShardingSpec` would be
+//   [[], [2, 0]]
+using SpmdTensorShardingSpec = std::vector<std::vector<int>>;
+
+// Holds the mesh name, sharding specs, and memory kind of a mesh tensor.
+struct NamedSpmdShardingSpec {
+  std::string mesh_name;
+  SpmdTensorShardingSpec tensor_spec;
+  std::optional<std::string> memory_kind;
+};
+
+// Holds the mesh names and sharding specs of each input and output.
+struct FunctionIOShardingSpecsAndMeshes {
+  std::vector<NamedSpmdShardingSpec> input_specs;
+  std::vector<NamedSpmdShardingSpec> output_specs;
+};
+
+// Describes the origin of a fragment.
+struct FragmentOrigin {
+  std::string computation_name;
+  int64_t transpose_count;
+
+  bool operator==(const FragmentOrigin& other) const {
+    return computation_name == other.computation_name &&
+           transpose_count == other.transpose_count;
+  }
+
+  bool operator!=(const FragmentOrigin& other) const {
+    return !(*this == other);
+  }
+
+  bool operator<(const FragmentOrigin& other) const {
+    return std::tie(computation_name, transpose_count) <
+           std::tie(other.computation_name, other.transpose_count);
+  }
+
+  friend llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                                       const FragmentOrigin& origin) {
+    os << "\"" << origin.computation_name << "\"";
+    if (origin.transpose_count > 0) {
+      os << "(" << origin.transpose_count << ")";
+    }
+    return os;
+  }
+
+  friend llvm::hash_code hash_value(const FragmentOrigin& origin) {
+    return llvm::hash_combine(origin.computation_name, origin.transpose_count);
+  }
+};
+
+// Holds the metadata of a fragment.
+struct FragmentInfo {
+  std::vector<FragmentOrigin> origins;
+  std::optional<int> stage_id;
+  std::optional<int> call_counter;
+
+  bool operator==(const FragmentInfo& other) const {
+    return llvm::equal(origins, other.origins) && stage_id == other.stage_id &&
+           call_counter == other.call_counter;
+  }
+
+  bool operator!=(const FragmentInfo& other) const { return !(*this == other); }
+
+  friend llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                                       const FragmentInfo& info) {
+    os << "FragmentInfo(origins=[";
+    llvm::interleave(info.origins, os, ",");
+    os << "]";
+    if (info.stage_id.has_value()) {
+      os << ",stage=" << info.stage_id.value();
+    }
+    if (info.call_counter.has_value()) {
+      os << ",call_counter=" << info.call_counter.value();
+    }
+    os << ")";
+    return os;
+  }
+};
+
+struct FragmentInfoMapInfo : public DenseMapInfo<FragmentInfo> {
+  static unsigned getHashValue(const FragmentInfo& info) {
+    return llvm::hash_combine(llvm::hash_combine_range(info.origins),
+                              info.stage_id, info.call_counter);
+  }
+  static bool isEqual(const FragmentInfo& lhs, const FragmentInfo& rhs) {
+    return lhs == rhs;
+  }
+
+  static inline FragmentInfo getEmptyKey() {
+    return FragmentInfo{/*origins=*/{},
+                        /*stage_id=*/DenseMapInfo<int>::getEmptyKey(),
+                        /*call_counter=*/DenseMapInfo<int>::getEmptyKey()};
+  }
+
+  static inline FragmentInfo getTombstoneKey() {
+    return FragmentInfo{/*origins=*/{},
+                        /*stage_id=*/DenseMapInfo<int>::getTombstoneKey(),
+                        /*call_counter=*/DenseMapInfo<int>::getTombstoneKey()};
+  }
+};
+
+// Describes a rule to merge fragments. A rule is defined by a list of sources
+// and a target. It is applied to a set of fragments when each one matches one
+// of the source info objects. The result of merging the source fragments is
+// labelled with the target info.
+struct FragmentMergeRule {
+  std::vector<FragmentInfo> sources;
+  FragmentInfo target;
+
+  friend llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                                       const FragmentMergeRule& rule) {
+    os << "FragmentMergeRule(sources=[";
+    llvm::interleave(rule.sources, os, ",");
+    return os << "],target=" << rule.target << ")";
+  }
+};
+
+using FragmentMergeRules = std::vector<FragmentMergeRule>;
+
+// Returns the fragment info of a fragment op.
+FragmentInfo GetFragmentInfo(FragmentOp fragment);
+
+// Sets the fragment info of a fragment op. Overwrites any existing info.
+void SetFragmentInfo(FragmentOp fragment, const FragmentInfo& metadata,
+                     RewriterBase& rewriter);
+
+// Given a function, extracts the mesh names, sharding specs and the memory kind
+// of each input and output. We assume that the function is a valid MPMD
+// program, and therefore we are guaranteed to get the mesh names and sharding
+// specs.
+//
+// If an input or output of the function has both a type with memory kind and an
+// attribute with memory kind, we use the memory kind from the type, even if
+// they are different. If no there's no memory_kind on value or attribute then
+// the memory kind is not set.
+//
+// TODO: b/374994155 - Instead we should drop the memory kind from the
+// attributes and move it to types only. If for some reason that's not possible
+// we should at least be consistent and use the memory kind from the type if
+// then we need at least some verification of their consistency.
+FunctionIOShardingSpecsAndMeshes ExtractFunctionIOShardingSpecsAndMeshes(
+    func::FuncOp func_op);
+
+// Retrieves the function named "main" from the given module, if it exists, and
+// fails otherwise.
+func::FuncOp GetMainFunction(ModuleOp module);
+
+// Returns true iff the module has a function which is annotated with a
+// topology.
+bool IsMpmdModule(ModuleOp module);
+
+// Returns true if the function is annotated with a topology.
+bool IsMpmdFunction(func::FuncOp func_op);
+
+// Returns true if the function is annotated with a mesh.
+bool IsSpmdFunction(func::FuncOp func_op);
+
+// Returns true if the function is either a SPMD or an MPMD function (see
+// above).
+bool IsDistributedFunction(func::FuncOp func_op);
+
+// A function is considered an entry point function if it is distributed and
+// public. Note: this means that other functions which are distributed but
+// not intended to be entry points must be set to private (e.g. MPMD CallOp
+// callees).
+bool IsEntryPointFunction(func::FuncOp func_op);
+
+// Returns the topology attribute of a function.
+// Precondition: `IsMpmdFunction(func_op)` must be true.
+TopologyAttr GetTopology(func::FuncOp func_op);
+
+// Returns the topology attribute of a module.
+// Precondition: `IsMpmdModule(module_op)` must be true.
+TopologyAttr GetTopology(ModuleOp module_op);
+
+// A flat mesh is a vector of (axis_name, axis_size) pairs.
+using FlatMesh = std::vector<std::pair<std::string, int>>;
+
+// Sets the topology used by the function enclosing op (which can be a function
+// itself).
+void SetTopology(
+    const std::vector<std::pair<std::string, FlatMesh>>& topology_shape,
+    Operation* op);
+
+// Returns the meshes of the topology attribute of an mpmd function.
+// Precondition: `IsMpmdFunction()` must be true.
+ArrayRef<NamedMeshAttr> GetTopologyMeshes(func::FuncOp func_op);
+
+// Converts a list of NamedMeshAttr to a name-to-mesh-attr map.
+llvm::DenseMap<StringRef, sdy::MeshAttr> GetMeshesByName(
+    ArrayRef<NamedMeshAttr> meshes);
+
+// Casts the type of `value` into a MeshTensorType and returns its local type.
+Type GetLocalTensorTypeFromMeshType(Value value, sdy::MeshAttr mesh_attr);
+
+// Casts the type of `value` into a MeshTensorType and returns its global type.
+Type GetGlobalTensorTypeFromMeshType(Value value, sdy::MeshAttr mesh_attr);
+
+// If `op` is an intra-mesh TransferOp returns it, otherwise returns nullptr.
+TransferOp DynCastIntraMeshTransfer(Operation* op);
+
+// If `op` is an inter-mesh TransferOp returns it, otherwise returns nullptr.
+TransferOp DynCastInterMeshTransfer(Operation* op);
+
+// Return true if `op` is an inter-mesh TransferOp.
+bool IsInterMeshTransfer(Operation* op);
+
+// Returns a vector with all functions in `module_op` that have a topology.
+SmallVector<func::FuncOp> GetMpmdFunctions(ModuleOp module_op);
+
+inline func::FuncOp GetCalleeFunc(CallOp call_op) {
+  return cast<func::FuncOp>(cast<CallOpInterface>(*call_op).resolveCallable());
+}
+
+// A dataflow edge with multiple sources and multiple targets used when
+// propagating info (e.g., mesh assignment, SPMD sharding propagation, etc)
+// through control-flow like ops (e.g., `mpmd.call` and `mpmd.for` ops).
+//
+// We allow for multiple sources in an edge, whenever we want to guarantee some
+// form of consistency among the different sources (even if they are different
+// values). For example, the first operand of all call_ops to the same function
+// must be assigned to meshes consistently. Similarly for multiple targets.
+struct MpmdDataflowEdge {
+  SmallVector<OpOperand*> sources;
+  SmallVector<Value> targets;
+};
+
+// Returns a vector of dataflow edges that connect block arguments of the
+// function with the operands of its call_ops, or an empty list if the function
+// isn't referenced by a mpmd.call_op.
+SmallVector<MpmdDataflowEdge> GetMpmdDataflowEdgesForFuncArgs(
+    func::FuncOp func_op);
+
+// Returns a vector of dataflow edges that connect operands of the function's
+// return op with the results of its call_ops, or an empty list if the function
+// isn't referenced by a mpmd.call_op.
+SmallVector<MpmdDataflowEdge> GetMpmdDataflowEdgesForFuncResults(
+    func::FuncOp func_op);
+
+// Returns all the call ops that with `func_op` as a target, ignoring unused
+// ones.
+SmallVector<CallOp> GetCallOps(func::FuncOp func_op);
+
+// Returns a vector of dataflow edges that connect block arguments of the
+// function with the operands of its call_ops and operands of the function's
+// return op with the results of its call_ops.
+//
+// Returns an empty list if the function isn't referenced by a
+// mpmd.call_op.
+//
+// See GetMpmdDataflowEdge.FuncOp test in utils_test for an example.
+//
+// Note: we collect edges on func_ops, instead of call_ops, because we need to
+// guarantees assignment/sharding consistency across different call_ops (e.g.,
+// the i-th operand of all call_ops to function `f` must be assigned to the
+// same mesh).
+SmallVector<MpmdDataflowEdge> GetMpmdDataflowEdges(func::FuncOp func_op);
+
+// Replaces the given `op` with a new FragmentOp that is assigned to `mesh_name`
+// and has a clone of `op` in its region whose results are returned.
+//
+// The created FragmentOp will take as operands all operands of `op`, and any
+// free tensor variables' in its regions, after assigning them to the same mesh,
+// and the results of the FragmentOp will be unassigned from the mesh so they
+// can replace the uses of `op`.
+//
+// This method only replaces uses of the original op for which
+// `should_replace_use` returns true.
+FragmentOp WrapOpWithFragment(
+    Operation* op, StringRef mesh_name, RewriterBase& rewriter,
+    std::function<bool(OpOperand&)> should_replace_use = [](OpOperand&) {
+      return true;
+    });
+
+// Returns all the transpose counts of a fragment.
+std::vector<int64_t> GetTransposeCounts(FragmentOp fragment);
+
+// If the fragment has one and only one transpose count, returns that value.
+// Otherwise, returns `nullopt`.
+std::optional<int64_t> TryToFindSingleTransposeCount(FragmentOp fragment);
+
+constexpr StringRef kCallCounterAttrName = "call_counter";
+
+// Returns the call counter of a fragment if defined. Otherwise, returns
+// `nullopt`.
+std::optional<uint32_t> TryToFindCallCounter(FragmentOp fragment);
+
+// Checks if `fragment2` appears immediately after `fragment1` in the program
+// respective to their mesh.
+// Assumes that both fragments are assigned to the same mesh.
+bool IsExecutedImmediatelyAfter(FragmentOp fragment1, FragmentOp fragment2);
+
+// Checks if all the meshes in the topology of `func` are identical to each
+// other.
+bool HasHomogeneousTopology(func::FuncOp func);
+
+// Returns the union of the origins of two different fragments.
+ArrayAttr GetFragmentOriginUnion(FragmentOp fragment1, FragmentOp fragment2,
+                                 RewriterBase& rewriter);
+
+// Returns whether the module has been lowered with sdy config in JAX.
+bool IsLoweredWithSdy(ModuleOp module);
+
+// Checks if an operation is the result of rematerialization (e.g.,
+// created during during `mpmd-loop-remat`).
+bool IsRemat(Operation* op);
+
+// Marks an operation as being the result of rematerialization.
+void MarkAsRemat(Operation* op, RewriterBase& rewriter);
+
+// Extracts the memory kind from the mesh name if it exists (e.g., "mesh#device"
+// or "mesh#pinned_host"), returning a pair with the mesh name and the memory
+// kind, or a pair with the mesh name and nullopt if the latter is not present
+// (e.g., "mesh").
+// Note: we do not check whether the memory kind is valid.
+std::pair<StringRef, std::optional<StringRef>>
+TryToExtractMemoryKindFromMeshName(StringRef mesh_name);
+
+// Updates the type of the value given a sharding, overriding the existing
+// sharding if present.
+void UpdateValueTypeWithSharding(Value value, sdy::TensorShardingAttr sharding);
+
+// Returns the location of the result info attribute of the given result, if
+// present, otherwise returns `nullopt`.
+std::optional<Location> GetResultInfoLoc(func::FuncOp func,
+                                         int64_t result_index);
+
+// Lookup the mesh attribute in a function that contains the operation.
+//
+// Returns an error if `op` isn't a fragment or the enclosing function doesn't
+// have a topology attribute.
+FailureOr<sdy::MeshAttr> GetMeshAttr(Operation* op);
+
+// Lookup the mesh attribute by its name in the topology in a function that
+// contains the operation.
+//
+// Returns an error if the enclosing function doesn't have a topology attribute.
+FailureOr<sdy::MeshAttr> GetMeshAttr(Operation* op, StringRef mesh_name);
+
+// Same as `GetMeshAttr(op)` but hards fail if an error is returned.
+sdy::MeshAttr GetMeshOrFail(Operation* op);
+
+// Same as `GetMeshAttr(op, mesh_name)` but hards fail if an error is returned.
+sdy::MeshAttr GetMeshOrFail(Operation* op, StringRef mesh_name);
+
+// Finds an operation inside `module` that carries an attribute named
+// `annotation`. Returns `nullptr` if such operation does not exist.
+Operation* FindAnnotatedOperation(ModuleOp module, StringRef annotation);
+
+// Checks whether given op is a supported binary reduction op and if so, returns
+// the corresponding type.
+std::optional<ReductionType> GetReductionOpType(Operation* op);
+
+// Creates an stablehlo binary reduction op, given a binary reduction type.
+Operation* CreateStablehloReduceOp(ReductionType reduction_type,
+                                   ValueRange values, Location loc,
+                                   OpBuilder& builder);
+
+// Checks that a closure that is the argument of a reduction computation, such
+// as the one encapsulated in stablehlo::ReduceOp or stablehlo:ScatterOp, is of
+// a specific op type (e.g. stablehlo::AddOp or stablehlo::MaxOp) per result,
+// and returns the corresponding ReductionType.
+std::optional<ReductionType> ComputeReductionType(Block& block);
+
+}  // namespace mlir::mpmd
+
+namespace llvm::cl {
+
+extern template class basic_parser<mlir::mpmd::FragmentMergeRule>;
+
+template <>
+class parser<mlir::mpmd::FragmentMergeRule>
+    : public basic_parser<mlir::mpmd::FragmentMergeRule> {
+ public:
+  parser(Option& opt) : basic_parser(opt) {}
+  bool parse(Option& opt, StringRef argName, StringRef arg,
+             mlir::mpmd::FragmentMergeRule& value);
+  StringRef getValueName() const override { return "fragment-merge-rule"; }
+  void printOptionDiff(const Option& opt,
+                       const mlir::mpmd::FragmentMergeRule& value,
+                       const OptVal& defaultValue, size_t globalWidth) const;
+  void anchor() override;
+};
+
+}  // namespace llvm::cl
+
+#endif  // SHARDY_DIALECT_MPMD_IR_UTILS_H_
diff --git a/shardy/dialect/mpmd/ir/utils_test.cc b/shardy/dialect/mpmd/ir/utils_test.cc
new file mode 100644
index 0000000..3d46435
--- /dev/null
+++ b/shardy/dialect/mpmd/ir/utils_test.cc
@@ -0,0 +1,1034 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/ir/utils.h"
+
+#include <optional>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/ScopedPrinter.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OwningOpRef.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Parser/Parser.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/register.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+#include <gmock/gmock.h>
+#include <gtest/gtest.h>
+
+using ::mlir::func::FuncOp;
+using ::testing::ElementsAre;
+using ::testing::Eq;
+using ::testing::FieldsAre;
+using ::testing::IsEmpty;
+using ::testing::Optional;
+
+namespace mlir::mpmd {
+namespace {
+
+FragmentOrigin MakeFragmentOrigin(const std::string& computation_name,
+                                  int transpose_count) {
+  return {computation_name, transpose_count};
+}
+
+FragmentInfo MakeFragmentInfo(const std::vector<FragmentOrigin>& origins,
+                              std::optional<int> stage_id = std::nullopt,
+                              std::optional<int> call_counter = std::nullopt) {
+  return {origins, stage_id, call_counter};
+}
+
+FragmentMergeRule MakeFragmentMergeRule(
+    const std::vector<FragmentInfo>& sources, const FragmentInfo& target) {
+  return {sources, target};
+}
+
+std::optional<std::string> GetMeshAttrString(Operation* op) {
+  FailureOr<sdy::MeshAttr> mesh_attr = GetMeshAttr(op);
+  if (failed(mesh_attr)) {
+    return std::nullopt;
+  }
+  return llvm::to_string(*mesh_attr);
+}
+
+TEST(FragmentInfo, GetFragmentInfo) {
+  const std::string kProgram = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(123), "f2"(123)]> (%arg0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      return %0 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+  FragmentOp fragment_op = cast<FragmentOp>(*main_func.getOps().begin());
+  FragmentInfo fragment_info = GetFragmentInfo(fragment_op);
+  EXPECT_THAT(fragment_info.origins,
+              ElementsAre(FieldsAre("f1", 123), FieldsAre("f2", 123)));
+  EXPECT_THAT(fragment_info.stage_id, Eq(std::nullopt));
+  EXPECT_THAT(fragment_info.call_counter, Eq(std::nullopt));
+}
+
+TEST(FragmentInfo, SetFragmentInfo) {
+  const std::string kProgram = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(123), "f2"(123)]> (%arg0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      return %0 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+  FragmentOp fragment_op = cast<FragmentOp>(*main_func.getOps().begin());
+
+  IRRewriter rewriter(&context);
+  SetFragmentInfo(fragment_op,
+                  MakeFragmentInfo({MakeFragmentOrigin("f3", 456)},
+                                   /*stage_id=*/1, /*call_counter=*/2),
+                  rewriter);
+  FragmentInfo fragment_info = GetFragmentInfo(fragment_op);
+  EXPECT_THAT(fragment_info.origins, ElementsAre(FieldsAre("f3", 456)));
+  EXPECT_THAT(fragment_info.stage_id, Eq(1));
+  EXPECT_THAT(fragment_info.call_counter, Eq(2));
+
+  SetFragmentInfo(fragment_op,
+                  MakeFragmentInfo({MakeFragmentOrigin("f4", 789)}), rewriter);
+  fragment_info = GetFragmentInfo(fragment_op);
+  EXPECT_THAT(fragment_info.origins, ElementsAre(FieldsAre("f4", 789)));
+  EXPECT_THAT(fragment_info.stage_id, Eq(std::nullopt));
+  EXPECT_THAT(fragment_info.call_counter, Eq(std::nullopt));
+}
+
+TEST(FragmentInfo, PrintFragmentInfo) {
+  FragmentInfo fragment_info = MakeFragmentInfo(
+      {MakeFragmentOrigin("f1", 123), MakeFragmentOrigin("f2", 456)},
+      /*stage_id=*/1, /*call_counter=*/2);
+  std::string str;
+  llvm::raw_string_ostream os(str);
+  os << fragment_info;
+  EXPECT_THAT(str, Eq("FragmentInfo(origins=[\"f1\"(123),\"f2\"(456)],stage=1,"
+                      "call_counter=2)"));
+}
+
+TEST(FragmentMergeRule, PrintFragmentMergeRule) {
+  FragmentMergeRule rule = MakeFragmentMergeRule(
+      {MakeFragmentInfo({MakeFragmentOrigin("f1", 123)}, /*stage_id=*/1),
+       MakeFragmentInfo({MakeFragmentOrigin("f2", 456)}, /*stage_id=*/1)},
+      MakeFragmentInfo(
+          {MakeFragmentOrigin("f1", 123), MakeFragmentOrigin("f2", 456)},
+          /*stage_id=*/1));
+  std::string str;
+  llvm::raw_string_ostream os(str);
+  os << rule;
+  EXPECT_THAT(str, Eq("FragmentMergeRule(sources=["
+                      "FragmentInfo(origins=[\"f1\"(123)],stage=1),"
+                      "FragmentInfo(origins=[\"f2\"(456)],stage=1)],"
+                      "target=FragmentInfo(origins=["
+                      "\"f1\"(123),\"f2\"(456)],stage=1))"));
+}
+
+TEST(ExtractFunctionIOShardingSpecsAndMeshes, FunctionWithSingleTransfer) {
+  const std::string kProgram = R"mlir(
+    func.func @main(%arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+        {mhlo.memory_kind = "arg"})
+        -> (!mpmd.mesh_tensor<"mesh2", tensor<12x16xf32>, sharding=<@mesh, [{"x"}, {?}]>> {mhlo.memory_kind = "res"}) attributes {
+        "topology"=#mpmd.topology<
+          <"mesh1": <["x"=2, "y"=4]>>,
+          <"mesh2": <["x"=2, "y"=4]>>
+        >} {
+      %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>) -> !mpmd.mesh_tensor<"mesh2", tensor<12x16xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+      func.return %0 : !mpmd.mesh_tensor<"mesh2", tensor<12x16xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto func_op = GetMainFunction(*module);
+  SDY_CHECK(func_op);
+
+  FunctionIOShardingSpecsAndMeshes specs_and_mesh =
+      ExtractFunctionIOShardingSpecsAndMeshes(func_op);
+
+  EXPECT_THAT(
+      specs_and_mesh.input_specs,
+      ElementsAre(FieldsAre("mesh1", SpmdTensorShardingSpec{{}, {1}}, "arg")));
+  EXPECT_THAT(
+      specs_and_mesh.output_specs,
+      ElementsAre(FieldsAre("mesh2", SpmdTensorShardingSpec{{0}, {}}, "res")));
+}
+
+TEST(ExtractFunctionIOShardingSpecsAndMeshes, MultipleInputMultipleOutput) {
+  const std::string kProgram = R"mlir(
+    func.func @main(
+      %arg0 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>,
+      %arg1 : !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+    ) -> (!mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>,
+          !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+      attributes {
+        "topology"=#mpmd.topology<
+          <"mesh1": <["x"=2, "y"=4]>>
+        >
+      }
+    {
+      func.return %arg0, %arg1 : !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, sharding=<@mesh, [{?}, {"y"}]>>,
+                                 !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto func_op = GetMainFunction(*module);
+  SDY_CHECK(func_op);
+
+  FunctionIOShardingSpecsAndMeshes specs_and_mesh =
+      ExtractFunctionIOShardingSpecsAndMeshes(func_op);
+  EXPECT_THAT(
+      specs_and_mesh.input_specs,
+      ElementsAre(
+          FieldsAre("mesh1", SpmdTensorShardingSpec{{}, {1}}, std::nullopt),
+          FieldsAre("mesh1", SpmdTensorShardingSpec{{0}, {}}, std::nullopt)));
+  EXPECT_THAT(
+      specs_and_mesh.output_specs,
+      ElementsAre(
+          FieldsAre("mesh1", SpmdTensorShardingSpec{{}, {1}}, std::nullopt),
+          FieldsAre("mesh1", SpmdTensorShardingSpec{{0}, {}}, std::nullopt)));
+}
+
+TEST(ExtractFunctionIOShardingSpecsAndMeshes, IOTypesHaveMemoryKinds) {
+  const std::string kProgram = R"mlir(
+    !input_type = !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, memory_kind="device">
+    !output_type = !mpmd.mesh_tensor<"mesh1", tensor<12x16xf32>, memory_kind="pinned_host">
+    func.func @main(%arg0 : !input_type {mhlo.memory_kind = "will_be_ignored"}) -> !output_type attributes {
+        "topology"=#mpmd.topology<<"mesh1": <["x"=2, "y"=4]>> >} {
+      %0 = mpmd.transfer %arg0 : (!input_type) -> !output_type
+      func.return %0 : !output_type
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto func_op = GetMainFunction(*module);
+
+  FunctionIOShardingSpecsAndMeshes specs_and_mesh =
+      ExtractFunctionIOShardingSpecsAndMeshes(func_op);
+
+  EXPECT_THAT(
+      specs_and_mesh.input_specs,
+      ElementsAre(FieldsAre("mesh1", SpmdTensorShardingSpec{}, "device")));
+
+  EXPECT_THAT(
+      specs_and_mesh.output_specs,
+      ElementsAre(FieldsAre("mesh1", SpmdTensorShardingSpec{}, "pinned_host")));
+}
+
+SmallVector<SmallVector<OpResult>> GetCallOpResults(FuncOp func_op) {
+  SmallVector<SmallVector<OpResult>> produced_values;
+  for (CallOp op : func_op.getBody().getOps<CallOp>()) {
+    produced_values.push_back(llvm::to_vector(op.getResults()));
+  }
+  return produced_values;
+}
+
+TEST(GetMeshAttrTest, NotInTheScopeOfAFragment) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<16xf32>) -> tensor<16xf32> attributes {
+      mesh_shape = #sdy.mesh<["x"=2]>
+    } {
+      %0 = stablehlo.add %arg0, %arg0 {focus} : tensor<16xf32>
+      func.return %0 : tensor<16xf32>
+    })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  Operation* op = FindAnnotatedOperation(*module, "focus");
+  SDY_CHECK(op);
+  EXPECT_THAT(GetMeshAttrString(op), Eq("#sdy.mesh<[\"x\"=2]>"));
+}
+
+TEST(GetMeshAttrTest, OperationMissingMeshShape) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  // This program is missing the mesh_shape attribute.
+  const std::string program = R"mlir(
+  func.func @main(%arg0: tensor<16xf32>) -> tensor<16xf32> attributes {
+  } {
+    %0 = stablehlo.add %arg0, %arg0 {focus} : tensor<16xf32>
+    func.return %0 : tensor<16xf32>
+  })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+
+  Operation* op = FindAnnotatedOperation(*module, "focus");
+  SDY_CHECK(op);
+
+  EXPECT_THAT(GetMeshAttrString(op), Eq(std::nullopt));
+}
+
+TEST(GetMeshAttrTest, ValueNotInTheScopeOfAFragment) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<16xf32>) -> tensor<16xf32> attributes {
+      mesh_shape = #sdy.mesh<["x"=2]>
+    } {
+      %0 = stablehlo.add %arg0, %arg0 {focus} : tensor<16xf32>
+      func.return %0 : tensor<16xf32>
+    })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  Operation* op = FindAnnotatedOperation(*module, "focus");
+  SDY_CHECK(op);
+  EXPECT_THAT(GetMeshAttrString(op), Eq("#sdy.mesh<[\"x\"=2]>"));
+}
+
+TEST(GetMeshAttrTest, NotInTheScopeOfAFragmentWithMeshAndTopology) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<16xf32>) -> tensor<16xf32> attributes {
+      mesh_shape = #sdy.mesh<["x"=2]>,
+      topology = #mpmd.topology<
+          <"mesh1": <["y"=2]>>
+        >
+    } {
+      %0 = stablehlo.add %arg0, %arg0 {focus} : tensor<16xf32>
+      func.return %0 : tensor<16xf32>
+    })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  Operation* op = FindAnnotatedOperation(*module, "focus");
+  SDY_CHECK(op);
+  // Expects the mesh to be the one in mesh_shape.
+  EXPECT_THAT(GetMeshAttrString(op), Eq("#sdy.mesh<[\"x\"=2]>"));
+}
+
+TEST(GetMeshAttrTest, InTheScopeOfAFragment) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string program = R"mlir(
+    func.func @main(%arg0: !mpmd.mesh_tensor<"mesh2", tensor<16xf32>>)
+      -> !mpmd.mesh_tensor<"mesh2", tensor<16xf32>> attributes {
+        topology = #mpmd.topology<
+          <"mesh1": <["x"=2]>>,
+          <"mesh2": <["x"=4]>>,
+          <"mesh3": <["y"=2]>>
+        >
+    } {
+      %1 = mpmd.fragment<mesh="mesh2", origin=["f1"]> (%arg0) (%arg1: tensor<16xf32>) {
+        %0 = stablehlo.add %arg1, %arg1 {focus} : tensor<16xf32>
+        mpmd.return %0 : tensor<16xf32>
+      } : (!mpmd.mesh_tensor<"mesh2", tensor<16xf32>>)
+       -> !mpmd.mesh_tensor<"mesh2", tensor<16xf32>>
+      func.return %1 : !mpmd.mesh_tensor<"mesh2", tensor<16xf32>>
+    })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  Operation* op = FindAnnotatedOperation(*module, "focus");
+  SDY_CHECK(op);
+  EXPECT_THAT(GetMeshAttrString(op), Eq("#sdy.mesh<[\"x\"=4]>"));
+}
+
+// This test is different from InTheScopeOfAFragment because it contains both
+// a topology and a mesh_shape in the function's attributes.
+TEST(GetMeshAttrTest, InTheScopeOfAFragmentWithTopologyAndMesh) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string program = R"mlir(
+    func.func @main(%arg0: !mpmd.mesh_tensor<"mesh1", tensor<16xf32>>)
+      -> !mpmd.mesh_tensor<"mesh1", tensor<16xf32>> attributes {
+        topology = #mpmd.topology<
+          <"mesh1": <["x"=2]>>
+        >,
+        mesh_shape = #sdy.mesh<["y"=4]>
+    } {
+      %1 = mpmd.fragment<mesh="mesh1", origin=["f"]> (%arg0) (%arg1: tensor<16xf32>) {
+        %0 = stablehlo.add %arg1, %arg1 {focus} : tensor<16xf32>
+        mpmd.return %0 : tensor<16xf32>
+      } : (!mpmd.mesh_tensor<"mesh1", tensor<16xf32>>)
+       -> !mpmd.mesh_tensor<"mesh1", tensor<16xf32>>
+      func.return %1 : !mpmd.mesh_tensor<"mesh1", tensor<16xf32>>
+    })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  Operation* op = FindAnnotatedOperation(*module, "focus");
+  SDY_CHECK(op);
+  // Expects the mesh to be the one in topology.
+  EXPECT_THAT(GetMeshAttrString(op), Eq("#sdy.mesh<[\"x\"=2]>"));
+}
+
+TEST(GetMeshAttrTest, OperationMissingMeshShapeAndTopology) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  // This program is missing both the mesh_shape and topology attributes.
+  const std::string program = R"mlir(
+  func.func @main(%arg0: tensor<16xf32>) -> tensor<16xf32> attributes {
+  } {
+    %0 = stablehlo.add %arg0, %arg0 {focus} : tensor<16xf32>
+    func.return %0 : tensor<16xf32>
+  })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+
+  Operation* op = FindAnnotatedOperation(*module, "focus");
+  SDY_CHECK(op);
+
+  EXPECT_THAT(GetMeshAttrString(op), Eq(std::nullopt));
+}
+
+TEST(GetMeshAttrTest, ValueInTheScopeOfAFragment) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string program = R"mlir(
+    func.func @main(%arg0: !mpmd.mesh_tensor<"mesh1", tensor<16xf32>>)
+      -> !mpmd.mesh_tensor<"mesh1", tensor<16xf32>> attributes {
+        topology = #mpmd.topology<
+          <"mesh1": <["x"=2]>>,
+          <"mesh2": <["x"=4]>>,
+          <"mesh3": <["y"=2]>>
+        >
+    } {
+      %1 = mpmd.fragment<mesh="mesh1", origin=["f1"]> (%arg0) (%arg1: tensor<16xf32>) {
+        %0 = stablehlo.add %arg1, %arg1 {focus} : tensor<16xf32>
+        mpmd.return %0 : tensor<16xf32>
+      } : (!mpmd.mesh_tensor<"mesh1", tensor<16xf32>>)
+       -> !mpmd.mesh_tensor<"mesh1", tensor<16xf32>>
+      func.return %1 : !mpmd.mesh_tensor<"mesh1", tensor<16xf32>>
+    })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  Operation* op = FindAnnotatedOperation(*module, "focus");
+  SDY_CHECK(op);
+  EXPECT_THAT(GetMeshAttrString(op), Eq("#sdy.mesh<[\"x\"=2]>"));
+}
+
+TEST(SetTopology, WhenUndefined) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<16xf32>, %arg1: tensor<16xf32>) -> tensor<16xf32> {
+      %1 = stablehlo.add %arg0, %arg0 : tensor<16xf32>
+      func.return %1 : tensor<16xf32>
+    })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  auto main_fn = cast<FuncOp>(module->lookupSymbol("main"));
+  std::vector<std::pair<std::string, FlatMesh>> topology_shape = {
+      {"mesh1", {{"x", 2}, {"y", 4}}},
+      {"mesh2", {{"x", 8}}},
+  };
+  SetTopology(topology_shape, main_fn);
+  SDY_CHECK(main_fn->hasAttr("topology"));
+  auto topology_attr = cast<TopologyAttr>(main_fn->getAttr("topology"));
+
+  // Expected meshes in the topology.
+  std::vector<sdy::MeshAxisAttr> mesh1 = {
+      sdy::MeshAxisAttr::get(&context, "x", 2),
+      sdy::MeshAxisAttr::get(&context, "y", 4)};
+  std::vector<sdy::MeshAxisAttr> mesh2 = {
+      sdy::MeshAxisAttr::get(&context, "x", 8)};
+  EXPECT_THAT(
+      topology_attr.getMeshes(),
+      ElementsAre(NamedMeshAttr::get(&context, "mesh1",
+                                     sdy::MeshAttr::get(&context, mesh1)),
+                  NamedMeshAttr::get(&context, "mesh2",
+                                     sdy::MeshAttr::get(&context, mesh2))));
+}
+
+TEST(SetTopology, WhenDefined) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<16xf32>, %arg1: tensor<16xf32>) -> tensor<16xf32> attributes {
+      topology = #mpmd.topology<<"mesh1": <["a"=4, "b"=2]>>, <"mesh2": <["b"=2]>>>
+    } {
+      %1 = stablehlo.add %arg0, %arg0 : tensor<16xf32>
+      func.return %1 : tensor<16xf32>
+    })mlir";
+
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  auto main_fn = cast<FuncOp>(module->lookupSymbol("main"));
+  std::vector<std::pair<std::string, FlatMesh>> topology_shape = {
+      {"m", {{"x", 8}}}};
+  SetTopology(topology_shape, main_fn);
+  SDY_CHECK(main_fn->hasAttr("topology"));
+  auto topology_attr = cast<TopologyAttr>(main_fn->getAttr("topology"));
+
+  // Expected mesh in the topology.
+  std::vector<sdy::MeshAxisAttr> mesh = {
+      sdy::MeshAxisAttr::get(&context, "x", 8)};
+  EXPECT_THAT(topology_attr.getMeshes(),
+              ElementsAre(NamedMeshAttr::get(
+                  &context, "m", sdy::MeshAttr::get(&context, mesh))));
+}
+
+using Sources = SmallVector<OpOperand*>;
+using Targets = SmallVector<Value>;
+
+TEST(GetMpmdDataflowEdge, FuncOp) {
+  const std::string kProgram = R"mlir(
+    module {
+      func.func public @main(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>, %arg2: tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>) attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>, <"mesh2" : <["x"=1]>>>} {
+        %0:2 = mpmd.call @f(%arg0, %arg1, %arg2) : (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+        %1:2 = mpmd.call @f(%arg0, %arg1, %arg2) : (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+        return %0#0, %1#1 : tensor<3x5xf32>, tensor<3x5xf32>
+      }
+      func.func private @f(%arg3: tensor<3x5xf32>, %arg4: tensor<3x5xf32>, %arg5: tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>) attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>, <"mesh2" : <["x"=1]>>>} {
+        %2 = stablehlo.add %arg3, %arg4 : tensor<3x5xf32>
+        %3 = stablehlo.add %2, %arg5 : tensor<3x5xf32>
+        return %2, %3 : tensor<3x5xf32>, tensor<3x5xf32>
+      }
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+
+  EXPECT_THAT(GetMpmdDataflowEdges(main_func), IsEmpty());
+
+  FuncOp private_func = dyn_cast_or_null<FuncOp>(module->lookupSymbol("f"));
+  SDY_CHECK(private_func);
+
+  auto return_op =
+      cast<func::ReturnOp>(private_func.getBody().front().getTerminator());
+  SmallVector<SmallVector<OpResult>> call_results = GetCallOpResults(main_func);
+  auto call_ops = llvm::to_vector(main_func.getBody().getOps<CallOp>());
+  CallOp c0 = call_ops[0];
+  CallOp c1 = call_ops[1];
+
+  EXPECT_THAT(GetMpmdDataflowEdges(private_func),
+              ElementsAre(
+                  // {%2} -> {%0#0, %1#0}
+                  FieldsAre(Sources{&return_op->getOpOperand(0)},
+                            Targets{call_results[0][0], call_results[1][0]}),
+                  // {%3} -> {%0#1, %1#1}
+                  FieldsAre(Sources{&return_op->getOpOperand(1)},
+                            Targets{call_results[0][1], call_results[1][1]}),
+                  // {%arg0 use 0, %arg0 use 1} -> {%arg3}
+                  FieldsAre(Sources{&c0->getOpOperand(0), &c1->getOpOperand(0)},
+                            Targets{private_func.getArgument(0)}),
+                  // {%arg1 use 0, %arg1 use 1} -> {%arg4}
+                  FieldsAre(Sources{&c0->getOpOperand(1), &c1->getOpOperand(1)},
+                            Targets{private_func.getArgument(1)}),
+                  // {%arg2 use 0, %arg2 use 1} -> {%arg5}
+                  FieldsAre(Sources{&c0->getOpOperand(2), &c1->getOpOperand(2)},
+                            Targets{private_func.getArgument(2)})));
+}
+
+TEST(TryToFindSingleTransposeCount, NoTransposeCount) {
+  const std::string kProgram = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      return %0 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+  FragmentOp fragment_op = cast<FragmentOp>(*main_func.getOps().begin());
+  EXPECT_THAT(TryToFindSingleTransposeCount(fragment_op), Eq(std::nullopt));
+}
+
+TEST(TryToFindSingleTransposeCount, OneTransposeCount) {
+  const std::string kProgram = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(123)]> (%arg0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      return %0 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+  FragmentOp fragment_op = cast<FragmentOp>(*main_func.getOps().begin());
+  EXPECT_THAT(TryToFindSingleTransposeCount(fragment_op), Optional(123));
+}
+
+TEST(TryToFindSingleTransposeCount, OneTransposeCountInMultipleOrigins) {
+  const std::string kProgram = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(123), "f2"(123)]> (%arg0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      return %0 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+  FragmentOp fragment_op = cast<FragmentOp>(*main_func.getOps().begin());
+  EXPECT_THAT(TryToFindSingleTransposeCount(fragment_op), Optional(123));
+}
+
+TEST(TryToFindSingleTransposeCount,
+     DifferentTransposeCountsShouldReturnNullopt) {
+  const std::string kProgram = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(123), "f2"(321)]> (%arg0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      return %0 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+  FragmentOp fragment_op = cast<FragmentOp>(*main_func.getOps().begin());
+  EXPECT_THAT(TryToFindSingleTransposeCount(fragment_op), Eq(std::nullopt));
+}
+
+TEST(IsExecutedImmediatelyAfter,
+     ShouldReturnTrueIfBackwardIsImmediatelyAfterForwardFragmentInSameMesh) {
+  const char kProgram[] = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      %forward_result = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      %transfer_result = mpmd.transfer %forward_result : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      %backward_result = mpmd.fragment<mesh="m1", origin=[]> (%forward_result) (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+      return %backward_result : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  Region::OpIterator it =
+      func_op.getOps().begin();  // it points to the forward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp fwd_fragment = cast<FragmentOp>(*it);
+  SDY_CHECK(it != func_op.getOps().end());
+  ++it;  // it points to the transfer op.
+  SDY_CHECK(it != func_op.getOps().end());
+  ++it;  // it points to the backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp bwd_fragment = cast<FragmentOp>(*it);
+
+  EXPECT_TRUE(IsExecutedImmediatelyAfter(fwd_fragment, bwd_fragment));
+}
+
+TEST(IsExecutedImmediatelyAfter,
+     ShouldReturnFalseIfAnotherFragmentInSameMeshBetweenTwoFragments) {
+  const char kProgram[] = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      %forward_result = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+       %another_fragment_result = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      %backward_result = mpmd.fragment<mesh="m1", origin=["f2"(1)]> (%forward_result) (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+      return %backward_result : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  Region::OpIterator it =
+      func_op.getOps().begin();  // it points to the forward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp fwd_fragment = cast<FragmentOp>(*it);
+  SDY_CHECK(it != func_op.getOps().end());
+  ++it;  // it points to the other fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  ++it;  // it points to the backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp bwd_fragment = cast<FragmentOp>(*it);
+
+  EXPECT_FALSE(IsExecutedImmediatelyAfter(fwd_fragment, bwd_fragment));
+}
+
+TEST(IsLoweredWithSdy, LoweredWithSdyIfModuleHasSdyLoweredAttr) {
+  const std::string kProgram = R"mlir(
+    module attributes {mpmd.sdy_lowered} {}
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+
+  EXPECT_TRUE(IsLoweredWithSdy(*module));
+}
+
+TEST(IsLoweredWithSdy, NotLoweredWithSdyIfModuleHasNoSdyLoweredAttr) {
+  const std::string kProgram = R"mlir(
+    module {}
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+
+  EXPECT_FALSE(IsLoweredWithSdy(*module));
+}
+
+TEST(IsLoweredWithSdy, NotLoweredWithSdyIfNoModuleAttr) {
+  const std::string kProgram = R"mlir(
+    func.func @main(%arg0: tensor<f32>) -> tensor<f32> {
+      return %arg0 : tensor<f32>
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+
+  EXPECT_FALSE(IsLoweredWithSdy(*module));
+}
+
+TEST(SdyGetSharding, ShouldGetCorrectFragmentArgsSharding) {
+  const std::string kProgram = R"mlir(
+   sdy.mesh @mesh = <["x"=4, "y"=2]>
+   !mesh_tensor_1 = !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>>
+    #topology = #mpmd.topology<<"mesh1" : <["x"=4, "y"=2]>>>
+   func.func public @main(%arg1: !mesh_tensor_1, %arg2: !mesh_tensor_1) -> (!mesh_tensor_1)
+    attributes {topology = #topology} {
+      %0 = mpmd.fragment<mesh="mesh1", origin=["stage1"], in_shardings=[<@mesh, [{"x", ?}, {?}, {?}]>, <@mesh, [{"y", ?}, {?}, {?}]>]> (%arg1, %arg2) (%arg3: tensor<16x10x3xf32>, %arg4: tensor<16x10x3xf32>) {
+       %r = stablehlo.add %arg3, %arg4 : tensor<16x10x3xf32>
+       mpmd.return %r : tensor<16x10x3xf32>
+      } : (!mesh_tensor_1, !mesh_tensor_1) -> !mesh_tensor_1
+      return %0 : !mesh_tensor_1
+  }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+
+  FragmentOp fragment = cast<FragmentOp>(*main_func.getOps().begin());
+  Value first_arg = (*fragment.getBody()->args_begin());
+  Value second_arg = (*(fragment.getBody()->args_rbegin()));
+
+  sdy::TensorShardingAttr sharded_on_x =
+      sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh")
+          .getSharded(0, "x");
+
+  sdy::TensorShardingAttr sharded_on_y =
+      sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh")
+          .getSharded(0, "y");
+
+  EXPECT_EQ(sharded_on_x, sdy::getSharding(first_arg));
+  EXPECT_EQ(sharded_on_y, sdy::getSharding(second_arg));
+}
+
+TEST(SdySetSharding, ShouldSetCorrectFragmentArgsSharding) {
+  const std::string kProgram = R"mlir(
+   sdy.mesh @mesh = <["x"=4, "y"=2]>
+   !mesh_tensor_1 = !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>>
+    #topology = #mpmd.topology<<"mesh1" : <["x"=4, "y"=2]>>>
+   func.func public @main(%arg1: !mesh_tensor_1, %arg2: !mesh_tensor_1) -> (!mesh_tensor_1)
+    attributes {topology = #topology} {
+      %0 = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%arg1, %arg2) (%arg3: tensor<16x10x3xf32>, %arg4: tensor<16x10x3xf32>) {
+       %r = stablehlo.add %arg3, %arg4 : tensor<16x10x3xf32>
+       mpmd.return %r : tensor<16x10x3xf32>
+      } : (!mesh_tensor_1, !mesh_tensor_1) -> !mesh_tensor_1
+      return %0 : !mesh_tensor_1
+  }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+
+  FragmentOp fragment = cast<FragmentOp>(*main_func.getOps().begin());
+  Value first_arg = (*fragment.getBody()->args_begin());
+  Value second_arg = (*(fragment.getBody()->args_rbegin()));
+
+  sdy::TensorShardingAttr sharded_on_x =
+      sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh")
+          .getSharded(0, "x");
+  sdy::TensorShardingAttr sharded_on_y =
+      sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh")
+          .getSharded(0, "y");
+
+  // There should be no sharding before setting them.
+  EXPECT_EQ(sdy::TensorShardingAttr(), sdy::getSharding(first_arg));
+  EXPECT_EQ(sdy::TensorShardingAttr(), sdy::getSharding(second_arg));
+
+  // Setting the sharding of the first argument should only set the first and
+  // the second fully open.
+  sdy::setSharding(first_arg, sharded_on_x);
+  EXPECT_EQ(sharded_on_x, sdy::getSharding(first_arg));
+  EXPECT_EQ(sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh"),
+            sdy::getSharding(second_arg));
+
+  // Setting the sharding of the second argument should only set the second and
+  // leave the first unchanged.
+  sdy::setSharding(second_arg, sharded_on_y);
+  EXPECT_EQ(sharded_on_x, sdy::getSharding(first_arg));
+  EXPECT_EQ(sharded_on_y, sdy::getSharding(second_arg));
+
+  // // Overriding existing sharding should be successful.
+  sdy::setSharding(first_arg, sharded_on_y);
+  EXPECT_EQ(sharded_on_y, sdy::getSharding(first_arg));
+}
+
+TEST(SdyGetArgsShardings, ShouldGetShardingsForAllArgs) {
+  const std::string kProgram = R"mlir(
+   sdy.mesh @mesh = <["x"=4, "y"=2]>
+   !mesh_tensor_1 = !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>>
+    #topology = #mpmd.topology<<"mesh1" : <["x"=4, "y"=2]>>>
+   func.func public @main(%arg1: !mesh_tensor_1, %arg2: !mesh_tensor_1) -> (!mesh_tensor_1)
+    attributes {topology = #topology} {
+      %0 = mpmd.fragment<mesh="mesh1", origin=["stage1"], in_shardings=[<@mesh, [{"x", ?}, {?}, {?}]>, <@mesh, [{"y", ?}, {?}, {?}]>]> (%arg1, %arg2) (%arg3: tensor<16x10x3xf32>, %arg4: tensor<16x10x3xf32>) {
+       %r = stablehlo.add %arg3, %arg4 : tensor<16x10x3xf32>
+       mpmd.return %r : tensor<16x10x3xf32>
+      } : (!mesh_tensor_1, !mesh_tensor_1) -> !mesh_tensor_1
+      return %0 : !mesh_tensor_1
+  }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+  FragmentOp fragment = cast<FragmentOp>(*main_func.getOps().begin());
+
+  auto expected_shardings = SmallVector<sdy::TensorShardingAttr>{
+      sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh")
+          .getSharded(0, "x"),
+      sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh")
+          .getSharded(0, "y")};
+
+  for (const auto& [i, sharding] :
+       llvm::enumerate(fragment.getBlockArgumentEdgeOwnerShardings())) {
+    EXPECT_EQ(expected_shardings[i], sharding);
+  }
+}
+
+TEST(SdySetArgShardings, ShouldSetShardingsForAllArgs) {
+  const std::string kProgram = R"mlir(
+   sdy.mesh @mesh = <["x"=4, "y"=2]>
+   !mesh_tensor_1 = !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>>
+    #topology = #mpmd.topology<<"mesh1" : <["x"=4, "y"=2]>>>
+   func.func public @main(%arg1: !mesh_tensor_1, %arg2: !mesh_tensor_1) -> (!mesh_tensor_1)
+    attributes {topology = #topology} {
+      %0 = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%arg1, %arg2) (%arg3: tensor<16x10x3xf32>, %arg4: tensor<16x10x3xf32>) {
+       %r = stablehlo.add %arg3, %arg4 : tensor<16x10x3xf32>
+       mpmd.return %r : tensor<16x10x3xf32>
+      } : (!mesh_tensor_1, !mesh_tensor_1) -> !mesh_tensor_1
+      return %0 : !mesh_tensor_1
+  }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  auto main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+
+  FragmentOp fragment = cast<FragmentOp>(*main_func.getOps().begin());
+  sdy::TensorShardingAttr sharded_on_x =
+      sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh")
+          .getSharded(0, "x");
+  sdy::TensorShardingAttr sharded_on_y =
+      sdy::TensorShardingAttr::getFullyOpen(&context, 3, "mesh")
+          .getSharded(1, "y");
+  Value first_arg = (*fragment.getBody()->args_begin());
+  Value second_arg = (*(fragment.getBody()->args_rbegin()));
+
+  fragment.setBlockArgumentEdgeOwnerShardings({sharded_on_x, sharded_on_y});
+
+  EXPECT_EQ(sharded_on_x, sdy::getSharding(first_arg));
+  EXPECT_EQ(sharded_on_y, sdy::getSharding(second_arg));
+}
+
+TEST(GetEdgeOwners, ShouldGetCorrectBlockArgumentAndResultEdgeOwners) {
+  const std::string kProgram = R"mlir(
+   sdy.mesh @mesh = <["x"=4, "y"=2]>
+   !mesh_tensor_1 = !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>>
+    #topology = #mpmd.topology<<"mesh1" : <["x"=4, "y"=2]>>>
+   func.func public @main(%arg1: !mesh_tensor_1, %arg2: !mesh_tensor_1) -> (!mesh_tensor_1)
+    attributes {topology = #topology} {
+      %0 = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%arg1, %arg2) (%arg3: tensor<16x10x3xf32>, %arg4: tensor<16x10x3xf32>) {
+       %r = stablehlo.add %arg3, %arg4 : tensor<16x10x3xf32>
+       mpmd.return %r : tensor<16x10x3xf32>
+      } : (!mesh_tensor_1, !mesh_tensor_1) -> !mesh_tensor_1
+      return %0 : !mesh_tensor_1
+  }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  FuncOp main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+
+  auto fragment = cast<FragmentOp>(*main_func.getOps().begin());
+
+  EXPECT_EQ(fragment.getBlockArgumentEdgeOwners(),
+            fragment.getBody()->getArguments());
+  EXPECT_EQ(fragment.getOpResultEdgeOwners(), fragment.getResults());
+}
+
+TEST(GetEdgeOwner, ShouldGetCorrectOwnerFromSourceOrTarget) {
+  const std::string kProgram = R"mlir(
+   sdy.mesh @mesh = <["x"=4, "y"=2]>
+   !mesh_tensor_1 = !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>>
+    #topology = #mpmd.topology<<"mesh1" : <["x"=4, "y"=2]>>>
+   func.func public @main(%arg1: !mesh_tensor_1, %arg2: !mesh_tensor_1) -> (!mesh_tensor_1)
+    attributes {topology = #topology} {
+      %0 = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%arg1, %arg2) (%arg3: tensor<16x10x3xf32>, %arg4: tensor<16x10x3xf32>) {
+       %r = stablehlo.add %arg3, %arg4 : tensor<16x10x3xf32>
+       mpmd.return %r : tensor<16x10x3xf32>
+      } : (!mesh_tensor_1, !mesh_tensor_1) -> !mesh_tensor_1
+      return %0 : !mesh_tensor_1
+  }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  SDY_CHECK(module);
+  FuncOp main_func = GetMainFunction(*module);
+  SDY_CHECK(main_func);
+
+  auto fragment = cast<FragmentOp>(*main_func.getOps().begin());
+  Value result = fragment.getResult(0);
+
+  EXPECT_EQ(fragment.getEdgeOwnerFromTarget(result), result);
+  for (OpOperand& operand : fragment->getOpOperands()) {
+    EXPECT_EQ(fragment.getEdgeOwnerFromSource(operand),
+              fragment.getBody()->getArgument(operand.getOperandNumber()));
+  }
+}
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/BUILD b/shardy/dialect/mpmd/transforms/BUILD
new file mode 100644
index 0000000..750ca00
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/BUILD
@@ -0,0 +1,19 @@
+# The MPMD dialect passes and pipelines.
+
+# load("@rules_cc//cc:cc_library.bzl", "cc_library")
+
+package(default_visibility = ["//visibility:public"])
+
+cc_library(
+    name = "passes",
+    srcs = ["passes.cc"],
+    hdrs = ["passes.h"],
+    deps = [
+        "//shardy/dialect/mpmd/transforms/common:passes",
+        "//shardy/dialect/mpmd/transforms/export:passes",
+        "//shardy/dialect/mpmd/transforms/import:passes",
+        "//shardy/dialect/mpmd/transforms/optimize:passes",
+        "//shardy/dialect/mpmd/transforms/sharding_propagation:passes",
+        "@llvm-project//mlir:Pass",
+    ],
+)
diff --git a/shardy/dialect/mpmd/transforms/common/BUILD b/shardy/dialect/mpmd/transforms/common/BUILD
new file mode 100644
index 0000000..c1aa58e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/BUILD
@@ -0,0 +1,127 @@
+# The MPMD common library and passes.
+
+# load("@rules_cc//cc:cc_library.bzl", "cc_library")
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library", "td_library")
+
+package(default_visibility = ["//visibility:public"])
+
+td_library(
+    name = "passes_td_files",
+    srcs = [
+        "passes.td",
+    ],
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+gentbl_cc_library(
+    name = "passes_inc",
+    tbl_outs = {
+        "passes.h.inc": [
+            "-gen-pass-decls",
+            "-name=MpmdCommon",
+        ],
+        "g3doc/mpmd_common_passes.md": ["-gen-pass-doc"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "passes.td",
+    deps = [":passes_td_files"],
+)
+
+cc_library(
+    name = "passes",
+    srcs = [
+        "absorb_inferred_fragments.cc",
+        "call_rewrites.cc",
+        "copy_constants.cc",
+        "fragment_dce.cc",
+        "fragment_dedup.cc",
+        "merge_fragments.cc",
+        "merge_transfers.cc",
+        "remove_transfer_cycles.cc",
+        "rule_based_merge.cc",
+        "split_bwd_fragments.cc",
+        "uniquify_function_inputs_outputs.cc",
+        "unroll_for_loops.cc",
+    ],
+    hdrs = [
+        "merge_fragments.h",
+        "passes.h",
+    ],
+    deps = [
+        ":distributed_function_pass",
+        ":passes_inc",
+        ":utils",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/transforms/optimize:utils",
+        "//shardy/dialect/sdy/ir:dialect",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:Analysis",
+        "@llvm-project//mlir:DataLayoutInterfaces",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:InliningUtils",
+        "@llvm-project//mlir:LoopLikeInterface",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Rewrite",
+        "@llvm-project//mlir:SideEffectInterfaces",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+        "@stablehlo//:stablehlo_ops",
+    ],
+)
+
+cc_library(
+    name = "distributed_function_pass",
+    srcs = ["distributed_function_pass.cc"],
+    hdrs = ["distributed_function_pass.h"],
+    deps = [
+        "//shardy/dialect/mpmd/ir:dialect",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:Pass",
+    ],
+)
+
+cc_library(
+    name = "simplify_region_op_base",
+    srcs = ["simplify_region_op_base.cc"],
+    hdrs = ["simplify_region_op_base.h"],
+    deps = [
+        ":utils",
+        "//shardy/common:logging",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:SideEffectInterfaces",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+    ],
+)
+
+cc_library(
+    name = "utils",
+    srcs = ["utils.cc"],
+    hdrs = ["utils.h"],
+    deps = [
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:InliningUtils",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+    ],
+)
+
+cc_library(
+    name = "testing_utils",
+    hdrs = ["testing_utils.h"],
+    deps = [
+        "//shardy/common:logging",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:Support",
+    ],
+)
diff --git a/shardy/dialect/mpmd/transforms/common/absorb_inferred_fragments.cc b/shardy/dialect/mpmd/transforms/common/absorb_inferred_fragments.cc
new file mode 100644
index 0000000..0d3647f
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/absorb_inferred_fragments.cc
@@ -0,0 +1,387 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_ABSORBINFERREDFRAGMENTSPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+// Checks if a fragment is a root fragment, i.e., if it is:
+// - a user fragment, or
+// - not used by any other fragment (e.g. a fragment used by the return op or a
+// transfer only), or
+// - not a user of a value produced by any other fragment (e.g., user of block
+// arguments or transfers).
+bool IsRoot(FragmentOp fragment) {
+  return fragment.isUserFragment() ||
+         llvm::none_of(
+             fragment->getUses(),
+             [](OpOperand& use) { return isa<FragmentOp>(use.getOwner()); }) ||
+         llvm::none_of(fragment->getOperands(), [](Value operand) {
+           return operand.getDefiningOp<FragmentOp>();
+         });
+}
+
+// Returns the closest consumer of `op` (in program order), or nullptr if there
+// is no consumer.
+Operation* ClosestConsumer(Operation* op) {
+  if (op->getUsers().empty()) {
+    return nullptr;
+  }
+  // TODO: b/364281760 - This isBeforeInBlock seems to be a bottleneck in large
+  // programs of many inferred fragments. This needs improvement, e.g., maybe
+  // it's enough to check for dependencies between the ops.
+  return *llvm::min_element(op->getUsers(), [](Operation* op1, Operation* op2) {
+    return op1->isBeforeInBlock(op2);
+  });
+}
+
+// Returns all the ops that produce values used by `op`.
+SmallVector<Operation*> GetProducers(Operation* op) {
+  SetVector<Operation*> producers;
+  for (Value operand : op->getOperands()) {
+    if (Operation* producer = operand.getDefiningOp()) {
+      producers.insert(producer);
+    }
+  }
+  return producers.takeVector();
+}
+
+// Returns the closest producer of `op` (in program order), or nullptr if there
+// is no producer.
+Operation* ClosestProducer(FragmentOp fragment) {
+  SmallVector<Operation*> producers = GetProducers(fragment);
+  if (producers.empty()) {
+    return nullptr;
+  }
+  // TODO: b/364281760 - This isBeforeInBlock seems to be a bottleneck in large
+  // programs of many inferred fragments. This needs improvement, e.g., maybe
+  // it's enough to check for dependencies between the ops.
+  return *llvm::min_element(producers, [](Operation* op1, Operation* op2) {
+    // Being before in block means being further way
+    // from the fragment.
+    return op2->isBeforeInBlock(op1);
+  });
+}
+
+// Returns all the transfers that are consumers of `fragment`.
+SmallVector<TransferOp> GetTransferConsumers(FragmentOp fragment) {
+  SetVector<TransferOp> transfers;
+  for (Operation* user : fragment->getUsers()) {
+    if (auto transfer = dyn_cast<TransferOp>(user)) {
+      transfers.insert(transfer);
+    }
+  }
+  return transfers.takeVector();
+}
+
+// Moves any transfer in `transfers` right before its first consumer, or removes
+// it if it has no consumer.
+void MoveTransfersToConsumerSites(ArrayRef<TransferOp> transfers,
+                                  RewriterBase& rewriter) {
+  for (TransferOp transfer : transfers) {
+    if (Operation* closest_consumer = ClosestConsumer(transfer)) {
+      rewriter.moveOpBefore(transfer, closest_consumer);
+    } else {
+      // If the transfer doesn't have a consumer, we simply remove it.
+      rewriter.eraseOp(transfer);
+    }
+  }
+}
+
+// Returns all the inferred producer fragments that can be merged into the root
+// fragment `root`. This is a set with any inferred fragment that is a producer
+// of `root` and has root as its closest consumer. This function may delay
+// transfers, by moving them next to their consumers, in order to create more
+// merging opportunities.
+SmallVector<FragmentOp> DelayTransfersAndGetMergeableInferredProducers(
+    FragmentOp root, RewriterBase& rewriter) {
+  SmallVector<Operation*> producers = GetProducers(root);
+  SetVector<FragmentOp> mergeable_producers;
+  for (Operation* producer : producers) {
+    if (auto producer_fragment = dyn_cast<FragmentOp>(producer)) {
+      if (producer_fragment.isUserFragment()) {
+        continue;
+      }
+
+      // Try to move any transfers that are consumers of `producer_fragment` out
+      // of the way.
+      SmallVector<TransferOp> transfers =
+          GetTransferConsumers(producer_fragment);
+      MoveTransfersToConsumerSites(transfers, rewriter);
+
+      if (ClosestConsumer(producer_fragment) != root) {
+        continue;
+      }
+      mergeable_producers.insert(producer_fragment);
+    }
+  }
+  return mergeable_producers.takeVector();
+}
+
+// Given a root fragment `R`, finds the closest inferred producer `P` of `R`
+// that can be merged into `R` and merges it.
+//
+// A fragment P can be merged into `R` if:
+// - it is an inferred fragment,
+// - it is the closest producer of `R` in program order, and
+// - it has `R` as its closest consumer.
+//
+// Transfers may be delayed in order to create more merging opportunities.
+//
+// For example, say we had the program:
+//
+//         /---> C
+//       P -------> R
+//     P' --------/
+//
+// where reading left-to-right we get operations executed in order: P', P, C, R.
+//
+// P is the closest producer of R in program order. However, if we merged P into
+// R, we would break op dominance, since C is a consumer of P and it would
+// appear before R (in program order). This merge cannot happen, unless C is a
+// transfer that does not depend on R. In this case, we could move C out of the
+// way, and merge P into R, obtaining:
+//
+//                 /--> C
+//               PR
+//   P' --------/
+//
+// Finally, P' too can be merged into PR, as well as C, if it's a fragment.
+//
+// Note that the location of R is preserved, as well as its attributes.
+//
+// TODO: b/364281760 - For now we don't actually check the dependency between C
+// and R. We simply check if the transfer producer appears before R. Going
+// forward, we should improve this.
+class AbsorbClosestProducerPattern : public OpRewritePattern<FragmentOp> {
+  using OpRewritePattern<FragmentOp>::OpRewritePattern;
+
+ public:
+  LogicalResult matchAndRewrite(FragmentOp op,
+                                PatternRewriter& rewriter) const override {
+    if (!IsRoot(op)) {
+      return failure();
+    }
+
+    SmallVector<FragmentOp> inferred_to_absorb =
+        DelayTransfersAndGetMergeableInferredProducers(op, rewriter);
+    if (inferred_to_absorb.empty()) {
+      return failure();
+    }
+
+    FragmentOp inferred_producer = inferred_to_absorb.front();
+
+    // We now merge `inferred_producer` into `op`, at the location of `op` and
+    // preserving its attributes.
+    DictionaryAttr discardable_attrs = op->getDiscardableAttrDictionary();
+    auto new_fragment = MergeRegionOps(
+        inferred_producer, op, rewriter,
+        /*num_static_args=*/0, /*replace_producer_use_in_consumer_block=*/
+        [](OpOperand&, Value) {
+          SDY_CHECK(false) << "Fragment ops shouldn't have free variables";
+        },
+        op.getOriginAttr(), op.getMeshNameAttr(),
+        /*stage_id=*/op.getStageIdAttr());
+    new_fragment->setDiscardableAttrs(discardable_attrs);
+    return success();
+  }
+};
+
+// Returns all the transfers that are producers of `fragment`.
+SetVector<TransferOp> GetTransferProducers(FragmentOp fragment) {
+  SetVector<TransferOp> transfers;
+  for (Operation* producer : GetProducers(fragment)) {
+    if (auto transfer = dyn_cast<TransferOp>(producer)) {
+      transfers.insert(transfer);
+    }
+  }
+  return transfers;
+}
+
+// Move each transfer next to its producer, or to the top of the block if it is
+// a block argument.
+void MoveTransfersToProducerSites(ArrayRef<TransferOp> transfers,
+                                  RewriterBase& rewriter) {
+  for (TransferOp transfer : transfers) {
+    if (auto arg = dyn_cast<BlockArgument>(transfer.getOperand())) {
+      rewriter.moveOpBefore(transfer, arg.getOwner(), arg.getOwner()->begin());
+    } else {
+      rewriter.moveOpAfter(transfer, transfer.getOperand().getDefiningOp());
+    }
+  }
+}
+
+// Returns all the inferred consumer fragments that can be merged into the root
+// fragment `root`. This is a set with any inferred fragment that is a consumer
+// of `root` and has root as its closest producer.
+SetVector<FragmentOp> EagerlyScheduleTransfersAndGetMergeableInferredConsumers(
+    FragmentOp root, RewriterBase& rewriter) {
+  SetVector<FragmentOp> consumers;
+  for (Operation* user : root->getUsers()) {
+    if (auto consumer_fragment = dyn_cast<FragmentOp>(user)) {
+      if (consumer_fragment.isUserFragment()) {
+        continue;
+      }
+
+      // Try to move any transfers that are producers of `consumer_fragment` out
+      // of the way.
+      SetVector<TransferOp> transfers = GetTransferProducers(consumer_fragment);
+      MoveTransfersToProducerSites(transfers.getArrayRef(), rewriter);
+
+      if (ClosestProducer(consumer_fragment) != root) {
+        continue;
+      }
+      consumers.insert(consumer_fragment);
+    }
+  }
+  return consumers;
+}
+
+// Given a root fragment `R`, finds the closest inferred consumer `C` of `R`
+// that can be merged into `R` and merges it.
+//
+// A fragment C can be merged into `R` if:
+// - it is an inferred fragment,
+// - it is the closest consumer of `R` in program order, and
+// - it has `R` as its closest producer.
+//
+// Transfers may be eagerly scheduled in order to create more merging
+// opportunities.
+//
+// For example, say we had the program:
+//     /------------> C'
+//   R --------> C
+//       P ---/
+// where reading left-to-right we get operations executed in order: R, P, C, C'.
+//
+// C is the closest consumer of R in program order. However, if we merged C into
+// R, we would break op dominance, since P is a producer of C and it would
+// appear after R (in program order). This merge cannot happen, unless P is a
+// transfer that does not depend on R. In this case, we could move it out of the
+// way, and merge C into R, obtaining:
+//        /------------> C'
+//      RC
+//   P -/
+//
+// Finally, C' too can be merged into R.
+//
+// Note that the location of R is preserved, as well as its attributes.
+//
+// TODO: b/364281760 - For now we don't actually check the dependency between X
+// and C. We simply check if the transfer producer appears before C. Going
+// forward, we should improve this.
+class AbsorbClosestConsumerPattern : public OpRewritePattern<FragmentOp> {
+  using OpRewritePattern<FragmentOp>::OpRewritePattern;
+
+ public:
+  LogicalResult matchAndRewrite(FragmentOp op,
+                                PatternRewriter& rewriter) const override {
+    if (!IsRoot(op)) {
+      return failure();
+    }
+
+    SetVector<FragmentOp> mergeable_consumers =
+        EagerlyScheduleTransfersAndGetMergeableInferredConsumers(op, rewriter);
+    if (mergeable_consumers.empty()) {
+      // Nothing to absorb.
+      return failure();
+    }
+
+    FragmentOp inferred_consumer = mergeable_consumers.front();
+    // We now merge `inferred_consumer` into `op`, at the location of `op` and
+    // preserving its attributes.
+    DictionaryAttr discardable_attrs = op->getDiscardableAttrDictionary();
+    Operation* new_fragment_dest = op->getNextNode();
+    if (new_fragment_dest == inferred_consumer) {
+      new_fragment_dest = new_fragment_dest->getNextNode();
+    }
+    FragmentOp new_fragment = MergeRegionOps(
+        op, inferred_consumer, rewriter,
+        /*num_static_args=*/0, /*replace_producer_use_in_consumer_block=*/
+        [](OpOperand&, Value) {
+          SDY_CHECK(false) << "Fragment ops shouldn't have free variables";
+        },
+        op.getOriginAttr(), op.getMeshNameAttr(),
+        /*stage_id=*/op.getStageIdAttr());
+    rewriter.moveOpBefore(new_fragment, new_fragment_dest);
+    new_fragment->setDiscardableAttrs(discardable_attrs);
+    return success();
+  }
+};
+
+class AbsorbInferredFragmentsPass
+    : public impl::AbsorbInferredFragmentsPassBase<
+          AbsorbInferredFragmentsPass> {
+  using AbsorbInferredFragmentsPassBase::AbsorbInferredFragmentsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func) override {
+    if (!IsMpmdFunction(func)) {
+      return;
+    }
+
+    if (!absorbOnEntryPointFunction && IsEntryPointFunction(func)) {
+      return;
+    }
+
+    RewritePatternSet merge_patterns(func.getContext());
+    merge_patterns
+        .add<AbsorbClosestProducerPattern, AbsorbClosestConsumerPattern>(
+            func.getContext());
+    GreedyRewriteConfig config;
+    config.setRegionSimplificationLevel(GreedySimplifyRegionLevel::Disabled)
+        .enableFolding(false);
+    if (failed(
+            applyPatternsGreedily(func, std::move(merge_patterns), config))) {
+      return signalPassFailure();
+    }
+
+    if (IsEntryPointFunction(func)) {
+      func.getBody().walk([](FragmentOp fragment) {
+        if (!fragment.isUserFragment()) {
+          SDY_LOG(WARNING)
+              << "Non entry-point MPMD function includes inferred "
+                 "fragments, which could cause performance issues.";
+        }
+      });
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/call_rewrites.cc b/shardy/dialect/mpmd/transforms/common/call_rewrites.cc
new file mode 100644
index 0000000..d1da0a6
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/call_rewrites.cc
@@ -0,0 +1,295 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <string_view>
+#include <vector>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Block.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Interfaces/CallInterfaces.h"
+#include "mlir/Interfaces/DataLayoutInterfaces.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassRegistry.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/Inliner.h"
+#include "mlir/Transforms/InliningUtils.h"
+#include "mlir/Transforms/Passes.h"
+#include "mlir/Transforms/RegionUtils.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_CALLINLINEPASS
+#define GEN_PASS_DEF_ERASEUNUSEDCALLEEBLOCKARGUMENTSPASS
+#define GEN_PASS_DEF_FROMUNROLLTOCALLCOUNTERPASS
+#define GEN_PASS_DEF_SINKNEGLIGIBLEOPSINTOCALLOPPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+using ::mlir::func::FuncOp;
+
+// TODO(jupvfranco): Consider whether inlining should be recursive to handle
+// nested calls.
+class CallOpInliner : public InlinerInterface {
+ public:
+  explicit CallOpInliner(MLIRContext* context, Operation* inlined_call)
+      : InlinerInterface(context), inlined_call_(inlined_call) {}
+
+ private:
+  // Iterates over all inlined blocks and sets attributes passed as input to
+  // this inliner.
+  void processInlinedBlocks(
+      iterator_range<Region::iterator> inlined_blocks) final {
+    for (Block& block : inlined_blocks) {
+      block.walk([&](Operation* op) {
+        if (isa<FragmentCallOp, FragmentOp>(op)) {
+          CopyAttributes(inlined_call_, op, /*elided_attrs_set=*/{"callee"});
+        }
+      });
+    }
+  }
+
+  // mpmd.call_ops are inlinable when using this inliner. Because we
+  // define this function, the one in the mpmd::MpmdDialectInlinerInterface
+  // won't be applied.
+  // NOTE: this is enough because we only used this InlinerInterface for
+  // mpmd.call ops. In fact, returning `true` would suffice here. We
+  // check if the `call` op is a CallOp for readability and to be more future
+  // proof.
+  bool isLegalToInline(Operation* call, Operation* callable,
+                       bool wouldBeCloned) const final {
+    return isa<CallOp>(call);
+  }
+
+  Operation* inlined_call_;
+};
+
+class CallInlinePass : public impl::CallInlinePassBase<CallInlinePass> {
+  using CallInlinePassBase::CallInlinePassBase;
+
+ private:
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+    module_op.walk([](CallOp call_op) {
+      CallOpInliner inliner(call_op->getContext(), call_op);
+      InlinerConfig config;
+      auto call_op_interface = cast<CallOpInterface>(*call_op);
+      FuncOp callable = cast<FuncOp>(call_op_interface.resolveCallable());
+      auto res = inlineCall(inliner, config.getCloneCallback(),
+                            call_op_interface, callable, &callable.getRegion(),
+                            /*shouldCloneInlinedRegion =*/true);
+      SDY_CHECK(res.succeeded())
+          << "Failed to inline " << std::string_view(callable.getSymName());
+      call_op->erase();
+    });
+  }
+};
+
+}  // namespace
+
+void AddCallInliningRelatedPasses(OpPassManager& pm) {
+  pm.addPass(createCallInlinePass());
+  // TODO(jupvfranco): Investigate whether a CSE here to remove duplication of
+  // sinked ops is a good idea.
+  // Remove any private function that is no longer needed.
+  pm.addPass(createSymbolDCEPass());
+}
+
+namespace {
+
+class SinkNegligibleOpsIntoCallOpPass
+    : public impl::SinkNegligibleOpsIntoCallOpPassBase<
+          SinkNegligibleOpsIntoCallOpPass> {
+  using SinkNegligibleOpsIntoCallOpPassBase::
+      SinkNegligibleOpsIntoCallOpPassBase;
+
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+    for (FuncOp func_op : GetMpmdFunctions(module_op)) {
+      runOnFunc(func_op);
+    }
+  }
+
+  void runOnFunc(FuncOp func_op) {
+    BitVector erase_arguments(func_op.getNumArguments());
+    OpBuilder block_builder = OpBuilder::atBlockBegin(&func_op.front());
+    for (const MpmdDataflowEdge& edge :
+         GetMpmdDataflowEdgesForFuncArgs(func_op)) {
+      OpOperand* first_operand = edge.sources.front();
+      if (!llvm::all_of(edge.sources, [first_operand](OpOperand* operand) {
+            return operand->get() == first_operand->get();
+          })) {
+        continue;
+      }
+      // We are guaranteed a single edge target (`arg`) in this case.
+      BlockArgument arg = cast<BlockArgument>(edge.targets.front());
+      // We only allow sinking of ops without operands, assuming that these will
+      // be negligible, as this may duplicate such ops.
+      if (Operation* operand_producer = first_operand->get().getDefiningOp();
+          operand_producer && operand_producer->getNumOperands() == 0 &&
+          operand_producer->getNumResults() == 1) {
+        Operation* cloned = Clone(block_builder, *operand_producer, {});
+        arg.replaceAllUsesWith(cloned->getResult(0));
+        erase_arguments.set(arg.getArgNumber());
+      }
+    }
+    if (erase_arguments.none()) {
+      return;
+    }
+    DenseSet<FuncOp> caller_functions;
+    (void)func_op.eraseArguments(erase_arguments);
+    for (CallOp call : GetCallOps(func_op)) {
+      call->eraseOperands(erase_arguments);
+      caller_functions.insert(call->getParentOfType<FuncOp>());
+    }
+    IRRewriter rewriter(func_op.getContext());
+    for (FuncOp caller_func : caller_functions) {
+      (void)simplifyRegions(rewriter, caller_func.getRegion());
+    }
+  }
+};
+
+class FromUnrollToCallCounterPass
+    : public impl::FromUnrollToCallCounterPassBase<
+          FromUnrollToCallCounterPass> {
+  using FromUnrollToCallCounterPassBase::FromUnrollToCallCounterPassBase;
+
+  void runOnFunc(func::FuncOp func_op) override {
+    func_op->walk([](CallOp call_op) {
+      if (auto unroll_counter =
+              call_op->getAttrOfType<IntegerAttr>(kUnrollCounterAttrName)) {
+        call_op->setAttr(kCallCounterAttrName, unroll_counter);
+        call_op->removeAttr(kUnrollCounterAttrName);
+      }
+    });
+  }
+};
+
+class EraseUnusedCalleeBlockArgumentsPass
+    : public impl::EraseUnusedCalleeBlockArgumentsPassBase<
+          EraseUnusedCalleeBlockArgumentsPass> {
+  using EraseUnusedCalleeBlockArgumentsPassBase::
+      EraseUnusedCalleeBlockArgumentsPassBase;
+
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+    for (FuncOp func_op : GetMpmdFunctions(module_op)) {
+      if (!IsEntryPointFunction(func_op)) {
+        runOnFunc(func_op);
+      }
+    }
+  }
+
+  void runOnFunc(FuncOp func_op) {
+    SmallVector<CallOp> call_ops = GetCallOps(func_op);
+    if (call_ops.empty()) {
+      return;
+    }
+    IRRewriter rewriter(func_op.getContext());
+    BitVector erase_arguments(func_op.getNumArguments());
+    BitVector erase_results(func_op.getNumResults());
+    // Maps the index of the call_op result to the index of the call_op operand,
+    // with which we will replace the result.
+    DenseMap<int, int> replacement_map;
+    Operation* terminator = func_op.getBody().front().getTerminator();
+    for (BlockArgument arg : func_op.getArguments()) {
+      if (HasOtherUsersExcept(arg, terminator)) {
+        // If the argument is used by any op other than the terminator, then it
+        // is needed by a computation and we cannot erase it from the function.
+        continue;
+      }
+      // At this point, we know that the arg is used by the terminator and only
+      // by the terminator.
+      erase_arguments.set(arg.getArgNumber());
+      for (OpOperand& use : arg.getUses()) {
+        erase_results.set(use.getOperandNumber());
+        replacement_map[use.getOperandNumber()] = arg.getArgNumber();
+      }
+    }
+
+    // If no argument is erased, then there's no work to do.
+    if (erase_arguments.none()) {
+      return;
+    }
+
+    // Replace any call op result that needs replacement.
+    for (CallOp call_op : call_ops) {
+      for (auto [result_index, operand_index] : replacement_map) {
+        rewriter.replaceAllUsesWith(call_op.getResult(result_index),
+                                    call_op.getOperand(operand_index));
+      }
+    }
+
+    if (erase_results.all()) {
+      // Now that we replaced the call_op results, if they have to be removed,
+      // then we know they are completely unused.
+      for (CallOp call_op : call_ops) {
+        rewriter.eraseOp(call_op);
+      }
+      rewriter.eraseOp(func_op);
+      return;
+    }
+
+    // Erase the arguments and results from the function itself.
+    if (erase_results.any()) {
+      (void)func_op.eraseResults(erase_results);
+      terminator->eraseOperands(erase_results);
+    }
+    // The arguments must be erased after the return's operands, so that they
+    // have no uses.
+    (void)func_op.eraseArguments(erase_arguments);
+    UpdateFunctionType(func_op);
+
+    // Flip the bits, so it tells us which results to keep.
+    BitVector& kept_results = erase_results.flip();
+    // Erase the operands and results from the call-ops.
+    for (CallOp call_op : call_ops) {
+      call_op->eraseOperands(erase_arguments);
+      std::vector<Type> result_types;
+      for (unsigned int index : kept_results.set_bits()) {
+        result_types.push_back(call_op->getResult(index).getType());
+      }
+      // Alas, we cannot directly erase results of an op, so we need to create
+      // a new call op, and use it to replace the old one.
+      rewriter.setInsertionPoint(call_op);
+      auto new_call_op = rewriter.create<CallOp>(call_op.getLoc(), result_types,
+                                                 call_op->getOperands(),
+                                                 call_op.getCalleeAttr());
+      new_call_op->setDiscardableAttrs(call_op->getDiscardableAttrDictionary());
+      for (auto [new_result, old_result_index] :
+           llvm::zip_equal(new_call_op.getResults(), kept_results.set_bits())) {
+        rewriter.replaceAllUsesWith(call_op->getResult(old_result_index),
+                                    new_result);
+      }
+      rewriter.eraseOp(call_op);
+    }
+  }
+};
+}  // namespace
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/copy_constants.cc b/shardy/dialect/mpmd/transforms/common/copy_constants.cc
new file mode 100644
index 0000000..90e898e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/copy_constants.cc
@@ -0,0 +1,85 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_COPYCONSTANTSPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+// Finds the HLO producer of `mesh_tensor`. I.e., given a tensor that lives at
+// the top level of the function (e.g., produced/consumed by a fragment), find
+// its producer, which is nested in a fragment.
+// TODO: jupvfranco - consider adapting this code to go through control-flow
+// ops.
+Operation* FindHloProducer(Value mesh_tensor) {
+  auto op_result = dyn_cast_or_null<OpResult>(mesh_tensor);
+  if (!op_result) {
+    return nullptr;
+  }
+  Operation* mpmd_producer = op_result.getOwner();
+
+  if (auto transfer = dyn_cast_or_null<TransferOp>(mpmd_producer)) {
+    return FindHloProducer(transfer.getOperand());
+  }
+
+  if (auto fragment = dyn_cast_or_null<FragmentOp>(mpmd_producer)) {
+    return fragment.getBody()
+        ->getTerminator()
+        ->getOperand(op_result.getResultNumber())
+        .getDefiningOp();
+  }
+
+  return nullptr;
+}
+
+class CopyConstantsPass
+    : public impl::CopyConstantsPassBase<CopyConstantsPass> {
+  using CopyConstantsPassBase::CopyConstantsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    IRRewriter rewriter(func_op.getContext());
+    Block& block = func_op.getBody().front();
+    for (FragmentOp fragment : llvm::reverse(block.getOps<FragmentOp>())) {
+      for (OpOperand& operand : fragment->getOpOperands()) {
+        if (auto hlo_producer = FindHloProducer(operand.get());
+            hlo_producer && hlo_producer->hasTrait<OpTrait::ConstantLike>()) {
+          rewriter.setInsertionPoint(fragment.getBody(),
+                                     fragment.getBody()->begin());
+          rewriter.replaceAllUsesWith(
+              fragment.getBody()->getArgument(operand.getOperandNumber()),
+              rewriter.clone(*hlo_producer)->getResult(0));
+        }
+      }
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/distributed_function_pass.cc b/shardy/dialect/mpmd/transforms/common/distributed_function_pass.cc
new file mode 100644
index 0000000..202a8de
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/distributed_function_pass.cc
@@ -0,0 +1,32 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/common/distributed_function_pass.h"
+
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+
+namespace mlir::mpmd {
+
+using ::mlir::func::FuncOp;
+
+void DistributedFunctionPass::runOnOperation() {
+  FuncOp func_op = getOperation();
+  if (IsDistributedFunction(func_op)) {
+    runOnFunc(func_op);
+  }
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/distributed_function_pass.h b/shardy/dialect/mpmd/transforms/common/distributed_function_pass.h
new file mode 100644
index 0000000..45c4947
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/distributed_function_pass.h
@@ -0,0 +1,47 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_DISTRIBUTED_FUNCTION_PASS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_DISTRIBUTED_FUNCTION_PASS_H_
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/Pass.h"
+
+namespace mlir::mpmd {
+
+// A base class for an OperationPass that should run only on distributed
+// functions (i.e., functions that have a mesh or topology in its attributes).
+// This is needed to omit any reduction specific functions for all our
+// partitioning and optimization passes.
+// TODO(aswietlik): Consider making this a ModuleOp pass with
+// GetMainFunction(module_op) instead.
+class DistributedFunctionPass : public OperationPass<func::FuncOp> {
+ public:
+  using OperationPass<func::FuncOp>::OperationPass;
+
+  DistributedFunctionPass() = default;
+  DistributedFunctionPass(const DistributedFunctionPass& other) = default;
+
+ protected:
+  // The pass will call this method on the distributed function.
+  virtual void runOnFunc(func::FuncOp func_op) {}
+
+ private:
+  void runOnOperation() final;
+};
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_DISTRIBUTED_FUNCTION_PASS_H_
diff --git a/shardy/dialect/mpmd/transforms/common/fragment_dce.cc b/shardy/dialect/mpmd/transforms/common/fragment_dce.cc
new file mode 100644
index 0000000..35aeb22
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/fragment_dce.cc
@@ -0,0 +1,149 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Iterators.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Interfaces/SideEffectInterfaces.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/RegionUtils.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_FRAGMENTDCEPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+bool EmptyFragment(FragmentOp fragment) {
+  return fragment.getResults().empty() && fragment->getOperands().empty() &&
+         llvm::all_of(fragment.getBody()->getOperations(),
+                      [](const Operation& op) { return isa<ReturnOp>(&op); });
+}
+
+FragmentOp EliminateUnusedResults(FragmentOp fragment, RewriterBase& rewriter) {
+  BitVector unused_results(fragment.getNumResults());
+  for (OpResult result : fragment->getResults()) {
+    if (result.use_empty()) {
+      unused_results.set(result.getResultNumber());
+    }
+  }
+  if (unused_results.none()) {
+    return fragment;
+  }
+
+  // Note: even if unused_results.all(), we may want to keep the fragment, in
+  // case it contains ops with side-effects.
+
+  rewriter.setInsertionPoint(fragment);
+
+  Operation* terminator = fragment->getRegion(0).front().getTerminator();
+  terminator->eraseOperands(unused_results);
+  auto new_fragment = rewriter.create<FragmentOp>(
+      fragment.getLoc(),
+      FilterRange<Type>(/*range=*/fragment.getResultTypes(),
+                        /*erase=*/unused_results),
+      fragment.getOperands(), fragment.getOriginAttr(),
+      fragment.getMeshNameAttr(), fragment.getStageIdAttr());
+  // Copy all attributes except `origin` and `mesh_name`, which were copied
+  // during the creation of the new fragment.
+  CopyAttributes(fragment, new_fragment,
+                 /*elided_attrs_set=*/{"origin", "mesh_name"});
+  new_fragment.getRegion().takeBody(fragment.getRegion());
+
+  BitVector& used_results = unused_results.flip();
+  for (auto [old_result_index, new_result] :
+       llvm::zip(used_results.set_bits(), new_fragment.getResults())) {
+    rewriter.replaceAllUsesWith(fragment->getResult(old_result_index),
+                                new_result);
+  }
+
+  rewriter.eraseOp(fragment);
+  return new_fragment;
+}
+
+// Simplifies the region of the fragment. This will apply several
+// simplifications to the region, such as removing dead code, which can cause
+// block arguments of the fragment to become unused, thus triggering the
+// `EliminateUnusedArgumentsPattern`. This is why we simplify the fragment with
+// a pattern instead of waiting for the pass to apply simplification at the end
+// of all rewrites.
+void SimplifyFragmentRegion(FragmentOp fragment, RewriterBase& rewriter) {
+  (void)simplifyRegions(rewriter, fragment.getRegion());
+}
+
+void EliminateUnusedArguments(FragmentOp fragment, RewriterBase& rewriter) {
+  Block& block = fragment.getRegion().front();
+  BitVector unused_arguments(fragment.getNumOperands());
+  for (BlockArgument arg : block.getArguments()) {
+    if (arg.use_empty()) {
+      unused_arguments.set(arg.getArgNumber());
+    }
+  }
+  if (unused_arguments.none()) {
+    return;
+  }
+
+  block.eraseArguments(unused_arguments);
+  fragment->setOperands(FilterRange<Value>(/*range=*/fragment.getOperands(),
+                                           /*erase=*/unused_arguments));
+}
+
+class FragmentDcePass : public impl::FragmentDcePassBase<FragmentDcePass> {
+  using FragmentDcePassBase::FragmentDcePassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    IRRewriter rewriter(func_op.getContext());
+    // Make sure we process users before producers.
+    func_op.walk<WalkOrder::PreOrder, ReverseIterator>(
+        [&rewriter](Operation* op) {
+          if (auto fragment = dyn_cast<FragmentOp>(op)) {
+            FragmentOp new_fragment =
+                EliminateUnusedResults(fragment, rewriter);
+            SimplifyFragmentRegion(new_fragment, rewriter);
+            EliminateUnusedArguments(new_fragment, rewriter);
+            if (EmptyFragment(new_fragment)) {
+              rewriter.eraseOp(new_fragment);
+            }
+            // Fragments cannot nest other fragments, so no need to visit the
+            // fragment's region.
+            return WalkResult::skip();
+          }
+          // Region simplification above cleans up ops inside fragments. Erasure
+          // here cleans up ops outside.
+          if (isa<TransferOp>(op)) {
+            if (op->use_empty() && isPure(op) &&
+                !op->hasTrait<OpTrait::IsTerminator>()) {
+              rewriter.eraseOp(op);
+              return WalkResult::skip();
+            }
+          }
+          return WalkResult::advance();
+        });
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/fragment_dedup.cc b/shardy/dialect/mpmd/transforms/common/fragment_dedup.cc
new file mode 100644
index 0000000..1f65c6e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/fragment_dedup.cc
@@ -0,0 +1,162 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstddef>
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/TypeRange.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/WalkResult.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_FRAGMENTDEDUPPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+// For each return operand in index `i`, finds its first occurrence
+// setting `first_indices[i]` to the index of the first occurrence. For example,
+// if values is [a, b, a, c, d, a] then the returned first_indices will be [0,
+// 1, 0, 2, 3, 0].
+//
+// Two return operands are considered duplicates if they are the same value and
+// the respective fragment results have the same type.
+//
+// Returns true if there are duplicate values in the fragment's return.
+bool FindDuplicateReturnOperands(FragmentOp fragment,
+                                 SmallVector<unsigned>& first_indices) {
+  bool found_duplicate = false;
+  ValueRange values = fragment.getBody()->getTerminator()->getOperands();
+  TypeRange types = fragment.getResultTypes();
+  DenseMap<std::pair<Value, Type>, size_t> index_of_first_occurrence;
+  for (auto [i, value_and_type] : llvm::enumerate(llvm::zip(values, types))) {
+    auto value_and_type_key = std::make_pair(std::get<0>(value_and_type),
+                                             std::get<1>(value_and_type));
+    auto it = index_of_first_occurrence.find(value_and_type_key);
+    if (it == index_of_first_occurrence.end()) {
+      // Found the first occurrence of a value. Insert it into the map and add
+      // its index to first_indices.
+      index_of_first_occurrence.try_emplace(value_and_type_key, i);
+      first_indices.push_back(i);
+    } else {
+      // Found a duplicate value. Add the index of its first occurrence to
+      // first_indices.
+      found_duplicate = true;
+      first_indices.push_back(it->second);
+    }
+  }
+  return found_duplicate;
+}
+
+// Erases any duplicate results.
+void RemoveDuplicatedResults(FragmentOp fragment, RewriterBase& rewriter) {
+  SmallVector<unsigned> index_of_first_occurrence;
+  index_of_first_occurrence.reserve(fragment.getNumResults());
+  if (!FindDuplicateReturnOperands(fragment, index_of_first_occurrence)) {
+    return;
+  }
+
+  for (OpResult result : fragment.getResults()) {
+    unsigned index = result.getResultNumber();
+    unsigned index_to_replace = index_of_first_occurrence[index];
+    if (index != index_to_replace) {
+      rewriter.replaceAllUsesWith(result, fragment.getResult(index_to_replace));
+    }
+  }
+}
+
+// For each value `fragment.getOperands()[i]`, finds its first occurrence
+// setting `first_indices[i]` to the index of the first occurrence. For example,
+// if the operands are [a, b, a, c, d, a] then the returned first_indices will
+// be [0, 1, 0, 2, 3, 0].
+//
+// The main difference from the logic in `FindDuplicateReturnOperands` is that
+// this function does not take types into account: if a value is passed as
+// operand to the fragment twice, then we are guaranteed that there two operand
+// types are the same.
+//
+// Returns true if there are duplicate values in `values`.
+bool FindDuplicateArguments(FragmentOp fragment,
+                            SmallVector<unsigned>& first_indices) {
+  bool found_duplicate = false;
+  DenseMap<Value, size_t> index_of_first_occurrence;
+  for (auto [i, value] : llvm::enumerate(fragment.getOperands())) {
+    auto it = index_of_first_occurrence.find(value);
+    if (it == index_of_first_occurrence.end()) {
+      // Found the first occurrence of a value. Insert it into the map and add
+      // its index to first_indices.
+      index_of_first_occurrence.insert({value, i});
+      first_indices.push_back(i);
+    } else {
+      // Found a duplicate value. Add the index of its first occurrence to
+      // first_indices.
+      found_duplicate = true;
+      first_indices.push_back(it->second);
+    }
+  }
+  return found_duplicate;
+}
+
+// Erases any duplicated operands.
+void RemoveDuplicatedOperands(FragmentOp fragment, RewriterBase& rewriter) {
+  Block& block = fragment.getRegion().front();
+
+  SmallVector<unsigned> index_of_first_occurrence;
+  index_of_first_occurrence.reserve(fragment.getNumOperands());
+  if (!FindDuplicateArguments(fragment, index_of_first_occurrence)) {
+    return;
+  }
+
+  for (OpOperand& operand : fragment->getOpOperands()) {
+    unsigned operand_index = operand.getOperandNumber();
+    unsigned index_to_replace = index_of_first_occurrence[operand_index];
+    if (operand_index != index_to_replace) {
+      rewriter.replaceAllUsesWith(block.getArgument(operand_index),
+                                  block.getArgument(index_to_replace));
+    }
+  }
+}
+
+// This pass deduplicates operands and results of fragments.
+class FragmentDedupPass
+    : public impl::FragmentDedupPassBase<FragmentDedupPass> {
+  using FragmentDedupPassBase::FragmentDedupPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    IRRewriter rewriter(func_op.getContext());
+    func_op.walk<WalkOrder::PreOrder>([&rewriter](FragmentOp fragment) {
+      RemoveDuplicatedOperands(fragment, rewriter);
+      RemoveDuplicatedResults(fragment, rewriter);
+      // We know that fragments cannot be nested in fragments, so there's
+      // nothing to visit in the body of `fragment`.
+      return WalkResult::skip();
+    });
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/merge_fragments.cc b/shardy/dialect/mpmd/transforms/common/merge_fragments.cc
new file mode 100644
index 0000000..36ed7bb
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/merge_fragments.cc
@@ -0,0 +1,691 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/common/merge_fragments.h"
+
+#include <algorithm>
+#include <cstdint>
+#include <iterator>
+#include <optional>
+#include <string>
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Iterators.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Interfaces/SideEffectInterfaces.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/WalkResult.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/optimize/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_MERGEINFERREDFRAGMENTSPASS
+#define GEN_PASS_DEF_MERGEFORWARDWITHBACKWARDPASS
+#define GEN_PASS_DEF_MERGEUSERDEFINEDFRAGMENTSINTOSCHEDULINGUNITSPASS
+#define GEN_PASS_DEF_VERIFYSTAGEMERGINGPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+#define DEBUG_TYPE "mpmd-merge-fragments"
+
+namespace {
+
+using ::mlir::func::FuncOp;
+
+bool AreStageIdsConsistent(FragmentOp producer_op, FragmentOp consumer_op) {
+  IntegerAttr producer_stage_id = producer_op.getStageIdAttr();
+  IntegerAttr consumer_stage_id = consumer_op.getStageIdAttr();
+  return !producer_stage_id || !consumer_stage_id ||
+         producer_stage_id == consumer_stage_id;
+}
+
+// Returns the stage_id integer attribute of the resulting merged fragment.
+IntegerAttr GetMergedStageIdAttribute(FragmentOp producer_op,
+                                      FragmentOp consumer_op) {
+  SDY_CHECK(AreStageIdsConsistent(producer_op, consumer_op))
+      << "Merging requires both fragments to have the same stage id.";
+  IntegerAttr producer_stage = producer_op.getStageIdAttr();
+  return producer_stage ? producer_stage : consumer_op.getStageIdAttr();
+}
+
+bool TypeHasOneElement(Type type) {
+  auto tensor_type = dyn_cast<RankedTensorType>(type);
+  if (!tensor_type || !tensor_type.hasStaticShape()) return false;
+  for (int64_t i = 0; i < tensor_type.getRank(); ++i) {
+    if (tensor_type.getDimSize(i) != 1) return false;
+  }
+  return true;
+}
+
+// Returns true if `op` is an inter-mesh TransferOp whose global type has only
+// one element.
+bool IsNonScalarInterMeshTransfer(Operation* op) {
+  TransferOp transfer_op = DynCastInterMeshTransfer(op);
+  return transfer_op &&
+         !TypeHasOneElement(transfer_op.getType().getGlobalTensorType());
+}
+
+// We only clone fragments with at most one non-return op.
+inline const int kInferredFragmentMaxSizeForCloning = 2;
+
+void PrintMergeDebug(FragmentOp producer_op) {
+  LLVM_DEBUG({
+    llvm::dbgs() << "\n\n=== Processing fragment: '"
+                 << PrintOperationForLog(
+                        producer_op,
+                        OpPrintingFlags().assumeVerified().skipRegions())
+                 << "'\n\n";
+  });
+}
+
+LogicalResult mergeFailure(StringRef message, bool log_failure = true) {
+  if (log_failure) {
+    LLVM_DEBUG({ llvm::dbgs() << "Merge failed.\n" << message << "\n"; });
+  }
+  return failure();
+}
+
+FragmentOp FragmentOrNull(FailureOr<FragmentOp> merge_result) {
+  if (succeeded(merge_result)) {
+    LLVM_DEBUG({
+      llvm::dbgs() << "Merge successful.\n"
+                   << (*merge_result)->getParentOfType<FuncOp>() << "\n";
+    });
+    return *merge_result;
+  }
+
+  return nullptr;
+}
+
+// Merges the call counters of the producer and consumer fragments.
+//
+// - If only one fragment has a call counter, we use that.
+// - If both fragments have the same call counter, we use that.
+// - If both fragments have different call counters but exactly one of them is
+// user-defined, then we return the user-defined fragment's call counter.
+// - Otherwise, we return std::nullopt. I.e. this information will be lost.
+std::optional<int> MergeCallCounters(FragmentOp producer_op,
+                                     FragmentOp consumer_op) {
+  std::optional<int> producer_call_count = TryToFindCallCounter(producer_op);
+  std::optional<int> consumer_call_count = TryToFindCallCounter(consumer_op);
+
+  if (producer_call_count && !consumer_call_count) {
+    return producer_call_count;
+  }
+  if (consumer_call_count && !producer_call_count) {
+    return consumer_call_count;
+  }
+
+  if (producer_call_count && consumer_call_count) {
+    if (producer_call_count == consumer_call_count) {
+      return producer_call_count;
+    }
+
+    std::string debug_info;
+    {
+      llvm::raw_string_ostream debug_stream(debug_info);
+      debug_stream << "producer counter=" << *producer_call_count
+                   << " vs consumer counter=" << *consumer_call_count
+                   << ". Producer origin=" << producer_op.getOrigin()
+                   << " Consumer origin=" << consumer_op.getOrigin();
+    }
+
+    // Both are either user-defined or inferred.
+    if (producer_op.isUserFragment() == consumer_op.isUserFragment()) {
+      SDY_LOG(INFO) << "[MpmdMergeFragments] Ignoring call_counter since "
+                       "different - "
+                    << debug_info;
+      return std::nullopt;
+    }
+
+    // Exactly one of the fragments is user-defined.
+    SDY_LOG(INFO) << "[MpmdMergeFragments] Preferring call_counter of "
+                     "user-defined fragment over inferred fragment - "
+                  << debug_info;
+    if (!producer_op.isUserFragment()) {
+      return consumer_call_count;
+    }
+    if (!consumer_op.isUserFragment()) {
+      return producer_call_count;
+    }
+    SDY_CHECK(false);
+  }
+
+  return std::nullopt;
+}
+
+// Returns a list of attributes that must be preserved in the merged fragment.
+// Note: origins are preserved by default and require no extra work.
+SmallVector<std::pair<StringRef, Attribute>> MergedAttributes(
+    FragmentOp producer_op, FragmentOp consumer_op) {
+  SmallVector<std::pair<StringRef, Attribute>> attributes;
+
+  if (std::optional<int> merged_call_count =
+          MergeCallCounters(producer_op, consumer_op)) {
+    IRRewriter rewriter(producer_op.getContext());
+    attributes.emplace_back(kCallCounterAttrName,
+                            rewriter.getUI32IntegerAttr(*merged_call_count));
+  }
+  return attributes;
+}
+
+}  // namespace
+
+FailureOr<FragmentOp> MergeFragmentBasePass::GetMergeCandidate(
+    FragmentOp producer_op, OpOrderMap& order) const {
+  SDY_CHECK(llvm::all_of(producer_op->getUsers(), [&](Operation* user) {
+    return producer_op->getBlock() == user->getBlock();
+  })) << "Expected all users of the producer op to be in the same block";
+
+  SmallVector<FragmentOp> sorted_fragment_users;
+  for (Operation* user : producer_op->getUsers()) {
+    if (auto fragment = dyn_cast<FragmentOp>(user)) {
+      sorted_fragment_users.push_back(fragment);
+    }
+  }
+  std::stable_sort(sorted_fragment_users.begin(), sorted_fragment_users.end(),
+                   [&](FragmentOp a, FragmentOp b) {
+                     return FastIsBeforeInBlock(a, b, order);
+                   });
+
+  if (sorted_fragment_users.empty()) {
+    return mergeFailure("Fragment is not used by any other fragment.");
+  }
+
+  if (AllowMergingWithAnyConsumer()) {
+    // Find the closest user that is mergeable.
+    auto* find_it = llvm::find_if(sorted_fragment_users, [&](FragmentOp user) {
+      return succeeded(AllowMerging(producer_op, user, /*log_failure=*/false));
+    });
+    if (find_it == sorted_fragment_users.end()) {
+      return mergeFailure("Failed to find a fragment user to merge with");
+    }
+    return *find_it;
+  }
+
+  // Merge with the closest user.
+  FragmentOp mergeable_user = *sorted_fragment_users.begin();
+  if (failed(AllowMerging(producer_op, mergeable_user, /*log_failure=*/true))) {
+    return failure();
+  }
+  return mergeable_user;
+}
+
+bool MergeFragmentBasePass::FastIsBeforeInBlock(Operation* op1, Operation* op2,
+                                                OpOrderMap& order) const {
+  auto it1 = order.find(op1);
+  auto it2 = order.find(op2);
+  SDY_CHECK(it1 != order.end() && it2 != order.end())
+      << "Expected both ops to be in the order map";
+  return it1->second < it2->second;
+}
+
+// Returns true if the producer can be merged with the consumer at the
+// position of the producer: i.e., all consumer's operands are produced by
+// the producer or by an earlier op.
+bool MergeFragmentBasePass::CanMergeAtProducer(Operation* producer,
+                                               Operation* consumer,
+                                               OpOrderMap& order) const {
+  return llvm::all_of(consumer->getOpOperands(), [&](OpOperand& operand) {
+    Operation* operand_producer = operand.get().getDefiningOp();
+    return !operand_producer || operand_producer == producer ||
+           FastIsBeforeInBlock(operand_producer, producer, order);
+  });
+}
+
+// Returns true if the producer can be merged with the consumer at the
+// position of the consumer: i.e., no ops in between the consumer and
+// producer uses the producer's results.
+bool MergeFragmentBasePass::CanMergeAtConsumer(Operation* producer,
+                                               Operation* consumer,
+                                               OpOrderMap& order) const {
+  return llvm::none_of(producer->getUsers(), [&](Operation* op) {
+    return FastIsBeforeInBlock(op, consumer, order);
+  });
+}
+
+// Tries to merge the fragment and returns the merged fragment, or an error
+// status if merging isn't possible.
+FailureOr<FragmentOp> MergeFragmentBasePass::MergeFragmentsRewrite(
+    FragmentOp producer_op, RewriterBase& rewriter, OpOrderMap& order) const {
+  FragmentOp mergeable_user;
+  if (FailureOr<FragmentOp> merge_candidate =
+          GetMergeCandidate(producer_op, order);
+      succeeded(merge_candidate)) {
+    mergeable_user = *merge_candidate;
+  } else {
+    return failure();
+  }
+
+  // Find the position to merge the producer and the consumer.
+  // By default, we merge with the consumer. Otherwise check if we can merge
+  // with the producer.
+  bool can_merge_at_consumer = true;
+  if (!CanMergeAtConsumer(producer_op, mergeable_user, order)) {
+    can_merge_at_consumer = false;
+    if (!CanMergeAtProducer(producer_op, mergeable_user, order)) {
+      return mergeFailure(
+          "The consumer and producer can't be merged at any position.");
+    }
+    mergeable_user->moveAfter(producer_op);
+  }
+
+  // Proceeding with actual merging.
+  // NOTE: Merging `producer_op` and `closest_user` may delay transfers
+  // depending on the definition of AllowMerging.
+
+  if (AllowCloningProducerFragment(producer_op)) {
+    rewriter.setInsertionPoint(producer_op);
+    FragmentOp cloned_producer_op =
+        cast<FragmentOp>(rewriter.clone(*producer_op));
+    for (auto [original_result, cloned_result] :
+         llvm::zip(producer_op.getResults(), cloned_producer_op.getResults())) {
+      rewriter.replaceUsesWithIf(
+          original_result, cloned_result, [mergeable_user](OpOperand& operand) {
+            // We also replace uses in any func return ops, so that the
+            // original fragment can be removed if it's completely merged.
+            return operand.getOwner() == mergeable_user ||
+                   isa<func::ReturnOp>(operand.getOwner());
+          });
+    }
+    order[cloned_producer_op] = order[producer_op];
+    producer_op = cloned_producer_op;
+  }
+
+  SmallVector<std::pair<StringRef, Attribute>> merged_attributes =
+      MergedAttributes(producer_op, mergeable_user);
+
+  // Now we can merge `producer_op` with `consumer_op`.
+  FragmentOp merged_fragment = MergeRegionOps(
+      producer_op, mergeable_user, rewriter,
+      /*num_static_args=*/0, /*replace_producer_use_in_consumer_block=*/
+      [](OpOperand&, Value) {
+        SDY_CHECK(false) << "Fragment ops shouldn't have free variables";
+      },
+      GetFragmentOriginUnion(producer_op, mergeable_user, rewriter),
+      producer_op.getMeshNameAttr(),
+      /*stage_id=*/GetMergedStageIdAttribute(producer_op, mergeable_user));
+
+  for (const auto [attr_name, attr] : merged_attributes) {
+    merged_fragment->setAttr(attr_name, attr);
+  }
+
+  // Notice that we merge at the producer by first moving the consumer right
+  // after the producer above, still `can_merge_at_consumer` is with respect
+  // to the original consumer, and hence we update the order map accordingly.
+  order[merged_fragment] =
+      can_merge_at_consumer ? order[mergeable_user] : order[producer_op];
+  return merged_fragment;
+}
+
+// Merges fragments recursively, attempting to clone the producer
+// fragment if possible. We may want to clone to avoid introducing
+// dependencies.
+//
+// Pre-condition: All users of the producer_op have been processed by this
+// rewrite, i.e., we do the rewrite in post-order traversal.
+void MergeFragmentBasePass::MergeFragmentsRecursivelyRewrite(
+    FragmentOp producer_op, RewriterBase& rewriter, OpOrderMap& order) const {
+  bool clone_producer_op = AllowCloningProducerFragment(producer_op);
+  FragmentOp last_merged = producer_op;
+  // Because all users have been processed, we will never need to merge
+  // transitive users. E.g., when processing f1, we won't have a mergeable
+  // chain f1 -> f2 -> f3 because if f2 and f3 could be merged, they would
+  // already be merged. But we still need to do recursive merging for when f1
+  // has multiple users.
+  while (last_merged) {
+    // TODO(b/313631663) - Remove cloning from merging.
+    FragmentOp to_merge = clone_producer_op ? producer_op : last_merged;
+    PrintMergeDebug(to_merge);
+    last_merged =
+        FragmentOrNull(MergeFragmentsRewrite(to_merge, rewriter, order));
+  }
+
+  if (clone_producer_op && producer_op.use_empty() && isPure(producer_op)) {
+    rewriter.eraseOp(producer_op);
+  }
+}
+
+void MergeFragmentBasePass::runOnFunc(FuncOp func_op) {
+  MLIRContext* context = func_op->getContext();
+  IRRewriter rewriter(context);
+
+  OpOrderMap order;
+  // Traversals are cheap but growing the map is expensive, so we pre-grow it.
+  order.reserve(
+      std::distance(func_op.getOps().begin(), func_op.getOps().end()) + 1);
+  for (auto [index, op] : llvm::enumerate(func_op.getOps())) {
+    // NB: we start at 1 to have a way to verify that an op is not in the map
+    // by checking if order[op] == 0.
+    order[&op] = index + 1;
+  }
+
+  // We do a post-order traversal as we want to process all users before the
+  // op itself, for guarantees around unused fragment removal (see below).
+  //
+  // We prove that every mergeable fragment indeed gets merged:
+  // Suppose not, i.e., there's a fragment f1 that can be merged with a
+  // consumer fragment f2, but was not merged. When f1 was visited, we
+  // didn't merge with f2. This means that f2 did not exist at that point
+  // (otherwise we would merge). I.e., f2 is the result of merging some
+  // fragments g1,..., gn, none of which can be merged into f1 and at
+  // least one of which is a consumer of f1. Assume g1 is this consumer
+  // fragment. But whatever makes g1 unmergeable with f1 will also make f2
+  // unmergeable with f1, e.g., having an inter-mesh transfer operand in g1.
+  // Thus such an f2 cannot exist.
+  func_op.walk<WalkOrder::PostOrder, ReverseIterator>([&](FragmentOp fragment) {
+    // We do some simple folding to avoid merging removable fragments.
+    // When we act on a fragment, all its users have been visited
+    // (because of the post-order traversal), so we are guaranteed not
+    // to merge with any removable fragments as they would've already
+    // been removed.
+    if (fragment.use_empty() && isPure(fragment)) {
+      rewriter.eraseOp(fragment);
+    } else {
+      // We need to merge recursively, because a fragment `f1` could be
+      // independently used by multiple other fragments, e.g., `f2(f1)`
+      // and `f3(f1)`. So we need to recursively merge until all
+      // eligible consuming fragments are merged.
+      MergeFragmentsRecursivelyRewrite(fragment, rewriter, order);
+    }
+    return WalkResult::skip();
+  });
+}
+
+namespace {
+
+class MergeInferredFragmentsPass
+    : public impl::MergeInferredFragmentsPassBase<MergeInferredFragmentsPass> {
+  using MergeInferredFragmentsPassBase::MergeInferredFragmentsPassBase;
+
+ protected:
+  LogicalResult AllowMerging(FragmentOp producer_op, FragmentOp consumer_op,
+                             bool log_failure) const final {
+    if (producer_op.isUserFragment() && consumer_op.isUserFragment()) {
+      return mergeFailure("Cannot merge two user fragments", log_failure);
+    }
+
+    if (IsSplitKeepTransferred(producer_op) &&
+        IsSplitDropTransferred(consumer_op)) {
+      return mergeFailure(
+          "Fragments were split, so they should not be merged back.",
+          log_failure);
+    }
+
+    // If the producer or the consumer op are inferred fragments, then we
+    // consider them negligible, i.e., can't delay transfers. Thus, it is safe
+    // to merge the fragments.
+    return success();
+  }
+
+  bool AllowCloningProducerFragment(FragmentOp producer_op) const final {
+    return cloneInferredFragments && !producer_op.isUserFragment() &&
+           isPure(producer_op) &&
+           producer_op.getBody()->getOperations().size() <=
+               kInferredFragmentMaxSizeForCloning &&
+           producer_op->getNumResults() == 1;
+  }
+
+  bool AllowMergingWithAnyConsumer() const final { return mergeAnyConsumer; }
+
+  FailureOr<FragmentOp> GetMergeCandidate(FragmentOp producer_op,
+                                          OpOrderMap& order) const final {
+    if (!mergeSideways) {
+      return MergeFragmentBasePass::GetMergeCandidate(producer_op, order);
+    }
+
+    SDY_CHECK(!cloneInferredFragments && !mergeAnyConsumer)
+        << "merge-sideways cannot be used with clone-inferred-fragments "
+           "or merge-any-consumer";
+
+    Operation* current = producer_op->getNextNode();
+    while (current) {
+      if (auto fragment = dyn_cast<FragmentOp>(current);
+          fragment && fragment.getMeshName() == producer_op.getMeshName()) {
+        current = fragment;
+        break;
+      }
+      if (auto transfer = dyn_cast<TransferOp>(current);
+          transfer &&
+          transfer.getType().getMeshName() == producer_op.getMeshName()) {
+        // If we transfer back into the same mesh, then abort, because the
+        // next node could have used the transferred value.
+        return mergeFailure(
+            "The closest fragment in the same mesh is a transfer. We do not "
+            "merge sideways for simplicity, to avoid having to check "
+            "dependencies.");
+      }
+      current = current->getNextNode();
+    }
+
+    if (!current) {
+      return mergeFailure("No mergeable fragment in the same mesh.");
+    }
+
+    auto merge_candidate = cast<FragmentOp>(current);
+    if (failed(AllowMerging(producer_op, merge_candidate,
+                            /*log_failure=*/true))) {
+      return failure();
+    }
+    return merge_candidate;
+  }
+};
+
+class MergeForwardWithBackwardPass
+    : public impl::MergeForwardWithBackwardPassBase<
+          MergeForwardWithBackwardPass> {
+  using MergeForwardWithBackwardPassBase::MergeForwardWithBackwardPassBase;
+
+ protected:
+  LogicalResult AllowMerging(FragmentOp producer_op, FragmentOp consumer_op,
+                             bool log_failure) const final {
+    // Check that the producer forward fragment and is immediately before
+    // the consumer backward fragment. This is only true for the last stage
+    // for 1F1B so it will not merge any fragments in previous stages, which
+    // is the intended behavior.
+    if (!IsExecutedImmediatelyAfter(producer_op, consumer_op)) {
+      return mergeFailure(
+          "The consumer fragment must appear immediately after the "
+          "producer fragment (modulo fragments in other meshes).",
+          log_failure);
+    }
+    if (!IsForwardFragment(producer_op) || !IsBackwardFragment(consumer_op)) {
+      return mergeFailure(
+          "The producer fragment must be a forward fragment and the "
+          "consumer fragment must be a backward fragment.",
+          log_failure);
+    }
+    return success();
+  }
+
+  bool AllowCloningProducerFragment(FragmentOp producer_op) const final {
+    return false;
+  }
+
+  bool AllowMergingWithAnyConsumer() const final { return false; }
+};
+
+class MergeUserDefinedFragmentsIntoSchedulingUnitsPass
+    : public impl::MergeUserDefinedFragmentsIntoSchedulingUnitsPassBase<
+          MergeUserDefinedFragmentsIntoSchedulingUnitsPass> {
+  using MergeUserDefinedFragmentsIntoSchedulingUnitsPassBase::
+      MergeUserDefinedFragmentsIntoSchedulingUnitsPassBase;
+
+ protected:
+  LogicalResult AllowMerging(FragmentOp producer_op, FragmentOp consumer_op,
+                             bool log_failure) const final {
+    if (!producer_op.isUserFragment() || !consumer_op.isUserFragment()) {
+      return mergeFailure("Cannot merge inferred fragments", log_failure);
+    }
+
+    std::optional<int> producer_transpose_count =
+        TryToFindSingleTransposeCount(producer_op);
+    SDY_CHECK(producer_transpose_count.has_value())
+        << "Expected one and only one transpose count for a user-defined "
+           "producer op.";
+
+    std::optional<int> consumer_transpose_count =
+        TryToFindSingleTransposeCount(consumer_op);
+    SDY_CHECK(consumer_transpose_count.has_value())
+        << "Expected one and only one transpose count for a user-defined "
+           "consumer op.";
+
+    if (*producer_transpose_count != *consumer_transpose_count) {
+      return mergeFailure(
+          "Cannot merge fragments with different transpose counts.",
+          log_failure);
+    }
+
+    std::optional<int> producer_call_count = TryToFindCallCounter(producer_op);
+    std::optional<int> consumer_call_count = TryToFindCallCounter(consumer_op);
+    if (producer_call_count.has_value() && consumer_call_count.has_value() &&
+        *producer_call_count != *consumer_call_count) {
+      return mergeFailure("Cannot merge fragments with different call counts.",
+                          log_failure);
+    }
+
+    if (!AreStageIdsConsistent(producer_op, consumer_op)) {
+      return mergeFailure(
+          "If both fragments have stage_ids, then they must be identical or "
+          "they cannot be merged.",
+          log_failure);
+    }
+    return success();
+  }
+
+  bool AllowCloningProducerFragment(FragmentOp producer_op) const final {
+    // User defined fragments can never be cloned.
+    return false;
+  }
+
+  bool AllowMergingWithAnyConsumer() const final { return false; }
+};
+
+// Holds data found in the attributes of a fragment, needed to check if stage
+// assignment is consistent.
+struct FragmentData {
+  StringRef mesh;
+  int64_t stage_id;
+  int64_t transpose_count;
+  std::optional<int64_t> call_count;
+};
+
+// Two fragments match _iff_ they have the same mesh assignment, same stage
+// assignment, same transpose_count, and the call_counts are consistent, i.e.,
+// they are the same or one of them is undefined. This property is not
+// transitive and therefore we don't define it as an equality operator.
+bool FragmentDatasMatch(FragmentData data, FragmentData other) {
+  return data.mesh == other.mesh && data.stage_id == other.stage_id &&
+         data.transpose_count == other.transpose_count &&
+         (!data.call_count.has_value() || !other.call_count.has_value() ||
+          *data.call_count == *other.call_count);
+}
+
+class VerifyStageMergingPass
+    : public impl::VerifyStageMergingPassBase<VerifyStageMergingPass> {
+  using VerifyStageMergingPassBase::VerifyStageMergingPassBase;
+
+ private:
+  void runOnFunc(FuncOp func_op) override {
+    // We keep track of the mesh, stage, transpose count, and potentially call
+    // count, of every fragment in the module. If we find any two equivalent
+    // fragments (see `FragmentData` above), then we emit an error.
+    std::vector<FragmentData> visited_fragments;
+    bool has_error = false;
+    func_op.walk([&](FragmentOp fragment) {
+      if (IntegerAttr stage_id_attr = fragment.getStageIdAttr()) {
+        StringRef mesh = fragment.getMeshName();
+        std::optional<int64_t> transpose_count =
+            TryToFindSingleTransposeCount(fragment);
+        // If the fragment has a stage, then it is a user-defined fragment,
+        // which means it has a transpose count.
+        SDY_CHECK(transpose_count.has_value());
+
+        FragmentData fragment_data = {mesh, stage_id_attr.getInt(),
+                                      *transpose_count,
+                                      TryToFindCallCounter(fragment)};
+        if (llvm::any_of(visited_fragments,
+                         [&fragment_data](const FragmentData& other) {
+                           return FragmentDatasMatch(fragment_data, other);
+                         })) {
+          emitError(fragment.getLoc())
+              << "A valid program cannot have more than one fragment with the "
+                 "same mesh, stage, transpose and call counts but found "
+                 "multiple fragments with the same attributes: [mesh="
+              << fragment_data.mesh << ", stage_id=" << fragment_data.stage_id
+              << ", transpose_count=" << fragment_data.transpose_count
+              << ", call_count=" << fragment_data.call_count.value_or(-1)
+              << "] for current fragment with origin: " << fragment.getOrigin();
+          has_error = true;
+        } else {
+          visited_fragments.push_back(fragment_data);
+        }
+      }
+    });
+
+    if (has_error) {
+      signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+
+void AddMergeInferredFragmentsPasses(OpPassManager& pm,
+                              bool absorb_on_entry_point_function,
+                              bool clone_inferred_fragments) {
+  // Absorb inferred fragments into user defined fragments to guarantee that
+  // the shape of the program in the body of a call-op or microbatching loop
+  // resembles the shape the user defined, via named computations + stage/mesh
+  // mapping (e.g., a V shaped pipeline).
+  pm.addNestedPass<FuncOp>(createAbsorbInferredFragmentsPass(
+      AbsorbInferredFragmentsPassOptions{absorb_on_entry_point_function}));
+
+  // Clean up the module by merging inferred fragments with user defined
+  // fragments (i.e., fragments that result from NamedComputations).
+  {
+    MergeInferredFragmentsPassOptions merge_inferred_options;
+    merge_inferred_options.cloneInferredFragments = clone_inferred_fragments;
+    merge_inferred_options.mergeAnyConsumer = true;
+    pm.addNestedPass<FuncOp>(
+        createMergeInferredFragmentsPass(std::move(merge_inferred_options)));
+  }
+  {
+    MergeInferredFragmentsPassOptions merge_inferred_options;
+    merge_inferred_options.mergeSideways = true;
+    pm.addNestedPass<FuncOp>(
+        createMergeInferredFragmentsPass(std::move(merge_inferred_options)));
+  }
+}
+
+}  // namespace mlir::mpmd
+
+#undef DEBUG_TYPE
diff --git a/shardy/dialect/mpmd/transforms/common/merge_fragments.h b/shardy/dialect/mpmd/transforms/common/merge_fragments.h
new file mode 100644
index 0000000..82a7c70
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/merge_fragments.h
@@ -0,0 +1,103 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_MERGE_FRAGMENTS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_MERGE_FRAGMENTS_H_
+
+#include <cstdint>
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Interfaces/LoopLikeInterface.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/TypeID.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/common/distributed_function_pass.h"
+
+namespace mlir::mpmd {
+
+class MergeFragmentBasePass : public DistributedFunctionPass {
+ public:
+  explicit MergeFragmentBasePass(TypeID passID)
+      : DistributedFunctionPass(passID) {}
+
+ protected:
+  // A map we will use to store the order of operations during merging.
+  using OpOrderMap = DenseMap<Operation*, int32_t>;
+
+  // Checks whether `producer_op` and `consumer_op` may be merged. Must be
+  // defined by subclasses of the pass.
+  //
+  // If `log_failure` is true, then the reason merging isn't allowed will be
+  // logged as a debug message.
+  virtual LogicalResult AllowMerging(FragmentOp producer_op,
+                                     FragmentOp consumer_op,
+                                     bool log_failure) const = 0;
+
+  // Checks whether a `producer_op` fragment may be cloned during merge. Must be
+  // defined by subclasses of the pass.
+  virtual bool AllowCloningProducerFragment(FragmentOp producer_op) const = 0;
+
+  // Whether a producer fragment should be merged with the closest mergeable
+  // consumer or with the closest consumer.
+  virtual bool AllowMergingWithAnyConsumer() const = 0;
+
+  virtual FailureOr<FragmentOp> GetMergeCandidate(FragmentOp producer_op,
+                                                  OpOrderMap& order) const;
+
+ private:
+  bool FastIsBeforeInBlock(Operation* op1, Operation* op2,
+                           OpOrderMap& order) const;
+
+  // Returns true if the producer can be merged with the consumer at the
+  // position of the producer: i.e., all consumer's operands are produced by
+  // the producer or by an earlier op.
+  bool CanMergeAtProducer(Operation* producer, Operation* consumer,
+                          OpOrderMap& order) const;
+
+  // Returns true if the producer can be merged with the consumer at the
+  // position of the consumer: i.e., no ops in between the consumer and
+  // producer uses the producer's results.
+  bool CanMergeAtConsumer(Operation* producer, Operation* consumer,
+                          OpOrderMap& order) const;
+
+  // Tries to merge the fragment and returns the merged fragment, or a failure
+  // if merging isn't possible.
+  FailureOr<FragmentOp> MergeFragmentsRewrite(FragmentOp producer_op,
+                                              RewriterBase& rewriter,
+                                              OpOrderMap& order) const;
+
+  // Merges fragments recursively, attempting to clone the producer
+  // fragment if possible. We may want to clone to avoid introducing
+  // dependencies.
+  //
+  // Pre-condition: All users of the producer_op have been processed by this
+  // rewrite, i.e., we do the rewrite in post-order traversal.
+  void MergeFragmentsRecursivelyRewrite(FragmentOp producer_op,
+                                        RewriterBase& rewriter,
+                                        OpOrderMap& order) const;
+
+  void runOnFunc(func::FuncOp func_op) override;
+};
+
+// Adds the sequence of passes that merges inferred fragments with user defined
+// fragments.
+void AddMergeInferredFragmentsPasses(mlir::OpPassManager& pm,
+  bool absorb_on_entry_point_function,
+  bool clone_inferred_fragments);
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_MERGE_FRAGMENTS_H_
diff --git a/shardy/dialect/mpmd/transforms/common/merge_transfers.cc b/shardy/dialect/mpmd/transforms/common/merge_transfers.cc
new file mode 100644
index 0000000..f742adb
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/merge_transfers.cc
@@ -0,0 +1,438 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <iterator>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/MapVector.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Block.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_MERGETRANSFERSPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+// Sets a threshold for the number of elements in a value that makes it a
+// candidate for transfer merging. Any transfer with more elements will not be
+// merged.
+// NOTE: this is set to 1 as it proved to be beneficial when training the Tier6
+// model. Though, increasing this threshold could be helpful when optimizing
+// other configurations or actually make it worse since it may increase the
+// memory footprint (even if slightly) and requires the introduction of concats
+// and slices, which could be costly in very small programs (e.g., serving).
+inline constexpr int kNumElementsThreshold = 1;
+inline constexpr StringRef kIsConcatTransfer = "concat_transfer";
+
+bool IsConcatTransfer(TransferOp transfer) {
+  return transfer->hasAttr(kIsConcatTransfer);
+}
+
+void SetConcatTransfer(TransferOp transfer) {
+  transfer->setAttr(kIsConcatTransfer, UnitAttr::get(transfer.getContext()));
+}
+
+// Returns all TransferOps that are users of `value` and were not introduced by
+// this pass.
+std::vector<TransferOp> GetNonConcatTransferUsers(Value value) {
+  std::vector<TransferOp> users;
+  for (Operation* user : value.getUsers()) {
+    if (auto transfer = dyn_cast<TransferOp>(user)) {
+      if (IsConcatTransfer(transfer)) {
+        continue;
+      }
+      users.push_back(transfer);
+    }
+  }
+  return users;
+}
+
+// Checks if a value is eligible for merging, i.e., if it is not pinned to host,
+// not sharded, and the number of elements is below the threshold.
+bool IsEligibleForMerging(Value value) {
+  auto mesh_tensor_type = cast<MeshTensorType>(value.getType());
+  // We do not concatenate values that are on the host or that are sharded.
+  if (mesh_tensor_type.getMemoryKind() &&
+      mesh_tensor_type.getMemoryKind().getValue() == kMemoryKindPinnedHost) {
+    return false;
+  }
+  if (mesh_tensor_type.getSharding() &&
+      !mesh_tensor_type.getSharding().isFullyReplicated()) {
+    return false;
+  }
+  RankedTensorType result_type = mesh_tensor_type.getRankedTensorType();
+  if (kNumElementsThreshold > -1 &&
+      result_type.getNumElements() > kNumElementsThreshold) {
+    // Being more aggressive here could impact runtime/memory performance.
+    // E.g., more concatenation could mean runtime degradation in very
+    // small/fast programs, or memory footprint increase since we duplicate
+    // the returned values when they're used by multiple consumers.
+    return false;
+  }
+  return true;
+}
+
+SetVector<FragmentOp> FragmentUsers(Operation* op) {
+  SetVector<FragmentOp> users;
+  for (Operation* user : op->getUsers()) {
+    if (auto fragment = dyn_cast<FragmentOp>(user)) {
+      users.insert(fragment);
+    }
+  }
+  return users;
+}
+
+using TypeAndConsumerFragment = std::pair<RankedTensorType, FragmentOp>;
+
+// Given a `producer` fragment, returns which values should be concatenated for
+// the merged transfer. The returned data structure maps:
+//   <Type, ConsumerFragment> -> vector{indices of results to concatenate}
+// The indices in the vector correspond to all the values of a given type that
+// are used by the same consumer fragment, via a transfer. E.g.,
+//
+//   ..., v1, ..., vn, ... = fragment ... {
+//   }, with vi: Ti.
+//   t1 = transfer v1 : mesh_tensor<m, T> -> mesh_tensor<m', T>
+//    ...
+//   tn = transfer vn : mesh_tensor<m, T> -> mesh_tensor<m', T>
+//   consumer = fragment ... (..., t1, ..., tn, ...)) { ...}
+//
+// The returned data structure will contain:
+//   <T1, consumer> -> sorted({indexof(v1), ..., indexof(vn)}})
+//
+// The vector of result indices is sorted so that know that after concatenation,
+// the result #i of the vector corresponds to the slice #i of the concatenated
+// value.
+//
+// Values that are sharded, or allocated in pinned_host, or have more elements
+// than the threshold are not considered for concatenation.
+llvm::MapVector<TypeAndConsumerFragment, std::vector<int>> FindValuesToConcat(
+    FragmentOp producer) {
+  llvm::MapVector<TypeAndConsumerFragment, std::vector<int>> values_to_concat;
+  for (OpResult result : producer.getResults()) {
+    if (!IsEligibleForMerging(result)) {
+      continue;
+    }
+    auto result_type =
+        cast<MeshTensorType>(result.getType()).getRankedTensorType();
+    for (TransferOp transfer : GetNonConcatTransferUsers(result)) {
+      // We iterate over the *set* (not vector) of fragment users to avoid
+      // adding the same consumer multiple times, in case a transfer is used by
+      // the same consumer multiple times.
+      for (FragmentOp consumer_fragment : FragmentUsers(transfer)) {
+        values_to_concat[{result_type, consumer_fragment}].push_back(
+            result.getResultNumber());
+      }
+    }
+  }
+  return values_to_concat;
+}
+// Creates a new fragment that is identical to fragment except that it
+// additionally returns result_value (of type result_type). Returns
+// the last result of the newly created FragmentOp, i.e. the result
+// corresponding to returned_value".
+Value AddNewResultToFragment(FragmentOp fragment, Value returned_value,
+                             MeshTensorType result_type,
+                             RewriterBase& rewriter) {
+  rewriter.setInsertionPoint(fragment);
+
+  Operation* terminator = fragment.getBody()->getTerminator();
+  terminator->insertOperands(terminator->getNumOperands(), returned_value);
+  std::vector<Type> result_types(fragment.getResultTypes().begin(),
+                                 fragment.getResultTypes().end());
+  result_types.push_back(result_type);
+  auto new_fragment = rewriter.create<FragmentOp>(
+      fragment.getLoc(), result_types, fragment.getOperands(),
+      fragment.getOriginAttr(), fragment.getMeshNameAttr(),
+      fragment.getStageIdAttr());
+  // Copy all attributes except `origin` and `mesh_name`, which were copied
+  // during the creation of the new fragment.
+  CopyAttributes(fragment, new_fragment,
+                 /*elided_attrs_set=*/{"origin", "mesh_name", "stage_id"});
+  new_fragment.getRegion().takeBody(fragment.getRegion());
+  rewriter.replaceOp(fragment, new_fragment.getResults().drop_back());
+  return new_fragment.getResult(terminator->getNumOperands() - 1);
+}
+
+// Concatenates the results of `producer` in indices `results_to_concat`. The
+// result of the concatenation is returned by the producer. Returns the newly
+// added result.
+//
+// results_to_concat[i] corresponds to the slice #i of the concatenated value.
+//
+// Requires: each result in `results_to_concat` has type `result_type`.
+//
+// Note: concatenated values are not removed as they may have multiple
+// consumers.
+//
+// Example:
+//   %producer:N = fragment ... {
+//      ...
+//      return ..., %v1, %v2, ... : ..., T, T, ...
+//   }  // with v1: T, v2: T and T = tensor<d1 x... x dn>
+//     ~~>
+//   %producer:N+1 = fragment ... {
+//      ...
+//      %r1 = reshape %v1 : tensor<1 x d1 x ... x dn>
+//      %r2 = reshape %v2 : tensor<1 x d1 x ... x dn>
+//      %concat = concat(%r1, %r2) : tensor<2 x d1 x ... x dn>
+//      return ..., %v1, %v2, ..., %concat
+//            : ..., T, T, ..., tensor<2 x d1 x ... x dn>
+//   }
+//
+Value ConcatResultsOnProducer(FragmentOp producer,
+                              const std::vector<int>& results_to_concat,
+                              RankedTensorType result_type,
+                              IRRewriter& rewriter) {
+  Operation* terminator = producer.getBody()->getTerminator();
+  rewriter.setInsertionPoint(terminator);
+
+  // The list of values that will be concatenated.
+  std::vector<Value> concat_operands;
+  concat_operands.reserve(results_to_concat.size());
+
+  // The shape of the value before it's concatenated to other values, i.e., it
+  // has a leading dimension of size 1.
+  SmallVector<int64_t> reshaped_dimensions;
+  reshaped_dimensions.reserve(result_type.getShape().size() + 1);
+  reshaped_dimensions.push_back(1);
+  llvm::copy(result_type.getShape(), std::back_inserter(reshaped_dimensions));
+
+  // Reshape each result in `results_to_concat` to have shape
+  // `reshaped_dimensions`/
+  for (int result_index : results_to_concat) {
+    OpResult result = producer->getResult(result_index);
+    Value operand = terminator->getOperand(result.getResultNumber());
+    SDY_CHECK(operand.getType() == result_type);
+    auto new_shape = RankedTensorType::get(reshaped_dimensions,
+                                           result_type.getElementType());
+    mlir::Value reshape = rewriter.create<stablehlo::ReshapeOp>(
+        producer.getLoc(), new_shape, operand);
+    concat_operands.push_back(reshape);
+  }
+
+  // Finally, concatenate the reshaped values and add the result to the
+  // producer.
+  Value concat = rewriter.create<stablehlo::ConcatenateOp>(producer.getLoc(),
+                                                           concat_operands, 0);
+  MeshTensorType new_result_type =
+      MeshTensorType::get(result_type.getContext(), producer.getMeshName(),
+                          cast<RankedTensorType>(concat.getType()));
+  return AddNewResultToFragment(producer, concat, new_result_type, rewriter);
+}
+
+// Creates a slice op to extract the `slice_index`th slice of shape `shape` from
+// the concatenated `concatenated_arg`.
+Value CreateSliceOnArgument(ArrayRef<int64_t> shape,
+                            BlockArgument concatenated_arg, int64_t slice_index,
+                            IRRewriter& rewriter) {
+  SmallVector<int64_t> start_indices{slice_index};
+  SmallVector<int64_t> limit_indices{slice_index + 1};
+  SmallVector<int64_t> strides{1};
+  for (int dim : shape) {
+    start_indices.push_back(0);
+    limit_indices.push_back(dim);
+    strides.push_back(1);
+  }
+  return rewriter.create<stablehlo::SliceOp>(concatenated_arg.getLoc(),
+                                             concatenated_arg, start_indices,
+                                             limit_indices, strides);
+}
+
+// Adds the result of `concat_transfer` to the operands (and block arguments) of
+// the `consumer` fragment and replaces all uses of each argument in
+// `consumer_args` with the respective slice of the concatenated transfer. E.g.,
+//
+//   %t1 = transfer %producer#...
+//   ...
+//   %tn = transfer %producer#...
+//   %concat_transfer = transfer %concatenated_result
+//                    : (mesh_tensor<m, tensor<n x d1 x ... x dn>>)
+//                    -> mesh_tensor<m', tensor<n x d1 x ... x dn>>
+//   %consumer = fragment ... (..., %t1, .., %tn, ...)
+//                          : (..., %arg1, ..., %argn, ...) {
+//      use(%arg1) ... use(%argn)
+//   }
+//   With %concatenated_result == concat(%t1, ..., %tn).
+//
+//   ~~>
+//
+//   %t1 = transfer %producer#...
+//   ...
+//   %tn = transfer %producer#...
+//   %concat_transfer = transfer %concatenated_result
+//                    : (mesh_tensor<m, tensor<n x d1 x ... x dn>>)
+//                    -> mesh_tensor<m', tensor<n x d1 x ... x dn>>
+//   %consumer = fragment ... (..., %t1, .., %tn, ..., %concat_transfer)
+//                          : (..., %arg1, ..., %argn, ...,
+//                             %arg_concat: tensor<n x d1 x ... x dn>) {
+//      %s1 = slice(%arg_concat, 0) : tensor<1 x d1 x ... x dn>
+//      %s1_reshape = reshape(%s1) : tensor<d1 x ... x dn>
+//      ...
+//      %sn = slice(%arg_concat, n) : tensor<1 x d1 x ... x dn>
+//      %sn_reshape = reshape(%sn) : tensor<d1 x ... x dn>
+//      ...
+//      use(%s1_reshape) ... use(%sn_reshape)
+//      // %arg1, ..., %argn are dead.
+//   }
+// Requires: every arg in `consumer_args` to be a block argument of `consumer`.
+void SliceConcatOnConsumer(
+    FragmentOp consumer,
+    const llvm::MapVector<BlockArgument, int>& consumer_args,
+    TransferOp concat_transfer, IRRewriter& rewriter) {
+  rewriter.setInsertionPoint(consumer.getBody(), consumer.getBody()->begin());
+  // Append the concatenated result to the operands of the consumer.
+  consumer->insertOperands(consumer->getNumOperands(),
+                           concat_transfer.getResult());
+
+  // Add a new argument to the consumer for the concatenated result.
+  BlockArgument concat_arg = consumer.getBody()->addArgument(
+      concat_transfer.getType().getRankedTensorType(),
+      concat_transfer.getLoc());
+  for (const auto& [arg, slice_index] : consumer_args) {
+    RankedTensorType arg_type = cast<RankedTensorType>(arg.getType());
+
+    // Create the respective slice of the concatenated result.
+    Value slice = CreateSliceOnArgument(arg_type.getShape(), concat_arg,
+                                        slice_index, rewriter);
+
+    // Drop the leading dimension of size 1 that results from the slicing.
+    Value reshape = rewriter.create<stablehlo::ReshapeOp>(consumer.getLoc(),
+                                                          arg_type, slice);
+
+    // Replace the argument with the slice, without erasing it. We do this
+    // for the sake of simplicity -- we can use the simplify passes to
+    // clean it up afterwards.
+    rewriter.replaceAllUsesWith(arg, reshape);
+  }
+}
+
+// Finds the consumer block arguments that need to be replaced with slices of
+// the concatenated transfer.
+llvm::MapVector<BlockArgument, int> FindConsumerArgumentsToReplace(
+    const std::vector<int>& result_indices, FragmentOp producer,
+    FragmentOp consumer) {
+  llvm::MapVector<BlockArgument, int> consumer_args;
+  for (auto [slice_index, result_index] : llvm::enumerate(result_indices)) {
+    for (TransferOp transfer :
+         GetNonConcatTransferUsers(producer.getResult(result_index))) {
+      for (OpOperand& transfer_use : transfer->getUses()) {
+        if (transfer_use.getOwner() == consumer) {
+          consumer_args[consumer.getBody()->getArgument(
+              transfer_use.getOperandNumber())] = slice_index;
+        }
+      }
+    }
+  }
+  return consumer_args;
+}
+
+// Given a `producer` fragment, concatenates sets of results that are smaller
+// than a given threshold and transferred to the same consumer fragment.
+void MergeTransfersProducedByFragment(FragmentOp producer,
+                                      IRRewriter& rewriter) {
+  llvm::MapVector<TypeAndConsumerFragment, std::vector<int>> values_to_concat =
+      FindValuesToConcat(producer);
+  FragmentOp current_producer = producer;
+  for (const auto& [type_and_consumer_fragment, result_indices] :
+       values_to_concat) {
+    auto [type, consumer_fragment] = type_and_consumer_fragment;
+    SDY_CHECK(!result_indices.empty());
+    if (result_indices.size() == 1) {
+      // Nothing to concatenate.
+      continue;
+    }
+
+    Value concat_result = ConcatResultsOnProducer(
+        current_producer, result_indices, type, rewriter);
+    rewriter.setInsertionPointAfterValue(concat_result);
+
+    MeshTensorType concat_result_type =
+        cast<MeshTensorType>(concat_result.getType());
+
+    // Transfer the concatenated result to the consumer.
+    auto concat_transfer = rewriter.create<TransferOp>(
+        concat_result.getLoc(),
+        MeshTensorType::get(concat_result_type.getContext(),
+                            consumer_fragment.getMeshName(),
+                            concat_result_type.getRankedTensorType()),
+        concat_result);
+
+    // Mark it as the result of a concatenation to guarantee that
+    // `concat_result_type` won't flow into a further concatenation.
+    SetConcatTransfer(concat_transfer);
+
+    // NOTE: at this point the producer is dead (as a consequence of
+    // concatenating results on the producer), so we need to be careful not to
+    // use it anymore. We update the current producer to the fragment that
+    // replaced `producer`.
+    current_producer = cast<FragmentOp>(concat_result.getDefiningOp());
+
+    // Find the arguments of the consumer that are uses of the results that
+    // were concatenated.
+    llvm::MapVector<BlockArgument, int> consumer_arguments =
+        FindConsumerArgumentsToReplace(result_indices, current_producer,
+                                       consumer_fragment);
+
+    // Replace the uses of the old results with slices of the new concatenated
+    // result.
+    SliceConcatOnConsumer(consumer_fragment, consumer_arguments,
+                          concat_transfer, rewriter);
+  }
+}
+
+class MergeTransfersPass
+    : public impl::MergeTransfersPassBase<MergeTransfersPass> {
+  using MergeTransfersPassBase::MergeTransfersPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func) override {
+    if (!IsMpmdFunction(func)) {
+      return;
+    }
+
+    IRRewriter rewriter(func.getContext());
+    Block& block = func.getBody().front();
+
+    // Copy all fragments to a vector so that replacing them with new fragments
+    // (in order to introduce new results) doesn't affect the iteration order.
+    SmallVector<FragmentOp> fragments(block.getOps<FragmentOp>().begin(),
+                                      block.getOps<FragmentOp>().end());
+    for (FragmentOp producer : fragments) {
+      MergeTransfersProducedByFragment(producer, rewriter);
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/passes.h b/shardy/dialect/mpmd/transforms/common/passes.h
new file mode 100644
index 0000000..2f4d0b1
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/passes.h
@@ -0,0 +1,45 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_PASSES_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_PASSES_H_
+
+// IWYU pragma: begin_keep
+
+#include <memory>
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/distributed_function_pass.h"
+#include "shardy/dialect/mpmd/transforms/common/merge_fragments.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+// IWYU pragma: end_keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DECL
+#define GEN_PASS_REGISTRATION
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+void AddCallInliningRelatedPasses(OpPassManager& pm);
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_PASSES_H_
diff --git a/shardy/dialect/mpmd/transforms/common/passes.td b/shardy/dialect/mpmd/transforms/common/passes.td
new file mode 100644
index 0000000..7316c85
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/passes.td
@@ -0,0 +1,488 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+// TODO: b/374694825 - This pass is not complete yet. In particular, we also
+// need to consider: (a) side-ways merging. We need to be careful with this as
+// it may have performance and jitting time implications. (b) relax the
+// condition in which we move transfers next to their producers/consumers. In
+// particular, we could move any transfer (perhaps any computation) that does
+// not depend on the root fragment. This should be safe and maybe we can use a
+// topological sort to do so, but needs more thinking.
+def AbsorbInferredFragmentsPass :
+    PassBase<"mpmd-absorb-inferred-fragments", "DistributedFunctionPass"> {
+  let summary = "Root fragments absorb inferred fragments.";
+  let description = [{
+    Makes root fragments absorb inferred fragments, i.e., by
+    merging inferred producer/consumer fragments into root fragments, where a
+    root fragment is any fragment that is:
+    - a user fragment, or
+    - not used by any other fragment (e.g. a fragment used by the return op or a
+    transfer only), or
+    - not a user of a value produced by any other fragment (e.g., user of block
+    arguments or transfers).
+
+    In order to do so, the pass applies the following patterns, until it reaches
+    a fixed point:
+
+    (1) Given a root fragment `rf`, if there is an inferred fragment `ipf` such
+    that `ipf` is a producer of `rf` and `rf` is the closest consumer of `ipf`,
+    then `ipf` is merged into `rf`.
+
+    And dually:
+
+    (2) Given a root fragment `rf`, if there is an inferred consumer `icf` such
+    that `icf` is a consumer of `rf` and `rf` is the closest producer of `icf`,
+    then `icf` is merged into `rf`.
+
+    This means we preserve the structure/shape of the program as defined by the
+    user, via named computations and stage/mesh assignment.
+
+    Note that this pass is quite aggressive in merging inferred fragments, and in
+    particular, it could cause small differences across different stages that
+    could increase the number of unique fragments to compile.
+
+    This pass will warn us if the final non entry-point functions still include
+    inferred fragments, as these could cause performance issues (e.g., gradient
+    accumulation gone wrong).
+
+    WARNING: if the program doesn't have any user-defined fragment and has lots
+    of inferred fragments, then this pass could be very slow.
+  }];
+
+  let options = [
+    Option<"absorbOnEntryPointFunction", "absorb-on-entry-point-function",
+           "bool", /*default=*/"false",
+           "Whether to absorb inferred fragments into user-defined fragments "
+           "on entry-point functions, in addition to targets of mpmd.calls.">
+  ];
+}
+
+//===----------------------------------------------------------------------===//
+// Start of - Call op passes
+//===----------------------------------------------------------------------===//
+
+def CallInlinePass : Pass<"mpmd-call-inline", "ModuleOp"> {
+  let summary = "Inlines all `mpmd.call` operations.";
+  let description = [{
+    Inlines `mpmd.call` operations, copying their attributes to any inlined
+    operations.
+  }];
+}
+
+def SinkNegligibleOpsIntoCallOpPass :
+    Pass<"mpmd-sink-negligible-ops-into-call-op", "ModuleOp"> {
+  let summary = "Sinks negligible ops into call-ops (i.e., the called function).";
+  let description = [{
+    Sinks (negligible) ops into called functions: if there is an op with zero
+    operands and a single result which is used as a specific operand of *all*
+    call ops to the same function, then we sink it into those call ops, i.e., we
+    clone it into the called function and replace all uses of the respective
+    argument with the clone. Sunken ops are removed from the caller function and
+    unused arguments of the callee (and operands of respective call ops)
+    removed. Note that this can potentially duplicate computation across many
+    microbatches, when using call ops for microbatching. Though, this
+    computation is most likely negligible as it takes no operands.
+  }];
+}
+
+// TODO: b/359837378 - We should erase the attribute from other ops too.
+def FromUnrollToCallCounterPass :
+    PassBase<"mpmd-from-unroll-to-call-counter", "DistributedFunctionPass"> {
+  let summary = "Converts the unroll counter attr of a call op to a call "
+                "counter attr.";
+  let description = [{
+    Whenever a call op has a `unroll_counter` attribute, this pass replaces it
+    with a `call_counter` attribute. This is needed for cases in which a
+    sequence of calls results from unrolling a loop in MLIR (e.g., via
+    `-mpmd-unroll-for-loops`) instead of from unrolling a loop at the python
+    level.
+  }];
+}
+
+def EraseUnusedCalleeBlockArgumentsPass :
+    Pass<"mpmd-erase-unused-callee-block-arguments", "ModuleOp"> {
+  let summary = "Erases any mpmd callee block argument that isn't used by an "
+                "(hlo) computation.";
+  let description = [{
+    Erases unused block arguments from functions called by mpmd.calls. We
+    consider a block argument to be unused if it has no used or is used only by
+    the function's terminator, i.e., if it is not used by any hlo computation.
+  }];
+}
+
+//===----------------------------------------------------------------------===//
+// End of - Call op passes
+//===----------------------------------------------------------------------===//
+
+// TODO: b/372460554 - Support nested mpmd.for loops.
+def UnrollForLoopsPass :
+    PassBase<"mpmd-unroll-for-loops", "DistributedFunctionPass"> {
+  let summary = "Fully unrolls `mpmd.for` loops.";
+  let description = [{
+    Creates a pass that completely unrolls `mpmd.for` ops, while
+    attaching an unroll_counter attribute to each unrolled op.
+
+    Requires: the unroll factor to be equal to the number of iterations.
+
+    NOTE: This pass does not support nested loops, nor partial unrolling.
+  }];
+
+  let dependentDialects = ["mlir::stablehlo::StablehloDialect"];
+}
+
+def CopyConstantsPass :
+    PassBase<"mpmd-copy-constants", "DistributedFunctionPass"> {
+  let summary = "Copies constants produced in one fragment to their consumers.";
+  let description = [{
+    Copies constants from their producer fragments to their consumer fragments,
+    possibly through transfers.
+
+    Example:
+
+    ```mlir
+    %f = fragment () () {
+      return constant
+    }
+    %t = transfer %f
+    fragment (%t) (%arg) {
+      op(... %arg ...)
+      ...
+    }
+
+     ~~>
+
+    %f = fragment () () {
+      return constant
+    }
+    %t = transfer %f
+    fragment (%t) (%arg) {
+      %c = constant
+      op(... %c ...)
+      ...
+    }
+    ```
+
+    Note: this pass doesn't cleanup any unused code.
+
+    This can be beneficial for runtime performance: it enables potential
+    optimizations by putting the constant together with its users and it avoids
+    transfers of constants. Additionally, it will improve memory usage: we
+    reduce the space needed for parameters of the computation.
+  }];
+}
+
+def FragmentDcePass :
+    PassBase<"mpmd-fragment-dce", "DistributedFunctionPass"> {
+  let summary = "Eliminates unused fragment arguments/results and simplifies "
+                "fragment regions.";
+  let description = [{
+    Removes unused fragment arguments and results, while simplifying fragment
+    regions, essentially eliminating dead MPMD code.
+  }];
+}
+
+def FragmentDedupPass :
+    PassBase<"mpmd-fragment-dedup", "DistributedFunctionPass"> {
+  let summary = "Removes any duplicated operands and results in fragments.";
+  let description = [{
+    Removes any duplicated used arguments and results in fragments. This will
+    leave the duplicate arguments and results unused. Other passes should be run
+    to remove the unused arguments and results.
+  }];
+}
+
+//===----------------------------------------------------------------------===//
+// Start of - Fragment merging passes
+//===----------------------------------------------------------------------===//
+
+// Collection of passes that merge producer-consumer fragment ops that are
+// assigned to the same mesh, within the same block, and if certain conditions
+// apply (c.f., each type of merge pass below).
+//
+// The origin of the new fragment will be the de-duplicated merge of the origins
+// of the two fragments.
+//
+// Note that we don't merge a producer fragment with its closest consumer
+// fragment if the former has other (non-fragment) users before the consumer
+// fragment (e.g. a transfer op), because we don't want to reorder any
+// operations in these passes (which would also require data flow dependency
+// analysis).
+//
+// Terminology:
+//   - An *inferred* fragment is a fragment created by the compiler. I.e., its
+//     `origin` attribute is empty.
+//   - A *user (-defined) fragment* is a fragment that resulted from a
+//     named_computation or from merging another user-defined fragment with an
+//     inferred fragment. I.e., its `origin` attribute is not empty.
+
+def MergeInferredFragmentsPass :
+    PassBase<"mpmd-merge-inferred-fragments", "MergeFragmentBasePass"> {
+  let summary = "Merges inferred fragments with user defined fragments.";
+  let description = [{
+    Merges inferred with user-defined or other inferred fragments. This pass is
+    useful to clean-up/simplify the module and can be useful after other
+    compiler passes that introduce inferred fragments, while
+    `-mpmd-transfer-aware-merge` above is more invasive and should be used for
+    optimization purposes only.
+
+    NOTE: We assume that merging an inferred fragment to any other fragment
+    never delays transfers.
+
+    When `clone_inferred_fragments=true`, then this merging pass allows for
+    certain fragments to be cloned. In particular, if we encounter a pair of
+    fragments f1 and f2 such that:
+    - f2 uses f1, and
+    - f1 is inferred, pure, and sufficiently simple (single non-return op and
+      single result),
+    then we merge a clone of f1 into f2, i.e., f1 itself (and other
+    users) remain independent of f2. It may be undesirable to merge inferred
+    producer fragments without cloning, because it can create unnecessary
+    dependencies between fragments. E.g.,
+
+    ```mlir
+    %inferred = frag m1 { return stablehlo.const … }
+    %frag1 = frag m1 (%inferred, …)
+    %frag2 = frag m1 (%inferred, …)
+
+    ~>
+
+    %inferred_frag1 = frag m1 (…) { … return const_m1, … }
+    %frag2 = frag m2 (inferred_frag1, …)
+    ```
+
+    So frag2 now depends on inferred_frag1 and we create a dependency.
+
+    However, sometimes we do want to merge in place, e.g., when the inferred
+    fragment has collectives inside.
+
+    NOTE: if the `mpmd.call` ops have been inlined, doing aggressive merging
+    with `merge-any-consumer=true` may create dependencies between fragments of
+    different microbatches, preventing certain reschedulings.
+  }];
+  let options = [
+    Option<"cloneInferredFragments", "clone-inferred-fragments", "bool",
+           /*default=*/"false",
+           "Whether to clone inferred fragments. Chains of clonable fragments "
+           "are merged one-by-one into their consumers and recursively.">,
+    Option<"mergeAnyConsumer", "merge-any-consumer", "bool",
+           /*default=*/"false",
+           "Whether to merge with any consumer or only the closest consumer.">,
+    Option<"mergeSideways", "merge-sideways", "bool", /*default=*/"false",
+           "Whether to merge with the next fragment in the same mesh "
+           "(neighbor), even if not a consumer.">
+  ];
+}
+
+def MergeForwardWithBackwardPass :
+    PassBase<"mpmd-merge-forward-with-backward", "MergeFragmentBasePass"> {
+  let summary = "Merge forward fragments with backward fragments.";
+  let description = [{
+    Merge a producer forward fragment with a consumer backward fragment, if the
+    former is immediately before the latter. This is only true for the last
+    stage in 1F1B schedule, so it will not merge any fragments in previous
+    stages, which is the intended behavior.
+  }];
+}
+
+def MergeUserDefinedFragmentsIntoSchedulingUnitsPass :
+    PassBase<"mpmd-merge-user-fragments-into-scheduling-units",
+             "MergeFragmentBasePass"> {
+  let summary = "Merge user based fragments pre pipeline scheduling passes.";
+  let description = [{
+    Merges pairs of user defined fragments to be used together with pipeline scheduling
+    passes.
+
+    NOTE: this pass requires every user-defined fragment to have one and only
+    one transpose count, meaning we cannot apply it after
+    `-mpmd-transfer-aware-merge`, which can result in fragments with multiple
+    transpose counts per fragment.
+  }];
+}
+
+def VerifyStageMergingPass :
+    PassBase<"mpmd-verify-stage-merging", "DistributedFunctionPass"> {
+  let summary = "Verifies that merging of fragments assigned to stages succeeded.";
+  let description = [{
+    Verifies that fragments with stage assignment have been correctly merged.
+    This means that it's not possible to have in the module any two equivalent
+    fragments in terms of assignment and counters.
+
+    Two fragments are equivalent in terms of assignment and counters _iff_
+    a. they are assigned to the same mesh,
+    b. they are assigned to the same stage,
+    c. they have the same transpose count, and
+    d. either both have the same call counter or one of them doesn't have a call
+    counter defined (i.e., an undefined call counter matches any call counter).
+
+    This is needed to guarantee to the user that any computation assigned to the
+    same stage is executed contiguously.
+  }];
+}
+
+def RuleBasedMergePass :
+    PassBase<"mpmd-rule-based-merge", "DistributedFunctionPass"> {
+  let summary = "Merges fragments based on user-defined rules.";
+  let description = [{
+    Merges fragments based on a specified list of rules, each specifying a list
+    of source fragment to merge (by their fragment info) and the target info to
+    label the merged fragment.
+  }];
+
+  let options = [
+    ListOption<"rules", "rules", "FragmentMergeRule",
+               "A list of fragment merge rules, each with a list of source "
+               "fragment infos and a target fragment info.">
+  ];
+}
+
+//===----------------------------------------------------------------------===//
+// End of - Fragment merging passes
+//===----------------------------------------------------------------------===//
+
+def MergeTransfersPass :
+    PassBase<"mpmd-merge-transfers", "DistributedFunctionPass"> {
+  let summary = "Merges sets of transfers that share the same producer and "
+                "consumer fragments.";
+  let description = [{
+    Merges sets of transfers of the same payload type which
+    share the same producer and consumer fragments. The payload values of these
+    transfers have less elements than a given threshold, are not sharded and do
+    not live in pinned_host.
+
+    Merging a set of transfers means: concatenating the transferred values at
+    producer site and splitting them at consumer site.
+
+    Note: if a producer fragment has a set of transfers that is used by distinct
+    consumers, this pass will duplicate the concatenation at the producer site,
+    which can cause an increase in memory footprint, and unnecessary operations.
+    Applying CSE and fragment IO dedup after this pass is recommended.
+  }];
+
+  let dependentDialects = ["mlir::stablehlo::StablehloDialect"];
+}
+
+def SplitBwdFragmentsPass : Pass<"mpmd-split-bwd-fragments", "func::FuncOp"> {
+  let summary = "Splits backward fragments based on transferred results.";
+  let description = [{
+    Splits backwards fragments so that any computation
+    that does not flow into transferred results becomes a fragment of its own.
+    The original fragment will return some residual values that will be passed in
+    as extra operands to the split-out fragments.
+
+    This split allows us to transfer results into other meshes earlier. One
+    canonical use of this optimization will be splitting the activation gradient
+    computation in back-propagation from the parameter gradient computation.
+    Note also that due to the need to potentially thread through some residual
+    values in the new fragments, memory pressure will increase.
+
+    Note: In practice we care only about transfers to different meshes, so this
+    pass should be best used after we've already fused transfers and eliminated
+    intra-mesh transfers.
+
+    Note: When splitting backward fragments we add extra residual values to the
+    original one and pass them as extra arguments to the split-out fragment. We
+    give these residual types fully replicated mesh types, which really assumes
+    that we have not run any form of SPMD propagation prior to this pass.
+  }];
+}
+
+def SplitAndPrioritizeTransferIndependentComputationsPass :
+    Pass<"mpmd-split-and-prioritize-transfer-independent-computations",
+         "func::FuncOp"> {
+  let summary = "Splits backward fragments based on transferred results.";
+  let description = [{
+    Splits a fragment into two fragments, so that we can start computation
+    early. I.e. we split the fragment into two fragments A -> B, where A does
+    not rely on any transfer result, and is maximally large, and B relies on
+    transfer results.
+
+    Note: When splitting fragments we add extra residual values to the original
+    one and pass them as extra arguments to the split-out fragment. We give
+    these residual types fully replicated mesh types, which really assumes that
+    we have not run any form of SPMD propagation prior to this pass.
+  }];
+}
+
+def RemoveTransferCyclesPass :
+    PassBase<"mpmd-remove-transfer-cycles", "DistributedFunctionPass"> {
+  let summary = "Removes device-only transfer cycles from the program, avoiding "
+                "unnecessary transfers.";
+  let description = [{
+    Removes transfer cycles.
+
+    E.g. in symbols:
+
+      x1 = transfer(x0) : m0 -> m1
+      x2 = transfer(x1) : m1 -> m2
+      x3 = transfer(x2) : m2 -> m3
+      x0_1 = transfer(x3) : m3 -> m0
+      x1_1 = transfer(x0_1) : m0 -> m1
+
+    ~~>
+
+      x1 = transfer(x0) : m0 -> m1
+      x2 = transfer(x1) : m1 -> m2
+      x3 = transfer(x2) : m2 -> m3
+      x0_1 = x0
+      x1_1 = x1
+
+    i.e. we then we break the cycle by using the existing values, removing the
+    unnecessary transfers.
+
+    Note that this could increase memory overhead, since transferring the data
+    away and back again means that there's a period where the data isn't on the
+    device. Thus, we only do this if the cycle only contains device-to-device
+    transfers, e.g. since a `device -> host -> device` cycle could be for memory
+    purposes.
+
+    This doesn't use the MLIR Canonicalizer, because that doesn't guarantee that
+    everything is canonicalized, and also it's more expensive to apply.
+  }];
+}
+
+// TODO(jupvfranco): we should create these copies using ifrt_ir reshard. We are
+// not ready for that yet as we need better support for donation in the
+// presence of reshards.
+def UniquifyFunctionInputsOutputsPass :
+    PassBase<"mpmd-uniquify-function-inputs-outputs", "DistributedFunctionPass"> {
+  let summary = "Uniquifies any value returned multiple times or any block "
+                "argument directly returned by the function.";
+  let description = [{
+    If a function returns the same value multiple times, creates multiple
+    versions for that value, by creating a fragment assigned to that value's
+    mesh which returns the value multiple times. After this pass, each return
+    operand is unique. This is important to ensure that the respective results
+    are allocated in different buffers, as in the following `jax.jit` example:
+
+    ```python
+    def f(x):
+      y = x + x
+      return y, y
+
+    z1, z2 = f(5)
+    z1 += 1
+    print(z1) ~~> 6
+    print(z2) ~~> 5
+    ```
+
+    Similarly, if a function returns a block argument, this pass creates an
+    identity fragment for that block argument, guaranteeing that values are
+    passed by value to the function, not by reference.
+  }];
+
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+}
diff --git a/shardy/dialect/mpmd/transforms/common/remove_transfer_cycles.cc b/shardy/dialect/mpmd/transforms/common/remove_transfer_cycles.cc
new file mode 100644
index 0000000..bc3c365
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/remove_transfer_cycles.cc
@@ -0,0 +1,88 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_REMOVETRANSFERCYCLESPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+// Walks the chain of device-only transfers leading to `transfer_op` to find an
+// operand with the same type. If so, this is a root of the cycle. Note that
+// this function finds the first such root.
+//
+// Returns nullptr if no such root is found.
+Value FindDeviceOnlyTransferCycleClosestRoot(TransferOp transfer_op) {
+  TransferOp transfer_parent = transfer_op;
+  while (transfer_parent) {
+    auto parent_operand_type = transfer_parent.getTensor().getType();
+    // TODO: b/397933351 - This doesn't handle memory kind attributes. We likely
+    // don't want to use attributes for the memory kinds on transfers, but if we
+    // do, then we should handle them here.
+    if (parent_operand_type.getMemoryKind() &&
+        parent_operand_type.getMemoryKind().getValue() != kMemoryKindDevice) {
+      return nullptr;
+    }
+    if (parent_operand_type == transfer_op.getType()) {
+      return transfer_parent.getTensor();
+    }
+    transfer_parent = dyn_cast_if_present<TransferOp>(
+        transfer_parent.getTensor().getDefiningOp());
+  }
+
+  return nullptr;
+}
+
+class RemoveTransferCyclesPass
+    : public impl::RemoveTransferCyclesPassBase<RemoveTransferCyclesPass> {
+  using RemoveTransferCyclesPassBase::RemoveTransferCyclesPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    IRRewriter rewriter(&getContext());
+    // Walk the func in reverse, so that we can delete user
+    // ops without having to worry about invalidating iterators.
+    for (Operation& op : llvm::make_early_inc_range(
+             llvm::reverse(func_op.front().getOperations()))) {
+      if (auto transfer_op = dyn_cast<TransferOp>(&op)) {
+        // Note that because we walk in reverse, it suffices to keep replacing
+        // the closest root we find.
+        if (Value root = FindDeviceOnlyTransferCycleClosestRoot(transfer_op)) {
+          rewriter.replaceAllUsesWith(transfer_op, root);
+          rewriter.eraseOp(transfer_op);
+        }
+      }
+    }
+  }
+};
+
+}  // namespace
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/rule_based_merge.cc b/shardy/dialect/mpmd/transforms/common/rule_based_merge.cc
new file mode 100644
index 0000000..a2cfc3c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/rule_based_merge.cc
@@ -0,0 +1,182 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <utility>
+#include <vector>
+
+#include "llvm/Support/ScopedPrinter.h"
+#include "mlir/Analysis/TopologicalSortUtils.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Dominance.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Rewrite/FrozenRewritePatternSet.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_RULEBASEDMERGEPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+using FragmentMergeRuleMap =
+    DenseMap<FragmentInfo, const FragmentMergeRule*, FragmentInfoMapInfo>;
+
+// TODO(petebu): Consider doing a forward walk through the function while
+// merging matching fragments instead of using a GreedyPatternRewriter.
+class RuleBasedMergingPattern : public OpRewritePattern<FragmentOp> {
+  using OpRewritePattern<FragmentOp>::OpRewritePattern;
+
+ public:
+  RuleBasedMergingPattern(MLIRContext* context,
+                          const FragmentMergeRules& merging_rules,
+                          const FragmentMergeRuleMap& fragment_merge_rule_map)
+      : OpRewritePattern<FragmentOp>(context),
+        fragment_merge_rule_map_(fragment_merge_rule_map) {}
+
+  LogicalResult matchAndRewrite(FragmentOp merge_into_fragment,
+                                PatternRewriter& rewriter) const override {
+    FragmentInfo fragment_info = GetFragmentInfo(merge_into_fragment);
+    // Find the merge rule for the fragment, if there is one.
+    auto it = fragment_merge_rule_map_.find(fragment_info);
+    if (it == fragment_merge_rule_map_.end()) {
+      return failure();
+    }
+    const FragmentMergeRule* rule = it->second;
+
+    // Get all the merge candidates for the merge rule.
+    std::vector<FragmentOp> merge_candidates =
+        GetMergeCandidates(merge_into_fragment, rule);
+    if (merge_candidates.empty()) {
+      return failure();
+    }
+    merge_candidates.push_back(merge_into_fragment);
+
+    // Sort the merge candidates topologically.
+    SmallVector<Operation*> sorted_merge_candidates;
+    sorted_merge_candidates.reserve(merge_candidates.size());
+    for (auto& merge_candidate : merge_candidates) {
+      sorted_merge_candidates.push_back(merge_candidate);
+    }
+    computeTopologicalSorting(sorted_merge_candidates);
+
+    // Merge the fragments.
+    Operation* new_fragment_dest = sorted_merge_candidates[0]->getNextNode();
+    FragmentOp new_fragment = dyn_cast<FragmentOp>(*sorted_merge_candidates[0]);
+    for (int i = 1; i < sorted_merge_candidates.size(); ++i) {
+      FragmentOp merge_candidate =
+          dyn_cast<FragmentOp>(*sorted_merge_candidates[i]);
+      if (new_fragment_dest == merge_candidate) {
+        new_fragment_dest = new_fragment_dest->getNextNode();
+      }
+      new_fragment = MergeRegionOps(
+          new_fragment, merge_candidate, rewriter,
+          /*num_static_args=*/0, /*replace_producer_use_in_consumer_block=*/
+          [](OpOperand&, Value) {
+            SDY_CHECK(false) << "Fragment ops shouldn't have free variables";
+          },
+          new_fragment.getOrigin(), new_fragment.getMeshNameAttr(),
+          /*stage_id=*/new_fragment.getStageIdAttr());
+    }
+    SetFragmentInfo(new_fragment, rule->target, rewriter);
+    // TODO(petebu): Consider making the position of the new fragment a
+    // parameter of the rule.
+    rewriter.moveOpBefore(new_fragment, new_fragment_dest);
+    return success();
+  }
+
+ private:
+  std::vector<FragmentOp> GetMergeCandidates(
+      FragmentOp merge_into_fragment, const FragmentMergeRule* rule) const {
+    Block& parent_body =
+        merge_into_fragment->getParentOfType<func::FuncOp>().getBody().front();
+
+    std::vector<FragmentOp> merge_candidates;
+    for (auto fragment : parent_body.getOps<FragmentOp>()) {
+      if (fragment == merge_into_fragment) {
+        continue;
+      }
+
+      // Merge candidates must be on the same mesh.
+      if (fragment.getMeshName() != merge_into_fragment.getMeshName()) {
+        continue;
+      }
+
+      FragmentInfo fragment_info = GetFragmentInfo(fragment);
+      auto it = fragment_merge_rule_map_.find(fragment_info);
+      if (it != fragment_merge_rule_map_.end() && it->second == rule) {
+        merge_candidates.push_back(fragment);
+      }
+    }
+
+    return merge_candidates;
+  }
+
+  const FragmentMergeRuleMap& fragment_merge_rule_map_;
+};
+
+class RuleBasedMergePass
+    : public impl::RuleBasedMergePassBase<RuleBasedMergePass> {
+  using RuleBasedMergePassBase::RuleBasedMergePassBase;
+
+ private:
+  FragmentMergeRuleMap fragment_merge_rule_map_;
+  FrozenRewritePatternSet patterns;
+
+  void runOnFunc(func::FuncOp func) override {
+    if (!IsMpmdFunction(func)) {
+      return;
+    }
+
+    GreedyRewriteConfig config;
+    config.setRegionSimplificationLevel(GreedySimplifyRegionLevel::Disabled);
+    config.enableFolding(false);
+    config.enableConstantCSE(false);
+    if (failed(applyPatternsGreedily(func, patterns, config))) {
+      return signalPassFailure();
+    }
+
+    sortTopologically(&func.getBody().front());
+  }
+
+  LogicalResult initialize(MLIRContext* context) final {
+    for (const FragmentMergeRule& rule : rules) {
+      for (const FragmentInfo& fragment : rule.sources) {
+        SDY_CHECK(!fragment_merge_rule_map_.contains(fragment))
+            << "Fragment " << llvm::to_string(fragment)
+            << " is already part of another merge rule.";
+        fragment_merge_rule_map_[fragment] = &rule;
+      }
+    }
+    RewritePatternSet patternsInternal(context);
+    patternsInternal.add<RuleBasedMergingPattern>(context, rules,
+                                                  fragment_merge_rule_map_);
+    patterns = std::move(patternsInternal);
+    return success();
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/simplify_region_op_base.cc b/shardy/dialect/mpmd/transforms/common/simplify_region_op_base.cc
new file mode 100644
index 0000000..85a50e3
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/simplify_region_op_base.cc
@@ -0,0 +1,235 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/common/simplify_region_op_base.h"
+
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Interfaces/SideEffectInterfaces.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/RegionUtils.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+
+namespace mlir::mpmd {
+
+namespace {
+
+// Returns a BitVector indicating for every source value whether it is a
+// duplicate of a previous source and should therefore be erased, and replaces
+// the corresponding target of each duplicate source with the corresponding
+// target value of the first occurrence of that source.
+BitVector DedupSrcTgtValues(ValueRange src_values, ValueRange tgt_values,
+                            PatternRewriter& rewriter) {
+  llvm::DenseMap<std::pair<Value, Type>, Value> mapping;
+  BitVector erase_values(src_values.size());
+  for (auto src_tgt_value_it :
+       llvm::enumerate(llvm::zip(src_values, tgt_values))) {
+    auto [src_value, tgt_value] = src_tgt_value_it.value();
+    auto key = std::make_pair(src_value, tgt_value.getType());
+    if (Value mapped_tgt = mapping.lookup(key)) {
+      rewriter.replaceAllUsesWith(tgt_value, mapped_tgt);
+      erase_values.set(src_tgt_value_it.index());
+    } else {
+      mapping[key] = tgt_value;
+    }
+  }
+
+  return erase_values;
+}
+
+// Remove all results whose corresponding return operands are block
+// arguments of the fragment, if the result type matches that of the operand
+// corresponding to the block argument, and replace those results with the
+// corresponding operand.
+//
+// In addition, remove operands whose corresponding block arguments have no more
+// uses.
+void RemoveNoopResults(Operation* op, BitVector& erase_operands,
+                       BitVector& erase_results, PatternRewriter& rewriter) {
+  SDY_CHECK_EQ(op->getNumRegions(), 1);
+  Block& block = op->getRegion(0).front();
+  Operation* return_op = block.getTerminator();
+  for (BlockArgument arg : block.getArguments()) {
+    int arg_num = arg.getArgNumber();
+    Value operand = op->getOperand(arg_num);
+    bool should_erase_operand = true;
+    for (OpOperand& use : arg.getUses()) {
+      int use_operand_num = use.getOperandNumber();
+      if (use.getOwner() == return_op &&
+          operand.getType() == op->getResult(use_operand_num).getType()) {
+        rewriter.replaceAllUsesWith(op->getResult(use_operand_num), operand);
+        erase_results.set(use_operand_num);
+      } else {
+        should_erase_operand = false;
+      }
+    }
+    if (should_erase_operand) {
+      erase_operands.set(arg_num);
+    }
+  }
+}
+
+// Checks if the result of an operation can be removed from its producer's
+// results, i.e., if it is unused and if it's removal doesn't cause any op
+// with side effects to be removed.
+//
+// If `result` is the only result of the region, then two things can happen:
+// if the region is recursively pure, then returns `true` so that the caller
+// will remove the whole region, which is safe (as long as `result` is not
+// used); otherwise, if the region is not pure, then removing the result would
+// create a region without results, which isn't well-formed, and therefore we do
+// not allow for the value to be removed.
+
+// In particular, it checks whether the value corresponds to a block
+// argument of the region or whether the nested op producing the result is
+// pure, in case the region results another value produced by an op with
+// side-effects.
+//
+// Example 1:
+//   %r:4 = region_based_op(...) (%arg0, %arg1, %arg2) {
+//      %0 = add %arg1, arg1
+//      %1 = all_gather arg2
+//      %2 = add %1, %1
+//      return %arg0, %0, %1, %2
+//   }
+//
+// When unused: %r#0 can be removed from the fragment's results as its
+// actual producer isn't nested in the region; %r#1 can also also be removed
+// because its actual producer (%0) is pure; %r#2 _cannot_ be removed because
+// it's actual producer (%1) is not pure; and %r#3 can be removed because it's
+// actual producer (%2) is pure, even though it derives from an all-gather.
+//
+// The resulting region is:
+//   %r = region_based_op(...) (%arg0, %arg1, %arg2) {
+//      %1 = all_gather arg2
+//      return %1
+//   }
+void RemoveUnusedResultsFromRegionOp(Operation* region_op,
+                                     BitVector& erase_results,
+                                     PatternRewriter& rewriter) {
+  // Mark any unused result to be erased.
+  for (OpResult result : region_op->getResults()) {
+    if (!result.use_empty()) {
+      // Cannot be removed as it is used.
+      continue;
+    }
+
+    if (region_op->getNumResults() == 1) {
+      if (isPure(region_op)) {
+        // Can be removed as long as this means that the whole region will be
+        // removed. Otherwise this could create a fragment without results,
+        // which isn't valid ATM.
+        // TODO(b/310958300): Revisit this once we can have 0-result fragments.
+        erase_results.set(result.getResultNumber());
+      }
+      // otherwise, keep the value alive so that the fragment's isn't removed.
+      continue;
+    }
+
+    Operation* terminator =
+        result.getOwner()->getRegion(0).front().getTerminator();
+    Operation* defining_op =
+        terminator->getOperand(result.getResultNumber()).getDefiningOp();
+    // The value can be removed if it is a block argument or if its defining op
+    // is pure.
+    if (!defining_op || isPure(defining_op)) {
+      erase_results.set(result.getResultNumber());
+    }
+  }
+}
+
+}  // namespace
+
+LogicalResult SimplifyRegionOp(Operation* op, PatternRewriter& rewriter,
+                               SimplifiedRegionOpCreateFn create_op) {
+  SDY_CHECK_EQ(op->getNumRegions(), 1);
+  Region& region = op->getRegion(0);
+  Block& block = region.front();
+  Operation* return_op = block.getTerminator();
+  bool has_operands = op->getNumOperands() > 0;
+  if (has_operands) {
+    SDY_CHECK_EQ(block.getNumArguments(), op->getNumOperands());
+  }
+  SDY_CHECK_EQ(return_op->getNumOperands(), op->getNumResults());
+
+  BitVector erase_results =
+      DedupSrcTgtValues(/*src_values=*/return_op->getOperands(),
+                        /*tgt_values=*/op->getResults(), rewriter);
+
+  BitVector erase_operands;
+  if (has_operands) {
+    erase_operands =
+        DedupSrcTgtValues(/*src_values=*/op->getOperands(),
+                          /*tgt_values=*/block.getArguments(), rewriter);
+    RemoveNoopResults(op, erase_operands, erase_results, rewriter);
+  }
+
+  RemoveUnusedResultsFromRegionOp(op, erase_results, rewriter);
+
+  // If all results must be erased, we erase the op.
+  if (erase_results.all()) {
+    rewriter.eraseOp(op);
+    return success();
+  }
+
+  if (erase_operands.none() && erase_results.none()) {
+    // No simplification needed.
+    return failure();
+  }
+
+  // NOTE: we need to erase return operands before we erase block arguments
+  // because the former might be a use of the latter.
+  return_op->eraseOperands(erase_results);
+  if (has_operands) {
+    block.eraseArguments(erase_operands);
+  }
+
+  SmallVector<Value> new_operands =
+      FilterRange<Value>(op->getOperands(), erase_operands);
+  SmallVector<Type> new_result_types =
+      FilterRange<Type>(op->getResultTypes(), erase_results);
+  Operation* new_op = create_op(new_result_types, new_operands, erase_results);
+  SDY_CHECK_EQ(new_op->getNumRegions(), 1);
+  Region& new_region = new_op->getRegion(0);
+  new_region.takeBody(region);
+
+  // Simplify the region to make sure we remove any dead code.
+  (void)simplifyRegions(rewriter, new_region);
+
+  // replace all results of `op` that weren't erased with the results of
+  // `new_op` (erased results were already replaced)
+  int new_result_num = 0;
+  for (OpResult result : op->getResults()) {
+    if (!erase_results.test(result.getResultNumber())) {
+      rewriter.replaceAllUsesWith(result, new_op->getResult(new_result_num++));
+    }
+  }
+
+  // Explicitly erase the op. This will cause the rewriter to add the operands
+  // to the worklist, and trigger simplification of the operands' producer
+  // fragments, thus reducing the number of iterations needed.
+  rewriter.eraseOp(op);
+  return success();
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/simplify_region_op_base.h b/shardy/dialect/mpmd/transforms/common/simplify_region_op_base.h
new file mode 100644
index 0000000..23e3153
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/simplify_region_op_base.h
@@ -0,0 +1,75 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_SIMPLIFY_REGION_OP_BASE_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_SIMPLIFY_REGION_OP_BASE_H_
+
+#include <functional>
+
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/TypeRange.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+
+namespace mlir::mpmd {
+
+using SimplifiedRegionOpCreateFn = std::function<Operation*(
+    TypeRange result_types, ValueRange operands, BitVector erased_results)>;
+
+// Simplifies the given `op`. In particular, it:
+//  - deduplicates results, and their corresponding return values;
+//  - deduplicates operands, and their corresponding block arguments, if the op
+//    has any operands;
+//  - removes results whose corresponding return operand is a block argument of
+//    the op;
+//  - removes operands whose corresponding block argument has no more uses (or
+//    didn't have any to begin with); and
+//  - removes results that are unused.
+//
+// NOTE: This method assumes that the op has the same number of results and
+// return values, and if the op has any operands we also assume that it has the
+// same number of operands and block arguments.
+LogicalResult SimplifyRegionOp(Operation* op, PatternRewriter& rewriter,
+                               SimplifiedRegionOpCreateFn create_op);
+
+// A base class for patterns that simplify a given op. See SimplifyRegionOp for
+// more information.
+template <class OpTy>
+class SimplifyRegionOpPatternBase : public OpRewritePattern<OpTy> {
+ public:
+  using OpRewritePattern<OpTy>::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(OpTy op,
+                                PatternRewriter& rewriter) const final {
+    return SimplifyRegionOp(
+        op, rewriter,
+        [&, this](TypeRange result_types, ValueRange operands,
+                  BitVector erased_results) -> Operation* {
+          return createNewOp(op, rewriter, result_types, operands,
+                             erased_results);
+        });
+  }
+
+ protected:
+  virtual OpTy createNewOp(OpTy op, PatternRewriter& rewriter,
+                           TypeRange result_types, ValueRange operands,
+                           BitVector erased_results) const = 0;
+};
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_SIMPLIFY_REGION_OP_BASE_H_
diff --git a/shardy/dialect/mpmd/transforms/common/split_bwd_fragments.cc b/shardy/dialect/mpmd/transforms/common/split_bwd_fragments.cc
new file mode 100644
index 0000000..9a0bf37
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/split_bwd_fragments.cc
@@ -0,0 +1,527 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <algorithm>
+#include <functional>
+#include <iterator>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/IRMapping.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/RegionUtils.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/optimize/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_SPLITBWDFRAGMENTSPASS
+#define GEN_PASS_DEF_SPLITANDPRIORITIZETRANSFERINDEPENDENTCOMPUTATIONSPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+bool IsBackwardsCandidateFragment(FragmentOp fragment) {
+  return IsBackwardFragment(fragment) &&
+         // The next are not strictly needed but make the pass idempotent.
+         !IsSplitDropTransferred(fragment) && !IsSplitKeepTransferred(fragment);
+}
+
+// Finds the results that do not have any transfer ops as users.
+BitVector FindNonTransferredResults(FragmentOp fragment) {
+  BitVector mask;
+  mask.reserve(fragment->getNumResults());
+  for (OpResult value : fragment.getResults()) {
+    mask.push_back(llvm::all_of(
+        value.getUsers(), [](Operation* op) { return !isa<TransferOp>(op); }));
+  }
+  return mask;
+}
+
+// Returns the effective operands of an operation. For an op with regions (that
+// is not isolated from above) this includes both its operands but also any
+// other SSA values from an outer scope referenced inside any inner regions of
+// the op. We use a set vector to preserve the original operand order, which
+// eases testing.
+SetVector<Value> GetEffectiveOperands(Operation* op) {
+  SetVector<Value> operands;
+  operands.insert(op->operand_begin(), op->operand_end());
+  for (Region& region : op->getRegions()) {
+    getUsedValuesDefinedAbove(region, operands);
+  }
+  return operands;
+}
+
+// Collects all operations that have results that have a dataflow to one of the
+// values in the `values` list.
+// NB: `values` are purposefully passed by-value as this function mutates them.
+DenseSet<Operation*> CollectOpsFlowingTo(std::vector<Value> values) {
+  DenseSet<Operation*> ops;
+  while (!values.empty()) {
+    Value value = values.back();
+    values.pop_back();
+
+    Operation* op = value.getDefiningOp();
+    if (!op || ops.contains(op)) continue;
+
+    ops.insert(op);
+    for (Value operand : GetEffectiveOperands(op)) {
+      values.push_back(operand);
+    }
+  }
+  return ops;
+}
+
+// Collect the values that represent root values of computations that (i)
+// flow to the values in the `values` and are either block arguments or produced
+// by the `boundary_ops`.
+SetVector<Value> CollectRootsForValues(
+    std::vector<Value> values, const DenseSet<Operation*>& boundary_ops) {
+  SetVector<Value> roots;
+  while (!values.empty()) {
+    Value value = values.back();
+    values.pop_back();
+
+    Operation* op = value.getDefiningOp();
+    if (!op || boundary_ops.contains(op)) {
+      roots.insert(value);
+    } else {
+      for (Value operand : GetEffectiveOperands(op)) {
+        values.push_back(operand);
+      }
+    }
+  }
+  return roots;
+}
+
+// Splits a list of values by a mask to the elements for which the mask is true,
+// and the elements for which the mask is false.
+std::pair<std::vector<Value>, std::vector<Value>> SplitValuesByMask(
+    ValueRange values, const BitVector& mask) {
+  std::vector<Value> true_results;
+  true_results.reserve(mask.count());
+  std::vector<Value> false_results;
+  false_results.reserve(values.size() - mask.count());
+  for (auto [index, value] : llvm::enumerate(values)) {
+    if (mask[index]) {
+      true_results.push_back(value);
+    } else {
+      false_results.push_back(value);
+    }
+  }
+  return std::make_pair(true_results, false_results);
+}
+
+// Returns `values` filtered to where the mask is set.
+std::vector<Value> FilterByMask(ValueRange values, const BitVector& mask) {
+  std::vector<Value> results;
+  results.reserve(mask.count());
+  for (auto [index, value] : llvm::enumerate(values)) {
+    if (mask[index]) {
+      results.push_back(value);
+    }
+  }
+  return results;
+}
+
+// Maps a set of operations through an IRMapping.
+DenseSet<Operation*> MapOpSet(const DenseSet<Operation*> ops,
+                              const IRMapping& mapping) {
+  DenseSet<Operation*> mapped_ops;
+  for (Operation* op : ops) {
+    mapped_ops.insert(mapping.lookup(op));
+  }
+  return mapped_ops;
+}
+
+// Traverses the region in reverse order, applies the mapping for each op, and
+// erases any (mapped) op that is not a terminator and for which `should_erase`
+// returns true. As such ops are not erased from the region, but rather the
+// image of the region through the mapping.
+void EraseOpsThroughIRMapping(Region& region, IRMapping& mapping,
+                              IRRewriter& rewriter,
+                              std::function<bool(Operation*)> should_erase) {
+  for (Operation& op : llvm::reverse(region.front().getOperations())) {
+    Operation* mapped_op = mapping.lookup(&op);
+    if (!mapped_op->hasTrait<OpTrait::IsTerminator>() &&
+        should_erase(mapped_op)) {
+      rewriter.eraseOp(mapped_op);
+    }
+  }
+}
+
+// Pulls computations out of a fragment based on the `pullable_mask` that
+// corresponds to the results of this fragment.
+//
+// For every result, if the corresponding bit of the mask is set, then we will
+// pull (i.e. extract) as much computation as we can out of this fragment into
+// another fragment -- subject to making sure that we keep any result with the
+// mask unset as result of the original fragment. In the process we may need to
+// pass some "residual" values returned from the first fragment into the second.
+//
+// In pseudocode (and assuming that the pullables and the non-pullables are
+// cleanly separated, just for the sake of simplicity), assume original:
+//
+//  %pullables, %non_pullables = fragment (...) {   // original fragment
+//     ...
+//     return %ps, %nps
+//  }
+// Becomes:
+//  %non_pullables, %residuals = fragment (...) {   // keep non pullables
+//     ...
+//     return %nps, %rs
+//  }
+//
+//  %pullables = fragment (..., %residuals) {       // keep pullables
+//     ...
+//     return %ps
+//  }
+void PullResultsOutOf(IRRewriter& rewriter, FragmentOp fragment,
+                      ArrayRef<Value> pullable_results,
+                      const DenseSet<Operation*>& non_pullable_ops,
+                      const BitVector& pullable_mask) {
+  MLIRContext* context = rewriter.getContext();
+
+  auto make_mesh_type = [&](Value value) -> Type {
+    // We create a fully replicated mesh type as we assume currently that the
+    // pass will run prior to SPMD propagation (otherwise we'd have to create
+    // a distributed type that matches the sharding specification of the value.)
+    return MeshTensorType::getFullyReplicated(
+        context, fragment.getMeshName(),
+        GetMeshOrFail(fragment, fragment.getMeshName()),
+        cast<RankedTensorType>(value.getType()));
+  };
+
+  Region& region = fragment.getRegion();
+
+  // If the non pullable ops and the terminator are all the ops of the fragment
+  // then there is simply nothing to pull.
+  if (non_pullable_ops.size() + 1 ==
+      fragment->getBlock()->getOperations().size()) {
+    return;
+  }
+  // We calculate the residuals as the root values flowing to the pullables.
+  SetVector<Value> residuals = CollectRootsForValues(
+      /*values=*/std::move(pullable_results),
+      /*boundary_ops=*/non_pullable_ops);
+
+  // 1. Create the fragment with only transferred and residual results.
+  // ------------------------------------------------------------------
+
+  // 1.1 Create the fragment return types.
+  std::vector<Type> transfer_fragment_types;
+  transfer_fragment_types.reserve(pullable_mask.size() - pullable_mask.count() +
+                                  residuals.size());
+  for (auto result : fragment.getResults()) {
+    if (!pullable_mask[result.getResultNumber()]) {
+      transfer_fragment_types.push_back(result.getType());
+    }
+  }
+  std::transform(residuals.begin(), residuals.end(),
+                 std::back_inserter(transfer_fragment_types), make_mesh_type);
+
+  // 1.2 Create the fragment.
+  rewriter.setInsertionPointAfter(fragment);
+  FragmentOp transfer_fragment = rewriter.create<FragmentOp>(
+      fragment.getLoc(), transfer_fragment_types, fragment->getOperands(),
+      fragment.getOriginAttr(), fragment.getMeshNameAttr(),
+      fragment.getStageIdAttr());
+  // TODO(jupvfranco): streamline fragment attribute copying in our codebase.
+  CopyAttributes(fragment, transfer_fragment);
+  transfer_fragment->setAttr(kSplitKeepTransferredAttrName,
+                             UnitAttr::get(context));
+
+  // 1.3 Clone the region, fix up the terminator, and clean up ops.
+  {
+    IRMapping mapping;
+    Region& new_region = transfer_fragment.getRegion();
+    region.cloneInto(&new_region, mapping);
+    std::vector<Value> ret_values;
+    auto* terminator = new_region.front().getTerminator();
+    for (OpOperand& ret_value : terminator->getOpOperands()) {
+      if (!pullable_mask[ret_value.getOperandNumber()]) {
+        ret_values.push_back(ret_value.get());
+      }
+    }
+    for (auto res_value : residuals) {
+      ret_values.push_back(mapping.lookup(res_value));
+    }
+    terminator->setOperands(ret_values);
+
+    // Erase the ops that are not in non_pullable_ops through the mapping.
+    DenseSet<Operation*> keep_ops = MapOpSet(non_pullable_ops, mapping);
+    EraseOpsThroughIRMapping(region, mapping, rewriter, [&](Operation* op) {
+      return !keep_ops.contains(op);
+    });
+  }
+
+  // 2. Create the fragment with residual arguments.
+  // -----------------------------------------------
+
+  // 2.1 Create the return types.
+  std::vector<Type> pulled_fragment_types;
+  pulled_fragment_types.reserve(pullable_mask.count());
+  for (auto result : fragment.getResults()) {
+    if (pullable_mask[result.getResultNumber()]) {
+      pulled_fragment_types.push_back(result.getType());
+    }
+  }
+  // 2.2 Create the operands.
+  std::vector<Value> pulled_fragment_operands(fragment->operand_begin(),
+                                              fragment->operand_end());
+  for (auto residual_value :
+       transfer_fragment.getResults().take_back(residuals.size())) {
+    pulled_fragment_operands.push_back(residual_value);
+  }
+
+  // 2.3 Create the actual fragment.
+  rewriter.setInsertionPointAfter(transfer_fragment);
+  FragmentOp pulled_fragment = rewriter.create<FragmentOp>(
+      fragment.getLoc(), pulled_fragment_types, pulled_fragment_operands,
+      fragment.getOriginAttr(), fragment.getMeshNameAttr(),
+      fragment.getStageIdAttr());
+  // TODO(jupvfranco): streamline fragment attribute copying in our codebase.
+  CopyAttributes(fragment, pulled_fragment);
+  pulled_fragment->setAttr(kSplitDropTransferredAttrName,
+                           UnitAttr::get(context));
+
+  // 2.4 Clone the region, fixup terminator and block arguments.
+  {
+    IRMapping mapping;
+    Region& new_region = pulled_fragment.getRegion();
+    region.cloneInto(&new_region, mapping);
+    // Add residual block arguments to the region and replace uses of roots.
+    for (auto res_value : residuals) {
+      rewriter.replaceAllUsesWith(
+          mapping.lookup(res_value),
+          new_region.addArgument(res_value.getType(), res_value.getLoc()));
+    }
+    std::vector<Value> ret_values;
+    Operation* terminator = new_region.front().getTerminator();
+    for (OpOperand& ret_value : terminator->getOpOperands()) {
+      if (pullable_mask[ret_value.getOperandNumber()]) {
+        ret_values.push_back(ret_value.get());
+      }
+    }
+    terminator->setOperands(ret_values);
+
+    // Erase any ops contained in the non_pullable_ops through the mapping.
+    DenseSet<Operation*> keep_ops = MapOpSet(non_pullable_ops, mapping);
+    EraseOpsThroughIRMapping(region, mapping, rewriter, [&](Operation* op) {
+      return keep_ops.contains(op);
+    });
+  }
+
+  // Replace each original fragment result with a result from either the
+  // transfer or the pulled fragment.
+  auto transfer_fragment_it = transfer_fragment->result_begin();
+  auto pulled_fragment_it = pulled_fragment->result_begin();
+  for (OpResult result : fragment.getResults()) {
+    if (!pullable_mask[result.getResultNumber()]) {
+      rewriter.replaceAllUsesWith(result, *transfer_fragment_it);
+      transfer_fragment_it++;
+    } else {
+      rewriter.replaceAllUsesWith(result, *pulled_fragment_it);
+      pulled_fragment_it++;
+    }
+  }
+  rewriter.eraseOp(fragment);
+}
+
+// Splits the fragment into two fragments, with as much computation as possible
+// pulled out of the original fragment into the second.
+void PullResultsMaximally(IRRewriter& rewriter, FragmentOp fragment,
+                          BitVector result_pullable_mask) {
+  const auto [pullable, non_pullable] = SplitValuesByMask(
+      fragment.getBody()->getTerminator()->getOperands(), result_pullable_mask);
+
+  PullResultsOutOf(rewriter, fragment, std::move(pullable),
+                   CollectOpsFlowingTo(std::move(non_pullable)),
+                   result_pullable_mask);
+}
+
+// Finds the results that are returned from the fragment, and marks them in the
+// `result_mask`. Skips values that are already seen.
+void FindAndMarkResults(Value val, DenseSet<Operation*>& seen,
+                        BitVector& result_mask) {
+  for (OpOperand& use : val.getUses()) {
+    if (auto return_op = dyn_cast<ReturnOp>(use.getOwner())) {
+      result_mask.set(use.getOperandNumber());
+      continue;
+    }
+
+    if (seen.contains(use.getOwner())) {
+      continue;
+    }
+    // Only add to `seen` if there are multiple operands,
+    // otherwise this will never be visited again.
+    if (use.getOwner()->getNumOperands() > 1) {
+      seen.insert(use.getOwner());
+    }
+
+    for (auto result : use.getOwner()->getResults()) {
+      FindAndMarkResults(result, seen, result_mask);
+    }
+  }
+}
+
+BitVector FindResultsFlowingFrom(ValueRange values, int num_results) {
+  BitVector result_mask(num_results);
+  DenseSet<Operation*> seen;
+  // An early guess to the number of ops seen.
+  seen.reserve(num_results);
+  for (Value val : values) {
+    FindAndMarkResults(val, seen, result_mask);
+  }
+
+  return result_mask;
+}
+
+DenseSet<Operation*> TransitiveUsersOf(std::vector<Value> values) {
+  DenseSet<Operation*> transitive_users;
+  while (!values.empty()) {
+    Value value = values.back();
+    values.pop_back();
+
+    for (Operation* op : value.getUsers()) {
+      if (transitive_users.contains(op)) {
+        continue;
+      }
+      transitive_users.insert(op);
+      for (Value res : op->getResults()) {
+        values.push_back(res);
+      }
+    }
+  }
+  return transitive_users;
+}
+
+DenseSet<Operation*> OpsNotRelyingOn(std::vector<Value> values,
+                                     FragmentOp fragment) {
+  DenseSet<Operation*> ops_to_avoid = TransitiveUsersOf(std::move(values));
+  DenseSet<Operation*> ops;
+
+  for (Operation& op : fragment.getBody()->getOperations()) {
+    if (ops_to_avoid.contains(&op)) {
+      continue;
+    }
+    ops.insert(&op);
+  }
+
+  return ops;
+}
+
+// Splits the fragment into two fragments, with as much computation as possible
+// pulled out of the original fragment into the first.
+void PullOperandsOutMaximally(IRRewriter& rewriter, FragmentOp fragment,
+                              BitVector arg_pullable_mask) {
+  std::vector<Value> args_to_pull =
+      FilterByMask(fragment.getBody()->getArguments(), arg_pullable_mask);
+  BitVector result_pullable_mask =
+      FindResultsFlowingFrom(args_to_pull, fragment.getNumResults());
+
+  std::vector<Value> pullable = FilterByMask(
+      fragment.getBody()->getTerminator()->getOperands(), result_pullable_mask);
+
+  DenseSet<Operation*> non_pullable_ops =
+      OpsNotRelyingOn(std::move(args_to_pull), fragment);
+
+  if (non_pullable_ops.empty() && result_pullable_mask.all()) {
+    // Everything will be pulled out, so we don't need to split.
+    return;
+  }
+
+  // We achieve the effect of pulling out the operands by pulling out the
+  // results that flow from the args, and maximally setting out the non-pullable
+  // ops.
+  PullResultsOutOf(rewriter, fragment, pullable, non_pullable_ops,
+                   result_pullable_mask);
+}
+
+class SplitBwdFragmentsPass
+    : public impl::SplitBwdFragmentsPassBase<SplitBwdFragmentsPass> {
+  using SplitBwdFragmentsPassBase::SplitBwdFragmentsPassBase;
+
+  void runOnOperation() final {
+    IRRewriter rewriter(&getContext());
+    getOperation().walk([&](FragmentOp fragment) {
+      if (IsBackwardsCandidateFragment(fragment)) {
+        // The pullable values are those that are non-transferred.
+        BitVector pullable_mask = FindNonTransferredResults(fragment);
+        if (!pullable_mask.all() && pullable_mask.any()) {
+          PullResultsMaximally(rewriter, fragment, pullable_mask);
+        }
+      }
+    });
+  }
+};
+
+// Finds the operands that are transfer results.
+BitVector FindTransferredArgs(FragmentOp fragment) {
+  BitVector mask(fragment->getNumOperands());
+  for (OpOperand& operand : fragment->getOpOperands()) {
+    if (isa_and_present<TransferOp>(operand.get().getDefiningOp())) {
+      mask.set(operand.getOperandNumber());
+    }
+  }
+
+  return mask;
+}
+
+bool IsSplitTransferIndependentCandidate(FragmentOp fragment) {
+  return !IsSplitDropTransferred(fragment) && !IsSplitKeepTransferred(fragment);
+}
+
+class SplitAndPrioritizeTransferIndependentComputationsPass
+    : public impl::SplitAndPrioritizeTransferIndependentComputationsPassBase<
+          SplitAndPrioritizeTransferIndependentComputationsPass> {
+  using SplitAndPrioritizeTransferIndependentComputationsPassBase::
+      SplitAndPrioritizeTransferIndependentComputationsPassBase;
+
+  void runOnOperation() final {
+    IRRewriter rewriter(&getContext());
+    Block& func_body = getOperation().front();
+    // Preorder walk to avoid walking into fragment bodies, since we don't need
+    // to.
+    func_body.walk<WalkOrder::PreOrder>([&](FragmentOp fragment) {
+      if (IsSplitTransferIndependentCandidate(fragment)) {
+        // The pullable values are those that are users of transfer
+        // operands.
+        BitVector operand_mask = FindTransferredArgs(fragment);
+        if (!operand_mask.all() && operand_mask.any()) {
+          PullOperandsOutMaximally(rewriter, fragment, operand_mask);
+        }
+      }
+      return WalkResult::skip();
+    });
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/test/BUILD b/shardy/dialect/mpmd/transforms/common/test/BUILD
new file mode 100644
index 0000000..3d7f509
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/BUILD
@@ -0,0 +1,21 @@
+# Lit tests for the MPMD common passes.
+
+load("//shardy:lit.bzl", "glob_lit_tests")
+
+package(default_visibility = ["//visibility:public"])
+
+filegroup(
+    name = "test_data",
+    testonly = True,
+    data = [
+        "//shardy/tools:mpmd_opt",
+        "@llvm-project//llvm:FileCheck",
+    ],
+)
+
+glob_lit_tests(
+    name = "all_tests",
+    data = [":test_data"],
+    driver = "@llvm-project//mlir:run_lit.sh",
+    test_file_exts = ["mlir"],
+)
diff --git a/shardy/dialect/mpmd/transforms/common/test/absorb_inferred_fragments.mlir b/shardy/dialect/mpmd/transforms/common/test/absorb_inferred_fragments.mlir
new file mode 100644
index 0000000..d82bf3c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/absorb_inferred_fragments.mlir
@@ -0,0 +1,461 @@
+// RUN: mpmd_opt %s -mpmd-absorb-inferred-fragments 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// The tests file exercise absortion of inferred fragments by User Defined
+// Fragments (UDFs) and other root fragments.
+
+// CHECK-LABEL: func private @simple_udf_absorbs_inferred_consumer_and_preserves_call_counter
+func.func private @simple_udf_absorbs_inferred_consumer_and_preserves_call_counter
+ (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1) {call_counter = 123 : ui32}
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   return
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1) {call_counter = 123 : ui32} (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // Note: this transfer is the first consumer of the UDF. This won't prevent
+  // merging from happening though.
+  %transfer = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1, %transfer : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @single_udf_producer_merges_all_inferred_consumers
+func.func private @single_udf_producer_merges_all_inferred_consumers
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   return
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %3 = mpmd.fragment<mesh="m1", origin=[]> (%0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1, %2, %3 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @udf_absorbs_consumer_after_transfer_is_moved_to_block_begin
+func.func private @udf_absorbs_consumer_after_transfer_is_moved_to_block_begin
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: transfer %arg1
+  // CHECK-NEXT: fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: transfer
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // Note: this transfer is the first consumer of the UDF. This won't prevent
+  // merging from happening though.
+  %transfer = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  // This is actually the closest producer of the inferred fragment. However,
+  // it doesn't depend on the UDF and it can be moved before the UDF.
+  %arg1_transfer = mpmd.transfer %arg1 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0, %arg1_transfer) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1, %transfer : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// Same as above, but closest producer is a transfer of an inferred fragment,
+// not a block argument.
+// CHECK-LABEL: func private @udf_absorbs_consumer_after_transfer_is_moved_to_producer
+func.func private @udf_absorbs_consumer_after_transfer_is_moved_to_producer
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: %[[C:.*]] = mpmd.fragment<mesh="m2", origin=[]> () ()
+  // CHECK-NEXT:   constant
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: transfer %[[C]]
+  // CHECK-NEXT: fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: transfer
+  // CHECK-NOT:  origin=[]
+
+  // There's nothing to absorb this inferred fragment, so it will remain in the
+  // final program.
+  %c = mpmd.fragment<mesh="m2", origin=[]> () () {
+    %4 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : () -> !mesh_2_tensor_4_8_f32
+
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // Note: this transfer is the first consumer of the UDF. This won't prevent
+  // merging from happening though.
+  %transfer = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  // This is actually the closest producer of the inferred fragment. However,
+  // it doesn't depend on the UDF and it can be moved before the UDF.
+  %c_transfer = mpmd.transfer %c : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0, %c_transfer) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1, %transfer : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// Same as above, but %c is defined after the udf. In this test nothing gets
+// absorbed (see explanation below).
+// CHECK-LABEL: func private @udf_cannot_absorb_because_transfer_cannot_be_moved
+func.func private @udf_cannot_absorb_because_transfer_cannot_be_moved(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // CHECK-NEXT: %[[C:.*]] = mpmd.fragment<mesh="m2", origin=[]> () ()
+  // CHECK-NEXT:   constant
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  %c = mpmd.fragment<mesh="m2", origin=[]> () () {
+    %4 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : () -> !mesh_2_tensor_4_8_f32
+
+  // This is actually the closest producer of the inferred fragment. Although it
+  // doesn't depend on the UDF, it cannot be moved before the UDF because it is
+  // defined after the UDF. And even though there's no reason for %c to be
+  // defined where it is, we haven't yet implemented logic to move it around.
+  // TODO: b/370062636 - We should change the merging algorithm to check if
+  // there are any dependencies between values, instead of simply relying on
+  // program order.
+  // CHECK-NEXT: transfer %[[C]]
+  %c_transfer = mpmd.transfer %c : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // CHECK-NEXT: fragment<mesh="m1", origin=[]>
+  // CHECK-NEXT:   multiply
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0, %c_transfer) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @inferred_root_absorbs_inferred_producer_and_then_is_absorbed_by_udf
+func.func private @inferred_root_absorbs_inferred_producer_and_then_is_absorbed_by_udf
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+  // CHECK-NEXT:   constant
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   divide
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   return
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // This is the closest producer of the inferred consumer of the UDF. However,
+  // the inferred consumer is also a root fragment (only used by the consumer),
+  // so everything will be merged.
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.constant dense<2.0> : tensor<4x8xf32>
+    %5 = stablehlo.divide %arg2, %4 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%0, %1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @closest_of_two_udf_producers_absorbs_inferred_consumer
+func.func private @closest_of_two_udf_producers_absorbs_inferred_consumer
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: %[[F:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: fragment<mesh="m1", origin=["g"]> (%arg0, %arg1, %[[F]])
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   divide
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NOT: origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["g"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // Although it has two UDF fragments as producers, it must be merged to the
+  // one that it's closer to, i.e., g.
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%0, %1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.divide %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @simple_udf_absorbs_inferred_producer
+func.func private @simple_udf_absorbs_inferred_producer
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: transfer
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   return
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // Note this transfer is the first producer of the UDF. This won't prevent
+  // merging from happening though.
+  %arg1_transfer = mpmd.transfer %arg1 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg1_transfer, %0) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @udf_absorbs_multiple_inferred_producers
+func.func private @udf_absorbs_multiple_inferred_producers
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: transfer
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   divide
+  // CHECK-NEXT:   return
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %arg1_transfer = mpmd.transfer %arg1 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg1_transfer, %0, %1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %4 = stablehlo.divide %arg3, %arg4 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @closest_of_two_udf_consumers_absorb_inferred_producer
+func.func private @closest_of_two_udf_consumers_absorb_inferred_producer
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["g"]>
+  // CHECK-NEXT:   divide
+  // CHECK-NEXT:   return
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["g"]> (%0, %1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.divide %arg3, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @delay_transfer_to_return_so_udf_absorbs_inferred_producer
+func.func private @delay_transfer_to_return_so_udf_absorbs_inferred_producer
+  (%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   divide
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: transfer
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // This is the closest consumer of the inferred fragment. However, it's not
+  // needed by the udf and can be postponed.
+  %transfer = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["f"]> (%0, %arg0) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.divide %arg3, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2, %transfer : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @udf_cannot_absorb_because_transfer_is_closest_consumer_and_cannot_be_postponed
+func.func private @udf_cannot_absorb_because_transfer_is_closest_consumer_and_cannot_be_postponed
+  (%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: transfer
+  // CHECK-NEXT: transfer
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   divide
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // This is the closest consumer of the inferred fragment and the closest
+  // consumer cannot be postponed. The program stays as it is.
+  %transfer_to_m2 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  %transfer_to_m1 = mpmd.transfer %transfer_to_m2 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["f"]> (%0, %arg0) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.divide %arg3, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2, %transfer_to_m1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @transfer_gets_removed_so_udf_absorbs_inferred_producer
+func.func private @transfer_gets_removed_so_udf_absorbs_inferred_producer
+  (%arg0: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   divide
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NOT: transfer
+  // CHECK-NOT:  origin=[]
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // This is the closest consumer of the inferred fragment. However, it's not
+  // needed by the udf and can be postponed.
+  %transfer = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["f"]> (%0, %arg0) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.divide %arg3, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/call_inline.mlir b/shardy/dialect/mpmd/transforms/common/test/call_inline.mlir
new file mode 100644
index 0000000..0f2b720
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/call_inline.mlir
@@ -0,0 +1,93 @@
+// RUN: mpmd_opt %s -mpmd-call-inline -symbol-dce -split-input-file 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: module
+// CHECK: func.func @test_no_inline_func_call
+func.func @test_no_inline_func_call(%arg0 : !mesh_1_tensor) -> !mesh_1_tensor attributes {
+  topology=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>>}
+{
+  // The mpmd-custom-inline pass must not inline call ops from other dialects.
+  // CHECK-NEXT: %[[CALL:.*]] = call @f(%arg0)
+  // CHECK-NEXT: return %[[CALL]]
+  %0 = call @f(%arg0) : (!mesh_1_tensor) -> !mesh_1_tensor
+  func.return %0 : !mesh_1_tensor
+}
+
+func.func @f(%arg0 : !mesh_1_tensor) -> !mesh_1_tensor
+    attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=4]>} {
+  func.return %arg0 : !mesh_1_tensor
+}
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: module
+// CHECK: func.func @test_no_inline_of_fragment_calls
+func.func @test_no_inline_of_fragment_calls(%arg0 : !mesh_tensor) -> !mesh_tensor attributes {
+  topology=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  // The mpmd-custom-inline pass must not inline fragment_calls.
+  // CHECK-NEXT: %[[FC:.*]] = mpmd.fragment_call<mesh="m1", origin=[]> @f(%arg0)
+  // CHECK-NEXT: return %[[FC]]
+  %0 = mpmd.fragment_call<mesh="m1", origin=[]> @f(%arg0) : (!mesh_tensor) -> !mesh_tensor
+  func.return %0 : !mesh_tensor
+}
+
+func.func @f(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {mesh_shape = #sdy.mesh<["x"=2]>} {
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// -----
+
+// CHECK-LABEL: module
+// CHECK: func.func @test_inline_of_mpmd_calls
+func.func @test_inline_of_mpmd_calls(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>>}
+{
+  // All mpmd.calls must be inlined. Only fragments and fragment_calls
+  // will get a call_counter.
+  // CHECK-NOT: mpmd.call
+  // CHECK-NEXT: %[[ASSIGN0:.*]] = mpmd.assign %arg0
+  // CHECK-NEXT: %[[FRAG0:.*]] = mpmd.fragment<mesh="mesh1", origin=["f"]> (%[[ASSIGN0]]) {call_counter = 0 : ui32} (%arg1: tensor<3x5xf32>) {
+  // CHECK-NEXT:   stablehlo.add %arg1, %arg1 : tensor<3x5xf32>
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[UNASSIGN0:.*]] = mpmd.unassign %[[FRAG0]]
+  %0 = mpmd.call @f(%arg0) {call_counter = 0 : ui32} : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  // CHECK-NEXT: %[[ASSIGN1:.*]] = mpmd.assign %[[UNASSIGN0]]
+  // CHECK-NEXT: %[[FRAG1:.*]] = mpmd.fragment<mesh="mesh1", origin=["f"]> (%[[ASSIGN1]]) {call_counter = 1 : ui32} (%arg1: tensor<3x5xf32>) {
+  // CHECK-NEXT:   stablehlo.add %arg1, %arg1 : tensor<3x5xf32>
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[UNASSIGN1:.*]] = mpmd.unassign %[[FRAG1]]
+  %1 = mpmd.call @f(%0) {call_counter = 1 : ui32} : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  // This call doesn't have a call_counter attribute. When we inline it's body,
+  // the inlined ops will not have the attribute either.
+  // CHECK-NEXT: %[[ASSIGN2:.*]] = mpmd.assign %[[UNASSIGN1]]
+  // CHECK-NEXT: %[[FRAG2:.*]] = mpmd.fragment<mesh="mesh1", origin=["f"]> (%[[ASSIGN2]]) (%arg1: tensor<3x5xf32>) {
+  // CHECK-NEXT:   stablehlo.add %arg1, %arg1 : tensor<3x5xf32>
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[UNASSIGN2:.*]] = mpmd.unassign %[[FRAG2]]
+  %2 = mpmd.call @f(%1) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  // CHECK-NEXT: return %[[UNASSIGN2]]
+  return %2 : tensor<3x5xf32>
+}
+
+!mesh_tensor = !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>
+
+// CHECK-NOT: func.func @f
+func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>>}
+{
+  %0 = mpmd.assign %arg0 : (tensor<3x5xf32>) -> !mesh_tensor
+  %1 = mpmd.fragment<mesh="mesh1", origin=["f"]> (%0) (%arg1: tensor<3x5xf32>) {
+    %a = stablehlo.add %arg1, %arg1 : tensor<3x5xf32>
+    mpmd.return %a : tensor<3x5xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  %2 = mpmd.unassign %1 : (!mesh_tensor) -> tensor<3x5xf32>
+  return %2 : tensor<3x5xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/copy_constants.mlir b/shardy/dialect/mpmd/transforms/common/test/copy_constants.mlir
new file mode 100644
index 0000000..9fcd891
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/copy_constants.mlir
@@ -0,0 +1,76 @@
+// RUN: mpmd_opt %s -mpmd-copy-constants 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @fragment_to_fragments()
+func.func @fragment_to_fragments() -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2, "y"=4]>>>}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f0"]> ()
+  // CHECK-NEXT:   constant
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"]> (%
+  // CHECK-NEXT:   %[[C:.*]] = stablehlo.constant
+  // CHECK-NEXT:   stablehlo.add %[[C]], %[[C]]
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f2"]> (%
+  // CHECK-NEXT:   %[[C:.*]] = stablehlo.constant
+  // CHECK-NEXT:   stablehlo.add %[[C]], %[[C]]
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+
+  %0 = mpmd.fragment<mesh="m1", origin=["f0"]> () () {
+    %c = stablehlo.constant dense<8.000000e+00> : tensor<4x8xf32>
+    mpmd.return %c : tensor<4x8xf32>
+  } : () -> !mesh_1_tensor
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%0) (%arg0: tensor<4x8xf32>) {
+    %add = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+    mpmd.return %add : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %2 = mpmd.fragment<mesh="m1", origin=["f2"]> (%0) (%arg0: tensor<4x8xf32>) {
+    %add = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+    mpmd.return %add : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  return %0, %1, %2 : !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @through_transfers
+func.func @through_transfers() -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_2_tensor)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2, "y"=4]>>, <"m2" : <["x"=2, "y"=4]>>>}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f0"]> ()
+  // CHECK-NEXT:   constant
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: transfer
+  // CHECK-NEXT: transfer
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"]>
+  // CHECK-NEXT:   %[[C:.*]] = stablehlo.constant
+  // CHECK-NEXT:   stablehlo.add %[[C]], %[[C]]
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: mpmd.fragment<mesh="m2", origin=["f2"]>
+  // CHECK-NEXT:   %[[C:.*]] = stablehlo.constant
+  // CHECK-NEXT:   stablehlo.add %[[C]], %[[C]]
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+
+  %0 = mpmd.fragment<mesh="m1", origin=["f0"]> () () {
+    %c = stablehlo.constant dense<8.000000e+00> : tensor<4x8xf32>
+    mpmd.return %c : tensor<4x8xf32>
+  } : () -> !mesh_1_tensor
+  %t1 = mpmd.transfer %0 : (!mesh_1_tensor) -> !mesh_2_tensor
+  %t2 = mpmd.transfer %t1 : (!mesh_2_tensor) -> !mesh_1_tensor
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%t2) (%arg0: tensor<4x8xf32>) {
+    %add = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+    mpmd.return %add : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %2 = mpmd.fragment<mesh="m2", origin=["f2"]> (%t1) (%arg0: tensor<4x8xf32>) {
+    %add = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+    mpmd.return %add : tensor<4x8xf32>
+  } : (!mesh_2_tensor) -> !mesh_2_tensor
+  return %0, %1, %2 : !mesh_1_tensor, !mesh_1_tensor, !mesh_2_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/custom_merge_forward_with_backward.mlir b/shardy/dialect/mpmd/transforms/common/test/custom_merge_forward_with_backward.mlir
new file mode 100644
index 0000000..5b848ac
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/custom_merge_forward_with_backward.mlir
@@ -0,0 +1,89 @@
+// RUN: mpmd_opt %s -mpmd-merge-forward-with-backward 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+
+// CHECK-LABEL: func @merge_forward_producer_with_backward_consumer
+func.func @merge_forward_producer_with_backward_consumer(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32) attributes {
+    "topology"=#topology} {
+  // CHECK: mpmd.fragment<mesh="m2", origin=["f2"]> (%arg1)
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1", "f1"(1)]> (%arg0)
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // This fragment is on a different mesh and should not block merging.
+  %2 = mpmd.fragment<mesh="m2", origin=["f2"]> (%arg1)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1, %2 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @do_not_merge_if_consumer_fragment_not_backward
+func.func @do_not_merge_if_consumer_fragment_not_backward(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#topology} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f2"]> (%0)
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %0, %1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @do_not_merge_if_producer_fragment_not_forward
+func.func @do_not_merge_if_producer_fragment_not_forward(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#topology} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0)
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f2"]> (%0)
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %0, %1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @do_not_merge_if_consumer_not_immediately_after_producer
+func.func @do_not_merge_if_consumer_not_immediately_after_producer(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+   "topology"=#topology} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"]>
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f2"]>
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"(1)]>
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // This fragment is on the same mesh which makes consumer not immediately after producer.
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1, %2 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/erase_unused_callee_block_arguments.mlir b/shardy/dialect/mpmd/transforms/common/test/erase_unused_callee_block_arguments.mlir
new file mode 100644
index 0000000..fc9121c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/erase_unused_callee_block_arguments.mlir
@@ -0,0 +1,193 @@
+// RUN: mpmd_opt %s -mpmd-erase-unused-callee-block-arguments 2>&1 | FileCheck %s
+
+!mesh2_t = !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+
+// CHECK-LABEL: func @used_by_return_only
+func.func @used_by_return_only(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> !mesh2_t
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[CALL1:.*]] = mpmd.call @f(%arg0) {call_counter = 0 : ui32}
+  // CHECK-NEXT: %[[CALL2:.*]] = mpmd.call @f(%[[CALL1]])
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign %[[CALL2]]
+  // CHECK-NEXT: return %[[ASSIGN]]
+  %0:2 = mpmd.call @f(%arg0, %arg1) {call_counter = 0 : ui32} : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  %1:2 = mpmd.call @f(%0#0, %0#1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  %2 = mpmd.assign %1#0 : (tensor<3x5xf32>) -> !mesh2_t
+  return %2 : !mesh2_t
+}
+
+// CHECK-LABEL: func private @f
+// CHECK-SAME: (%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+func.func private @f(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg0
+  // CHECK-NEXT: return %[[ADD]] : tensor<3x5xf32>
+  %1 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  // %arg1 is only used by the return.
+  return %1, %arg1 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func @call_op_is_not_needed
+func.func @call_op_is_not_needed(%arg0: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: return %arg0
+  %0 = mpmd.call @g(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  %1 = mpmd.call @g(%0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %1 : tensor<3x5xf32>
+}
+
+// CHECK-NOT: @g
+func.func private @g(%arg0: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // %arg1 is only used by the return.
+  return %arg0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func @arg_is_completely_unused
+func.func @arg_is_completely_unused(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @h(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  // CHECK-NEXT: return %[[CALL]] : tensor<3x5xf32>
+  %0 = mpmd.call @h(%arg0, %arg1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func private @h
+// CHECK-SAME: (%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+func.func private @h(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg0
+  // CHECK-NEXT: return %[[ADD]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func @duplicate_operand_in_call
+func.func @duplicate_operand_in_call(%arg0: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @i(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  // CHECK-NEXT: return %[[CALL]] : tensor<3x5xf32>
+  // Only the second use of %arg0 should be erased from the call op's operands.
+  %0 = mpmd.call @i(%arg0, %arg0) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func private @i
+// CHECK-SAME: (%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+func.func private @i(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg0
+  // CHECK-NEXT: return %[[ADD]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// This test is a noop.
+// CHECK-LABEL: func @arg_is_used_by_op
+func.func @arg_is_used_by_op(%arg0: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @j(%arg0, %arg0) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<3x5xf32>
+  // CHECK-NEXT: return %[[CALL]] : tensor<3x5xf32>
+  %0 = mpmd.call @j(%arg0, %arg0) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func private @j
+// CHECK-SAME: (%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>) -> tensor<3x5xf32>
+func.func private @j(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[ADD0:.*]] = stablehlo.add %arg0, %arg0
+  // CHECK-NEXT: %[[ADD1:.*]] = stablehlo.add %arg1, %arg1
+  // CHECK-NEXT: return %[[ADD0]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  // arg1 is actually used and therefore we cannot erase it.
+  %1 = stablehlo.add %arg1, %arg1 : tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func private @not_called
+// CHECK-SAME: (%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>) -> tensor<3x5xf32>
+func.func private @not_called(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[ADD0:.*]] = stablehlo.add %arg0, %arg0
+  // CHECK-NEXT: return %[[ADD0]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func @call_w_many_results
+func.func @call_w_many_results(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> !mesh2_t
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[CALL1:.*]]:2 = mpmd.call @k(%arg0)
+  // CHECK-NEXT: %[[CALL2:.*]]:2 = mpmd.call @k(%[[CALL1]]#0)
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign %[[CALL2]]#0
+  // CHECK-NEXT: return %[[ASSIGN]]
+  %0:4 = mpmd.call @k(%arg0, %arg1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+  %1:4 = mpmd.call @k(%0#0, %0#1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+  %2 = mpmd.assign %1#0 : (tensor<3x5xf32>) -> !mesh2_t
+  return %2 : !mesh2_t
+}
+
+// CHECK-LABEL: func private @k
+// CHECK-SAME: (%arg0: tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+func.func private @k(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg0
+  // CHECK-NEXT: return %[[ADD]], %[[ADD]]
+  %1 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  // %arg1 is only used by the return.
+  return %1, %arg1, %arg1, %1 : tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func @call_inside_for_loop
+func.func @call_inside_for_loop(%arg0: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: mpmd.for (%arg0) {iterations = 3 : ui32, unroll_factor = 3 : ui32} (%arg1: tensor<3x5xf32>, %index: tensor<ui32>) {
+  // CHECK-NEXT:   %[[CALL:.*]] = mpmd.call @l(%arg1) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  // CHECK-NEXT:   return %[[CALL]] : tensor<3x5xf32>
+  // CHECK-NEXT: } : tensor<3x5xf32>
+  %0 = mpmd.for (%arg0) {iterations = 3 : ui32, unroll_factor = 3 : ui32} (%arg1: tensor<3x5xf32>, %index: tensor<ui32>) {
+    %1 = mpmd.call @l(%arg1, %index) : (tensor<3x5xf32>, tensor<ui32>) -> tensor<3x5xf32>
+    mpmd.return %1 : tensor<3x5xf32>
+  } : tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func private @l
+// CHECK-SAME: (%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+func.func private @l(%arg0: tensor<3x5xf32>, %arg1: tensor<ui32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg0
+  // CHECK-NEXT: return %[[ADD]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/fragment_dce.mlir b/shardy/dialect/mpmd/transforms/common/test/fragment_dce.mlir
new file mode 100644
index 0000000..d8f5c3f
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/fragment_dce.mlir
@@ -0,0 +1,111 @@
+// RUN: mpmd_opt %s -mpmd-fragment-dce 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @unused_operand
+func.func @unused_operand(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+// CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+// CHECK-SAME:   (%arg2: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[FRAGMENT]]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+  func.return %0 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @unused_result
+func.func @unused_result(%arg0: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+// CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+// CHECK-SAME:   (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[FRAGMENT]]
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+      %1 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      // This value is not used outside the fragment. It will be removed.
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (!mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor)
+  func.return %0#0 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @unused_result_causes_operand_to_be_removed
+func.func @unused_result_causes_operand_to_be_removed(%arg0: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+// CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+// CHECK-SAME:   (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[FRAGMENT]]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+      mpmd.return %arg2 : tensor<4x8xf32>
+    } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %1:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %0)
+    (%arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>) {
+      %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+      // This value is not used outside the fragment. It will be removed and
+      // so it the fragment that produces the operand re to %arg2.
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (!mesh_1_tensor, !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor)
+  func.return %1#0 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @pure_ops_are_removed_but_side_effecting_ops_are_not
+func.func @pure_ops_are_removed_but_side_effecting_ops_are_not(%arg0: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+// CHECK-NEXT: %[[F:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[CC:.*]] = stablehlo.custom_call @Sharding(%arg1)
+// CHECK-NEXT:   mpmd.return %arg1
+// CHECK-NEXT: }
+  %1:3 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+      // This is an unused pure op and will be removed.
+      %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+      // This is an unused side-effecting op and will not be removed.
+      %2 = stablehlo.custom_call @Sharding(%arg1) {has_side_effect = true} : (tensor<4x8xf32>) -> tensor<4x8xf32>
+      mpmd.return %1, %2, %arg1 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+    } : (!mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+  func.return %1#2 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @transfer_becomes_unused
+func.func @transfer_becomes_unused(%arg0: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+// CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+// CHECK-SAME:   (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[FRAGMENT]]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+      mpmd.return %arg2 : tensor<4x8xf32>
+    } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %t = mpmd.transfer %0 : (!mesh_1_tensor) -> !mesh_1_tensor
+  %1:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %t)
+    (%arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>) {
+      %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+      // This value is not used outside the fragment. It will be removed and
+      // so it the fragment that produces the operand re to %arg2.
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (!mesh_1_tensor, !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor)
+  func.return %1#0 : !mesh_1_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/fragment_dedup.mlir b/shardy/dialect/mpmd/transforms/common/test/fragment_dedup.mlir
new file mode 100644
index 0000000..7b0d784
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/fragment_dedup.mlir
@@ -0,0 +1,67 @@
+// RUN: mpmd_opt %s -mpmd-fragment-dedup 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_1_tensor_dist_x = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+!mesh_1_tensor_dist_y = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"y"}, {?}]>>
+#topology =#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>
+
+module {
+
+// CHECK-LABEL: func @duplicate_operands
+func.func @duplicate_operands(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor, %arg2: !mesh_1_tensor)
+  -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+  attributes {"topology"=#topology} {
+  %0:3 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1, %arg2, %arg0, %arg2, %arg2)
+    (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>,
+     %arg6: tensor<4x8xf32>, %arg7: tensor<4x8xf32>, %arg8: tensor<4x8xf32>) {
+    // %arg3, %arg4, %arg5 are the unique operands. They should be used instead of
+    // %arg6, %arg7, %arg8.
+    // CHECK: %1 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+    // CHECK: %2 = stablehlo.add %arg5, %arg3 : tensor<4x8xf32>
+    // CHECK: %3 = stablehlo.add %arg5, %arg5 : tensor<4x8xf32>
+    %1 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+    %2 = stablehlo.add %arg5, %arg6 : tensor<4x8xf32>
+    %3 = stablehlo.add %arg7, %arg8 : tensor<4x8xf32>
+    mpmd.return %1, %2, %3 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor,
+       !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+      -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+  func.return %0#0, %0#1, %0#2 : !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @duplicate_results
+func.func @duplicate_results(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+  -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+    attributes { "topology"=#topology} {
+  // CHECK: %[[FRAGMENT:.*]]:4 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+  %0:4 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %1, %1, %1, %1 :
+      tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>,
+      tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor)
+      -> (!mesh_1_tensor, !mesh_1_tensor,
+          !mesh_1_tensor, !mesh_1_tensor)
+  // Only use the first result since all the others are duplicates.
+  // CHECK: %[[FRAGMENT]]#0, %[[FRAGMENT]]#0, %[[FRAGMENT]]#0, %[[FRAGMENT]]#0
+  func.return %0#0, %0#1, %0#2, %0#3 :
+    !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor,
+    !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @duplicate_results_but_with_different_types
+func.func @duplicate_results_but_with_different_types(%arg0: !mesh_1_tensor_dist_x) -> (!mesh_1_tensor_dist_x, !mesh_1_tensor_dist_y)
+  attributes {"topology"=#topology}
+{
+  // CHECK-NEXT: %[[F:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) (%arg1: tensor<4x8xf32>)
+  // CHECK-NEXT:   mpmd.return %arg1, %arg1 : tensor<4x8xf32>, tensor<4x8xf32>
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[F]]#0, %[[F]]#1
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1, %arg1 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_dist_x) -> (!mesh_1_tensor_dist_x, !mesh_1_tensor_dist_y)
+  func.return %0#0, %0#1 : !mesh_1_tensor_dist_x, !mesh_1_tensor_dist_y
+}
+
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/fragment_dedup_and_dce.mlir b/shardy/dialect/mpmd/transforms/common/test/fragment_dedup_and_dce.mlir
new file mode 100644
index 0000000..b647730
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/fragment_dedup_and_dce.mlir
@@ -0,0 +1,83 @@
+// RUN: mpmd_opt %s -mpmd-fragment-dedup -mpmd-fragment-dce 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+#topology =#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["y"=2]>>>
+!mesh_1_tensor_dist_x = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+!mesh_1_tensor_dist_y = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"y"}, {?}]>>
+
+// CHECK-LABEL: func @duplicate_operands
+func.func @duplicate_operands(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor, %arg2: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+// CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1, %arg2)
+// CHECK-SAME:   (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg3, %arg4
+// CHECK-NEXT:   %[[MUL:.*]] = stablehlo.multiply %arg5, %arg3
+// CHECK-NEXT:   %[[SUB:.*]] = stablehlo.subtract %[[ADD]], %arg5
+// CHECK-NEXT:   %[[DIV:.*]] = stablehlo.divide %[[MUL]], %arg5
+// CHECK-NEXT:   %[[POW:.*]] = stablehlo.power %[[SUB]], %[[DIV]]
+// CHECK-NEXT:   mpmd.return %[[POW]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[FRAGMENT]]
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1, %arg2, %arg0, %arg2, %arg2)
+    (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>,
+     %arg6: tensor<4x8xf32>, %arg7: tensor<4x8xf32>, %arg8: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+    %2 = stablehlo.multiply %arg5, %arg6 : tensor<4x8xf32>
+    %3 = stablehlo.subtract %1, %arg7 : tensor<4x8xf32>
+    %4 = stablehlo.divide %2, %arg8 : tensor<4x8xf32>
+    %5 = stablehlo.power %3, %4 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor,
+       !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+      -> !mesh_1_tensor
+  func.return %0 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @duplicate_results
+func.func @duplicate_results(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+  -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor,
+      !mesh_1_tensor)
+    attributes { "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+// CHECK-NEXT: %[[FRAGMENT:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+// CHECK-SAME:   (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:   %[[MUL:.*]] = stablehlo.multiply %arg2, %arg3
+// CHECK-NEXT:   mpmd.return %[[ADD]], %[[MUL]]
+// CHECK-NEXT: }
+  %0:4 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    %2 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %1, %2, %1, %2 :
+      tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>,
+      tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor)
+      -> (!mesh_1_tensor, !mesh_1_tensor,
+          !mesh_1_tensor, !mesh_1_tensor)
+  func.return %0#0, %0#1, %0#2, %0#3 :
+    !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor,
+    !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @duplicate_operands_and_results
+func.func @duplicate_operands_and_results(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+  -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+// CHECK-NEXT: %[[FRAGMENT:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+// CHECK-SAME:   (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:   %[[MUL:.*]] = stablehlo.multiply %[[ADD]], %arg2
+// CHECK-NEXT:   mpmd.return %[[ADD]], %[[MUL]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[FRAGMENT]]#0, %[[FRAGMENT]]#1
+  %0:3 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1, %arg0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    %2 = stablehlo.multiply %1, %arg4 : tensor<4x8xf32>
+    mpmd.return %1, %2, %2 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+      -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+  func.return %0#0, %0#1, %0#2 :
+    !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/from_unroll_to_call_counter.mlir b/shardy/dialect/mpmd/transforms/common/test/from_unroll_to_call_counter.mlir
new file mode 100644
index 0000000..966e899
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/from_unroll_to_call_counter.mlir
@@ -0,0 +1,29 @@
+// RUN: mpmd_opt %s -mpmd-from-unroll-to-call-counter 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func.func @only_calls_are_annotated
+func.func @only_calls_are_annotated(%arg0: tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>}
+{
+
+  // CHECK-NEXT: %[[C1:.*]] = mpmd.call @f(%arg0) {call_counter = 0 : ui32} :
+  %0 = mpmd.call @f(%arg0) {unroll_counter = 0 : ui32} : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  // CHECK-NEXT: %[[C2:.*]] = mpmd.call @f(%[[C1]]) {call_counter = 1 : ui32} :
+  %1 = mpmd.call @f(%0) {unroll_counter = 1 : ui32} : (tensor<3x5xf32>) -> tensor<3x5xf32>
+
+  // This call doesn't have an unroll_counter, so it doesn't get a call_counter
+  // either.
+  // CHECK-NEXT: %[[C3:.*]] = mpmd.call @f(%[[C2]]) :
+  %2 = mpmd.call @f(%1) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+
+  // Any other op that is not a call doesn't get a call_counter either, even if
+  // it has an unroll_counter.
+  // CHECK-NEXT: mpmd.assign {unroll_counter = 2 : ui32}
+  %3 = mpmd.assign {unroll_counter = 2 : ui32} %2 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+  return %3 : !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+}
+
+func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>}
+{
+  return %arg0 : tensor<3x5xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/merge_inferred_any_consumer.mlir b/shardy/dialect/mpmd/transforms/common/test/merge_inferred_any_consumer.mlir
new file mode 100644
index 0000000..033c852
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/merge_inferred_any_consumer.mlir
@@ -0,0 +1,45 @@
+// RUN: mpmd_opt %s -mpmd-merge-inferred-fragments=merge-any-consumer=true 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @inferred_fragment_is_last_user
+func.func @inferred_fragment_is_last_user(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+  // CHECK-NEXT:   stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+  // CHECK-NEXT:   stablehlo.subtract %3, %3 : tensor<4x8xf32>
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // This is not a mergeable consumer as both fragments are user-defined fragments.
+  // CHECK-NEXT:  mpmd.fragment<mesh="m1", origin=["g"]>
+  %1 = mpmd.fragment<mesh="m1", origin=["g"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // The transfer won't impact merging as this is the merge-inferred pass
+  %2 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  // An inferred fragment that can be merged with the producer of its operand.
+  // CHECK-NOT: mpmd.fragment<mesh="m1", origin=[]>
+  %3 = mpmd.fragment<mesh="m1", origin=[]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %5 = stablehlo.subtract %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %3, %2, %1 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments.mlir b/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments.mlir
new file mode 100644
index 0000000..c542240
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments.mlir
@@ -0,0 +1,499 @@
+// RUN: mpmd_opt %s -mpmd-merge-inferred-fragments 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @producer_user_is_inter_mesh_transfer_consumer_is_inferred
+func.func @producer_user_is_inter_mesh_transfer_consumer_is_inferred(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: %[[MERGED_FRAGMENT:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+// CHECK:      %[[TRANSFER:.*]] = mpmd.transfer %[[MERGED_FRAGMENT]]#0
+// CHECK-NEXT: mpmd.fragment<mesh="m2", origin=["use_transfer"]> (%[[TRANSFER]])
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4, %arg3 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0#0)
+    (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.transfer %0#1 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  // If the transfer was only used by the func return it wouldn't block merging.
+  %3 = mpmd.fragment<mesh="m2", origin=["use_transfer"]> (%2)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  func.return %1, %3 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @noop_fragment_doesnt_prevent_merging
+func.func @noop_fragment_doesnt_prevent_merging(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // Note: when merging two fragments, in which one of them is a noop, we use
+  // the name of the other fragment as a name of the resulting fragment.
+  // CHECK-NEXT: %[[MERGED_FRAGMENT:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+  // CHECK:      %[[TRANSFER:.*]] = mpmd.transfer %[[MERGED_FRAGMENT]]#0
+  // CHECK-NEXT: mpmd.fragment<mesh="m2", origin=["use_transfer"]> (%[[TRANSFER]])
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4, %arg3 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0#0)
+    (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.transfer %0#1 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  // If the transfer was only used by the func return it wouldn't block merging.
+  %3 = mpmd.fragment<mesh="m2", origin=["use_transfer"]> (%2)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  func.return %1, %3 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @consumer_operand_is_inter_mesh_transfer
+func.func @consumer_operand_is_inter_mesh_transfer_producer_is_inferred(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg1
+// CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]> (%arg0, %[[TRANSFER]])
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.transfer %arg1 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["f"]> (%0, %1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @both_fragments_are_negligible
+func.func @both_fragments_are_negligible(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: %[[MERGED_FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+// CHECK-SAME:   (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   mpmd.return %arg1
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[MERGED_FRAGMENT]]
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @consumer_origin_is_empty
+func.func @consumer_origin_is_empty(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: %[[MERGED_FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+// CHECK-SAME:   (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   mpmd.return %arg1
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[MERGED_FRAGMENT]]
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @origins_are_empty
+func.func @origins_are_empty(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: %[[MERGED_FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+// CHECK-SAME:   (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   mpmd.return %arg1
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[MERGED_FRAGMENT]]
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// Same as @small_inferred_fragment_is_cloned in mpmd_merge_fragments_with_cloning.mlir.
+// However, the pass tested in this file does not allow for fragments to be cloned.
+
+// CHECK-LABEL: func @small_inferred_fragment_is_not_cloned
+func.func @small_inferred_fragment_is_not_cloned(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[FRAG1:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply
+  // CHECK-NEXT:   return %[[ADD]], %[[MULT]]
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[FRAG2:.*]] = mpmd.fragment<mesh="m1", origin=["g"]> (%[[FRAG1]]#0)
+  // CHECK-NEXT:   stablehlo.add
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[FRAG1]]#1, %[[FRAG2]]
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=["g"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1, %2 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @inferred_producer_can_be_merged_into_stage
+func.func @inferred_producer_can_be_merged_into_stage(%arg0: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0)
+  // CHECK-NOT: mpmd.fragment
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @stage_can_be_merged_into_inferred_consumer
+func.func @stage_can_be_merged_into_inferred_consumer(%arg0: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0)
+  // CHECK-NOT: mpmd.fragment
+  %0 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_merge_if_producer_result_used_by_another_mesh
+func.func @should_merge_if_producer_result_used_by_another_mesh(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[MERGED_PRODUCER_RESULT:.*]]:2 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT:   %[[ADD2:.*]] = stablehlo.add
+  // CHECK-NEXT:   return %[[ADD]], %[[ADD2:.*]]
+  // CHECK-NEXT: }
+  %produced_result = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  // CHECK-NEXT:      %[[TRANSFER:.*]] = mpmd.transfer %[[MERGED_PRODUCER_RESULT]]#
+  %result_transferred_to_another_mesh = mpmd.transfer %produced_result : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  // CHECK-NEXT:      mpmd.fragment<mesh="m2", origin=["diff_mesh_consumer"]>
+  %result_from_diff_mesh = mpmd.fragment<mesh="m2", origin=["diff_mesh_consumer"]> (%result_transferred_to_another_mesh)
+    (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  %result_from_inferred_consumer = mpmd.fragment<mesh="m1", origin=[]> (%produced_result)
+    (%arg2: tensor<4x8xf32>) {
+    %5 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %result_from_inferred_consumer, %result_from_diff_mesh : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @can_merge_if_all_consumer_operands_produced_before_producer
+func.func @can_merge_if_all_consumer_operands_produced_before_producer(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[EARLIER_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["older_than_producer"]> (%arg0)
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT:   return %[[ADD]]
+  // CHECK-NEXT: }
+  // We don't merge the %earlier_result fragment with the inferred consumer because it would reorder the fragments.
+  %earlier_result = mpmd.fragment<mesh="m1", origin=["older_than_producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK-NEXT: %[[MERGED_PRODUCER_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]> (%[[EARLIER_RESULT]], %arg0)
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT:   %[[ADD2:.*]] = stablehlo.add
+  // CHECK-NEXT:   return %[[ADD2:.*]]
+  // CHECK-NEXT: }
+  %later_result = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %final_result = mpmd.fragment<mesh="m1", origin=[]> (%earlier_result, %later_result) // inferred consumer.
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %5 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %final_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_merge_into_producer_if_producer_results_used_before_consumer
+func.func @should_merge_into_producer_if_producer_results_used_before_consumer(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[MERGED_FRAGMENT:.*]]:2 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT:   %[[ADD2:.*]] = stablehlo.add
+  // CHECK-NEXT:   return %[[ADD:.*]], %[[ADD2:.*]]
+  // CHECK-NEXT: }
+  %producer_result = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %[[MERGED_FRAGMENT]]#
+  %transfer_result = mpmd.transfer %producer_result : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  %final_result = mpmd.fragment<mesh="m1", origin=[]> (%producer_result) // inferred consumer.
+    (%arg2: tensor<4x8xf32>) {
+    %5 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %final_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_merge_into_consumer_if_consumer_uses_result_in_between
+func.func @should_merge_into_consumer_if_consumer_uses_result_in_between(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0
+  // CHECK-NEXT: %[[TRANSFER2:.*]] = mpmd.transfer %[[TRANSFER]]
+  %producer_result = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %transfer_result_1 = mpmd.transfer %arg0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  %transfer_result_2 = mpmd.transfer %transfer_result_1 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK-NEXT: %[[MERGED_FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0, %[[TRANSFER2]])
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT:   %[[ADD2:.*]] = stablehlo.add
+  // CHECK-NEXT:   return %[[ADD2]]
+  // CHECK-NEXT: }
+  %final_result = mpmd.fragment<mesh="m1", origin=[]> (%producer_result, %transfer_result_2) // inferred consumer.
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %5 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %final_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_not_merge_if_it_would_create_cyclic_dependency
+func.func @should_not_merge_if_it_would_create_cyclic_dependency(
+  %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[PRODUCER_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT:   return %[[ADD]]
+  // CHECK-NEXT: }
+  %producer_result = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK-NEXT: %[[TRANSFER_RESULT:.*]] = mpmd.transfer %[[PRODUCER_RESULT]]
+  // CHECK-NEXT: %[[TRANSFER2_RESULT:.*]] = mpmd.transfer %[[TRANSFER_RESULT]]
+  %transfer_result_1 = mpmd.transfer %producer_result : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  %transfer_result_2 = mpmd.transfer %transfer_result_1 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=[]> (%[[PRODUCER_RESULT]], %[[TRANSFER2_RESULT]])
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT:   return %[[ADD]]
+  // CHECK-NEXT: }
+  %final_result = mpmd.fragment<mesh="m1", origin=[]> (%producer_result, %transfer_result_2) // inferred consumer.
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %5 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %final_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @call_counter_user_is_preferred_for_consumer_user
+func.func @call_counter_user_is_preferred_for_consumer_user(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT:  {call_counter = 1 : ui32}
+// CHECK-NOT: call_counter
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%0) {call_counter = 1 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @call_counter_user_is_preferred_for_producer_user
+func.func @call_counter_user_is_preferred_for_producer_user(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: {call_counter = 0 : ui32}
+// CHECK-NOT: call_counter
+  %0 = mpmd.fragment<mesh="m1", origin=["f0"]> (%arg0) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0) {call_counter = 1 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @call_counter_removed_if_both_inferred
+func.func @call_counter_removed_if_both_inferred(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NOT: call_counter
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0) {call_counter = 1 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments_sideways.mlir b/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments_sideways.mlir
new file mode 100644
index 0000000..a5b9d84
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments_sideways.mlir
@@ -0,0 +1,71 @@
+// RUN: mpmd_opt %s -mpmd-merge-inferred-fragments='merge-sideways=true' 2>&1 | FileCheck %s
+
+!m1_4x8 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!m2_4x8 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+#topo = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+
+// CHECK-LABEL: func @simple
+func.func @simple(%arg0: !m1_4x8, %arg1: !m1_4x8)
+  -> (!m1_4x8, !m1_4x8) attributes {topology=#topo} {
+  // CHECK-NEXT: %[[FRAG:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   stablehlo.add
+  // CHECK-NEXT:   stablehlo.multiply
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[FRAG]]#0, %[[FRAG]]#1
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!m1_4x8, !m1_4x8) -> !m1_4x8
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg1)
+    (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!m1_4x8) -> !m1_4x8
+
+  func.return %0, %1 : !m1_4x8, !m1_4x8
+}
+
+// CHECK-LABEL: func @interleaved
+func.func @interleaved(%arg0: !m1_4x8, %arg1: !m1_4x8)
+  -> (!m1_4x8, !m1_4x8) attributes {topology=#topo} {
+  // CHECK-NEXT: %[[FRAG_F:.*]] = mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK-NEXT:   stablehlo.add
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[FRAG_G:.*]]:2 = mpmd.fragment<mesh="m1", origin=["g"]>
+  // CHECK-NEXT:   stablehlo.multiply
+  // CHECK-NEXT:   stablehlo.subtract
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[FRAG_G]]#1, %[[FRAG_G]]#0
+  //
+  // We have interleaved fragments which don't rely on one another:
+  // %arg0 -> %0 -> %2
+  // %arg1 -> %1
+  //
+  // With sideways merging, %1 merges into %2.
+
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!m1_4x8) -> !m1_4x8
+
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%arg1)
+    (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!m1_4x8) -> !m1_4x8
+
+
+  %2 = mpmd.fragment<mesh="m1", origin=["g"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.subtract %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!m1_4x8) -> !m1_4x8
+
+  func.return %2, %1 : !m1_4x8, !m1_4x8
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments_with_cloning.mlir b/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments_with_cloning.mlir
new file mode 100644
index 0000000..3fe12c9
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/merge_inferred_fragments_with_cloning.mlir
@@ -0,0 +1,159 @@
+// RUN: mpmd_opt %s -mpmd-merge-inferred-fragments=clone-inferred-fragments=true 2>&1 | FileCheck %s
+
+!mesh_1_tensor_f32 = !mpmd.mesh_tensor<"m1", tensor<f32>>
+!mesh_1_tensor_2_8_f32 = !mpmd.mesh_tensor<"m1", tensor<2x8xf32>>
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @small_inferred_fragment_is_cloned
+func.func @small_inferred_fragment_is_cloned(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     stablehlo.multiply
+// CHECK-NEXT:     return
+// CHECK-NEXT: }
+// CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["g"]> (%arg0)
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     return
+// CHECK-NEXT: }
+// CHECK-NEXT: return
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=["g"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1, %2 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @return_operand_is_replaced_with_clone
+func.func @return_operand_is_replaced_with_clone(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: %[[F:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     stablehlo.multiply
+// CHECK-NEXT:     return
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[F]]#0, %[[F]]#1
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %0, %1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @chain_of_inferred_fragments
+func.func @chain_of_inferred_fragments()
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]> ()
+// CHECK-NEXT:     stablehlo.const
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     stablehlo.multiply
+// CHECK-NEXT:     stablehlo.multiply
+// CHECK-NEXT:     return
+// CHECK-NEXT: }
+// CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["g"]> ()
+// CHECK-NEXT:     stablehlo.const
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     stablehlo.multiply
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     return
+// CHECK-NEXT: }
+// CHECK-NEXT: return
+  %0 = mpmd.fragment<mesh="m1", origin=[]> () () {
+    %3 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : () -> !mesh_1_tensor_4_8_f32
+
+  %10 = mpmd.fragment<mesh="m1", origin=[]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %11 = mpmd.fragment<mesh="m1", origin=[]> (%10)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%11)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=["g"]> (%11)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1, %2 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @large_inferred_fragment_is_not_cloned
+func.func @large_inferred_fragment_is_not_cloned(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"]>
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     stablehlo.multiply
+// CHECK-NEXT:     return
+// CHECK-NEXT: }
+// CHECK-NEXT:  mpmd.fragment<mesh="m1", origin=["g"]>
+// CHECK-NEXT:     stablehlo.add
+// CHECK-NEXT:     return
+// CHECK-NEXT: }
+// CHECK-NEXT: return
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    %3 = stablehlo.add %4, %4 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=["g"]> (%0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1, %2 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/merge_transfers.mlir b/shardy/dialect/mpmd/transforms/common/test/merge_transfers.mlir
new file mode 100644
index 0000000..6698895
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/merge_transfers.mlir
@@ -0,0 +1,267 @@
+// RUN: mpmd_opt %s -mpmd-merge-transfers 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<f32>>
+!mesh_1_tensor_2 = !mpmd.mesh_tensor<"m1", tensor<1xf32>>
+!mesh_1_tensor_3 = !mpmd.mesh_tensor<"m1", tensor<bf16>>
+
+sdy.mesh @mesh = <["x"=1]>
+!mesh_1_tensor_sharded = !mpmd.mesh_tensor<"m1", tensor<1xf32>, sharding=<@mesh, [{"x"}]>>
+!mesh_1_tensor_over_threshold = !mpmd.mesh_tensor<"m1", tensor<2xf32>>
+
+
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<f32>>
+!mesh_2_tensor_2 = !mpmd.mesh_tensor<"m2", tensor<1xf32>>
+!mesh_2_tensor_3 = !mpmd.mesh_tensor<"m2", tensor<bf16>>
+!mesh_2_tensor_sharded = !mpmd.mesh_tensor<"m2", tensor<1xf32>, sharding=<@mesh, [{"x"}]>>
+!mesh_2_tensor_over_threshold = !mpmd.mesh_tensor<"m2", tensor<2xf32>>
+
+
+!mesh_3_tensor = !mpmd.mesh_tensor<"m3", tensor<f32>>
+
+// CHECK-LABEL: func @one_producer_many_consumers
+func.func @one_producer_many_consumers(%arg0: !mesh_1_tensor)
+  -> (!mesh_2_tensor, !mesh_3_tensor, !mesh_2_tensor) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=4]>>, <"m2": <["x"=4]>>, <"m3": <["x"=4]>>>
+  }
+{
+
+// The producer fragment.
+// CHECK-NEXT: %[[PROD:.*]]:6 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%[[ARG1:.*]]:
+// Reshapes + Concat for one of the consumers.
+// CHECK-NEXT:   %[[RESHAPE1:.*]] = stablehlo.reshape %[[ARG1]] : (tensor<f32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE2:.*]] = stablehlo.reshape %[[ARG1]] : (tensor<f32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[CONCAT1:.*]] = stablehlo.concatenate %[[RESHAPE1]], %[[RESHAPE2]], dim = 0
+// CHECKS-SAME:     : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
+// Reshapes + Concat for another consumer.
+// CHECK-NEXT:   %[[RESHAPE3:.*]] = stablehlo.reshape %[[ARG1]] : (tensor<f32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE4:.*]] = stablehlo.reshape %[[ARG1]] : (tensor<f32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[CONCAT2:.*]] = stablehlo.concatenate %[[RESHAPE3]], %[[RESHAPE4]], dim = 0
+// CHECK-SAME:      : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
+// Reshapes + Concat for another consumer.
+// CHECK-NEXT:   %[[RESHAPE5:.*]] = stablehlo.reshape %[[ARG1]] : (tensor<f32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE6:.*]] = stablehlo.reshape %[[ARG1]] : (tensor<f32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE7:.*]] = stablehlo.reshape %[[ARG1]] : (tensor<f32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[CONCAT3:.*]] = stablehlo.concatenate %[[RESHAPE5]], %[[RESHAPE6]], %[[RESHAPE7]], dim = 0
+// CHECK-SAME:      : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<3xf32>
+// The fragment returns the same results as before the rewrite and all the
+// concat ops.
+// CHECK-NEXT:   return %[[ARG1]], %[[ARG1]], %[[ARG1]], %[[CONCAT1]], %[[CONCAT2]], %[[CONCAT3]]
+
+  %0:3 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<f32>) {
+    mpmd.return %arg2, %arg2, %arg2 : tensor<f32>, tensor<f32>, tensor<f32>
+  } : (!mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor)
+
+// The concatenated transfers. One per consumer fragment.
+// CHECK-DAG: %[[CONCAT_TRANSFER1:.*]] = mpmd.transfer {concat_transfer} %[[PROD]]#5
+// CHECK-SAME:   (!mpmd.mesh_tensor<"m1", tensor<3xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<3xf32>>
+// CHECK-DAG: %[[CONCAT_TRANSFER2:.*]] = mpmd.transfer {concat_transfer} %[[PROD]]#4
+// CHECK-SAME:   (!mpmd.mesh_tensor<"m1", tensor<2xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<2xf32>>
+// CHECK-DAG: %[[CONCAT_TRANSFER3:.*]] = mpmd.transfer {concat_transfer} %[[PROD]]#3
+// CHECK-SAME:   (!mpmd.mesh_tensor<"m1", tensor<2xf32>>) -> !mpmd.mesh_tensor<"m3", tensor<2xf32>>
+
+// Existing transfers are not removed/rewriten.
+// CHECK-DAG: %[[TRANSFER1:.*]] = mpmd.transfer %[[PROD]]#0
+// CHECK-DAG: %[[TRANSFER2:.*]] = mpmd.transfer %[[PROD]]#1
+// CHECK-DAG: %[[TRANSFER3:.*]] = mpmd.transfer %[[PROD]]#2
+  %t1 = mpmd.transfer %0#0 : (!mesh_1_tensor) -> !mesh_2_tensor
+  %t2 = mpmd.transfer %0#1 : (!mesh_1_tensor) -> !mesh_2_tensor
+  %t3 = mpmd.transfer %0#2 : (!mesh_1_tensor) -> !mesh_2_tensor
+
+// A consumer fragment.
+// CHECK-DAG: fragment<mesh="m2", origin=["f2"]> (%[[TRANSFER1]], %[[TRANSFER2]], %[[TRANSFER3]], %[[CONCAT_TRANSFER1]]) ({{.*}}, {{.*}}, {{.*}}, %[[ARG4:.*]]:
+// The argument respective to concat transfer (%[[ARG4]]) is split into three tensors.
+// CHECK-NEXT:   %[[SLICE1:.*]] = stablehlo.slice %[[ARG4]] [0:1] : (tensor<3xf32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE1:.*]] = stablehlo.reshape %[[SLICE1]] : (tensor<1xf32>) -> tensor<f32>
+// CHECK-NEXT:   %[[SLICE2:.*]] = stablehlo.slice %[[ARG4]] [1:2] : (tensor<3xf32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE2:.*]] = stablehlo.reshape %[[SLICE2]] : (tensor<1xf32>) -> tensor<f32>
+// CHECK-NEXT:   %[[SLICE3:.*]] = stablehlo.slice %[[ARG4]] [2:3] : (tensor<3xf32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE3:.*]] = stablehlo.reshape %[[SLICE3]] : (tensor<1xf32>) -> tensor<f32>
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %[[RESHAPE1]], %[[RESHAPE2]]
+// CHECK-NEXT:   stablehlo.add %[[ADD]], %[[RESHAPE3]]
+  %1 = mpmd.fragment<mesh="m2", origin=["f2"]> (%t1, %t2, %t3)
+    (%arg2: tensor<f32>, %arg3: tensor<f32>, %arg4: tensor<f32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<f32>
+    %5 = stablehlo.add %4, %arg4 : tensor<f32>
+    mpmd.return %5 : tensor<f32>
+  } : (!mesh_2_tensor, !mesh_2_tensor, !mesh_2_tensor) -> !mesh_2_tensor
+
+// Another consumer in a different mesh from the first consumer.
+// CHECK-DAG: %[[TRANSFER4:.*]] = mpmd.transfer %[[PROD]]#0
+// CHECK-SAME:   (!mpmd.mesh_tensor<"m1", tensor<f32>>) -> !mpmd.mesh_tensor<"m3", tensor<f32>>
+// CHECK-DAG: %[[TRANSFER5:.*]] = mpmd.transfer %[[PROD]]#1
+// CHECK-SAME:   (!mpmd.mesh_tensor<"m1", tensor<f32>>) -> !mpmd.mesh_tensor<"m3", tensor<f32>>
+  %t4 = mpmd.transfer %0#0 : (!mesh_1_tensor) -> !mesh_3_tensor
+  %t5 = mpmd.transfer %0#1 : (!mesh_1_tensor) -> !mesh_3_tensor
+// CHECK-DAG: fragment<mesh="m3", origin=["f3"]> (%[[TRANSFER4]], %[[TRANSFER5]], %[[CONCAT_TRANSFER3]]) ({{.*}}, {{.*}}, %[[ARG3:.*]]:
+// CHECK-NEXT:   %[[SLICE4:.*]] = stablehlo.slice %[[ARG3]] [0:1] : (tensor<2xf32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE4:.*]] = stablehlo.reshape %[[SLICE4]] : (tensor<1xf32>) -> tensor<f32>
+// CHECK-NEXT:   %[[SLICE5:.*]] = stablehlo.slice %[[ARG3]] [1:2] : (tensor<2xf32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE5:.*]] = stablehlo.reshape %[[SLICE5]] : (tensor<1xf32>) -> tensor<f32>
+// CHECK-NEXT:   stablehlo.add %[[RESHAPE4]], %[[RESHAPE5]]
+  %2 = mpmd.fragment<mesh="m3", origin=["f3"]> (%t4, %t5)
+    (%arg2: tensor<f32>, %arg3: tensor<f32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<f32>
+    mpmd.return %4 : tensor<f32>
+  } : (!mesh_3_tensor, !mesh_3_tensor) -> !mesh_3_tensor
+
+// Another consumer in the same mesh from the first consumer.
+// CHECK-DAG:  mpmd.fragment<mesh="m2", origin=["f4"]> (%[[TRANSFER1]], %[[TRANSFER2]], %[[CONCAT_TRANSFER2]]) ({{.*}}, {{.*}}, %[[ARG3:.*]]:
+// CHECK-NEXT:   %[[SLICE6:.*]] = stablehlo.slice %[[ARG3]] [0:1] : (tensor<2xf32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE6:.*]] = stablehlo.reshape %[[SLICE6]] : (tensor<1xf32>) -> tensor<f32>
+// CHECK-NEXT:   %[[SLICE7:.*]] = stablehlo.slice %[[ARG3]] [1:2] : (tensor<2xf32>) -> tensor<1xf32>
+// CHECK-NEXT:   %[[RESHAPE7:.*]] = stablehlo.reshape %[[SLICE7]] : (tensor<1xf32>) -> tensor<f32>
+// CHECK-NEXT:   stablehlo.add %[[RESHAPE6]], %[[RESHAPE7]]
+
+  %3 = mpmd.fragment<mesh="m2", origin=["f4"]> (%t1, %t2)
+    (%arg2: tensor<f32>, %arg3: tensor<f32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<f32>
+    mpmd.return %4 : tensor<f32>
+  } : (!mesh_2_tensor, !mesh_2_tensor) -> !mesh_2_tensor
+
+  func.return %1, %2, %3 : !mesh_2_tensor, !mesh_3_tensor, !mesh_2_tensor
+}
+
+// CHECK-LABEL: func @different_shapes_do_not_merge
+func.func @different_shapes_do_not_merge(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor_2)
+  -> (!mesh_2_tensor, !mesh_2_tensor_2) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=4]>>, <"m2": <["x"=4]>>, <"m3": <["x"=4]>>>
+  }
+{
+// CHECK-NOT: concat_transfer
+// CHECK-NEXT: %[[PROD:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+// CHECK-NEXT:   mpmd.return %arg2, %arg3 : tensor<f32>, tensor<1xf32>
+// CHECK-NEXT: }
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+    (%arg2: tensor<f32>, %arg3: tensor<1xf32>) {
+    mpmd.return %arg2, %arg3 : tensor<f32>, tensor<1xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor_2) -> (!mesh_1_tensor, !mesh_1_tensor_2)
+// CHECK-NEXT: %[[TRANSFER1:.*]] = mpmd.transfer %[[PROD]]#0
+// CHECK-NEXT: %[[TRANSFER2:.*]] = mpmd.transfer %[[PROD]]#1
+  %t1 = mpmd.transfer %0#0 : (!mesh_1_tensor) -> !mesh_2_tensor
+  %t2 = mpmd.transfer %0#1 : (!mesh_1_tensor_2) -> !mesh_2_tensor_2
+// CHECK-NEXT: fragment<mesh="m2", origin=["f2"]> (%[[TRANSFER1]], %[[TRANSFER2]])
+// CHECK-NEXT:   mpmd.return %arg2, %arg3
+  %1:2 = mpmd.fragment<mesh="m2", origin=["f2"]> (%t1, %t2)
+    (%arg2: tensor<f32>, %arg3: tensor<1xf32>) {
+    mpmd.return %arg2, %arg3 : tensor<f32>, tensor<1xf32>
+  } : (!mesh_2_tensor, !mesh_2_tensor_2) -> (!mesh_2_tensor, !mesh_2_tensor_2)
+
+  func.return %1#0, %1#1 : !mesh_2_tensor, !mesh_2_tensor_2
+}
+
+// CHECK-LABEL: func @different_dtypes_do_not_merge
+func.func @different_dtypes_do_not_merge(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor_3)
+  -> (!mesh_2_tensor, !mesh_2_tensor_3) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=4]>>, <"m2": <["x"=4]>>, <"m3": <["x"=4]>>>
+  }
+{
+// CHECK-NEXT: %[[PROD:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+// CHECK-NEXT:   mpmd.return %arg2, %arg3 : tensor<f32>, tensor<bf16>
+// CHECK-NEXT: }
+// CHECK-NOT: concat_transfer
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+    (%arg2: tensor<f32>, %arg3: tensor<bf16>) {
+    mpmd.return %arg2, %arg3 : tensor<f32>, tensor<bf16>
+  } : (!mesh_1_tensor, !mesh_1_tensor_3) -> (!mesh_1_tensor, !mesh_1_tensor_3)
+// CHECK-NEXT: %[[TRANSFER1:.*]] = mpmd.transfer %[[PROD]]#0
+// CHECK-NEXT: %[[TRANSFER2:.*]] = mpmd.transfer %[[PROD]]#1
+  %t1 = mpmd.transfer %0#0 : (!mesh_1_tensor) -> !mesh_2_tensor
+  %t2 = mpmd.transfer %0#1 : (!mesh_1_tensor_3) -> !mesh_2_tensor_3
+// CHECK-NEXT: fragment<mesh="m2", origin=["f2"]> (%[[TRANSFER1]], %[[TRANSFER2]])
+// CHECK-NEXT:   mpmd.return %arg2, %arg3
+  %1:2 = mpmd.fragment<mesh="m2", origin=["f2"]> (%t1, %t2)
+    (%arg2: tensor<f32>, %arg3: tensor<bf16>) {
+    mpmd.return %arg2, %arg3 : tensor<f32>, tensor<bf16>
+  } : (!mesh_2_tensor, !mesh_2_tensor_3) -> (!mesh_2_tensor, !mesh_2_tensor_3)
+
+  func.return %1#0, %1#1 : !mesh_2_tensor, !mesh_2_tensor_3
+}
+
+
+// CHECK-LABEL: func @sharded_tensors_do_not_merge
+func.func @sharded_tensors_do_not_merge(%arg0: !mesh_1_tensor_sharded, %arg1: !mesh_1_tensor_sharded)
+  -> (!mesh_2_tensor_sharded, !mesh_2_tensor_sharded) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=1]>>, <"m2": <["x"=1]>>>
+  }
+{
+// CHECK-NOT: concat_transfer
+// CHECK-NEXT: %[[PROD:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+// CHECK-NEXT:   mpmd.return %arg2, %arg3 : tensor<1xf32>, tensor<1xf32>
+// CHECK-NEXT: }
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+    (%arg2: tensor<1xf32>, %arg3: tensor<1xf32>) {
+    mpmd.return %arg2, %arg3 : tensor<1xf32>, tensor<1xf32>
+  } : (!mesh_1_tensor_sharded, !mesh_1_tensor_sharded) -> (!mesh_1_tensor_sharded, !mesh_1_tensor_sharded)
+// CHECK-NEXT: %[[TRANSFER1:.*]] = mpmd.transfer %[[PROD]]#0
+// CHECK-NEXT: %[[TRANSFER2:.*]] = mpmd.transfer %[[PROD]]#1
+  %t1 = mpmd.transfer %0#0 : (!mesh_1_tensor_sharded) -> !mesh_2_tensor_sharded
+  %t2 = mpmd.transfer %0#1 : (!mesh_1_tensor_sharded) -> !mesh_2_tensor_sharded
+// CHECK-NEXT: fragment<mesh="m2", origin=["f2"]> (%[[TRANSFER1]], %[[TRANSFER2]])
+// CHECK-NEXT:   mpmd.return %arg2, %arg3
+  %1:2 = mpmd.fragment<mesh="m2", origin=["f2"]> (%t1, %t2)
+    (%arg2: tensor<1xf32>, %arg3: tensor<1xf32>) {
+    mpmd.return %arg2, %arg3 : tensor<1xf32>, tensor<1xf32>
+  } : (!mesh_2_tensor_sharded, !mesh_2_tensor_sharded) -> (!mesh_2_tensor_sharded, !mesh_2_tensor_sharded)
+
+  func.return %1#0, %1#1 : !mesh_2_tensor_sharded, !mesh_2_tensor_sharded
+}
+
+
+// We're using the default threshold of 1 element.
+// CHECK-LABEL: func @too_many_elements_do_not_merge
+func.func @too_many_elements_do_not_merge(%arg0: !mesh_1_tensor_over_threshold, %arg1: !mesh_1_tensor_over_threshold)
+  -> (!mesh_2_tensor_over_threshold, !mesh_2_tensor_over_threshold) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=1]>>, <"m2": <["x"=1]>>>
+  }
+{
+// CHECK-NOT: concat_transfer
+// CHECK-NEXT: %[[PROD:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+// CHECK-NEXT:   mpmd.return %arg2, %arg3 : tensor<2xf32>, tensor<2xf32>
+// CHECK-NEXT: }
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+    (%arg2: tensor<2xf32>, %arg3: tensor<2xf32>) {
+    mpmd.return %arg2, %arg3 : tensor<2xf32>, tensor<2xf32>
+  } : (!mesh_1_tensor_over_threshold, !mesh_1_tensor_over_threshold) -> (!mesh_1_tensor_over_threshold, !mesh_1_tensor_over_threshold)
+// CHECK-NEXT: %[[TRANSFER1:.*]] = mpmd.transfer %[[PROD]]#0
+// CHECK-NEXT: %[[TRANSFER2:.*]] = mpmd.transfer %[[PROD]]#1
+  %t1 = mpmd.transfer %0#0 : (!mesh_1_tensor_over_threshold) -> !mesh_2_tensor_over_threshold
+  %t2 = mpmd.transfer %0#1 : (!mesh_1_tensor_over_threshold) -> !mesh_2_tensor_over_threshold
+// CHECK-NEXT: fragment<mesh="m2", origin=["f2"]> (%[[TRANSFER1]], %[[TRANSFER2]])
+// CHECK-NEXT:   mpmd.return %arg2, %arg3
+  %1:2 = mpmd.fragment<mesh="m2", origin=["f2"]> (%t1, %t2)
+    (%arg2: tensor<2xf32>, %arg3: tensor<2xf32>) {
+    mpmd.return %arg2, %arg3 : tensor<2xf32>, tensor<2xf32>
+  } : (!mesh_2_tensor_over_threshold, !mesh_2_tensor_over_threshold) -> (!mesh_2_tensor_over_threshold, !mesh_2_tensor_over_threshold)
+
+  func.return %1#0, %1#1 : !mesh_2_tensor_over_threshold, !mesh_2_tensor_over_threshold
+}
+
+// CHECK-LABEL: func @duplicate_transfer_use
+func.func @duplicate_transfer_use(%arg0: !mesh_1_tensor)
+  -> !mesh_2_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=4]>>, <"m2": <["x"=4]>>, <"m3": <["x"=4]>>>
+  }
+{
+
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<f32>) {
+    mpmd.return %arg2, %arg2 : tensor<f32>, tensor<f32>
+  } : (!mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor)
+
+// CHECK:      %[[CONCAT_TRANSFER:.*]] = mpmd.transfer {concat_transfer} %0#2 : (!mpmd.mesh_tensor<"m1", tensor<2xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<2xf32>>
+// CHECK:      mpmd.fragment<mesh="m2", origin=["f2"]> (%{{.*}}, %{{.*}}, %{{.*}}, %[[CONCAT_TRANSFER]])
+// CHECK-SAME:   (%arg1: tensor<f32>, %arg2: tensor<f32>, %arg3: tensor<f32>, %arg4: tensor<2xf32>) {
+// CHECK-DAG:      stablehlo.slice %arg4 [0:1]
+// CHECK-DAG:      stablehlo.slice %arg4 [0:1]
+// CHECK-DAG:      stablehlo.slice %arg4 [1:2]
+
+  %t1 = mpmd.transfer %0#0 : (!mesh_1_tensor) -> !mesh_2_tensor
+  %t2 = mpmd.transfer %0#1 : (!mesh_1_tensor) -> !mesh_2_tensor
+
+  %1 = mpmd.fragment<mesh="m2", origin=["f2"]> (%t2, %t1, %t1)
+    (%arg2: tensor<f32>, %arg3: tensor<f32>, %arg4: tensor<f32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<f32>
+    mpmd.return %4 : tensor<f32>
+  } : (!mesh_2_tensor, !mesh_2_tensor, !mesh_2_tensor) -> !mesh_2_tensor
+
+  func.return %1 : !mesh_2_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/merge_user_fragments_into_schedule_units.mlir b/shardy/dialect/mpmd/transforms/common/test/merge_user_fragments_into_schedule_units.mlir
new file mode 100644
index 0000000..9f4828f
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/merge_user_fragments_into_schedule_units.mlir
@@ -0,0 +1,171 @@
+// RUN: mpmd_opt %s -mpmd-merge-user-fragments-into-scheduling-units 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @simple_merge
+func.func @simple_merge(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1", "f2"]> (%arg0)
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @transpose_counts_do_not_match
+func.func @transpose_counts_do_not_match(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"]>
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"(1)]>
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @call_counts_do_not_match
+func.func @call_counts_do_not_match(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) {call_counter = 0 : ui32}
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%[[FRAGMENT]]) {call_counter = 1 : ui32}
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) {call_counter = 0 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @call_counts_match
+func.func @call_counts_match(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) {call_counter = 2 : ui32}
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) {call_counter = 2 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%0) {call_counter = 2 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @producer_call_count_is_undefined
+func.func @producer_call_count_is_undefined(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) {call_counter = 2 : ui32}
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%0) {call_counter = 2 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @consumer_call_count_is_undefined
+func.func @consumer_call_count_is_undefined(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) {call_counter = 3 : ui32}
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) {call_counter = 3 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%0)(%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @same_stage_can_be_merged
+func.func @same_stage_can_be_merged(%arg0: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1", "f2"], stage=0> (%arg0)
+  // CHECK-NOT: mpmd.fragment
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"], stage=0> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"], stage=0> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @different_stages_cannot_be_merged
+func.func @different_stages_cannot_be_merged(%arg0: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"], stage=0>
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f2"], stage=1>
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"], stage=0> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"], stage=1> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/merged_stages_verification.mlir b/shardy/dialect/mpmd/transforms/common/test/merged_stages_verification.mlir
new file mode 100644
index 0000000..18596db
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/merged_stages_verification.mlir
@@ -0,0 +1,99 @@
+// RUN: mpmd_opt %s -mpmd-verify-stage-merging -split-input-file 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// The fragments are assigned to different stages and therefore this is a valid
+// program.
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  %0 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"], stage=1> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// One of the fragments is not assigned to a stage. This is a valid program.
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"], stage=1> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// -----
+
+// Although both fragments are assigned to the same stage, they have different
+// transpose_counts, and therefore this is a valid program.
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  %0 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"(1)], stage=0> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// -----
+
+// Although both fragments are assigned to the same stage and have the same
+// transpose_count, they have different call_counter, and therefore this is a
+// valid program.
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  %0 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%0) {call_counter = 1 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/merged_stages_verification_failure.mlir b/shardy/dialect/mpmd/transforms/common/test/merged_stages_verification_failure.mlir
new file mode 100644
index 0000000..63a7d32
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/merged_stages_verification_failure.mlir
@@ -0,0 +1,76 @@
+// RUN: mpmd_opt %s -mpmd-verify-stage-merging -split-input-file -verify-diagnostics 2>&1
+
+// One of the fragments does not have a call_counter while the other does. This
+// is not a valid program.
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  %0 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // expected-error@+1 {{A valid program cannot have more than one fragment with the same mesh, stage, transpose and call counts but found multiple fragments with the same attributes: [mesh=m1, stage_id=0, transpose_count=0, call_count=1] for current fragment with origin: [#mpmd.user_origin<"f">]}}
+  %1 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%0) {call_counter = 1 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// -----
+
+// Both fragments have the same call counter. This is not a valid program.
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  %0 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0) {call_counter = 1 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // expected-error@+1 {{A valid program cannot have more than one fragment with the same mesh, stage, transpose and call counts but found multiple fragments with the same attributes: [mesh=m1, stage_id=0, transpose_count=0, call_count=1] for current fragment with origin: [#mpmd.user_origin<"f">]}}
+  %1 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%0) {call_counter = 1 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// -----
+
+// The fragments do not have have a call counter but they are both assigned to
+// the same stage. This is not a valid program.
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  %0 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // expected-error@+1 {{A valid program cannot have more than one fragment with the same mesh, stage, transpose and call counts but found multiple fragments with the same attributes: [mesh=m1, stage_id=0, transpose_count=0, call_count=-1] for current fragment with origin: [#mpmd.user_origin<"f">]}}
+  %1 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/remove_transfer_cycles.mlir b/shardy/dialect/mpmd/transforms/common/test/remove_transfer_cycles.mlir
new file mode 100644
index 0000000..33ebfe2
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/remove_transfer_cycles.mlir
@@ -0,0 +1,64 @@
+// RUN: mpmd_opt %s -mpmd-remove-transfer-cycles 2>&1 | FileCheck %s
+
+// Reshards out of Call Op are merged.
+
+!m1_4x8 = !mpmd.mesh_tensor<"mesh1", tensor<4x8xf32>>
+!m2_4x8 = !mpmd.mesh_tensor<"mesh2", tensor<4x8xf32>, memory_kind="device">
+!m2_4x8_host = !mpmd.mesh_tensor<"mesh2", tensor<4x8xf32>, memory_kind="pinned_host">
+!m3_4x8 = !mpmd.mesh_tensor<"mesh3", tensor<4x8xf32>>
+
+#topo = #mpmd.topology<<"mesh1" : <["x"=4]>>, <"mesh2" : <["x"=4]>>, <"mesh3" : <["x"=4]>>>
+
+// CHECK: func.func public @single_cycle_is_removed
+func.func public @single_cycle_is_removed(%arg0: !m1_4x8)
+  -> (!m1_4x8, !m3_4x8) attributes {topology = #topo} {
+// CHECK-NEXT: %[[T2:.*]] = mpmd.transfer %arg0
+// CHECK-NEXT: %[[T3:.*]] = mpmd.transfer %[[T2]]
+// CHECK-NEXT: return %arg0, %[[T3]]
+
+  %t2 = mpmd.transfer %arg0 : (!m1_4x8) -> !m2_4x8
+  %t3 = mpmd.transfer %t2 : (!m2_4x8) -> !m3_4x8
+
+  %t1_1 = mpmd.transfer %t3 : (!m3_4x8) -> !m1_4x8
+
+  return %t1_1, %t3 : !m1_4x8, !m3_4x8
+}
+
+// CHECK: func.func public @triple_cycle_is_removed
+func.func public @triple_cycle_is_removed(%arg0: !m1_4x8)
+  -> (!m1_4x8, !m2_4x8, !m3_4x8) attributes {topology = #topo} {
+// CHECK-NEXT: %[[T2:.*]] = mpmd.transfer %arg0
+// CHECK-NEXT: %[[T3:.*]] = mpmd.transfer %[[T2]]
+// CHECK-NEXT: return %arg0, %[[T2]], %[[T3]]
+
+  %t2 = mpmd.transfer %arg0 : (!m1_4x8) -> !m2_4x8
+  %t3 = mpmd.transfer %t2 : (!m2_4x8) -> !m3_4x8
+
+  %t1_1 = mpmd.transfer %t3 : (!m3_4x8) -> !m1_4x8
+  %t2_1 = mpmd.transfer %t1_1 : (!m1_4x8) -> !m2_4x8
+  %t3_1 = mpmd.transfer %t2_1 : (!m2_4x8) -> !m3_4x8
+
+  %t1_2 = mpmd.transfer %t3_1 : (!m3_4x8) -> !m1_4x8
+  %t2_2 = mpmd.transfer %t1_2 : (!m1_4x8) -> !m2_4x8
+  %t3_2 = mpmd.transfer %t2_2 : (!m2_4x8) -> !m3_4x8
+
+  %t1_3 = mpmd.transfer %t3_2 : (!m3_4x8) -> !m1_4x8
+
+  return %t1_3, %t2_2, %t3_2 : !m1_4x8, !m2_4x8, !m3_4x8
+}
+
+// CHECK: func.func public @cycle_with_pinned_host_is_kept
+func.func public @cycle_with_pinned_host_is_kept(%arg0: !m1_4x8)
+  -> (!m1_4x8, !m3_4x8) attributes {topology = #topo} {
+// CHECK-NEXT: %[[T2:.*]] = mpmd.transfer %arg0
+// CHECK-NEXT: %[[T3:.*]] = mpmd.transfer %[[T2]]
+// CHECK-NEXT: %[[T1:.*]] = mpmd.transfer %[[T3]]
+// CHECK-NEXT: return %[[T1]], %[[T3]]
+
+  %t2 = mpmd.transfer %arg0 : (!m1_4x8) -> !m2_4x8_host
+  %t3 = mpmd.transfer %t2 : (!m2_4x8_host) -> !m3_4x8
+
+  %t1_1 = mpmd.transfer %t3 : (!m3_4x8) -> !m1_4x8
+
+  return %t1_1, %t3 : !m1_4x8, !m3_4x8
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/sink_negligible_ops_into_call_op.mlir b/shardy/dialect/mpmd/transforms/common/test/sink_negligible_ops_into_call_op.mlir
new file mode 100644
index 0000000..83e0020
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/sink_negligible_ops_into_call_op.mlir
@@ -0,0 +1,137 @@
+// RUN: mpmd_opt %s -mpmd-sink-negligible-ops-into-call-op -split-input-file 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0 : tensor<i32>, %arg1 : tensor<i32>) -> (tensor<i32>, tensor<i32>) attributes {
+  "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: mpmd.call @fn(%arg0)
+  // CHECK-NEXT: mpmd.call @fn(%arg1)
+  // CHECK-NEXT: return
+  %0 = stablehlo.constant dense<0> : tensor<i32>
+  %1 = mpmd.call @fn(%0, %arg0) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  %2 = mpmd.call @fn(%0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %1, %2 : tensor<i32>, tensor<i32>
+}
+
+// CHECK-LABEL: func private @fn(%arg0: tensor<i32>)
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: %[[C:.*]] = stablehlo.constant
+  // CHECK-NEXT: add %[[C]], %arg0
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
+
+// -----
+
+// The op can be sunk, but it can't be DCE's as it's used by another op.
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0 : tensor<i32>, %arg1 : tensor<i32>) -> (tensor<i32>, tensor<i32>, tensor<i32>) attributes {
+  "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: %[[C:.*]] = stablehlo.constant
+  // CHECK-NEXT: mpmd.call @fn(%arg0)
+  // CHECK-NEXT: mpmd.call @fn(%arg1)
+  // CHECK-NEXT: return %[[C]],
+  %0 = stablehlo.constant dense<0> : tensor<i32>
+  %1 = mpmd.call @fn(%0, %arg0) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  %2 = mpmd.call @fn(%0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0, %1, %2 : tensor<i32>, tensor<i32>, tensor<i32>
+}
+
+// CHECK-LABEL: func private @fn(%arg0: tensor<i32>)
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: %[[C:.*]] = stablehlo.constant
+  // CHECK-NEXT: add %[[C]], %arg0
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
+
+// -----
+
+// An op with operands cannot be sunk.
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0 : tensor<i32>, %arg1 : tensor<i32>) -> (tensor<i32>, tensor<i32>) attributes {
+  "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg1
+  // CHECK-NEXT: mpmd.call @fn(%[[ADD]], %arg0)
+  // CHECK-NEXT: mpmd.call @fn(%[[ADD]], %arg1)
+  // CHECK-NEXT: return
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  %1 = mpmd.call @fn(%0, %arg0) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  %2 = mpmd.call @fn(%0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %1, %2 : tensor<i32>, tensor<i32>
+}
+
+// CHECK-LABEL: func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>)
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: add %arg0, %arg1
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
+
+// -----
+
+// An op with multiple results cannot be sunk.
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0 : tensor<i32>, %arg1 : tensor<i32>) -> (tensor<i32>, tensor<i32>) attributes {
+  "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: %[[ADD:.*]]:2 = call @my_add
+  // CHECK-NEXT: mpmd.call @fn(%[[ADD]]#0, %arg0)
+  // CHECK-NEXT: mpmd.call @fn(%[[ADD]]#1, %arg1)
+  // CHECK-NEXT: return
+  %0:2 = call @my_add(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> (tensor<i32>, tensor<i32>)
+  %1 = mpmd.call @fn(%0#0, %arg0) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  %2 = mpmd.call @fn(%0#1, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %1, %2 : tensor<i32>, tensor<i32>
+}
+
+// CHECK-LABEL: func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>)
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: add %arg0, %arg1
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
+
+func.func private @my_add(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32>, tensor<i32>) {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0, %0 : tensor<i32>, tensor<i32>
+}
+
+// -----
+
+// The op is not consistently used in all calls, so it cannot be sunk.
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0 : tensor<i32>, %arg1 : tensor<i32>) -> (tensor<i32>, tensor<i32>) attributes {
+  "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: %[[C:.*]] = stablehlo.constant
+  // CHECK-NEXT: mpmd.call @fn(%arg0, %arg1)
+  // CHECK-NEXT: mpmd.call @fn(%[[C]], %arg1)
+  %0 = stablehlo.constant dense<0> : tensor<i32>
+  %1 = mpmd.call @fn(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  %2 = mpmd.call @fn(%0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %1, %2 : tensor<i32>, tensor<i32>
+}
+
+// CHECK-LABEL: func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>)
+func.func private @fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["z"=2]>>>
+} {
+  // CHECK-NEXT: add %arg0, %arg1
+  %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
+  return %0 : tensor<i32>
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/split_bwd_fragments.mlir b/shardy/dialect/mpmd/transforms/common/test/split_bwd_fragments.mlir
new file mode 100644
index 0000000..b12f4cc
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/split_bwd_fragments.mlir
@@ -0,0 +1,242 @@
+// RUN: mpmd_opt %s -mpmd-split-bwd-fragments -mpmd-fragment-dedup -mpmd-fragment-dce -canonicalize -mpmd-fragment-dce 2>&1 | FileCheck %s
+
+!mesh1_8x8_i1 = !mpmd.mesh_tensor<"mesh1", tensor<8x8xi1>>
+!mesh1_8x8_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x8xf32>>
+!mesh0_8x8_f32 = !mpmd.mesh_tensor<"mesh0", tensor<8x8xf32>>
+!mesh0_16x8_f32 = !mpmd.mesh_tensor<"mesh0", tensor<16x8xf32>>
+!mesh0_8x16_f32 = !mpmd.mesh_tensor<"mesh0", tensor<8x16xf32>>
+!mesh0_16x64_f32 = !mpmd.mesh_tensor<"mesh0", tensor<16x64xf32>>
+!mesh0_8x64_f32 = !mpmd.mesh_tensor<"mesh0", tensor<8x64xf32>>
+!mesh0_16x16_f32 = !mpmd.mesh_tensor<"mesh0", tensor<16x16xf32>>
+!mesh1_16x8_f32 = !mpmd.mesh_tensor<"mesh1", tensor<16x8xf32>>
+!mesh1_8x16_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x16xf32>>
+!mesh1_16x64_f32 = !mpmd.mesh_tensor<"mesh1", tensor<16x64xf32>>
+!mesh1_8x64_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x64xf32>>
+!mesh1_16x16_f32 = !mpmd.mesh_tensor<"mesh1", tensor<16x16xf32>>
+
+// CHECK-LABEL: @split_simple
+func.func public @split_simple(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh0_16x16_f32, !mesh1_16x64_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+    // CHECK:     %[[F1:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1) {split_keep_transferred}
+    // CHECK-NEXT:   %[[D1:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_one}
+    // CHECK-NEXT:   mpmd.return %[[D1]]
+
+    // CHECK:     %[[F2:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg2, %[[F1]]) {split_drop_transferred}
+    // CHECK-NEXT:   %[[D2:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_two}
+    // CHECK-NEXT:   mpmd.return %[[D2]]
+
+    // CHECK:     %[[T:.*]] = mpmd.transfer %[[F1]]
+    // CHECK-NEXT: return %[[T]], %[[F2]]
+    %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+        %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+        mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+    %4 = mpmd.transfer %0#0 : (!mesh1_16x16_f32) -> !mesh0_16x16_f32
+    func.return %4, %0#1 : !mesh0_16x16_f32, !mesh1_16x64_f32
+}
+
+// This is exactly like `split_simple` but contains the second dot inside a
+// region. As a result %1 does not appear directly as operand of %2, rather it
+// is treated as an extra operand (see `GetEffectiveOperands()` in
+// split_bwd_fragments.cc)
+// CHECK-LABEL: @split_simple_region
+func.func public @split_simple_region(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh0_16x16_f32, !mesh1_16x64_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+    // CHECK:     %[[F1:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1) {split_keep_transferred}
+    // CHECK:       %[[D1:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_one}
+    // CHECK-NEXT:  mpmd.return %[[D1]]
+
+    // CHECK:     %[[F2:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%[[F1]], %arg2) {split_drop_transferred}
+    // CHECK:     %[[WHILE:.*]] = stablehlo.while
+    // CHECK:        %[[D2:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_two}
+    // CHECK-NEXT:     stablehlo.return %[[D2]]
+    // CHECK:        mpmd.return %[[WHILE]]
+
+    // CHECK:     %[[T:.*]] = mpmd.transfer %[[F1]]
+    // CHECK-NEXT: return %[[T]], %[[F2]]
+    %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+
+        %2 = stablehlo.while(%iterArg = %arg12) : tensor<16x64xf32>
+             cond {
+               %3 = stablehlo.constant dense<true> : tensor<i1>
+               stablehlo.return %3 : tensor<i1>
+             } do {
+               %4 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+               stablehlo.return %4 : tensor<16x64xf32>
+             }
+        mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+    %4 = mpmd.transfer %0#0 : (!mesh1_16x16_f32) -> !mesh0_16x16_f32
+    func.return %4, %0#1 : !mesh0_16x16_f32, !mesh1_16x64_f32
+}
+
+
+// CHECK-LABEL: @no_transfer_no_split
+func.func public @no_transfer_no_split(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+   // CHECK: %[[F:.*]]:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+   // CHECK-NEXT: %[[D1:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_one}
+   // CHECK-NEXT: %[[D2:.*]] = stablehlo.dot %[[D1]], %[[_:.*]] {dot_two}
+   // CHECK-NEXT: mpmd.return %[[D1]], %[[D2]]
+   // CHECK: return %[[F]]#0, %[[F]]#1
+    %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+        %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+        mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+    func.return %0#0, %0#1 : !mesh1_16x16_f32, !mesh1_16x64_f32
+}
+
+// CHECK-LABEL: @only_transfer_no_split
+func.func public @only_transfer_no_split(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh0_16x16_f32, !mesh0_16x64_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+   // CHECK: %[[F:.*]]:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+   // CHECK-NEXT: %[[D1:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_one}
+   // CHECK-NEXT: %[[D2:.*]] = stablehlo.dot %[[D1]], %[[_:.*]] {dot_two}
+   // CHECK-NEXT: mpmd.return %[[D1]], %[[D2]]
+   // CHECK-DAG: %[[T1:.*]] = mpmd.transfer %[[F]]#0
+   // CHECK-DAG: %[[T2:.*]] = mpmd.transfer %[[F]]#1
+   // CHECK: return %[[T1]], %[[T2]]
+    %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+        %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+        mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+    %4 = mpmd.transfer %0#0 : (!mesh1_16x16_f32) -> !mesh0_16x16_f32
+    %5 = mpmd.transfer %0#1 : (!mesh1_16x64_f32) -> !mesh0_16x64_f32
+    func.return %4, %5 : !mesh0_16x16_f32, !mesh0_16x64_f32
+}
+
+
+// The following test shows how the split fragments contain split ops, whether
+// they have side-effects or not. This is, arguably, somewhat questionable as
+// we do not directly have knowledge of exactly what side-effects the fragment
+// ops perform, but it is a behaviour we adopt in other parts too (e.g. merging)
+// CHECK-LABEL: @split_simple_with_side_effects
+func.func public @split_simple_with_side_effects(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh0_16x16_f32, !mesh1_16x64_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+    // CHECK: mpmd.fragment
+    // CHECK-NEXT: stablehlo.dot
+    // CHECK-NEXT: mpmd.return
+    // CHECK: mpmd.fragment
+    // CHECK-NEXT: stablehlo.custom_call @custom_dot
+    %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+        %2 = stablehlo.custom_call @custom_dot(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+        mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+    %4 = mpmd.transfer %0#0 : (!mesh1_16x16_f32) -> !mesh0_16x16_f32
+    func.return %4, %0#1 : !mesh0_16x16_f32, !mesh1_16x64_f32
+}
+
+// CHECK-LABEL: @split_simple_cant_pull
+func.func public @split_simple_cant_pull(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh0_16x64_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+   // CHECK: %[[F:.*]]:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+   // CHECK-NEXT: %[[D1:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_one}
+   // CHECK-NEXT: %[[D2:.*]] = stablehlo.dot %[[D1]], %[[_:.*]] {dot_two}
+   // CHECK-NEXT: mpmd.return %[[D1]], %[[D2]]
+    %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+        %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+        mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+    // We transfer %0#1 so we will attempt to split %0#0 out, but it is not associated with any
+    // computation that can be pulled, so the split fragment will just disappear after simplification.
+    %4 = mpmd.transfer %0#1 : (!mesh1_16x64_f32) -> !mesh0_16x64_f32
+    func.return %0#0, %4 : !mesh1_16x16_f32, !mesh0_16x64_f32
+}
+
+// CHECK-LABEL: @split_simple_multiple_matmuls
+func.func public @split_simple_multiple_matmuls(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh0_16x16_f32, !mesh1_8x64_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+    // CHECK:     %[[F1:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1) {split_keep_transferred}
+    // CHECK-NEXT:   %[[D1:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_one}
+    // CHECK-NEXT:   mpmd.return %[[D1]]
+
+    // CHECK:     %[[F2:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg2, %[[F1]], %arg1) {split_drop_transferred}
+    // CHECK-DAG:   %[[D2:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_two}
+    // CHECK-DAG:   %[[D3:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_three}
+    // CHECK-DAG:   mpmd.return %[[D3]]
+    %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+        %2 = "stablehlo.dot"(%1, %arg12) {dot_two}: (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+        %3 = "stablehlo.dot"(%arg11, %2) {dot_three}: (tensor<8x16xf32>, tensor<16x64xf32>) -> tensor<8x64xf32>
+        mpmd.return %1, %3 : tensor<16x16xf32>, tensor<8x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_8x64_f32)
+    %4 = mpmd.transfer %0#0 : (!mesh1_16x16_f32) -> !mesh0_16x16_f32
+    func.return %4, %0#1 : !mesh0_16x16_f32, !mesh1_8x64_f32
+}
+
+// CHECK-LABEL: @split_simple_multiple_transfers
+func.func public @split_simple_multiple_transfers(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh0_16x16_f32, !mesh0_16x64_f32, !mesh1_8x64_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+    // CHECK:     %[[F1:.*]]:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2) {split_keep_transferred}
+    // CHECK-NEXT:   %[[D1:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_one}
+    // CHECK-NEXT:   %[[D2:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_two}
+    // CHECK-NEXT:   mpmd.return %[[D1]], %[[D2]]
+
+    // CHECK:     %[[F2:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%[[F1]]#1, %arg1) {split_drop_transferred}
+    // CHECK-NEXT:   %[[D3:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_three}
+    // CHECK-NEXT:   mpmd.return %[[D3]]
+    %0:3 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+        %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+        %3 = "stablehlo.dot"(%arg11, %2) {dot_three} : (tensor<8x16xf32>, tensor<16x64xf32>) -> tensor<8x64xf32>
+        mpmd.return %1, %2, %3 : tensor<16x16xf32>, tensor<16x64xf32>, tensor<8x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32, !mesh1_8x64_f32)
+    // Here by transferring two values, we block the pulling of {dot_two}
+    %4 = mpmd.transfer %0#0 : (!mesh1_16x16_f32) -> !mesh0_16x16_f32
+    %5 = mpmd.transfer %0#1 : (!mesh1_16x64_f32) -> !mesh0_16x64_f32
+    func.return %4, %5, %0#2 : !mesh0_16x16_f32, !mesh0_16x64_f32, !mesh1_8x64_f32
+}
+
+
+// This is one of the backward fragments of a two-layer MLP.
+// CHECK-LABEL: @split_bwd_mlp
+func.func public @split_bwd_mlp(%arg0: !mesh1_8x8_i1, %arg1: !mesh1_8x8_f32, %arg2: !mesh1_8x8_f32, %arg3: !mesh1_8x8_i1, %arg4: !mesh1_8x8_f32, %arg5: !mesh1_8x8_f32) -> (!mesh0_8x8_f32, !mesh1_8x8_f32, !mesh1_8x8_f32)
+  attributes {topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>} {
+    // CHECK-NEXT: %[[FTRAN:.*]]:3 = mpmd.fragment<mesh="mesh1", origin=["layer_3"(1), "layer_2"(1)]> (%arg0, %arg2, %arg3, %arg5) {call_counter = 0 : ui32, split_keep_transferred}
+    // CHECK-DAG: %[[S1:.*]] = stablehlo.select %[[_:.*]], %[[_:.*]], %[[_:.*]]
+    // CHECK-DAG: %[[D1:.*]] = stablehlo.dot_general %[[S1]], %[[_:.*]]
+    // CHECK-DAG: %[[T1:.*]] = stablehlo.transpose %[[D1]]
+    // CHECK-DAG: %[[S2:.*]] = stablehlo.select %[[_:.*]], %[[T1]], %[[_:.*]]
+    // CHECK-DAG: %[[D2:.*]] = stablehlo.dot_general %[[S2]], %[[_:.*]]
+    // CHECK-DAG: %[[T2:.*]] = stablehlo.transpose %[[D2]]
+    // CHECK-DAG: mpmd.return %[[T2]], %[[S2]], %[[S1]]
+
+    // CHECK: %[[FRES:.*]]:2 = mpmd.fragment<mesh="mesh1", origin=["layer_3"(1), "layer_2"(1)]> (%arg4, %[[FTRAN]]#1, %arg1, %[[FTRAN]]#2) {call_counter = 0 : ui32, split_drop_transferred}
+    // CHECK-DAG: %[[G1:.*]] = stablehlo.dot_general %arg9, %arg8
+    // CHECK-DAG: %[[G2:.*]] = stablehlo.dot_general %arg7, %arg6
+    // CHECK-DAG: mpmd.return %[[G1]], %[[G2]] : tensor<8x8xf32>, tensor<8x8xf32>
+
+    // CHECK: %[[TRANSFER:.*]] = mpmd.transfer %[[FTRAN]]#0
+    // CHECK-DAG: %[[TRANSFER]], %[[FRES]]#0, %[[FRES]]#1
+    %4:3 = mpmd.fragment<mesh="mesh1", origin=["layer_3"(1), "layer_2"(1)]> (%arg0, %arg1, %arg2, %arg3, %arg4, %arg5) {call_counter = 0 : ui32}
+    (%arg11: tensor<8x8xi1>, %arg6: tensor<8x8xf32>, %arg7: tensor<8x8xf32>, %arg8: tensor<8x8xi1>, %arg9: tensor<8x8xf32>, %arg10: tensor<8x8xf32>) {
+      %9 = stablehlo.constant dense<1.000000e+00> : tensor<8x8xf32>
+      %10 = stablehlo.constant dense<0.000000e+00> : tensor<8x8xf32>
+      %11 = stablehlo.select %arg11, %9, %10 : tensor<8x8xi1>, tensor<8x8xf32>
+      %12 = "stablehlo.dot_general"(%11, %arg7) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [0], rhs_contracting_dimensions = [0]>, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]}> : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
+      %13 = stablehlo.transpose %12, dims = [1,0] : (tensor<8x8xf32>) -> tensor<8x8xf32>
+      %14 = "stablehlo.dot_general"(%11, %arg6) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [1]>, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]}> : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
+      %15 = stablehlo.constant dense<0.000000e+00> : tensor<8x8xf32>
+      %16 = stablehlo.select %arg8, %13, %15 : tensor<8x8xi1>, tensor<8x8xf32>
+      %17 = "stablehlo.dot_general"(%16, %arg10) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [0], rhs_contracting_dimensions = [0]>, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]}> : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
+      %18 = stablehlo.transpose %17, dims = [1,0] : (tensor<8x8xf32>) -> tensor<8x8xf32>
+      %19 = "stablehlo.dot_general"(%16, %arg9) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [1]>, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]}> : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
+      mpmd.return %14, %19, %18 : tensor<8x8xf32>, tensor<8x8xf32>, tensor<8x8xf32>
+    } : (!mesh1_8x8_i1, !mesh1_8x8_f32, !mesh1_8x8_f32, !mesh1_8x8_i1, !mesh1_8x8_f32, !mesh1_8x8_f32) -> (!mesh1_8x8_f32, !mesh1_8x8_f32, !mesh1_8x8_f32)
+    %5 = mpmd.transfer %4#2 : (!mesh1_8x8_f32) -> !mesh0_8x8_f32
+    func.return %5, %4#0, %4#1 : !mesh0_8x8_f32, !mesh1_8x8_f32, !mesh1_8x8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/split_fragment_prioritize_transfer_independent_computations.mlir b/shardy/dialect/mpmd/transforms/common/test/split_fragment_prioritize_transfer_independent_computations.mlir
new file mode 100644
index 0000000..ae8646e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/split_fragment_prioritize_transfer_independent_computations.mlir
@@ -0,0 +1,108 @@
+// RUN: mpmd_opt %s -mpmd-split-and-prioritize-transfer-independent-computations -canonicalize -mpmd-fragment-dedup -mpmd-fragment-dce 2>&1 \
+// RUN:   | FileCheck  --implicit-check-not split_keep_transferred --implicit-check-not split_drop_transferred %s
+
+!mesh1_8x8_i1 = !mpmd.mesh_tensor<"mesh1", tensor<8x8xi1>>
+!mesh1_8x8_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x8xf32>>
+!mesh0_8x8_f32 = !mpmd.mesh_tensor<"mesh0", tensor<8x8xf32>>
+!mesh0_16x8_f32 = !mpmd.mesh_tensor<"mesh0", tensor<16x8xf32>>
+!mesh0_8x16_f32 = !mpmd.mesh_tensor<"mesh0", tensor<8x16xf32>>
+!mesh0_16x64_f32 = !mpmd.mesh_tensor<"mesh0", tensor<16x64xf32>>
+!mesh0_8x64_f32 = !mpmd.mesh_tensor<"mesh0", tensor<8x64xf32>>
+!mesh0_16x16_f32 = !mpmd.mesh_tensor<"mesh0", tensor<16x16xf32>>
+!mesh1_16x8_f32 = !mpmd.mesh_tensor<"mesh1", tensor<16x8xf32>>
+!mesh1_8x16_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x16xf32>>
+!mesh1_16x64_f32 = !mpmd.mesh_tensor<"mesh1", tensor<16x64xf32>>
+!mesh1_8x64_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x64xf32>>
+!mesh1_16x16_f32 = !mpmd.mesh_tensor<"mesh1", tensor<16x16xf32>>
+#topology = #mpmd.topology<<"mesh0" : <["x"=1]>>, <"mesh1" : <["x"=1]>>>
+
+
+
+func.func public @split_simple(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh0_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+  attributes {topology = #topology} {
+  // CHECK:     %[[T:.*]] = mpmd.transfer %arg2
+  // CHECK:     %[[F1:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1) {split_keep_transferred}
+  // CHECK-NEXT:   %[[D1:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_one}
+  // CHECK-NEXT:   mpmd.return %[[D1]]
+  // CHECK-NEXT: }
+
+  // CHECK:     %[[F2:.*]] = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%[[T]], %[[F1]]) {split_drop_transferred}
+  // CHECK-NEXT:   %[[D2:.*]] = stablehlo.dot %[[_:.*]], %[[_:.*]] {dot_two}
+  // CHECK-NEXT:   mpmd.return %[[D2]]
+  // CHECK-NEXT: }
+
+  // CHECK-NEXT: return %[[F1]], %[[F2]]
+  %t2 = mpmd.transfer %arg2 : (!mesh0_16x64_f32) -> !mesh1_16x64_f32
+  %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %t2)
+    (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+      %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+      %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+      mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+    } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+  func.return %0#0, %0#1 : !mesh1_16x16_f32, !mesh1_16x64_f32
+}
+
+
+
+// CHECK-LABEL: @no_split_all_compute_relies_on_transferred_first_arg
+func.func public @no_split_all_compute_relies_on_transferred_first_arg(%arg0: !mesh0_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+  attributes {topology = #topology} {
+  // Only one fragment, since the split would pull out all the compute.
+  // CHECK:     mpmd.fragment
+  // CHECK-NOT: mpmd.fragment
+  %t0 = mpmd.transfer %arg0 : (!mesh0_16x8_f32) -> !mesh1_16x8_f32
+  %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%t0, %arg1, %arg2)
+    (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+      %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+      %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+      mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+    } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+  func.return %0#0, %0#1 : !mesh1_16x16_f32, !mesh1_16x64_f32
+}
+
+// CHECK-LABEL: @no_split_only_one_arg_which_is_transferred
+func.func public @no_split_only_one_arg_which_is_transferred(%arg0: !mesh0_16x8_f32) -> !mesh1_16x8_f32
+  attributes {topology = #topology} {
+  // Only one fragment, since the split would pull out all the compute.
+  // CHECK:     mpmd.fragment
+  // CHECK-NOT: mpmd.fragment
+  %t0 = mpmd.transfer %arg0 : (!mesh0_16x8_f32) -> !mesh1_16x8_f32
+  %0 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%t0)
+    (%arg10: tensor<16x8xf32>) {
+      %1 = stablehlo.add %arg10, %arg10 : tensor<16x8xf32>
+      mpmd.return %1 : tensor<16x8xf32>
+    } : (!mesh1_16x8_f32) -> !mesh1_16x8_f32
+  func.return %0 : !mesh1_16x8_f32
+}
+
+// CHECK-LABEL: @no_transfer_no_split
+func.func public @no_transfer_no_split(%arg0: !mesh1_16x8_f32, %arg1 : !mesh1_8x16_f32, %arg2: !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+  attributes {topology = #topology} {
+    // CHECK:     mpmd.fragment
+    // CHECK-NOT: mpmd.fragment
+    %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%arg0, %arg1, %arg2)
+      (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+        %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+        %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+        mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+      } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+    func.return %0#0, %0#1 : !mesh1_16x16_f32, !mesh1_16x64_f32
+}
+
+// CHECK-LABEL: @only_transfer_no_split
+func.func public @only_transfer_no_split(%arg0: !mesh0_16x8_f32, %arg1 : !mesh0_8x16_f32, %arg2: !mesh0_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+  attributes {topology = #topology} {
+  // CHECK:     mpmd.fragment
+  // CHECK-NOT: mpmd.fragment
+  %t0 = mpmd.transfer %arg0 : (!mesh0_16x8_f32) -> !mesh1_16x8_f32
+  %t1 = mpmd.transfer %arg1 : (!mesh0_8x16_f32) -> !mesh1_8x16_f32
+  %t2 = mpmd.transfer %arg2 : (!mesh0_16x64_f32) -> !mesh1_16x64_f32
+  %0:2 = mpmd.fragment<mesh="mesh1", origin=["block"(1)]> (%t0, %t1, %t2)
+    (%arg10: tensor<16x8xf32>, %arg11 : tensor<8x16xf32>, %arg12: tensor<16x64xf32>) {
+      %1 = "stablehlo.dot"(%arg10, %arg11) {dot_one} : (tensor<16x8xf32>, tensor<8x16xf32>) -> tensor<16x16xf32>
+      %2 = "stablehlo.dot"(%1, %arg12) {dot_two} : (tensor<16x16xf32>, tensor<16x64xf32>) -> tensor<16x64xf32>
+      mpmd.return %1, %2 : tensor<16x16xf32>, tensor<16x64xf32>
+    } : (!mesh1_16x8_f32, !mesh1_8x16_f32, !mesh1_16x64_f32) -> (!mesh1_16x16_f32, !mesh1_16x64_f32)
+  func.return %0#0, %0#1 : !mesh1_16x16_f32, !mesh1_16x64_f32
+}
+
diff --git a/shardy/dialect/mpmd/transforms/common/test/uniquify_function_inputs_outputs.mlir b/shardy/dialect/mpmd/transforms/common/test/uniquify_function_inputs_outputs.mlir
new file mode 100644
index 0000000..789cdbb
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/uniquify_function_inputs_outputs.mlir
@@ -0,0 +1,148 @@
+// RUN: mpmd_opt %s -mpmd-uniquify-function-inputs-outputs -split-input-file 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4xf32>>
+
+// CHECK-LABEL: func @no_work_needed
+func.func @no_work_needed(%arg0: !mesh_1_tensor, %arg1: !mesh_2_tensor) -> (!mesh_1_tensor, !mesh_2_tensor) attributes {
+  "topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+} {
+  // CHECK-NEXT: %[[F1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]>
+  // CHECK:      %[[F2:.*]] = mpmd.fragment<mesh="m2", origin=["f2"]>
+  // CHECK:      return %[[F1]], %[[F2]]
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4xf32>) {
+    %1 = stablehlo.add %arg2, %arg2 : tensor<4xf32>
+    mpmd.return %1 : tensor<4xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %1 = mpmd.fragment<mesh="m2", origin=["f2"]> (%arg1) (%arg2: tensor<4xf32>) {
+    %1 = stablehlo.add %arg2, %arg2 : tensor<4xf32>
+    mpmd.return %1 : tensor<4xf32>
+  } : (!mesh_2_tensor) -> !mesh_2_tensor
+  return %0, %1 : !mesh_1_tensor, !mesh_2_tensor
+}
+
+
+// CHECK-LABEL: func @single_mesh_one_return_operand
+func.func @single_mesh_one_return_operand(%arg0: !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor) attributes {
+  "topology"=#mpmd.topology<<"m1": <["x"=2]>>>
+} {
+  // CHECK-NEXT: %[[F1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]>
+  // CHECK:      %[[F2:.*]] = mpmd.fragment<mesh="m1", origin=["f2"]>
+  // CHECK:      %[[UF:.*]]:2 = mpmd.fragment<mesh="m1", origin=[]> (%[[F1]]) (%arg1: tensor<4xf32>) {
+  // CHECK:         mpmd.return %arg1, %arg1 : tensor<4xf32>, tensor<4xf32>
+  // CHECK:      %[[F2]], %[[UF]]#0, %[[UF]]#1
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg1: tensor<4xf32>) {
+    %1 = stablehlo.add %arg1, %arg1 : tensor<4xf32>
+    mpmd.return %1 : tensor<4xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%0) (%arg1: tensor<4xf32>) {
+    mpmd.return %arg1 : tensor<4xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  return %1, %0, %0 : !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @needs_fragment_for_m1_with_many_values
+func.func @needs_fragment_for_m1_with_many_values(%arg0: !mesh_1_tensor, %arg1: !mesh_2_tensor
+) -> (!mesh_2_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor) attributes {
+  "topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+} {
+  // CHECK-NEXT: %[[F1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]>
+  // CHECK:      %[[F2:.*]] = mpmd.fragment<mesh="m2", origin=["f2"]>
+  // CHECK:      %[[F3:.*]] = mpmd.fragment<mesh="m1", origin=["f3"]>
+  // CHECK:      %[[UF:.*]]:5 = mpmd.fragment<mesh="m1", origin=[]> (%[[F1]], %[[F3]]) (%[[A1:.*]]: tensor<4xf32>, %[[A2:.*]]: tensor<4xf32>)
+  // CHECK-NEXT:   mpmd.return %[[A1]], %[[A1]], %[[A2]], %[[A2]], %[[A2]]
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[F2]], %[[UF]]#0, %[[UF]]#2, %[[UF]]#1, %[[UF]]#3, %[[UF]]#4
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4xf32>) {
+    mpmd.return %arg2 : tensor<4xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %1 = mpmd.fragment<mesh="m2", origin=["f2"]> (%arg1) (%arg2: tensor<4xf32>) {
+    mpmd.return %arg2 : tensor<4xf32>
+  } : (!mesh_2_tensor) -> !mesh_2_tensor
+  %2 = mpmd.fragment<mesh="m1", origin=["f3"]> (%arg0) (%arg2: tensor<4xf32>) {
+    mpmd.return %arg2 : tensor<4xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  return %1, %0, %2, %0, %2, %2 : !mesh_2_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @needs_fragment_for_m1_and_m2
+func.func @needs_fragment_for_m1_and_m2(%arg0: !mesh_1_tensor, %arg1: !mesh_2_tensor
+) -> (!mesh_1_tensor, !mesh_2_tensor, !mesh_2_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor) attributes {
+  "topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+} {
+  // CHECK: %[[UF1:.*]]:4 = mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK: %[[UF2:.*]]:2 = mpmd.fragment<mesh="m2", origin=[]>
+  // CHECK: return %[[UF1]]#0, %[[UF2]]#0, %[[UF2]]#1, %[[UF1]]#2, %[[UF1]]#1, %[[UF1]]#3
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4xf32>) {
+    mpmd.return %arg2 : tensor<4xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %1 = mpmd.fragment<mesh="m2", origin=["f2"]> (%arg1) (%arg2: tensor<4xf32>) {
+    mpmd.return %arg2 : tensor<4xf32>
+  } : (!mesh_2_tensor) -> !mesh_2_tensor
+  %2 = mpmd.fragment<mesh="m1", origin=["f3"]> (%arg0) (%arg2: tensor<4xf32>) {
+    mpmd.return %arg2 : tensor<4xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  return %0, %1, %1, %2, %0, %2 : !mesh_1_tensor, !mesh_2_tensor, !mesh_2_tensor, !mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor
+}
+
+// -----
+
+!dist_mesh_tensor = !mpmd.mesh_tensor<"m1", tensor<4xf32>, sharding=<@mesh, [{"x"}]>>
+
+module {
+
+// CHECK-LABEL: func @single_mesh_one_return_operand
+func.func @single_mesh_one_return_operand_with_global_view(%arg0: !dist_mesh_tensor) -> (!dist_mesh_tensor, !dist_mesh_tensor, !dist_mesh_tensor) attributes {
+  "topology"=#mpmd.topology<<"m1": <["x"=2]>>>
+} {
+  // CHECK-NEXT: %[[F1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]>
+  // CHECK:      %[[F2:.*]] = mpmd.fragment<mesh="m1", origin=["f2"]>
+  // CHECK:      %[[UF:.*]]:2 = mpmd.fragment<mesh="m1", origin=[]> (%[[F1]]) (%arg1: tensor<4xf32>) {
+  // CHECK:         mpmd.return %arg1, %arg1 : tensor<4xf32>, tensor<4xf32>
+  // CHECK:      %[[F2]], %[[UF]]#0, %[[UF]]#1
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg1: tensor<4xf32>) {
+    %1 = stablehlo.add %arg1, %arg1 : tensor<4xf32>
+    mpmd.return %1 : tensor<4xf32>
+  } : (!dist_mesh_tensor) -> !dist_mesh_tensor
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%0) (%arg1: tensor<4xf32>) {
+    mpmd.return %arg1 : tensor<4xf32>
+  } : (!dist_mesh_tensor) -> !dist_mesh_tensor
+  return %1, %0, %0 : !dist_mesh_tensor, !dist_mesh_tensor, !dist_mesh_tensor
+}
+}
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m", tensor<4xui32>, sharding=<@mesh, [{"x"}]>>
+
+// CHECK-LABEL: func @f
+func.func @f(%arg0: !mesh_tensor) -> (!mesh_tensor, !mesh_tensor, !mesh_tensor)
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=2]>>>}
+{
+  // CHECK-NEXT: %[[F1:.*]] = mpmd.fragment<mesh="m", origin=["f"]> (%arg0) (%arg1: tensor<4xui32>) {
+  // CHECK-NEXT:   return %arg1
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[F2:.*]]:2 = mpmd.fragment<mesh="m", origin=[]> (%arg0) (%arg1: tensor<4xui32>) {
+  // CHECK-NEXT:   return %arg1, %arg1
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[F2]]#0, %[[F1]], %[[F2]]#1
+  %0 = mpmd.fragment<mesh="m", origin=["f"]>(%arg0) (%arg1: tensor<4xui32>) {
+    mpmd.return %arg1 : tensor<4xui32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  func.return %arg0, %0, %arg0 : !mesh_tensor, !mesh_tensor, !mesh_tensor
+}
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m", tensor<4xui32>, sharding=<@mesh, [{"x"}]>>
+
+// CHECK-LABEL: func @identity_function
+func.func @identity_function(%arg0: !mesh_tensor) -> !mesh_tensor
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=2]>>>}
+{
+  // CHECK-NEXT: %[[F:.*]] = mpmd.fragment<mesh="m", origin=[]> (%arg0) (%arg1: tensor<4xui32>) {
+  // CHECK-NEXT:   return %arg1
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[F]]
+  func.return %arg0 : !mesh_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/common/test/unroll_for_loops.mlir b/shardy/dialect/mpmd/transforms/common/test/unroll_for_loops.mlir
new file mode 100644
index 0000000..580f567
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/test/unroll_for_loops.mlir
@@ -0,0 +1,30 @@
+// RUN: mpmd_opt %s -mpmd-unroll-for-loops 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @unroll_for_loop_of_three_iterations
+func.func @unroll_for_loop_of_three_iterations(%arg0: tensor<10xui32>, %arg1: tensor<10xui32>) -> (tensor<10xui32>, tensor<10xui32>)
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>}
+{
+  // CHECK-NEXT: %[[I0:.*]] = stablehlo.constant {unroll_counter = 0 : ui32} dense<0>
+  // CHECK-NEXT: %[[BCAST0:.*]] = stablehlo.broadcast_in_dim %[[I0]], {{.*}} {unroll_counter = 0 : ui32
+  // CHECK-NEXT: %[[ADD0_0:.*]] = stablehlo.add %arg0, %[[BCAST0]] {unroll_counter = 0 : ui32
+  // CHECK-NEXT: %[[ADD0_1:.*]] = stablehlo.add %arg0, %arg1 {unroll_counter = 0 : ui32
+  // CHECK-NEXT: %[[I1:.*]] = stablehlo.constant {unroll_counter = 1 : ui32} dense<1>
+  // CHECK-NEXT: %[[BCAST1:.*]] = stablehlo.broadcast_in_dim %[[I1]], {{.*}} {unroll_counter = 1 : ui32
+  // CHECK-NEXT: %[[ADD1_0:.*]] = stablehlo.add %[[ADD0_0]], %[[BCAST1]] {unroll_counter = 1 : ui32
+  // CHECK-NEXT: %[[ADD1_1:.*]] = stablehlo.add %[[ADD0_0]], %[[ADD0_1]]
+  // CHECK-NEXT: %[[I2:.*]] = stablehlo.constant {unroll_counter = 2 : ui32} dense<2>
+  // CHECK-NEXT: %[[BCAST2:.*]] = stablehlo.broadcast_in_dim %[[I2]], {{.*}} {unroll_counter = 2 : ui32
+  // CHECK-NEXT: %[[ADD2_0:.*]] = stablehlo.add %[[ADD1_0]], %[[BCAST2]] {unroll_counter = 2 : ui32
+  // CHECK-NEXT: %[[ADD2_1:.*]] = stablehlo.add %[[ADD1_0]], %[[ADD1_1]] {unroll_counter = 2 : ui32
+  // CHECK-NEXT: return %[[ADD2_0]], %[[ADD2_1]]
+
+  // CHECK-NOT: mpmd.for
+  %0:2 = mpmd.for (%arg0, %arg1) {iterations = 3 : ui32, unroll_factor = 3 : ui32}
+  (%arg2: tensor<10xui32>, %arg3: tensor<10xui32>, %index: tensor<ui32>) {
+    %1 = stablehlo.broadcast_in_dim %index, dims = [] : (tensor<ui32>) -> tensor<10xui32>
+    %2 = stablehlo.add %arg2, %1 : tensor<10xui32>
+    %3 = stablehlo.add %arg2, %arg3 : tensor<10xui32>
+    mpmd.return %2, %3 : tensor<10xui32>, tensor<10xui32>
+  } : tensor<10xui32>, tensor<10xui32>
+  func.return %0#0, %0#1 : tensor<10xui32>, tensor<10xui32>
+}
diff --git a/shardy/dialect/mpmd/transforms/common/testing_utils.h b/shardy/dialect/mpmd/transforms/common/testing_utils.h
new file mode 100644
index 0000000..6d4ea1b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/testing_utils.h
@@ -0,0 +1,41 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_TESTING_UTILS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_TESTING_UTILS_H_
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+
+namespace mlir::mpmd {
+
+// Finds an op of type `OpTy` that contains the attribute `attr_name`, as a
+// means of identifying the specific place we'd like to test.
+template <typename OpTy>
+OpTy GetOpWithAttribute(func::FuncOp fn, StringRef attr_name) {
+  OpTy found_op;
+  fn.walk([&](OpTy op) {
+    if (op->hasAttr(attr_name)) {
+      found_op = op;
+    }
+  });
+  SDY_CHECK(found_op);
+  return found_op;
+}
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_TESTING_UTILS_H_
diff --git a/shardy/dialect/mpmd/transforms/common/uniquify_function_inputs_outputs.cc b/shardy/dialect/mpmd/transforms/common/uniquify_function_inputs_outputs.cc
new file mode 100644
index 0000000..0dde263
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/uniquify_function_inputs_outputs.cc
@@ -0,0 +1,138 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <utility>
+
+#include "llvm/ADT/MapVector.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/sdy/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_UNIQUIFYFUNCTIONINPUTSOUTPUTSPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+using ValueToReturnIndices = llvm::MapVector<Value, SmallVector<int64_t>>;
+
+void CreateReturnFragmentForMesh(StringRef mesh_name, Operation* return_op,
+                                 ValueToReturnIndices& value_to_return_indices,
+                                 OpBuilder& builder) {
+  // We remove any entries that require no work, in order to avoid too many
+  // checks.
+  value_to_return_indices.remove_if([](const auto& it) {
+    if (it.second.size() == 1) {
+      Value v = it.first;
+      return !isa<BlockArgument>(v);
+    }
+    return it.second.empty();
+  });
+
+  builder.setInsertionPoint(return_op);
+  SmallVector<Value> fragment_operands;
+  fragment_operands.reserve(value_to_return_indices.size());
+  SmallVector<Type> fragment_return_types;
+  for (const auto& [value, return_indices] : value_to_return_indices) {
+    fragment_operands.push_back(value);
+    fragment_return_types.insert(fragment_return_types.end(),
+                                 return_indices.size(),
+                                 cast<MeshTensorType>(value.getType()));
+  }
+
+  if (fragment_operands.empty()) {
+    return;
+  }
+
+  auto loc = return_op->getLoc();
+  auto fragment_op = builder.create<FragmentOp>(
+      loc, fragment_return_types, fragment_operands,
+      /*user_origin=*/ArrayAttr::get(builder.getContext(), {}),
+      /*mesh_name=*/mesh_name, /*stage_id=*/IntegerAttr());
+  Block& fragment_block = fragment_op.getRegion().emplaceBlock();
+
+  SmallVector<Value> returned_values;
+  returned_values.reserve(fragment_return_types.size());
+  // The index of the fragment result that we should use to replace the
+  // function return op operand.
+  int fragment_result_index = 0;
+  sdy::MeshAttr mesh_attr = GetMeshOrFail(fragment_op, mesh_name);
+  for (const auto& [value, return_indices] : value_to_return_indices) {
+    // Add a single block argument for this value and return it as many times
+    // as it's used.
+    returned_values.insert(
+        returned_values.end(), return_indices.size(),
+        fragment_block.addArgument(
+            GetGlobalTensorTypeFromMeshType(value, mesh_attr),
+            value.getLoc()));
+
+    for (int64_t index : return_indices) {
+      return_op->setOperand(index,
+                            fragment_op->getResult(fragment_result_index++));
+    }
+  }
+  auto block_builder = OpBuilder::atBlockEnd(&fragment_block);
+  block_builder.create<ReturnOp>(loc, returned_values);
+}
+
+class UniquifyFunctionInputOutputsPass
+    : public impl::UniquifyFunctionInputsOutputsPassBase<
+          UniquifyFunctionInputOutputsPass> {
+  using UniquifyFunctionInputsOutputsPassBase::
+      UniquifyFunctionInputsOutputsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    if (!IsMpmdFunction(func_op)) {
+      // This is not the main function. Do nothing.
+      return;
+    }
+
+    Operation* return_op = func_op.getBody().front().getTerminator();
+    // value_to_return_indices_per_mesh[mesh_name] = value_to_return_indices
+    // where value_to_return_indices[v] contains a sequence of the indices in
+    // return op where v is used.
+    llvm::MapVector<StringRef, ValueToReturnIndices>
+        value_to_return_indices_per_mesh;
+    for (OpOperand& operand : return_op->getOpOperands()) {
+      auto mesh_type = dyn_cast<MeshTensorType>(operand.get().getType());
+      SDY_CHECK(mesh_type);
+      StringRef mesh_name = mesh_type.getMeshName();
+      value_to_return_indices_per_mesh[mesh_name][operand.get()].push_back(
+          operand.getOperandNumber());
+    }
+
+    OpBuilder builder(&getContext());
+    for (auto& [mesh_name, value_to_return_indices] :
+         value_to_return_indices_per_mesh) {
+      CreateReturnFragmentForMesh(mesh_name, return_op, value_to_return_indices,
+                                  builder);
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/unroll_for_loops.cc b/shardy/dialect/mpmd/transforms/common/unroll_for_loops.cc
new file mode 100644
index 0000000..02202fc
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/unroll_for_loops.cc
@@ -0,0 +1,146 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <vector>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/IRMapping.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/WalkResult.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_UNROLLFORLOOPSPASS
+#include "shardy/dialect/mpmd/transforms/common/passes.h.inc"
+
+namespace {
+
+void SetUnrollCounter(Operation* op, int counter, RewriterBase& rewriter) {
+  op->setAttr(kUnrollCounterAttrName, rewriter.getUI32IntegerAttr(counter));
+}
+
+Value GetUint32Constant(RewriterBase& rewriter, Location loc, uint32_t value) {
+  auto shape = RankedTensorType::get(
+      {}, rewriter.getIntegerType(32, /*isSigned=*/false));
+  return rewriter.create<stablehlo::ConstantOp>(
+      loc, DenseIntElementsAttr::get(shape, value));
+}
+
+// Requires: the unroll factor to be equal to the number of iterations.
+void FullyUnrollForOp(ForOp for_op, RewriterBase& rewriter) {
+  const uint32_t num_iterations = for_op.getIterations();
+
+  Block* for_body = for_op.getBody();
+  rewriter.setInsertionPoint(for_op);
+
+  for (uint32_t unroll_counter = 0; unroll_counter < num_iterations;
+       ++unroll_counter) {
+    // Create the unrolled index constant.
+    Value unrolled_index =
+        GetUint32Constant(rewriter, for_op.getLoc(), unroll_counter);
+    SetUnrollCounter(unrolled_index.getDefiningOp(), unroll_counter, rewriter);
+
+    IRMapping block_args_to_for_op_operands;
+    // The unrolled index argument is mapped to the newly created index
+    // constant.
+    block_args_to_for_op_operands.map(for_body->getArguments().back(),
+                                      unrolled_index);
+    // Map all arguments of the block to the respective for operands.
+    for (auto [operand, arg] :
+        llvm::zip(for_op.getOperands(), for_body->getArguments().drop_back())) {
+      block_args_to_for_op_operands.map(arg, operand);
+    }
+
+    for (Operation& for_body_op : for_body->getOperations()) {
+      if (for_body_op.hasTrait<OpTrait::IsTerminator>()) {
+        // The new operands of the for-loop are the results of the last
+        // unrolled iteration.
+        std::vector<Value> new_operands;
+        for (Value operand : for_body_op.getOperands()) {
+          // We have no free-variables in the for-loop, so all operands are
+          // block arguments.
+          new_operands.push_back(
+              block_args_to_for_op_operands.lookup(operand));
+        }
+        if (unroll_counter == num_iterations - 1) {
+          // In the last iteration of the loop, the users of the `for` loop are
+          // other ops.
+          rewriter.replaceAllOpUsesWith(for_op, new_operands);
+        } else {
+          // In any other iteration of the loop, the uses of the `for` loop are
+          // the `for` loop itself.
+          for_op->setOperands(new_operands);
+        }
+      } else {
+        Operation* unrolled_op =
+            rewriter.clone(for_body_op, block_args_to_for_op_operands);
+        // We annotate all unrolled ops with the unroll counter, so that we have
+        // enough information to generate the pipeline schedule.
+        SetUnrollCounter(unrolled_op, unroll_counter, rewriter);
+      }
+    }
+  }
+  rewriter.eraseOp(for_op);
+}
+
+class UnrollForLoopsPass
+    : public impl::UnrollForLoopsPassBase<UnrollForLoopsPass> {
+  using UnrollForLoopsPassBase::UnrollForLoopsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    IRRewriter rewriter(func_op.getContext());
+    auto walk_result = func_op.getBody().walk([&rewriter](ForOp op) {
+      // Crashes if pre-condition is not met.
+      SDY_CHECK(op.getIterations() == op.getUnrollFactor())
+          << "The unroll factor is required to be the same as the number of "
+          << "iterations.";
+      // TODO: b/372460554 - Support nested mpmd.for loops.
+      // NOTE: At the moment, users will be able to nest for loops using
+      // mpmd.calls as an indirection, this could create odd pipeline schedules,
+      // and needs to be addressed (either disallow pipeline scheduling on
+      // nested loops, or change the scheduler to consider more than one
+      // call/unroll_counter).
+      if (op->getParentOfType<ForOp>()) {
+        op->emitError(
+            "Nested fori loops aren't supported. Please contact OWNERs if you ")
+            << "need this feature.";
+        return WalkResult::interrupt();
+      }
+      FullyUnrollForOp(op, rewriter);
+      // No nested for loops, so no need to visit the body.
+      return WalkResult::skip();
+    });
+    if (walk_result.wasInterrupted()) {
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/utils.cc b/shardy/dialect/mpmd/transforms/common/utils.cc
new file mode 100644
index 0000000..4643c4e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/utils.cc
@@ -0,0 +1,467 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+
+#include <functional>
+#include <iterator>
+#include <optional>
+#include <string>
+#include <utility>
+
+#include "llvm/ADT/DenseSet.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/IRMapping.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Pass/PassInstrumentation.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/InliningUtils.h"
+#include "mlir/Transforms/LocationSnapshot.h"
+#include "mlir/Transforms/RegionUtils.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+bool HasOtherUsersExcept(Value value, Operation* user) {
+  return llvm::any_of(value.getUsers(), [&user](Operation* other_user) {
+    return !user->isAncestor(other_user);
+  });
+}
+
+Operation* GetAncestorIf(Operation* op, std::function<bool(Operation*)> pred,
+                         bool strict) {
+  while (op && !pred(op)) {
+    op = op->getParentOp();
+    if (strict) {
+      SDY_CHECK(op)
+          << "`op` does not have an ancestor that satisfies predicate.";
+    }
+  }
+  return op;
+}
+
+Operation* GetAncestorInBlock(Block* block, Operation* op, bool strict) {
+  return GetAncestorIf(
+      op, [block](Operation* op) { return op->getBlock() == block; }, strict);
+}
+
+bool HasAncestorInBlock(Block* block, Operation* op) {
+  return GetAncestorInBlock(block, op, /*strict=*/false);
+}
+
+void UpdateFunctionType(func::FuncOp func_op) {
+  func_op.setType(FunctionType::get(
+      func_op.getContext(), func_op.getBody().getArgumentTypes(),
+      func_op.getBody().front().getTerminator()->getOperandTypes()));
+}
+
+Operation* Clone(OpBuilder& builder, Operation& operation,
+                 ArrayRef<Value> new_operands) {
+  Operation* new_operation = builder.clone(operation);
+  new_operation->setOperands(new_operands);
+  return new_operation;
+}
+
+void CopyAttributes(
+    Operation* source, Operation* destination,
+    const llvm::SmallDenseSet<mlir::StringRef> elided_attrs_set) {
+  for (NamedAttribute attr : source->getAttrs()) {
+    if (!elided_attrs_set.contains(attr.getName().strref())) {
+      destination->setAttr(attr.getName(), attr.getValue());
+    }
+  }
+}
+
+std::string OperationToString(Operation* op, const OpPrintingFlags& flags) {
+  std::string out;
+  {
+    llvm::raw_string_ostream os(out);
+    op->print(os, flags);
+  }
+  return out;
+}
+
+std::string PrintOperationForLog(Operation* op, OpPrintingFlags flags) {
+  return OperationToString(op, flags);
+}
+
+namespace {
+
+void PrintFileLoc(FileLineColLoc file_loc,
+                  llvm::raw_string_ostream& loc_stream) {
+  loc_stream << "\n"
+             << file_loc.getFilename() << ":" << file_loc.getLine() << ":"
+             << file_loc.getStartColumn() << " to " << file_loc.getEndColumn();
+}
+
+// Recurses into the child locations of some of location types to find a nested
+// file location and prints info if it is found. Returns true if a file location
+// is found.
+bool RecursivelyPrintLoc(Location loc, llvm::raw_string_ostream& loc_stream) {
+  return llvm::TypeSwitch<LocationAttr, bool>(loc)
+      .Case([&](CallSiteLoc call_loc) -> bool {
+        // We recurse into the callee of a call site, as the caller will be
+        // emitted in a different note on the main diagnostic.
+        return RecursivelyPrintLoc(call_loc.getCallee(), loc_stream);
+      })
+      .Case([&](FileLineColLoc file_loc) -> bool {
+        PrintFileLoc(file_loc, loc_stream);
+        return true;
+      })
+      .Case([&](FusedLoc fused_loc) -> bool {
+        // Fused location is unique in that we try to find a sub-location to
+        // show, rather than the top-level location itself.
+        for (Location childLoc : fused_loc.getLocations()) {
+          if (RecursivelyPrintLoc(childLoc, loc_stream)) {
+            return true;
+          }
+        }
+        return false;
+      })
+      .Case([&](NameLoc name_loc) -> bool {
+        if (RecursivelyPrintLoc(name_loc.getChildLoc(), loc_stream)) {
+          loc_stream << "\n\t ^ " << name_loc.getName();
+          return true;
+        };
+        return false;
+      })
+      .Case([&](OpaqueLoc opaque_loc) -> bool {
+        // OpaqueLoc always falls back to a different source location.
+        return RecursivelyPrintLoc(opaque_loc.getFallbackLocation(),
+                                   loc_stream);
+      })
+      .Case([](UnknownLoc) -> bool {
+        // Prefer not to show unknown locations.
+        return false;
+      });
+}
+
+// Finds a nested call site location in the given location.
+std::optional<CallSiteLoc> GetCallSiteLoc(Location loc) {
+  if (dyn_cast<NameLoc>(loc))
+    return GetCallSiteLoc(cast<NameLoc>(loc).getChildLoc());
+  if (auto callLoc = dyn_cast<CallSiteLoc>(loc)) {
+    return callLoc;
+  }
+  if (dyn_cast<FusedLoc>(loc)) {
+    for (auto subLoc : cast<FusedLoc>(loc).getLocations()) {
+      // If fused, just get the first call site location.
+      if (auto callLoc = GetCallSiteLoc(subLoc)) {
+        return callLoc;
+      }
+    }
+    return std::nullopt;
+  }
+  return std::nullopt;
+}
+
+void PrintStackTraceFromLoc(Location loc,
+                            llvm::raw_string_ostream& loc_stream) {
+  if (auto call_loc = GetCallSiteLoc(loc)) {
+    // Print the info from the current loc.
+    RecursivelyPrintLoc(*call_loc, loc_stream);
+    // Print the file locations of the callers.
+    PrintStackTraceFromLoc(call_loc->getCaller(), loc_stream);
+  }
+}
+
+}  // namespace
+
+std::string PrintStackTraceFromLoc(Location loc) {
+  std::string loc_str;
+  llvm::raw_string_ostream loc_stream(loc_str);
+  PrintStackTraceFromLoc(loc, loc_stream);
+  return loc_str;
+}
+
+std::string PrintLocation(Location loc) {
+  std::string loc_str;
+  llvm::raw_string_ostream loc_stream(loc_str);
+
+  PrintStackTraceFromLoc(loc, loc_stream);
+  if (loc_str.empty()) {
+    if (auto name_loc = dyn_cast<NameLoc>(loc);
+        name_loc && isa<UnknownLoc>(name_loc.getChildLoc())) {
+      loc_stream << name_loc.getName();
+    } else {
+      loc.print(loc_stream);
+    }
+  }
+  return loc_str;
+}
+
+bool IsSplitDropTransferred(FragmentOp fragment) {
+  return fragment->hasAttr(kSplitDropTransferredAttrName);
+}
+
+bool IsSplitKeepTransferred(FragmentOp fragment) {
+  return fragment->hasAttr(kSplitKeepTransferredAttrName);
+}
+
+namespace detail {
+namespace {
+
+// Add all unique operands of the producer op to `new_operands` and map them
+// to the corresponding block argument. Erase all arguments whose
+// corresponding operand is already mapped to another argument, and replace
+// their uses with the mapped argument.
+void ProcessProducerOperands(Operation* producer_op, Block& producer_block,
+                             RewriterBase& rewriter, int num_static_args,
+                             SmallVector<Value>& new_operands,
+                             IRMapping& mapping) {
+  ArrayRef<BlockArgument> dynamic_producer_args =
+      producer_block.getArguments().drop_front(num_static_args);
+
+  llvm::BitVector erase_args(producer_block.getNumArguments());
+  for (auto [operand, arg] :
+       llvm::zip(producer_op->getOperands(), dynamic_producer_args)) {
+    if (Value mapped_arg = mapping.lookupOrNull(operand)) {
+      rewriter.replaceAllUsesWith(arg, mapped_arg);
+      erase_args.set(arg.getArgNumber());
+    } else {
+      new_operands.push_back(operand);
+      mapping.map(operand, arg);
+    }
+  }
+  producer_block.eraseArguments(erase_args);
+}
+
+// Add all unique operands of the consumer op, that aren't the result of the
+// producer op, to `new_operands` and returns a replacement value for each
+// dynamic argument as follows:
+// - if the operand is the result of the producer op, add the corresponding
+//   return operand of the producer op to the result.
+// - otherwise if the operand is already mapped to a block argument, add that
+//   argument to the result.
+// - otherwise, add that operand to `new_operands`, create a corresponding block
+//   argument in `producer_block` and add it to the result, and map the operand
+//   to the new block argument.
+SmallVector<Value> ProcessConsumerOperands(
+    Operation* consumer_op, Block& consumer_block, Operation* producer_op,
+    Block& producer_block, int num_static_args,
+    SmallVector<Value>& new_operands, IRMapping& mapping) {
+  SmallVector<Value> new_consumer_args;
+  new_consumer_args.reserve(consumer_block.getNumArguments());
+
+  // Add all static arguments from the producer op to `new_consumer_args`.
+  llvm::copy(producer_block.getArguments().take_front(num_static_args),
+               std::back_inserter(new_consumer_args));
+
+  Operation* return_op = producer_block.getTerminator();
+  ArrayRef<BlockArgument> dynamic_consumer_args =
+      consumer_block.getArguments().drop_front(num_static_args);
+
+  for (auto [operand, arg] :
+       llvm::zip(consumer_op->getOperands(), dynamic_consumer_args)) {
+    if (auto op_result = dyn_cast<OpResult>(operand);
+        op_result && op_result.getOwner() == producer_op) {
+      new_consumer_args.push_back(
+          return_op->getOperand(op_result.getResultNumber()));
+    } else if (Value mapped_arg = mapping.lookupOrNull(operand)) {
+      new_consumer_args.push_back(mapped_arg);
+    } else {
+      new_operands.push_back(operand);
+      new_consumer_args.push_back(
+          producer_block.addArgument(arg.getType(), operand.getLoc()));
+      mapping.map(operand, new_consumer_args.back());
+    }
+  }
+
+  return new_consumer_args;
+}
+
+// Returns all the results of the producer op that are not just used by the
+// consumer, adds their type to `new_result_types`, and adds the corresponding
+// return operands to `new_return_operands`.
+//
+// In addition, replace all uses of the producer op that were nested in the
+// consumer block (we assume the consumer block was already merged into the
+// producer block) using the provided `replace_producer_use_in_consumer_block`.
+SmallVector<Value> ProcessProducerResults(
+    Operation* producer_op, Operation* consumer_op,
+    Operation* producer_return_op, RewriterBase& rewriter,
+    SmallVector<Type>& new_result_types,
+    SmallVector<Value>& new_return_operands,
+    std::function<void(OpOperand&, Value)>
+        replace_producer_use_in_consumer_block) {
+  SmallVector<Value> producer_results_to_replace;
+  producer_results_to_replace.reserve(producer_op->getNumResults());
+
+  for (auto [result, return_operand] : llvm::zip(
+           producer_op->getResults(), producer_return_op->getOperands())) {
+    // We need to replace any nested uses of the result in the merged block
+    // before calling consumer_op.
+    for (OpOperand& use : llvm::make_early_inc_range(result.getUses())) {
+      if (producer_op->isProperAncestor(use.getOwner())) {
+        replace_producer_use_in_consumer_block(use, return_operand);
+      }
+    }
+
+    if (HasOtherUsersExcept(result, consumer_op)) {
+      producer_results_to_replace.push_back(result);
+      new_result_types.push_back(result.getType());
+      new_return_operands.push_back(return_operand);
+    }
+  }
+
+  return producer_results_to_replace;
+}
+
+}  // namespace
+
+Operation* MergeRegionOps(
+    Operation* producer_op, Operation* consumer_op, RewriterBase& rewriter,
+    int num_static_args,
+    std::function<void(OpOperand&, Value)>
+        replace_producer_use_in_consumer_block,
+    std::function<Operation*(Location, TypeRange, ValueRange)>
+        create_merged_op) {
+  SDY_CHECK_EQ(producer_op->getNumRegions(), 1);
+  SDY_CHECK_EQ(consumer_op->getNumRegions(), 1);
+  Block& producer_block = producer_op->getRegion(0).front();
+  Block& consumer_block = consumer_op->getRegion(0).front();
+
+  // Fast path if the producer is trivial to avoid cloning argument lists.
+  // This is a bit of a special case, but is very effective since we typically
+  // merge in a bottom-up fashion.
+  if (producer_op->hasOneUse() && producer_block.getNumArguments() == 1 &&
+      // NB: despite the naming, the consumer_op may not be a true consumer.
+      // This is WAI, although naming this as consumer_op is a bit misleading.
+      // TODO(dvytin): rename consistently in all merge variants.
+      *producer_op->getUsers().begin() == consumer_op &&
+      producer_op->getNumResults() == 1 && num_static_args == 0) {
+    // Find where is the producer op used.
+    int64_t operand_index = producer_op->getUses().begin()->getOperandNumber();
+
+    auto fused_loc =
+        rewriter.getFusedLoc({producer_op->getLoc(), consumer_op->getLoc()});
+    // It is ugly to have to set the insertion point here, but typically the
+    // callback will capture the same rewriter (!) so we have to set the
+    // insertion point before we call the callback. This is sad. We should be
+    // passing the rewriter ref to the callback.
+    rewriter.setInsertionPoint(consumer_op);
+    // It is also a little sad we cannot reuse the consumer_op entirely in place
+    // and have to allocate operand lists for it. But the callback deals with
+    // setting the attributes correctly in the fused op. This deserves some
+    // refactoring to decouple the expensive parts and allow us to fully reuse
+    // the consumer_op, in-place.
+    //
+    // TODO(dvytin): refactor the callbacks to MergeRegionOps.
+    Operation* fused_op = create_merged_op(
+        fused_loc, consumer_op->getResultTypes(), consumer_op->getOperands());
+    fused_op->getRegion(0).takeBody(consumer_op->getRegion(0));
+    Block& fused_block = fused_op->getRegion(0).front();
+    // Replace the operand with the producer op operand.
+    fused_op->setOperand(operand_index, producer_op->getOperand(0));
+    Operation* terminator = producer_block.getTerminator();
+    Value yielded_value = terminator->getOperand(0);
+    BlockArgument arg = fused_block.getArgument(operand_index);
+    rewriter.replaceAllUsesWith(arg, yielded_value);
+    arg.setType(producer_block.getArgument(0).getType());
+    rewriter.inlineBlockBefore(&producer_block, &fused_block.front(), arg);
+    rewriter.eraseOp(terminator);
+    rewriter.replaceOp(consumer_op, fused_op->getResults());
+    rewriter.eraseOp(producer_op);
+    return fused_op;
+  }
+
+  SmallVector<Value> new_operands;
+  new_operands.reserve(producer_op->getNumOperands() +
+                       consumer_op->getNumOperands());
+  IRMapping mapping;
+
+  ProcessProducerOperands(producer_op, producer_block, rewriter,
+                          num_static_args, new_operands, mapping);
+
+  SmallVector<Value> new_consumer_args = ProcessConsumerOperands(
+      consumer_op, consumer_block, producer_op, producer_block, num_static_args,
+      new_operands, mapping);
+
+  // We take the producer return op before merging the blocks.
+  Operation* producer_return_op = producer_block.getTerminator();
+
+  // Inline the consumer block at the end of the producer block, to get a merged
+  // block.
+  rewriter.mergeBlocks(&consumer_block, &producer_block, new_consumer_args);
+
+  int max_num_results =
+      producer_op->getNumResults() + consumer_op->getNumResults();
+  SmallVector<Type> new_result_types;
+  SmallVector<Value> new_return_operands;
+  new_result_types.reserve(max_num_results);
+  new_return_operands.reserve(max_num_results);
+
+  SmallVector<Value> producer_results_to_replace = ProcessProducerResults(
+      producer_op, consumer_op, producer_return_op, rewriter, new_result_types,
+      new_return_operands, replace_producer_use_in_consumer_block);
+
+  // Now we can erase the return op of the producer op as we'll need to create a
+  // new return op for the merged block.
+  rewriter.eraseOp(producer_return_op);
+
+  // Add all result types of the consumer op to `new_result_types`.
+  llvm::copy(consumer_op->getResultTypes(),
+               std::back_inserter(new_result_types));
+
+  // Add all return operands of the consumer op to `new_return_operands`. Note
+  // that this needs to be done after the call to `mergeBlocks` because the
+  // block arguments have been replaced for the consumer block.
+  Operation* return_op = producer_block.getTerminator();
+  llvm::copy(return_op->getOperands(),
+               std::back_inserter(new_return_operands));
+
+  // Set the operands of the return op to those of the merged block.
+  return_op->setOperands(new_return_operands);
+
+  // Finally create the merged op with a fused location right before consumer
+  // op, take the merged block from the producer, and replace the results of
+  // both the producer and consumer ops with the corresponding results of the
+  // merged op.
+  rewriter.setInsertionPoint(consumer_op);
+  Location fused_loc =
+      rewriter.getFusedLoc({producer_op->getLoc(), consumer_op->getLoc()});
+  Operation* new_op =
+      create_merged_op(fused_loc, new_result_types, new_operands);
+
+  new_op->getRegion(0).takeBody(producer_op->getRegion(0));
+
+  for (auto [old_result, new_result] :
+       llvm::zip_first(producer_results_to_replace, new_op->getResults())) {
+    rewriter.replaceAllUsesExcept(old_result, new_result, consumer_op);
+  }
+  rewriter.replaceOp(consumer_op, new_op->getResults().take_back(
+                                      consumer_op->getNumResults()));
+  rewriter.eraseOp(producer_op);
+
+  return new_op;
+}
+
+}  // namespace detail
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/common/utils.h b/shardy/dialect/mpmd/transforms/common/utils.h
new file mode 100644
index 0000000..eaa8520
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/common/utils.h
@@ -0,0 +1,182 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_UTILS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_UTILS_H_
+
+#include <functional>
+#include <string>
+
+#include "llvm/ADT/DenseSet.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/TypeRange.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+// The name of the attribute that keeps track of how many times a loop has been
+// unrolled.
+constexpr StringRef kUnrollCounterAttrName = "unroll_counter";
+
+// TODO(joelwee): rename these attributes to be more generic and less tied to
+// splitting for near-zero-bubble pipelining.
+// Attribute to mark a fragment after some splitting has been applied to it. The
+// distinguishing feature is that the fragment has kept its transferred results.
+constexpr StringRef kSplitKeepTransferredAttrName = "split_keep_transferred";
+// Attribute to mark a fragment that has been split out of another one. The
+// distinguishing feature is that this fragment has dropped any transferred
+// results of the original, i.e. its results are never transferred.
+constexpr StringRef kSplitDropTransferredAttrName = "split_drop_transferred";
+
+// Returns true if `value` is used by any op that isn't `user` or nested in a
+// region of `user`.
+bool HasOtherUsersExcept(Value value, Operation* user);
+
+// Returns the closest surrounding operation for which `pred` returns true
+// including `op` itself, or fails if there is no such operation.
+//
+// If `strict` is false, returns nullptr rather than failing.
+Operation* GetAncestorIf(Operation* op, std::function<bool(Operation*)> pred,
+                         bool strict = true);
+
+// Returns the closest surrounding operation of `op` that is in the given
+// `block`, or fails if there is no such operation.
+//
+// If `strict` is false, returns nullptr rather than failing.
+Operation* GetAncestorInBlock(Block* block, Operation* op, bool strict = true);
+
+// Returns true if `op` has a surrounding operation that is in the given `block`
+bool HasAncestorInBlock(Block* block, Operation* op);
+
+// Updates the FunctionType of the given `func_op` to match the block arguments
+// and return operands in its region.
+void UpdateFunctionType(func::FuncOp func_op);
+
+// Clone an operation and replace its operands.
+Operation* Clone(OpBuilder& builder, Operation& operation,
+                 ArrayRef<Value> new_operands);
+
+// Copies any attribute not contained in `elided_attrs_set` from `source` to
+// `destination`.
+// Note: it may overwrite an attribute in the `destination` op.
+void CopyAttributes(Operation* source, Operation* destination,
+                    llvm::SmallDenseSet<StringRef> elided_attrs_set = {});
+
+// Serializes the MLIR operation as a string.
+std::string OperationToString(Operation* op, const OpPrintingFlags& flags);
+
+// Print `op` to string with large constants elided.
+std::string PrintOperationForLog(
+    Operation* op,
+    OpPrintingFlags flags = OpPrintingFlags().elideLargeElementsAttrs());
+
+// Prints a stack trace from the given location.
+std::string PrintStackTraceFromLoc(Location loc);
+
+// Prints loc formatted as:
+// - If has stack trace, then print it.
+// - If NameLoc without child (e.g. arg or result), print just the name.
+// - Else defaults to default printing.
+std::string PrintLocation(Location loc);
+
+// Returns true if this is one of the two results of the split -- the one that
+// drops the transferred results.
+bool IsSplitDropTransferred(FragmentOp fragment);
+
+// Returns true if this is one of the two results of the split -- the one that
+// keeps the transferred results.
+bool IsSplitKeepTransferred(FragmentOp fragment);
+
+// Returns all items in `range` for which `erase` is set to false.
+template <class T, class RangeT>
+SmallVector<T> FilterRange(RangeT range, const BitVector& erase) {
+  SmallVector<T> result;
+  result.reserve(range.size());
+  for (auto it : llvm::enumerate(range)) {
+    if (!erase.test(it.index())) {
+      result.push_back(it.value());
+    }
+  }
+
+  return result;
+}
+
+namespace detail {
+
+// A non-templated version of MergeRegionOps<OpTy> that takes a callback for
+// creating the merged op. Note that the `consumer_op` may have some (or even
+// none) uses of the `producer_op` results -- i.e. it is not guaranteed to be
+// a true consumer. This is implied in the templated version but not here.
+Operation* MergeRegionOps(
+    Operation* producer_op, Operation* consumer_op,
+    RewriterBase& rewriter, int num_static_args,
+    std::function<void(OpOperand&, Value)>
+        replace_producer_use_in_consumer_block,
+    std::function<Operation*(Location, TypeRange,
+                                   ValueRange)>
+        create_merged_op);
+
+}  // namespace detail
+
+// Merges `producer_op` and `consumer_op` into a single OpTy that returns all
+// the results of `producer_op` that are not just used by `consumer_op`, as
+// well as all the results of `consumer_op`. All uses of `producer_op` that
+// were by `consumer_op` directly (if any) will be replaced with the
+// corresponding producer return operands in the merged block, and any uses
+// that were nested within the region of `consumer_op` (if any) will be
+// replaced using the provided `replace_producer_use_in_consumer_block`.
+//
+// The merged op won't have duplicate operands.
+//
+// `num_static_args` specifies the number of static block arguments that both
+// `producer_op` and `consumer_op` have as the first arguments, whose types
+// are assumed to be identical for both ops, before any dynamic block
+// arguments, i.e. block arguments that correspond to operands.
+//
+// `builder_args` should include any builder argument that should be forwarded
+// to `rewriter.create<OpTy>()` in addition to result types and operands.
+//
+// NOTE: we assume OpTy is an op with a single region, that has
+// `num_static_args` static block arguments and an additional block argument
+// for each operand, and that all uses of `producer_op` are at or after
+// `consumer_op`.
+template <class OpTy, class... BuilderArgs>
+OpTy MergeRegionOps(OpTy producer_op, OpTy consumer_op,
+                    RewriterBase& rewriter, int num_static_args,
+                    std::function<void(OpOperand&, Value)>
+                        replace_producer_use_in_consumer_block,
+                    BuilderArgs&&... builder_args) {
+  return cast<OpTy>(detail::MergeRegionOps(
+      producer_op, consumer_op, rewriter, num_static_args,
+      replace_producer_use_in_consumer_block,
+      [&](Location loc, TypeRange result_types,
+          ValueRange operands) {
+        return rewriter.create<OpTy>(
+            loc, result_types, operands,
+            std::forward<BuilderArgs>(builder_args)...);
+      }));
+}
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_COMMON_UTILS_H_
diff --git a/shardy/dialect/mpmd/transforms/export/BUILD b/shardy/dialect/mpmd/transforms/export/BUILD
new file mode 100644
index 0000000..86f3e7f
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/BUILD
@@ -0,0 +1,123 @@
+# The MPMD export passes and pipeline.
+
+# load("@rules_cc//cc:cc_library.bzl", "cc_library")
+# load("@rules_cc//cc:cc_test.bzl", "cc_test")
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library", "td_library")
+
+package(default_visibility = ["//visibility:public"])
+
+td_library(
+    name = "passes_td_files",
+    srcs = [
+        "passes.td",
+    ],
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+gentbl_cc_library(
+    name = "passes_inc",
+    tbl_outs = {
+        "passes.h.inc": [
+            "-gen-pass-decls",
+            "-name=MpmdExport",
+        ],
+        "g3doc/mpmd_export_passes.md": ["-gen-pass-doc"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "passes.td",
+    deps = [":passes_td_files"],
+)
+
+cc_library(
+    name = "passes",
+    srcs = [
+        "export_pipeline.cc",
+        "lower_to_fragment_calls.cc",
+        "mark_aliasing_and_donation.cc",
+        "mark_fragment_reserved_memory.cc",
+        "mark_input_output_with_layouts.cc",
+        "mark_offloaded_input_output.cc",
+        "reschedule_ops.cc",
+    ],
+    hdrs = [
+        "passes.h",
+    ],
+    deps = [
+        ":naming_utils",
+        ":passes_inc",
+        ":utils",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/ir:fragment_arg_res_attrs",
+        "//shardy/dialect/mpmd/transforms/common:distributed_function_pass",
+        "//shardy/dialect/mpmd/transforms/common:passes",
+        "//shardy/dialect/mpmd/transforms/common:utils",
+        "//shardy/dialect/sdy/ir:dialect",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:Analysis",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+        "@stablehlo//:stablehlo_ops",
+    ],
+)
+
+cc_library(
+    name = "naming_utils",
+    srcs = ["naming_utils.cc"],
+    hdrs = ["naming_utils.h"],
+    deps = [
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/transforms/common:utils",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_test(
+    name = "naming_utils_test",
+    srcs = ["naming_utils_test.cc"],
+    deps = [
+        ":naming_utils",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "@com_google_googletest//:gtest_main",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Parser",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_library(
+    name = "utils",
+    srcs = ["utils.cc"],
+    hdrs = ["utils.h"],
+    deps = [
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/ir:fragment_arg_res_attrs",
+        "@llvm-project//mlir:Analysis",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_test(
+    name = "utils_test",
+    srcs = ["utils_test.cc"],
+    deps = [
+        ":utils",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/ir:register",
+        "@com_google_googletest//:gtest_main",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Parser",
+        "@llvm-project//mlir:Support",
+    ],
+)
diff --git a/shardy/dialect/mpmd/transforms/export/export_pipeline.cc b/shardy/dialect/mpmd/transforms/export/export_pipeline.cc
new file mode 100644
index 0000000..3f9bf97
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/export_pipeline.cc
@@ -0,0 +1,142 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <utility>
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassRegistry.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/Passes.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"
+#include "shardy/dialect/mpmd/transforms/export/passes.h"
+
+namespace mlir::mpmd {
+
+using ::mlir::func::FuncOp;
+
+void addExportPipeline(OpPassManager& pm, const ExportOptions& options) {
+  // CSE the graph as this will deduplicate any duplicated transfers at the top
+  // level of the function and hlo computations nested within fragments.
+  // NOTE: a possible issue with applying CSE here is that in the *very
+  // unlikely* scenario in which we have:
+  //      fwd_fragment:N = fragment ...
+  //      remat_fragment:N = fragment ...
+  //      bwd_fragment = fragment (remat_fragment)
+  // with fwd_fragment and remat_fragment being identical and both pure, then we
+  // will deduplicate them and use fwd_fragment, which will cause a memory usage
+  // regression. However, this is very unlikely to happen and we can always
+  // revisit this if it does.
+  pm.addNestedPass<FuncOp>(createCSEPass());
+
+  if (options.copyConstantsFromProducerToConsumer) {
+    // Apply this pass before DCE as it will leave some operations unused.
+    pm.addNestedPass<FuncOp>(createCopyConstantsPass());
+  }
+
+  // Canonicalize the program and dedup operands and results of fragments.
+  // TODO: jupvfranco - consider using `enabledPatterns` to apply only fragment
+  // canonicalization patterns.
+  pm.addPass(createCanonicalizerPass(
+      GreedyRewriteConfig().setRegionSimplificationLevel(
+          GreedySimplifyRegionLevel::Disabled)));
+  pm.addNestedPass<FuncOp>(createFragmentDedupPass());
+
+  // This optimization may affect certain use cases negatively. Thus, it's
+  // disabled by default, but users can enable it on a per-module basis.
+  if (options.applyMergeTransfers) {
+    // Merges transfers that share the same producer and consumer fragments to
+    // minimize the number of transfers. This pass does not cleanup unused
+    // fragment results/args, so we should run it before fragment DCE.
+    // We also need to run it after deduping fragment operands/results in order
+    // to reduce the size of the concats (i.e., in case a fragment result is
+    // duplicated with both duplicates used by the same consumer).
+    // TODO: jupvfranco - consider applying this in the optimize pipeline. We
+    // cannot do that yet, because we need to run it after Shardy prop, which
+    // happens in between the optimization and export passes.
+    pm.addNestedPass<FuncOp>(createMergeTransfersPass());
+
+    // Run CSE and fragment dedup again after merging transfers as the
+    // -mpmd-merge-transfers pass may have created duplicated concat, slice and
+    // reshape ops.
+    pm.addNestedPass<FuncOp>(createFragmentDedupPass());
+    pm.addNestedPass<FuncOp>(createCSEPass());
+  }
+
+  // Remove any dead-code by eliminating unused fragment results and arguments
+  // and by DCE'ing the fragment bodies.
+  pm.addNestedPass<FuncOp>(createFragmentDcePass());
+
+  // Must be applied after the last -mpmd-fragment-dedup, as it may add
+  // duplicated fragment results and after -canonicalize, as it may add
+  // identity fragments, which would be canonicalized away.
+  pm.addNestedPass<FuncOp>(createUniquifyFunctionInputsOutputsPass());
+
+  // The fragments created by the pass above maybe slowdown compilation (more
+  // fragments to compile) and may cause performance regressions. Thus, we merge
+  // them with other fragments.
+  pm.addNestedPass<FuncOp>(createMergeInferredFragmentsPass());
+
+  // Mark each fragment with the inputs and outputs which are offloaded to host
+  // memory.
+  pm.addNestedPass<FuncOp>(createMarkOffloadedInputOutputPass());
+
+  // Propagate `mhlo.layout_mode` attributes from program inputs to fragments
+  // that are consumers of program input, and propagate `mhlo.layout_mode`
+  // attributes from program outputs to fragments that are output producers.
+  pm.addNestedPass<FuncOp>(createMarkInputOutputWithLayoutsPass());
+
+  // Before we create any dependencies between fragments, delay the
+  // execution of inferred fragments to as late as possible, not to create
+  // unnecessary dependencies on them, which could potentially delay the
+  // execution of user-defined fragments, or even increase memory usage (the
+  // produced tensors say live for longer).
+  pm.addNestedPass<FuncOp>(createDelayInferredFragmentsPass());
+  // Delay the execution of transfers from CPU to as late as possible to reduce
+  // the amount of data in memory.
+  pm.addNestedPass<FuncOp>(createDelayTransfersFromCpuPass());
+
+  // This pass marks input and output aliasing or donation. For each fragment op
+  // whose input can be aliased with an output, it adds an XLA
+  // `tf.aliasing_output` attribute. Otherwise, for each input that can be
+  // donated, it adds a `jax.buffer_donor` attribute. When lowering the fragment
+  // op to fragment calls, the `tf.aliasing_output` and `jax.buffer_donor`
+  // attributes will be set for the corresponding argument.
+  pm.addNestedPass<FuncOp>(createMarkAliasingAndDonationPass());
+
+  // Mark each fragment with how much memory should be left free to account for
+  // live buffers produced by other fragments. This should be run after the
+  // offloading and aliasing passes.
+  pm.addNestedPass<FuncOp>(createMarkFragmentReservedMemoryPass());
+
+  // This pass should be applied after all passes that operate on fragment ops.
+  LowerToFragmentCallsPassOptions lower_to_fragment_calls_options;
+  lower_to_fragment_calls_options.groupAcrossMeshes =
+      options.groupFragmentsAcrossMeshes;
+  lower_to_fragment_calls_options.verboseLogging = options.verboseLogging;
+  pm.addPass(createLowerToFragmentCallsPass(
+      std::move(lower_to_fragment_calls_options)));
+}
+
+void registerExportPipeline() {
+  PassPipelineRegistration<>(
+      "mpmd-export-pipeline",
+      "Run the standard set of passes to export an MPMD program.",
+      [](OpPassManager& pm) {
+        addExportPipeline(pm);
+      });
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/lower_to_fragment_calls.cc b/shardy/dialect/mpmd/transforms/export/lower_to_fragment_calls.cc
new file mode 100644
index 0000000..30255aa
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/lower_to_fragment_calls.cc
@@ -0,0 +1,448 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <algorithm>
+#include <cstdint>
+#include <optional>
+#include <string>
+#include <string_view>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/Hashing.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Block.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/SymbolTable.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/export/naming_utils.h"
+#include "shardy/dialect/mpmd/transforms/export/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/export/utils.h"
+#include "shardy/dialect/sdy/ir/constants.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_LOWERTOFRAGMENTCALLSPASS
+#include "shardy/dialect/mpmd/transforms/export/passes.h.inc"
+
+namespace {
+
+using ::mlir::func::FuncOp;
+
+constexpr StringRef kFragmentNamePrefix = "p";
+constexpr StringRef kGroupId = "group_id";
+constexpr StringRef kGroupName = "group_name";
+
+void SetIntegerAttr(Operation* op, StringRef name, int64_t value,
+                    IRRewriter& rewriter) {
+  op->setAttr(name, rewriter.getI64IntegerAttr(value));
+}
+
+std::optional<int64_t> GetIntegerAttr(Operation* op, StringRef name) {
+  if (auto attr = dyn_cast_or_null<IntegerAttr>(op->getAttr(name))) {
+    return attr.getInt();
+  }
+  return std::nullopt;
+}
+
+// Two fragments are considered to be equivalent if their bodies are equivalent
+// and they have the same mesh name. Note that using the mesh name rather than
+// the mesh attribute in the topology is important to avoid false sharing in
+// systems where we have the same mesh attribute bound to different names.
+// That said, there may be situations where two meshes are equal as mesh
+// attributes, and spanning over the same devices, but have different names,
+// in which case we may want to still merge (to reduce code bloat). We do not,
+// currently, have such a mechanism to know whether two equal meshes span over
+// the same devices and hence we pick the more conservative choice of comparing
+// mesh names, which is important for heterogeneous systems.
+//
+// TODO(dvytin): consider merging even across different equivalent meshes.
+struct FragmentBodyEquivalenceBaseInfo : public DenseMapInfo<FragmentOp> {
+  static unsigned getHashValue(FragmentOp fragment_op,
+                               bool group_across_meshes) {
+    llvm::hash_code hash;
+    if (group_across_meshes) {
+      // Hash the mesh shape.
+      hash = hash_value(*mpmd::GetMeshAttr(fragment_op));
+    } else {
+      // Hash the mesh name to avoid deduping fragments executed on different
+      // meshes (which may be unsafe in heterogeneous settings).
+      hash = llvm::hash_value(fragment_op.getMeshName());
+    }
+    // Hash the fragment argument attributes.
+    hash = llvm::hash_combine(hash, fragment_op->getAttr(kArgAttrName));
+    hash = llvm::hash_combine(hash, fragment_op->getAttr(kResAttrName));
+
+    // Hash the fragment body.
+    //
+    // Note that since we don't hash the operands of operations in the body or
+    // take the number of block arguments into account, there might be
+    // collisions between the hash value of two fragments that aren't truly
+    // equivalent, e.g. they have exactly the same operations but the order of
+    // operands is different. This is ok because the hash value is only used to
+    // lookup the bucket for a fragment, where hashing the operations' signature
+    // is strong enough for minimising collisions, and those that do occur are
+    // resolved with the isEqual check below.
+    fragment_op.getRegion().walk([&](Operation* op) {
+      hash = llvm::hash_combine(
+          hash, OperationEquivalence::computeHash(
+                    op, /*hashOperands=*/OperationEquivalence::ignoreHashValue,
+                    /*hashResults=*/OperationEquivalence::ignoreHashValue,
+                    OperationEquivalence::IgnoreLocations));
+    });
+
+    return hash;
+  }
+
+  static bool isEqual(FragmentOp lhs, FragmentOp rhs,
+                      bool group_across_meshes) {
+    if (lhs == rhs) {
+      return true;
+    }
+    if (lhs == getTombstoneKey() || lhs == getEmptyKey() ||
+        rhs == getTombstoneKey() || rhs == getEmptyKey()) {
+      return false;
+    }
+
+    bool equal_meshes;
+    if (group_across_meshes) {
+      equal_meshes = *mpmd::GetMeshAttr(lhs) == *mpmd::GetMeshAttr(rhs);
+    } else {
+      // Compare the mesh names to avoid deduping fragments executed on
+      // different meshes (which may be unsafe in heterogeneous settings).
+      equal_meshes = lhs.getMeshName() == rhs.getMeshName();
+    }
+
+    return equal_meshes &&
+           lhs->getAttr(kArgAttrName) == rhs->getAttr(kArgAttrName) &&
+           lhs->getAttr(kResAttrName) == rhs->getAttr(kResAttrName) &&
+           OperationEquivalence::isRegionEquivalentTo(
+               &lhs.getRegion(), &rhs.getRegion(),
+               OperationEquivalence::IgnoreLocations);
+  }
+};
+
+struct FragmentBodyEquivalenceCrossMeshGroupingInfo
+    : FragmentBodyEquivalenceBaseInfo {
+  static unsigned getHashValue(FragmentOp fragment_op) {
+    return FragmentBodyEquivalenceBaseInfo::getHashValue(
+        fragment_op, /*group_across_meshes=*/true);
+  }
+
+  static bool isEqual(FragmentOp lhs, FragmentOp rhs) {
+    return FragmentBodyEquivalenceBaseInfo::isEqual(
+        lhs, rhs, /*group_across_meshes=*/true);
+  }
+};
+
+struct FragmentBodyEquivalenceSameMeshGroupingInfo
+    : FragmentBodyEquivalenceBaseInfo {
+  static unsigned getHashValue(FragmentOp fragment_op) {
+    return FragmentBodyEquivalenceBaseInfo::getHashValue(
+        fragment_op, /*group_across_meshes=*/false);
+  }
+
+  static bool isEqual(FragmentOp lhs, FragmentOp rhs) {
+    return FragmentBodyEquivalenceBaseInfo::isEqual(
+        lhs, rhs, /*group_across_meshes=*/false);
+  }
+};
+
+// Auxiliary data structure for fragment grouping.
+struct FragmentGroupInfo {
+  std::optional<int64_t> hbm_bytes;
+  // The unique identifier for this group.
+  int64_t group_id;
+  // Call-sites for all fragments in this group, grouped by mesh.
+  MeshToCallSites mesh_call_sites = MeshToCallSites();
+};
+
+StringRef DropJitPrefix(StringRef name) {
+  const std::string_view kJitPrefix = "jit_";
+  // jax jitted modules have a jit_ prefix, which we drop for simplicity.
+  if (name.starts_with(kJitPrefix)) {
+    return name.substr(kJitPrefix.size());
+  }
+  return name;
+}
+
+StringRef GetModuleName(ModuleOp module_op) {
+  return module_op.getName().value_or("main");
+}
+
+std::string PrettyPrintUserOrigin(ArrayRef<Attribute> origins) {
+  std::string result;
+  llvm::raw_string_ostream stream(result);
+  auto concat_origin = [&stream](UserOriginAttr origin) -> void {
+    stream << origin.getUserName().getValue() << "(";
+    if (origin.getTransposeCount() == 0) {
+      stream << "fwd";
+    } else if (origin.getTransposeCount() == 1) {
+      stream << "bwd";
+    } else {
+      stream << "TransposeCount=" << origin.getTransposeCount();
+    }
+    stream << ")";
+  };
+
+  stream << "[";
+  if (origins.empty()) {
+    stream << "]";
+    return result;
+  }
+  Attribute tail = origins.back();
+  for (Attribute attr : origins.drop_back(1)) {
+    auto origin_attr = cast<UserOriginAttr>(attr);
+    concat_origin(origin_attr);
+    stream << ", ";
+  }
+  concat_origin(cast<UserOriginAttr>(tail));
+  stream << "]";
+  return result;
+}
+
+// Groups fragments by body and mesh shape equivalence, and gives each group a
+// unique identifier, and an updated hbm reserved bytes number. Marks each
+// fragment with its group id, name, and hbm bytes id.
+//
+// Returns all fragments in the module.
+template <typename FragmentEquivalenceInfo>
+std::vector<FragmentOp> GroupFragmentsAndMarkWithGroupName(
+    ModuleOp module_op, IRRewriter& rewriter) {
+  DenseMap<FragmentOp, FragmentGroupInfo, FragmentEquivalenceInfo> fragment_map;
+
+  // Step 1: Group all fragments by body and mesh shape equivalence, and give
+  // each group a unique identifier, and an updated hbm reserved bytes number.
+  int64_t group_id = 0;
+  std::vector<FragmentOp> all_fragments;
+  // Walk the module, collecting fragments in program order, as we rely on that
+  //  below to log the schedule.
+  module_op.walk([&](FragmentOp fragment) {
+    all_fragments.push_back(fragment);
+
+    std::optional<int64_t> hbm_reserved_bytes =
+        GetIntegerAttr(fragment, kReservedHbmBytes);
+
+    auto [it, inserted] =
+        fragment_map.try_emplace(fragment, FragmentGroupInfo{});
+    FragmentGroupInfo& fragment_group = it->getSecond();
+    if (inserted) {
+      fragment_group.group_id = group_id++;
+    }
+    // TODO(dvytin): Experiment with different policies.
+    // std::nullopt < any int64_t, hence std::max works with std::nullopt.
+    fragment_group.hbm_bytes =
+        std::max(fragment_group.hbm_bytes, hbm_reserved_bytes);
+
+    std::string name = GetFullNameFromMetadata(fragment.getOrigin().getValue(),
+                                               fragment.getStageId());
+    std::optional<uint32_t> call_counter = TryToFindCallCounter(fragment);
+    fragment_group.mesh_call_sites[fragment.getMeshName()].emplace_back(
+        std::move(name), call_counter);
+  });
+
+  // Step 2: Mark all fragments with their calculated group ids, names, and
+  // hbm bytes.
+  for (auto fragment : all_fragments) {
+    const auto& [hbm_bytes, group_id, call_sites] = fragment_map[fragment];
+    if (hbm_bytes.has_value()) {
+      SetIntegerAttr(fragment, kReservedHbmBytes, *hbm_bytes, rewriter);
+    }
+    SetIntegerAttr(fragment, kGroupId, group_id, rewriter);
+    std::string group_name;
+
+    llvm::raw_string_ostream stream(group_name);
+    std::string fragment_name = GetCallSitesSummaryName(call_sites);
+    // Append a unique id. We do this first to guarantee it isn't affected by
+    // truncation.
+    stream << kFragmentNamePrefix << group_id << "_";
+
+    // Append the fragment name.
+    stream << fragment_name << ".";
+
+    // Find function name in the module.
+    StringRef module_name = GetModuleName(module_op);
+    // Drop the jit_ prefix if present.
+    module_name = DropJitPrefix(module_name);
+    // Truncate the function name to 32 characters.
+    std::string truncated_module_name = Truncate(module_name, 32);
+    stream << truncated_module_name;
+
+    int max_group_name_length = 64 - truncated_module_name.size();
+    group_name = Truncate(group_name, max_group_name_length);
+    fragment->setAttr(kGroupName, rewriter.getStringAttr(group_name));
+  }
+
+  return all_fragments;
+}
+
+class LowerToFragmentCallsPass
+    : public impl::LowerToFragmentCallsPassBase<LowerToFragmentCallsPass> {
+  using LowerToFragmentCallsPassBase::LowerToFragmentCallsPassBase;
+
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+    MLIRContext& ctx = getContext();
+    bool is_sdy_partitioned = mpmd::IsLoweredWithSdy(module_op);
+
+    IRRewriter rewriter(&ctx);
+
+    std::vector<FragmentOp> all_fragments =
+        groupAcrossMeshes ? GroupFragmentsAndMarkWithGroupName<
+                                FragmentBodyEquivalenceCrossMeshGroupingInfo>(
+                                module_op, rewriter)
+                          : GroupFragmentsAndMarkWithGroupName<
+                                FragmentBodyEquivalenceSameMeshGroupingInfo>(
+                                module_op, rewriter);
+
+    // Step 3: Log the fragment naming per mesh, for debugging purposes.
+    if (auto func = dyn_cast_or_null<FuncOp>(module_op.lookupSymbol("main"))) {
+      ArrayRef<mpmd::NamedMeshAttr> meshes = mpmd::GetTopologyMeshes(func);
+      for (mpmd::NamedMeshAttr mesh : meshes) {
+        if (verboseLogging) {
+          SDY_LOG(INFO) << "Module "
+                        << std::string_view(GetModuleName(module_op))
+                        << " on mesh " << std::string_view(mesh.getName())
+                        << " will execute the following XLA programs:";
+        }
+        for (FragmentOp fragment : all_fragments) {
+          if (fragment.getMeshName() != mesh.getName()) {
+            continue;
+          }
+          std::string stage_and_call_counter;
+          llvm::raw_string_ostream stream(stage_and_call_counter);
+          if (std::optional<int64_t> stage_id = fragment.getStageId()) {
+            stream << ", stage id " << *stage_id;
+          }
+          if (std::optional<uint32_t> call_counter =
+                  TryToFindCallCounter(fragment)) {
+            stream << ", call counter " << *call_counter;
+          }
+          StringRef group_name =
+              fragment->getAttrOfType<StringAttr>(kGroupName).getValue();
+          if (verboseLogging) {
+            SDY_LOG(INFO) << "\t- " << group_name.str()
+                          << " from program with origins "
+                          << PrettyPrintUserOrigin(
+                                 fragment.getOrigin().getValue())
+                          << stage_and_call_counter << ".";
+          }
+        }
+      }
+    }
+
+    // Step 4: For each fragment, extract a function if not already done.
+    SymbolTableCollection symbol_table_collection;
+    SymbolTable& symbol_table =
+        symbol_table_collection.getSymbolTable(module_op);
+
+    for (FragmentOp fragment : all_fragments) {
+      // We use the marked attributes instead of looking up in fragment_map
+      // because we will be doing rewriter replacements.
+      std::optional<int64_t> hbm_bytes =
+          GetIntegerAttr(fragment, kReservedHbmBytes);
+      StringRef group_name =
+          fragment->getAttrOfType<StringAttr>(kGroupName).getValue();
+      if (!symbol_table.lookup(group_name)) {
+        // This is the first encountered fragment for the equivalence group.
+        Block& block = *fragment.getBody();
+        auto func_op = FuncOp::create(
+            fragment.getLoc(), group_name,
+            FunctionType::get(&ctx, block.getArgumentTypes(),
+                              block.getTerminator()->getOperandTypes()));
+        // TODO(b/425890780): Remove references to kMeshShapeAttr.
+        func_op->setAttr(mpmd::kMeshShapeAttr, *mpmd::GetMeshAttr(fragment));
+        // TODO(b/298362694): Try to experiment with different policies when
+        // selecting a value for the flag.
+        if (hbm_bytes.has_value()) {
+          SetIntegerAttr(func_op, kReservedHbmBytes, *hbm_bytes, rewriter);
+        }
+        // Set the argument and result attributes, for now it only has aliasing
+        // and host offload information.
+        if (Attribute arg_attr = fragment->getAttr(kArgAttrName)) {
+          func_op.setArgAttrsAttr(cast<ArrayAttr>(arg_attr));
+        }
+        if (Attribute res_attr = fragment->getAttr(kResAttrName)) {
+          func_op.setResAttrsAttr(cast<ArrayAttr>(res_attr));
+        }
+
+        // Set the argument and result attr to include the sharding in the type.
+        // This is needed for shardy XLA to read the sharding later when
+        // importing.
+        if (is_sdy_partitioned) {
+          for (OpOperand& arg : fragment->getOpOperands()) {
+            auto arg_type = dyn_cast<MeshTensorType>(arg.get().getType());
+            if (!arg_type) {
+              continue;
+            }
+            if (sdy::TensorShardingAttr arg_sharding = arg_type.getSharding()) {
+              func_op.setArgAttr(arg.getOperandNumber(), sdy::kShardingAttr,
+                                 arg_sharding);
+            }
+          }
+          for (auto result : fragment->getResults()) {
+            auto res_type = dyn_cast<MeshTensorType>(result.getType());
+            if (!res_type) {
+              continue;
+            }
+            if (sdy::TensorShardingAttr res_sharding = res_type.getSharding()) {
+              sdy::setFuncResultSharding(func_op, result.getResultNumber(),
+                                         res_sharding);
+            }
+          }
+        }
+
+        sdy::inlineRegionAndConvertTerminatorOp<func::ReturnOp>(
+            fragment.getRegion(), func_op.getBody());
+        symbol_table.insert(func_op);
+      }
+      rewriter.setInsertionPoint(fragment);
+      bool is_remat = mpmd::IsRemat(fragment);
+      bool is_gspmd_partitioned = fragment->hasAttr(kIsGspmdPartitioned);
+      auto fragment_call_op = rewriter.replaceOpWithNewOp<FragmentCallOp>(
+          fragment, fragment.getResultTypes(), fragment.getOperands(),
+          fragment.getOrigin(), fragment.getMeshName(), group_name);
+      // The following is just for debugging (nb: remat is not an explicit
+      // attribute of fragment calls.).
+      if (is_remat) {
+        mpmd::MarkAsRemat(fragment_call_op, rewriter);
+      }
+      if (is_gspmd_partitioned) {
+        fragment_call_op->setAttr(kIsGspmdPartitioned, rewriter.getUnitAttr());
+      }
+      if (is_sdy_partitioned) {
+        fragment_call_op->setAttr(kIsSdyPartitioned, rewriter.getUnitAttr());
+      }
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/mark_aliasing_and_donation.cc b/shardy/dialect/mpmd/transforms/export/mark_aliasing_and_donation.cc
new file mode 100644
index 0000000..0735de3
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/mark_aliasing_and_donation.cc
@@ -0,0 +1,200 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <optional>
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Analysis/Liveness.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/export/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/export/utils.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_MARKALIASINGANDDONATIONPASS
+#include "shardy/dialect/mpmd/transforms/export/passes.h.inc"
+
+namespace {
+
+// If the input can be aliased with an output, then it returns the index of the
+// output it can be aliased with. An index cannot be aliased to an output if
+// the output index has already been used for aliasing (i.e., it is present in
+// the `aliased_outputs` set)
+std::optional<unsigned int> FindAliasingOutput(
+    FragmentOp op, unsigned int input_index,
+    const DenseSet<unsigned int>& aliased_outputs) {
+  Type input_type = op->getOperand(input_index).getType();
+  for (OpResult output : op->getResults()) {
+    unsigned output_index = output.getResultNumber();
+    // The input can only be aliased with an output if
+    // 1. They are the same type,
+    // 2. the output has not been aliased yet, and
+    // 3. the output is not on host memory.
+    // 4. the input and output have the same layout.
+    if (output.getType() != input_type || IsResultOnHost(output) ||
+        !IsInputOutputLayoutMatch(op, input_index, output_index)) {
+      continue;
+    }
+    if (!aliased_outputs.contains(output_index)) {
+      return output_index;
+    }
+  }
+  return std::nullopt;
+}
+
+// Constructs a set of indices of donated inputs and a map between an input
+// index to an output index that it can be  aliased within a given region-based
+// `op`, given `donatable_input_indices`, a vector of input indices whose last
+// usage is the given operation, `aliased_block_args` a set of block
+// arguments the user has marked to be aliased, and `donated_block_args` a set
+// of block arguments the user has marked to be donated.
+std::pair<DenseSet<unsigned int>, DenseMap<unsigned int, unsigned int>>
+ConstructDonationSetAndIOAliasingMap(
+    FragmentOp op, ArrayRef<unsigned int> donatable_input_indices,
+    const DenseSet<BlockArgument>& aliased_block_args,
+    const DenseSet<BlockArgument>& donated_block_args) {
+  DenseSet<unsigned int> donated_input_indices_set;
+  DenseMap<unsigned int, unsigned int> input_output_aliasing_map;
+
+  // Keep track of which outputs have been aliased. Each output can be aliased
+  // at most once.
+  DenseSet<unsigned int> aliased_outputs;
+
+  for (unsigned int input_index : donatable_input_indices) {
+    // Don't donate values which are on host.
+    if (IsArgOnHost(op, input_index)) {
+      continue;
+    }
+    Value input_value = op->getOperand(input_index);
+
+    // Do not donate inputs that are used in transfer ops because transfer and
+    // fragment ops could overlap which makes the input not suitable for
+    // donation.
+    if (sdy::hasAnyUserOfType<TransferOp>(input_value)) {
+      continue;
+    }
+
+    if (auto block_arg = dyn_cast<BlockArgument>(input_value)) {
+      // Only donate/alias a block argument if the user has marked it to
+      // be donated/aliased.
+      if (donated_block_args.contains(block_arg)) {
+        donated_input_indices_set.insert(input_index);
+      } else if (aliased_block_args.contains(block_arg)) {
+        if (auto aliased_output_idx =
+                FindAliasingOutput(op, input_index, aliased_outputs)) {
+          input_output_aliasing_map[input_index] = *aliased_output_idx;
+          SDY_CHECK(aliased_outputs.insert(*aliased_output_idx).second);
+        } else {
+          // Donate the input if no aliasing output was found.
+          donated_input_indices_set.insert(input_index);
+        }
+      }
+    } else {
+      // Try to find an alias. If no alias is found, then donate the input.
+      if (auto aliased_output_idx =
+              FindAliasingOutput(op, input_index, aliased_outputs)) {
+        input_output_aliasing_map[input_index] = *aliased_output_idx;
+        SDY_CHECK(aliased_outputs.insert(*aliased_output_idx).second);
+      } else {
+        donated_input_indices_set.insert(input_index);
+      }
+    }
+  }
+  return std::make_pair(donated_input_indices_set, input_output_aliasing_map);
+}
+
+void MarkOperandsForAliasingAndDonation(func::FuncOp main_func,
+                                        MLIRContext* ctx) {
+  DenseSet<BlockArgument> aliased_entrypoint_func_args =
+      GetAliasedBlockArguments(main_func);
+  DenseSet<BlockArgument> donated_entrypoint_func_args =
+      GetDonatedBlockArguments(main_func);
+  DenseMap<Operation*, SmallVector<unsigned int>> donation_candidates =
+      OperandsForDeletionMapping(main_func);
+  IRRewriter rewriter(ctx);
+
+  for (auto [op, donatable_input_indices] : donation_candidates) {
+    // Only analyze fragment ops because they are the only ops converted to
+    // functions and we can set the aliasing or donation attributes.
+    auto fragment_op = dyn_cast<FragmentOp>(op);
+    if (!fragment_op) {
+      continue;
+    }
+    // Sort the indices to ensure a deterministic order.
+    llvm::sort(donatable_input_indices);
+
+    // Step 1: Add the indices of the donated block arguments.
+    // Step 2: Construct the aliasing map. The inputs that are not aliasable
+    // will be donated.
+    std::pair<DenseSet<unsigned int>, DenseMap<unsigned int, unsigned int>>
+        donated_idx_set_and_io_aliasing_map =
+            ConstructDonationSetAndIOAliasingMap(
+                fragment_op, donatable_input_indices,
+                aliased_entrypoint_func_args, donated_entrypoint_func_args);
+
+    // Only set the donation and aliasing attributes if there is anything to
+    // alias or donate.
+    if (donated_idx_set_and_io_aliasing_map.first.empty() &&
+        donated_idx_set_and_io_aliasing_map.second.empty()) {
+      continue;
+    }
+
+    SmallVector<Attribute> aliasing_and_donation_attributes =
+        GetArgAttrsOrCreateDefault(fragment_op);
+    for (auto input_index : donated_idx_set_and_io_aliasing_map.first) {
+      InsertAttr(aliasing_and_donation_attributes[input_index],
+                 kBufferDonationAttrName, rewriter.getBoolAttr(true));
+    }
+    for (auto [input_index, output_index] :
+         donated_idx_set_and_io_aliasing_map.second) {
+      InsertAttr(aliasing_and_donation_attributes[input_index],
+                 kAliasingAttrName, rewriter.getI32IntegerAttr(output_index));
+    }
+    SetArgAttrs(op, aliasing_and_donation_attributes);
+  }
+}
+
+class MarkAliasingAndDonationPass
+    : public impl::MarkAliasingAndDonationPassBase<
+          MarkAliasingAndDonationPass> {
+  using MarkAliasingAndDonationPassBase::MarkAliasingAndDonationPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp main_func) override {
+    if (IsMpmdFunction(main_func)) {
+      MarkOperandsForAliasingAndDonation(main_func, main_func.getContext());
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/mark_fragment_reserved_memory.cc b/shardy/dialect/mpmd/transforms/export/mark_fragment_reserved_memory.cc
new file mode 100644
index 0000000..69a49a0
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/mark_fragment_reserved_memory.cc
@@ -0,0 +1,192 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cmath>
+#include <cstdint>
+#include <string_view>
+
+#include "llvm/ADT/DenseSet.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Analysis/Liveness.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/export/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/export/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_MARKFRAGMENTRESERVEDMEMORYPASS
+#include "shardy/dialect/mpmd/transforms/export/passes.h.inc"
+
+namespace {
+
+using OperandLastUseMap = DenseMap<Operation*, SmallVector<unsigned int>>;
+
+// Mapping from the mesh name to how much memory is being used on it (in bytes).
+using LiveMemoryMap = DenseMap<StringRef, double>;
+
+// Gets the size of the tensor in bytes.
+double GetMeshTensorSize(MeshTensorType mesh_tensor_type, Operation* op) {
+  RankedTensorType local_tensor_type = mesh_tensor_type.getLocalTensorType(op);
+  return local_tensor_type.getNumElements() *
+         (static_cast<double>(local_tensor_type.getElementTypeBitWidth()) / 8);
+}
+
+// Gets the current amount of live memory on the mesh the fragment lives
+// on. But any operands need to be excluded from this calculation since
+// XLA already knows about them.
+template <typename FragOpT>
+int64_t GetLiveMemoryBytes(FragOpT op, OpBuilder& builder,
+                           LiveMemoryMap current_memory_usage_per_mesh) {
+  auto it_memory = current_memory_usage_per_mesh.find(op.getMeshName());
+  SDY_CHECK(it_memory != current_memory_usage_per_mesh.end())
+      << "Required mesh_name missing: " << std::string_view(op.getMeshName());
+  double current_size_bytes = it_memory->second;
+
+  llvm::DenseSet<Value> already_seen;
+  for (auto [index, operand] : llvm::enumerate(op.getOperands())) {
+    // If a fragment takes a value multiple times, then only subtract from
+    // the size of live buffers once. We also don't subtract args on host, since
+    // they're not in HBM.
+    if (already_seen.contains(operand) || IsArgOnHost(op, index)) {
+      continue;
+    }
+    already_seen.insert(operand);
+
+    MeshTensorType mesh_tensor_type = cast<MeshTensorType>(operand.getType());
+    current_size_bytes -=
+        GetMeshTensorSize(mesh_tensor_type, op.getOperation());
+  }
+  // Don't expect a program to have values less than a byte, but better to
+  // be safe.
+  return ceil(current_size_bytes);
+}
+
+class MarkFragmentReservedMemoryPass
+    : public impl::MarkFragmentReservedMemoryPassBase<
+          MarkFragmentReservedMemoryPass> {
+  using MarkFragmentReservedMemoryPassBase::MarkFragmentReservedMemoryPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp main_func) override {
+    if (!mpmd::IsMpmdFunction(main_func)) {
+      return;
+    }
+
+    LiveMemoryMap current_memory_usage_per_mesh;
+    Liveness liveness(main_func);
+
+    // Set initial bytes on all meshes to 0.
+    ArrayRef<mpmd::NamedMeshAttr> topology = mpmd::GetTopologyMeshes(main_func);
+    for (auto mesh : topology) {
+      auto [_, inserted] =
+          current_memory_usage_per_mesh.try_emplace(mesh.getName(), 0);
+      SDY_CHECK(inserted) << "Expected mapping to be empty, already saw mesh '"
+                          << std::string_view(mesh.getName()) << "'.";
+    }
+
+    for (BlockArgument& arg : main_func.getArguments()) {
+      // Do not add live values for args that are on the host or that are
+      // donated and not used.
+      if (!IsArgOnHost(main_func, arg.getArgNumber()) &&
+          (!IsArgDonated(main_func, arg.getArgNumber()) || !arg.use_empty())) {
+        MeshTensorType type = cast<MeshTensorType>(arg.getType());
+        AddLiveValue(type, current_memory_usage_per_mesh, main_func);
+      }
+    }
+
+    // Traversal of all ops is in order. Can add to tracked memory usage for
+    // every new result, and remove any values which are the last usage.
+    OpBuilder builder(main_func.getContext());
+    for (Operation& op : main_func.getOps()) {
+      if (auto fragment_op = dyn_cast<FragmentOp>(op)) {
+        fragment_op->setAttr(
+            kReservedHbmBytes,
+            IntegerAttr::get(
+                builder.getI64Type(),
+                GetLiveMemoryBytes(fragment_op, builder,
+                                   current_memory_usage_per_mesh)));
+      }
+
+      if (isa<TransferOp>(op) || isa<FragmentOp>(op)) {
+        // Remove any of the operands which are the last use. If an operand is
+        // an argument of the MPMD program then we can only subtract it here
+        // if it has been donated to the program. Otherwise, we cannot safely
+        // subtract it because we do not know if there are other references to
+        // it outside of the program that would keep it alive. We take the safe
+        // option here and possibly overestimate live buffers.
+        llvm::DenseSet<Value> already_seen;
+        for (auto [index, operand] : llvm::enumerate(op.getOperands())) {
+          if (!IsArgOnHost(&op, index) && already_seen.insert(operand).second &&
+              liveness.isDeadAfter(operand, &op)) {
+            if (auto block_arg = dyn_cast_or_null<BlockArgument>(operand);
+                !block_arg ||
+                IsArgDonated(main_func, block_arg.getArgNumber())) {
+              MeshTensorType mesh_tensor_type =
+                  cast<MeshTensorType>(operand.getType());
+              SubtractLastUse(mesh_tensor_type, current_memory_usage_per_mesh,
+                              &op);
+            }
+          }
+        }
+        // Add sizes of the results which are now live.
+        for (OpResult result : op.getResults()) {
+          if (!IsResultOnHost(result) && !result.use_empty()) {
+            MeshTensorType type = cast<MeshTensorType>(result.getType());
+            AddLiveValue(type, current_memory_usage_per_mesh, &op);
+          }
+        }
+      } else if (isa<func::ReturnOp>(op)) {
+        continue;
+      } else {
+        op.emitError(
+            "Expected only TransferOp, FragmentOp, FragmentCallOp and ReturnOp "
+            "in the function body.");
+        signalPassFailure();
+      }
+    }
+  }
+
+  void AddLiveValue(MeshTensorType type,
+                    LiveMemoryMap& current_memory_usage_per_mesh,
+                    Operation* op) {
+    StringRef mesh_name = type.getMeshName();
+    auto it_memory = current_memory_usage_per_mesh.find(mesh_name);
+    SDY_CHECK(it_memory != current_memory_usage_per_mesh.end())
+        << "Required mesh_name missing: " << std::string_view(mesh_name);
+    it_memory->second += GetMeshTensorSize(type, op);
+  }
+
+  void SubtractLastUse(MeshTensorType type,
+                       LiveMemoryMap& current_memory_usage_per_mesh,
+                       Operation* op) {
+    StringRef mesh_name = type.getMeshName();
+    auto it_memory = current_memory_usage_per_mesh.find(mesh_name);
+    SDY_CHECK(it_memory != current_memory_usage_per_mesh.end())
+        << "Required mesh_name missing: " << std::string_view(mesh_name);
+    it_memory->second -= GetMeshTensorSize(type, op);
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/mark_input_output_with_layouts.cc b/shardy/dialect/mpmd/transforms/export/mark_input_output_with_layouts.cc
new file mode 100644
index 0000000..a4d8e5d
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/mark_input_output_with_layouts.cc
@@ -0,0 +1,256 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/export/passes.h"  // IWYU pragma: keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_MARKINPUTOUTPUTWITHLAYOUTSPASS
+#include "shardy/dialect/mpmd/transforms/export/passes.h.inc"
+
+namespace {
+
+using ::mlir::func::FuncOp;
+
+bool IsAutoLayout(StringAttr layout) {
+  return layout && layout == kLayoutModeAuto;
+}
+
+// A layout attribute denotes a default layout if it is empty or if it is a
+// `default` string.
+bool IsDefaultLayout(StringAttr layout) {
+  return !layout || layout == kLayoutModeDefault;
+}
+
+// Two layouts are compatible if: 1) they are both default layouts, 2) they are
+// identical custom layouts, or 3) one or both are auto layouts.
+bool AreLayoutsCompatible(StringAttr layout1, StringAttr layout2) {
+  if (IsDefaultLayout(layout1) && IsDefaultLayout(layout2)) {
+    return true;
+  }
+  if (IsAutoLayout(layout1) || IsAutoLayout(layout2)) {
+    return true;
+  }
+  return layout1 == layout2;
+}
+
+class MarkInputOutputWithLayoutsPass
+    : public impl::MarkInputOutputWithLayoutsPassBase<
+          MarkInputOutputWithLayoutsPass> {
+  using MarkInputOutputWithLayoutsPassBase::MarkInputOutputWithLayoutsPassBase;
+
+ private:
+  void runOnFunc(FuncOp func) final {
+    if (!IsEntryPointFunction(func)) {
+      return;
+    }
+
+    // Propagate layouts for func args that are returned from the func.
+    // It is necessary to do this before propagating layouts from func args or
+    // results to fragment arg or results because this function might refine
+    // func arg/result layouts (e.g., an auto arg layout is converted to a
+    // custom layout if the arg's corresponding result has a custom layout).
+    if (!PropagateLayoutsForReturnedFuncArgs(func)) {
+      signalPassFailure();
+      return;
+    }
+
+    // Propagate layouts from func args/results to fragment arg/results.
+    for (FragmentOp frag : func.getOps<FragmentOp>()) {
+      if (!PropagateFuncLayoutsToFragments(frag, func)) {
+        signalPassFailure();
+        return;
+      }
+    }
+  }
+
+  // Propagates layouts from func args/results to fragment arg/results.
+  // Returns false if a fragment result is returned multiple times with
+  // incompatible layouts. See `AreLayoutsCompatible` for the definition of when
+  // layouts are considered compatible.
+  bool PropagateFuncLayoutsToFragments(FragmentOp frag, FuncOp parent) {
+    SmallVector<Attribute> arg_attrs = GetArgAttrsOrCreateDefault(frag);
+    for (OpOperand& operand : frag->getOpOperands()) {
+      if (auto block_arg = dyn_cast<BlockArgument>(operand.get())) {
+        SDY_CHECK_EQ(block_arg.getOwner()->getParentOp(), parent);
+        if (auto layout_attr = parent.getArgAttrOfType<StringAttr>(
+                block_arg.getArgNumber(), kLayoutModeAttr)) {
+          if (IsDefaultLayout(layout_attr)) {
+            // Remove the layout attribute because the default layout is
+            // implicit.
+            parent.removeArgAttr(
+                block_arg.getArgNumber(),
+                StringAttr::get(parent.getContext(), kLayoutModeAttr));
+          } else {
+            InsertAttr(arg_attrs[operand.getOperandNumber()], kLayoutModeAttr,
+                       layout_attr);
+          }
+        }
+      }
+    }
+    SetArgAttrs(frag, arg_attrs);
+
+    SmallVector<Attribute> res_attrs = GetResAttrsOrCreateDefault(frag);
+    for (const OpResult& res : frag->getOpResults()) {
+      StringAttr frag_result_layout_attr;
+      int propagated_from_result_idx = -1;
+      for (OpOperand& operand : res.getUses()) {
+        // Only propagate result layouts from the func op to the fragment op.
+        if (operand.getOwner() != parent.front().getTerminator()) {
+          continue;
+        }
+        auto func_result_layout_attr = parent.getResultAttrOfType<StringAttr>(
+            operand.getOperandNumber(), kLayoutModeAttr);
+        if (propagated_from_result_idx == -1 ||
+            IsAutoLayout(frag_result_layout_attr)) {
+          // Propagate from the func result if no layout has been propagated
+          // yet or the propagated layout is auto. It is always safe to
+          // propagate to auto layouts because in the worst case we'll get
+          // another auto layout.
+          propagated_from_result_idx = operand.getOperandNumber();
+          frag_result_layout_attr = func_result_layout_attr;
+        } else if (!AreLayoutsCompatible(func_result_layout_attr,
+                                         frag_result_layout_attr)) {
+          emitError(operand.getOwner()->getLoc())
+              << "Result #" << operand.getOperandNumber()
+              << " is also returned as result #" << propagated_from_result_idx
+              << ", but with incompatible layouts: " << func_result_layout_attr
+              << " vs. " << frag_result_layout_attr;
+          return false;
+        }
+      }
+
+      if (propagated_from_result_idx > -1 && frag_result_layout_attr) {
+        if (!IsDefaultLayout(frag_result_layout_attr)) {
+          InsertAttr(res_attrs[res.getResultNumber()], kLayoutModeAttr,
+                     frag_result_layout_attr);
+        }
+        // Update the parent result layout in case the fragment result is
+        // returned multiple times, and this has helped refined the result
+        // layouts further.
+        for (OpOperand& operand : res.getUses()) {
+          if (operand.getOwner() != parent.front().getTerminator()) {
+            continue;
+          }
+          if (IsDefaultLayout(frag_result_layout_attr)) {
+            parent.removeResultAttr(
+                operand.getOperandNumber(),
+                StringAttr::get(parent.getContext(), kLayoutModeAttr));
+          } else {
+            parent.setResultAttr(operand.getOperandNumber(), kLayoutModeAttr,
+                                 frag_result_layout_attr);
+          }
+        }
+      }
+    }
+    SetResAttrs(frag, res_attrs);
+    return true;
+  }
+
+  // Propagates layouts for func args that are returned from the func. Emits
+  // an error and returns false if a func arg and its corresponding results have
+  // incompatible layouts. Otherwise, it finds the layout that is compatible and
+  // possibly specified (i.e., device and custom layouts) from the func arg and
+  // its corresponding results layouts, and updates their attributes to this
+  // layout.
+  bool PropagateLayoutsForReturnedFuncArgs(FuncOp func) {
+    for (BlockArgument arg : func.getArguments()) {
+      if (arg.use_empty()) {
+        continue;
+      }
+
+      // Find the possibly specified and compatible layout from the results
+      // corresponding to the arg.
+      StringAttr res_layout_attr;
+      int propagated_from_result_idx = -1;
+      for (OpOperand& use : arg.getUses()) {
+        if (!isa<func::ReturnOp>(use.getOwner())) {
+          continue;
+        }
+        auto func_res_layout_attr = func.getResultAttrOfType<StringAttr>(
+            use.getOperandNumber(), kLayoutModeAttr);
+        if (propagated_from_result_idx == -1 || IsAutoLayout(res_layout_attr)) {
+          res_layout_attr = func_res_layout_attr;
+          propagated_from_result_idx = use.getOperandNumber();
+        } else if (!AreLayoutsCompatible(res_layout_attr,
+                                         func_res_layout_attr)) {
+          emitError(func->getLoc())
+              << "Arg #" << arg.getArgNumber() << " is returned as result #"
+              << propagated_from_result_idx << " and result #"
+              << use.getOperandNumber()
+              << ", but with incompatible layouts: " << res_layout_attr
+              << " vs. " << func_res_layout_attr;
+          return false;
+        }
+      }
+
+      // No propagation is required because the arg is not returned.
+      if (propagated_from_result_idx == -1) {
+        continue;
+      }
+      if (auto arg_layout_attr = func.getArgAttrOfType<StringAttr>(
+              arg.getArgNumber(), kLayoutModeAttr);
+          AreLayoutsCompatible(res_layout_attr, arg_layout_attr)) {
+        if (IsAutoLayout(arg_layout_attr)) {
+          arg_layout_attr = res_layout_attr;
+        }
+        // Update the arg layout and the corresponding result layouts to the
+        // compatible layout. If the compatible layout is device default then
+        // we remove the layout attribute because the default layout is
+        // implicit.
+        if (IsDefaultLayout(arg_layout_attr)) {
+          func.removeArgAttr(arg.getArgNumber(), kLayoutModeAttr);
+        } else {
+          func.setArgAttr(arg.getArgNumber(), kLayoutModeAttr, arg_layout_attr);
+        }
+        for (OpOperand& use : arg.getUses()) {
+          if (isa<func::ReturnOp>(use.getOwner())) {
+            if (IsDefaultLayout(arg_layout_attr)) {
+              func.removeResultAttr(
+                  use.getOperandNumber(),
+                  StringAttr::get(func.getContext(), kLayoutModeAttr));
+            } else {
+              func.setResultAttr(use.getOperandNumber(), kLayoutModeAttr,
+                                 arg_layout_attr);
+            }
+          }
+        }
+      } else {
+        emitError(func->getLoc())
+            << "Arg #" << arg.getArgNumber() << " is returned as result #"
+            << propagated_from_result_idx
+            << ", but with incompatible layouts: " << arg_layout_attr << " vs. "
+            << res_layout_attr;
+      }
+    }
+    return true;
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/mark_offloaded_input_output.cc b/shardy/dialect/mpmd/transforms/export/mark_offloaded_input_output.cc
new file mode 100644
index 0000000..de4e170
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/mark_offloaded_input_output.cc
@@ -0,0 +1,334 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <optional>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "mlir/Analysis/Liveness.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Interfaces/CallInterfaces.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/export/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/export/utils.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_MARKOFFLOADEDINPUTOUTPUTPASS
+#include "shardy/dialect/mpmd/transforms/export/passes.h.inc"
+
+namespace {
+
+using ::mlir::func::FuncOp;
+using ::mlir::stablehlo::CustomCallOp;
+
+// Name of the custom_call which indicates moving data from host to device
+// or device to host. E.g. %host_v = mhlo.custom_call
+// @annotate_device_placement(%v) {
+//   mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+// }
+inline constexpr StringRef kOffloadCustomCallName = "annotate_device_placement";
+// Name of Attr which holds further information in a dict.
+inline constexpr StringRef kMhloFrontendAttr = "mhlo.frontend_attributes";
+// Attr in the `kMhloFrontendAttr` dict attr which indicates where the buffer is
+// on: host or device.
+inline constexpr StringRef kXlaBufferPlacementAttr = "_xla_buffer_placement";
+// Attr in the `kMhloFrontendAttr` dict attr which indicates where the compute
+// will take place: host or device.
+inline constexpr StringRef kXlaComputeTypeAttr = "_xla_compute_type";
+inline constexpr StringRef kXlaComputeTypeHost = "host";
+
+// These are compatible custom calls that we have identified are compatible with
+// offload. The list may not be exhaustive. The sharding custom_calls are
+// compatible because they are just annotations for sharding, and are noops
+// otherwise.
+bool IsOffloadCompatibleCustomCall(Operation* op) {
+  if (auto custom_call = dyn_cast<CustomCallOp>(op)) {
+    return custom_call.getCallTargetName() == "Sharding" ||
+           custom_call.getCallTargetName() == "SPMDShardToFullShape" ||
+           custom_call.getCallTargetName() == "SPMDFullToShardShape";
+  }
+  return false;
+}
+
+// Returns true if the reshape just modifies trivial dims, i.e. dims of size 1.
+bool IsTrivialReshape(Operation* op) {
+  if (auto reshape = dyn_cast<stablehlo::ReshapeOp>(op)) {
+    // Check equality excluding dims of size 1.
+    return llvm::equal(
+        llvm::make_filter_range(reshape.getResult().getType().getShape(),
+                                [](int64_t dim) { return dim != 1; }),
+        llvm::make_filter_range(reshape.getOperand().getType().getShape(),
+                                [](int64_t dim) { return dim != 1; }));
+  }
+  if (auto broadcast = dyn_cast<stablehlo::BroadcastInDimOp>(op)) {
+    // Check equality excluding dims of size 1.
+    return llvm::equal(
+        llvm::make_filter_range(broadcast.getResult().getType().getShape(),
+                                [](int64_t dim) { return dim != 1; }),
+        llvm::make_filter_range(broadcast.getOperand().getType().getShape(),
+                                [](int64_t dim) { return dim != 1; }));
+  }
+  return false;
+}
+
+// Retrieves the offload destination of `op` if it is an offload custom call.
+std::optional<StringRef> GetOffloadValueIfExists(Operation* op) {
+  auto custom_call = dyn_cast_if_present<CustomCallOp>(op);
+  if (!custom_call ||
+      custom_call.getCallTargetName() != kOffloadCustomCallName) {
+    return std::nullopt;
+  }
+
+  auto frontend_attr =
+      custom_call->getAttrOfType<DictionaryAttr>(kMhloFrontendAttr);
+  SDY_CHECK(frontend_attr);
+  auto buffer_attr = frontend_attr.getAs<StringAttr>(kXlaBufferPlacementAttr);
+  SDY_CHECK(buffer_attr);
+  return buffer_attr.getValue();
+}
+
+// Returns the operand corresponding to the result of the op, for ops which are
+// compatible with host offload. If the op is not compatible, return nullptr;
+Value WalkBackwardThroughOffloadCompatibleResult(OpResult res) {
+  Operation* op = res.getOwner();
+  if (auto barrier = dyn_cast<stablehlo::OptimizationBarrierOp>(op)) {
+    return barrier->getOperand(res.getResultNumber());
+  }
+  if (auto while_op = dyn_cast<stablehlo::WhileOp>(op)) {
+    return while_op.getBody().front().getTerminator()->getOperand(
+        res.getResultNumber());
+  }
+  if (auto dynamic_update_slice = dyn_cast<stablehlo::DynamicUpdateSliceOp>(op);
+      dynamic_update_slice && dynamic_update_slice.getResult() == res) {
+    return dynamic_update_slice.getUpdate();
+  }
+  if (IsTrivialReshape(op) || IsOffloadCompatibleCustomCall(op)) {
+    return op->getOperand(0);
+  }
+  if (auto call = dyn_cast<func::CallOp>(op)) {
+    auto callee = cast<FuncOp>(cast<CallOpInterface>(*call).resolveCallable());
+    return callee.front().getTerminator()->getOperand(res.getResultNumber());
+  }
+  return nullptr;
+}
+
+// Returns whether a value is a result, and whether the result is stored on the
+// host memory via an annotate custom call on the source of the result, or if
+// the result was computed on host.
+bool IsResultAndOnHostMemory(Value val) {
+  auto res = dyn_cast_if_present<OpResult>(val);
+  if (!res) {
+    return false;
+  }
+
+  if (auto mhlo_frontend_attr =
+          res.getOwner()->getAttrOfType<DictionaryAttr>(kMhloFrontendAttr)) {
+    if (auto compute_type =
+            mhlo_frontend_attr.getAs<StringAttr>(kXlaComputeTypeAttr)) {
+      return compute_type.getValue() == kXlaComputeTypeHost;
+    }
+  }
+  if (Value operand = WalkBackwardThroughOffloadCompatibleResult(res)) {
+    return IsResultAndOnHostMemory(operand);
+  }
+
+  return GetOffloadValueIfExists(res.getOwner()) == kMemoryKindPinnedHost;
+}
+
+// Gets memory kind from user.
+//
+// We don't currently consider transfers, because we don't currently
+// propagate host memory kinds across transfers.
+Attribute GetMemoryKindFromUser(OpOperand& use, FuncOp func) {
+  if (isa<FragmentOp>(use.getOwner())) {
+    return GetArgAttr(use.getOwner(), use.getOperandNumber(), kMemoryKindAttr);
+  }
+  if (isa<func::ReturnOp>(use.getOwner())) {
+    return func.getResultAttr(use.getOperandNumber(), kMemoryKindAttr);
+  }
+  return nullptr;
+}
+
+// Retrieve memory kind from the users of `arg`, checking that they all match.
+// Emits an error otherwise.
+StringAttr GetMemoryKindFromUsers(BlockArgument arg, FuncOp func,
+                                  bool& has_error) {
+  if (arg.use_empty()) {
+    return nullptr;
+  }
+
+  OpOperand& first_use = *arg.getUses().begin();
+  Attribute memory_kind = GetMemoryKindFromUser(first_use, func);
+  for (OpOperand& use : llvm::drop_begin(arg.getUses())) {
+    Attribute user_memory_kind = GetMemoryKindFromUser(use, func);
+
+    if (memory_kind != user_memory_kind) {
+      emitError(arg.getLoc())
+          << "Memory kind mismatch between users of arg " << arg.getArgNumber()
+          << ": " << memory_kind << " vs " << user_memory_kind << "\n"
+          << PrintOperationForLog(first_use.getOwner(),
+                                  OpPrintingFlags().skipRegions())
+          << " vs \n"
+          << PrintOperationForLog(use.getOwner(),
+                                  OpPrintingFlags().skipRegions());
+      has_error = true;
+    }
+  }
+
+  return dyn_cast_if_present<StringAttr>(memory_kind);
+}
+
+// Retrieve memory kind from the defining op of `val`.
+StringAttr GetMemoryKindFromDefiningOp(Value val, FuncOp func) {
+  if (auto block_arg = dyn_cast<BlockArgument>(val)) {
+    return func.getArgAttrOfType<StringAttr>(block_arg.getArgNumber(),
+                                             kMemoryKindAttr);
+  }
+
+  auto op_result = cast<OpResult>(val);
+  if (auto frag = dyn_cast<FragmentOp>(op_result.getOwner())) {
+    return dyn_cast_if_present<StringAttr>(
+        GetResAttr(frag, op_result.getResultNumber(), kMemoryKindAttr));
+  }
+
+  return nullptr;
+}
+
+bool IsOnDevice(StringAttr memory_kind) {
+  return !memory_kind || memory_kind.getValue() == kMemoryKindDevice;
+}
+
+bool HasMismatchedMemories(StringAttr memory_a, StringAttr memory_b) {
+  // Memory is either on device or host.
+  return IsOnDevice(memory_a) != IsOnDevice(memory_b);
+}
+
+class MarkOffloadedInputOutputPass
+    : public impl::MarkOffloadedInputOutputPassBase<
+          MarkOffloadedInputOutputPass> {
+  using MarkOffloadedInputOutputPassBase::MarkOffloadedInputOutputPassBase;
+
+ private:
+  void runOnFunc(FuncOp func) final {
+    bool has_error = false;
+
+    if (!IsEntryPointFunction(func)) {
+      return;
+    }
+
+    // Mark func body first.
+    for (FragmentOp frag : func.getOps<FragmentOp>()) {
+      PropagateHostMemoryKindOnFragments(frag, func);
+    }
+
+    // Mark func args.
+    for (BlockArgument arg : func.getArguments()) {
+      StringAttr arg_memory_kind = func.getArgAttrOfType<StringAttr>(
+          arg.getArgNumber(), kMemoryKindAttr);
+      if (arg.use_empty()) {
+        continue;
+      }
+      StringAttr user_memory_kind =
+          GetMemoryKindFromUsers(arg, func, has_error);
+      if (HasMismatchedMemories(arg_memory_kind, user_memory_kind)) {
+        emitError(arg.getLoc())
+            << "Memory kind mismatch between arg " << arg.getArgNumber()
+            << " and users: " << arg_memory_kind << " vs " << user_memory_kind;
+        has_error = true;
+      }
+    }
+
+    // Mark func results.
+    for (OpOperand& return_operand :
+         func.front().getTerminator()->getOpOperands()) {
+      StringAttr res_memory_kind = func.getResultAttrOfType<StringAttr>(
+          return_operand.getOperandNumber(), kMemoryKindAttr);
+      StringAttr defining_op_memory_kind =
+          GetMemoryKindFromDefiningOp(return_operand.get(), func);
+      if (HasMismatchedMemories(res_memory_kind, defining_op_memory_kind)) {
+        emitError(return_operand.getOwner()->getLoc())
+            << "Memory kind mismatch between result "
+            << return_operand.getOperandNumber()
+            << " and defining op: " << res_memory_kind << " vs "
+            << defining_op_memory_kind;
+        has_error = true;
+      }
+    }
+
+    if (has_error) {
+      signalPassFailure();
+    }
+  }
+
+  // This propagates "host" memory kinds for:
+  // - Func args which are on host.
+  // - Frag args where the operands are on host (e.g. result of an offloaded
+  // fragment result).
+  // - Frag results which come from "activation offloaded" values.
+  // - Frag results where the return value was computed on host.
+  //
+  // It does not currently handle:
+  // - Propagating host annotation across transfers.
+  // - Propagating host annotation from fragment args to fragment result.
+  // - Propagating host annotation through ops implicitly computed on host.
+  //
+  // We don't propagate device memory kinds, because that's the default.
+  void PropagateHostMemoryKindOnFragments(FragmentOp frag, FuncOp parent) {
+    SmallVector<Attribute> arg_attrs = GetArgAttrsOrCreateDefault(frag);
+    for (OpOperand& operand : frag->getOpOperands()) {
+      if (auto result = dyn_cast<OpResult>(operand.get());
+          result && isa<FragmentOp>(result.getOwner()) &&
+          IsResultOnHost(result)) {
+        InsertAttr(arg_attrs[operand.getOperandNumber()], kMemoryKindAttr,
+                   StringAttr::get(frag.getContext(), kMemoryKindPinnedHost));
+        continue;
+      }
+      if (auto block_arg = dyn_cast<BlockArgument>(operand.get());
+          block_arg && IsArgOnHost(parent, block_arg.getArgNumber())) {
+        InsertAttr(arg_attrs[operand.getOperandNumber()], kMemoryKindAttr,
+                   StringAttr::get(frag.getContext(), kMemoryKindPinnedHost));
+        continue;
+      }
+    }
+    SetArgAttrs(frag, arg_attrs);
+
+    SmallVector<Attribute> res_attrs = GetResAttrsOrCreateDefault(frag);
+    for (auto [idx, return_operand] : llvm::enumerate(
+             frag.getRegion().front().getTerminator()->getOperands())) {
+      if (IsResultAndOnHostMemory(return_operand)) {
+        InsertAttr(res_attrs[idx], kMemoryKindAttr,
+                   StringAttr::get(frag.getContext(), kMemoryKindPinnedHost));
+      }
+    }
+    SetResAttrs(frag, res_attrs);
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/naming_utils.cc b/shardy/dialect/mpmd/transforms/export/naming_utils.cc
new file mode 100644
index 0000000..67fc7a8
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/naming_utils.cc
@@ -0,0 +1,373 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/export/naming_utils.h"
+
+#include <array>
+#include <cstddef>
+#include <cstdint>
+#include <functional>
+#include <iterator>
+#include <map>
+#include <optional>
+#include <set>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+
+namespace mlir::mpmd {
+
+namespace {
+
+// Splits a string of the form "abcd_21354" to (a StringRef) "abcd" and (an
+// integer) 21354. If the input cannot be split like this, then the function
+// returns failure(); otherwise success().
+// TODO(dvytin): we might need to generalize this based on further examples.
+LogicalResult SplitToPrefixAndCounter(StringRef name, StringRef& prefix,
+                                            int64_t& counter) {
+  size_t last_separator_pos = name.find_last_of('_');
+  if (last_separator_pos != StringRef::npos) {
+    StringRef suffix = name.substr(last_separator_pos + 1);
+    if (!suffix.consumeInteger(/*Radix=*/10, counter) && suffix.empty()) {
+      prefix = name.substr(0, last_separator_pos);
+      return success();
+    }
+  }
+  return failure();
+}
+
+// Parses the origin attribute and returns a string in a way that also takes any
+// transpose count into account, that we refer to as ``head''. If the original
+// name of the origin was splittable into a prefix and a counter, it also
+// returns the counter otherwise returns nullopt. Some examples:
+//    #mpmd.origin<"foo"(2)>    ~>  head="foo(2)", counter=nullopt
+//    #mpmd.origin<"boo_1">     ~>  head="boo",    counter=1
+//    #mpmd.origin<"boo_1"(3)>  ~>  head="boo(3)", counter=1
+std::pair<std::string, std::optional<int64_t>> ParseToHeadAndCounter(
+    UserOriginAttr origin_attr, bool keep_transpose_count = true) {
+  StringRef origin_ref = origin_attr.getUserName().getValue();
+
+  StringRef head_ref;
+  int64_t counter = 0;
+  bool did_split =
+      SplitToPrefixAndCounter(origin_ref, head_ref, counter).succeeded();
+
+  std::string head;
+  llvm::raw_string_ostream stream(head);
+  stream << (did_split ? head_ref : origin_ref);
+
+  if (keep_transpose_count && origin_attr.getTransposeCount() > 0) {
+    stream << "(" << origin_attr.getTransposeCount() << ")";
+  }
+  std::optional<int64_t> parsed_counter =
+      did_split ? std::make_optional(counter) : std::nullopt;
+  return std::make_pair(std::move(head), parsed_counter);
+}
+
+// An auxiliary struct for creating an informative fragment name for sequences
+// of UserOrigin attributes.
+struct PrinterState {
+  std::string head;
+  // TODO(dvytin): modify to allow bwd origin counters to be printed in reverse.
+  int64_t min;
+  int64_t max;
+
+  std::string ToString() const {
+    std::string ret = StrCat(this->head, "_", this->min);
+    if (this->max != this->min) {
+      return StrCat(ret, ":", this->max);
+    }
+    return ret;
+  }
+};
+
+}  // namespace
+
+std::string Truncate(StringRef str, int64_t max_length) {
+  SDY_CHECK_GE(max_length, 32);  // A comfortable width to fit the unique id.
+  StringRef delimiter = "<...>";
+  if (str.size() > max_length) {
+    return StrCat(str.substr(0, max_length - delimiter.size()), delimiter);
+  }
+  return str.str();
+}
+
+std::string GetInformativeFragmentName(ArrayRef<Attribute> origin) {
+  std::vector<std::string> constructed;
+  std::optional<PrinterState> current_state = std::nullopt;
+
+  for (Attribute attr : origin) {
+    UserOriginAttr origin_attr = cast<UserOriginAttr>(attr);
+    const auto& [head, parsed_counter] = ParseToHeadAndCounter(origin_attr);
+    if (!parsed_counter.has_value()) {
+      // Failed to split. Dump and reset any current state.
+      if (current_state.has_value()) {
+        constructed.push_back((*current_state).ToString());
+        current_state = std::nullopt;
+      }
+      // And then dump this attribute's head.
+      constructed.push_back(std::move(head));
+      continue;
+    }
+    // Successful split.
+    if (!current_state.has_value()) {
+      current_state = PrinterState{std::move(head), /*min=*/*parsed_counter,
+                                   /*max=*/*parsed_counter};
+      continue;
+    }
+    // Current state has value (and successful split).
+    if (head == current_state->head) {
+      // Same head.
+      if (*parsed_counter == current_state->min - 1) {
+        current_state->min = *parsed_counter;
+      } else if (*parsed_counter == current_state->max + 1) {
+        current_state->max = *parsed_counter;
+      } else {
+        constructed.push_back((*current_state).ToString());
+        current_state->min = *parsed_counter;
+        current_state->max = *parsed_counter;
+      }
+    } else {
+      // Different head.
+      constructed.push_back((*current_state).ToString());
+      current_state->head = std::move(head);
+      current_state->min = *parsed_counter;
+      current_state->max = *parsed_counter;
+    }
+  }
+  if (current_state.has_value()) {
+    constructed.push_back((*current_state).ToString());
+  }
+
+  std::string result;
+  llvm::raw_string_ostream stream(result);
+
+  if (constructed.empty()) {
+    stream << "inferred";
+  }
+  llvm::interleave(constructed, stream, ".");
+  return result;
+}
+
+namespace {
+
+inline constexpr StringRef kForwardKeyword = "fwd";
+inline constexpr StringRef kBackwardKeyword = "bwd";
+inline constexpr StringRef kTransposeKeyword = "transpose";
+
+std::string GetMostFrequentName(ArrayRef<Attribute> origins) {
+  // Keeps track of each name and the counters associated with it.
+  // We use a sorted map since in case of draws in popularity, we use
+  // lexicographic order to break ties.
+  std::map<std::string, std::vector<std::optional<int64_t>>, std::less<>>
+      name_to_counters;
+  bool is_bwd = true;
+  for (Attribute attr : origins) {
+    UserOriginAttr origin_attr = cast<UserOriginAttr>(attr);
+    const auto& [head, parsed_counter] =
+        ParseToHeadAndCounter(origin_attr, /*keep_transpose_count=*/false);
+    name_to_counters[head].push_back(parsed_counter);
+    is_bwd = is_bwd && origin_attr.getTransposeCount() == 1;
+  }
+
+  // When we have many candidates for naming, we pick the most popular one and
+  // append "..." to the end of the name.
+  StringRef tail = name_to_counters.size() == 1 ? "" : "...";
+
+  // Find the most popular name.
+  auto it =
+      llvm::max_element(name_to_counters, [](const auto& a, const auto& b) {
+        return a.second.size() < b.second.size();
+      });
+  std::set<int64_t> counters;
+  for (const auto& counter : it->second) {
+    if (counter.has_value()) {
+      counters.insert(*counter);
+    }
+  }
+  if (counters.empty()) {
+    // There's no counter associated with this name.
+    return StrCat(it->first, tail);
+  }
+  if (counters.size() == 1) {
+    return StrCat(it->first, "_", *counters.begin(), tail);
+  }
+  // TODO: jupvfranco - Consider the rare cases in which the counters are not
+  // contiguous.
+  int64_t min = *llvm::min_element(counters);
+  int64_t max = *llvm::max_element(counters);
+  return is_bwd ? StrCat(it->first, "_", max, ":", min, tail)
+                : StrCat(it->first, "_", min, ":", max, tail);
+}
+
+void GetPhasesFromUserOrigins(ArrayRef<Attribute> origins,
+                              std::vector<std::string>& constructed) {
+  std::set<int64_t> transpose_counts;
+  for (Attribute attr : origins) {
+    auto origin = cast<UserOriginAttr>(attr);
+    transpose_counts.insert(origin.getTransposeCount());
+  }
+  llvm::transform(transpose_counts, std::back_inserter(constructed),
+                  [](int64_t transpose_count) -> std::string {
+                    if (transpose_count == 0) {
+                      return kForwardKeyword.str();
+                    }
+                    if (transpose_count == 1) {
+                      return kBackwardKeyword.str();
+                    }
+                    return StrCat(kTransposeKeyword, transpose_count);
+                  });
+}
+
+}  // namespace
+
+// Note: unique id and function name aren't included.
+std::string GetFullNameFromMetadata(ArrayRef<Attribute> origins,
+                                    std::optional<int64_t> stage_id) {
+  std::vector<std::string> constructed;
+
+  // Step 0. Find a name for the fragment.
+  if (stage_id.has_value()) {
+    constructed.push_back(StrCat("stage", *stage_id));
+  } else if (origins.empty()) {
+    constructed.push_back("inferred");
+  } else {
+    constructed.push_back(GetMostFrequentName(origins));
+  }
+
+  // Step 1. Find the phases.
+  GetPhasesFromUserOrigins(origins, constructed);
+
+  std::string result;
+  llvm::raw_string_ostream stream(result);
+  llvm::interleave(constructed, stream, "_");
+  return result;
+}
+
+namespace {
+
+bool AllHaveCallCounters(ArrayRef<CallSite> call_sites) {
+  return llvm::all_of(call_sites, [](const CallSite& call_site) {
+    return call_site.second.has_value();
+  });
+}
+
+bool IsAscendingAndContiguous(ArrayRef<CallSite> call_sites) {
+  for (int i = 1; i < call_sites.size(); ++i) {
+    if (*call_sites[i - 1].second + 1 != *call_sites[i].second) {
+      return false;
+    }
+  }
+  return true;
+}
+
+bool IsDescendingAndContiguous(ArrayRef<CallSite> call_sites) {
+  for (int i = 1; i < call_sites.size(); ++i) {
+    if (*call_sites[i - 1].second - 1 != *call_sites[i].second) {
+      return false;
+    }
+  }
+  return true;
+}
+
+// Requires: call_sites.size() > 1;
+bool IsNamingConsistent(ArrayRef<CallSite> call_sites) {
+  SDY_CHECK_GT(call_sites.size(), 1);
+  for (const CallSite& call_site : call_sites) {
+    if (call_site.first != call_sites[0].first) {
+      return false;
+    }
+  }
+  return true;
+}
+
+std::optional<std::string> GetMeshCallCountersSummary(
+    ArrayRef<CallSite> call_sites) {
+  SDY_CHECK(!call_sites.empty());
+
+  if (call_sites.size() == 1) {
+    auto [name, call_counter] = call_sites[0];
+    if (call_counter.has_value()) {
+      return StrCat("call", *call_counter);
+    }
+    return std::nullopt;
+  }
+
+  if (!IsNamingConsistent(call_sites)) {
+    return std::nullopt;
+  }
+
+  if (!AllHaveCallCounters(call_sites)) {
+    return std::nullopt;
+  }
+  auto compare_fn = [](const CallSite& a, const CallSite& b) {
+    return a.second.value() < b.second.value();
+  };
+
+  if (IsAscendingAndContiguous(call_sites)) {
+    const uint32_t min = *llvm::min_element(call_sites, compare_fn)->second;
+    const uint32_t max = *llvm::max_element(call_sites, compare_fn)->second;
+    return StrCat("calls", min, "to", max);
+  }
+
+  if (IsDescendingAndContiguous(call_sites)) {
+    const uint32_t min = *llvm::min_element(call_sites, compare_fn)->second;
+    const uint32_t max = *llvm::max_element(call_sites, compare_fn)->second;
+    return StrCat("calls", max, "from", min);
+  }
+
+  return std::nullopt;
+}
+
+}  // namespace
+
+std::string GetCallSitesSummaryName(const MeshToCallSites& mesh_call_sites) {
+  std::set<std::string> all_summaries;
+  bool has_name_without_call_counter = false;
+  // Keep all names in a sorted set to ensure deterministic behavior.
+  std::set<std::string> all_names;
+  for (const auto& [_, call_sites] : mesh_call_sites) {
+    all_names.insert(call_sites[0].first);
+    std::optional<std::string> summary = GetMeshCallCountersSummary(call_sites);
+    if (summary.has_value()) {
+      all_summaries.insert(*summary);
+    } else {
+      has_name_without_call_counter = true;
+    }
+  }
+
+  SDY_CHECK(!all_names.empty());
+  StringRef name = *all_names.begin();
+  if (all_summaries.size() == 1 && !has_name_without_call_counter) {
+    const std::optional<std::string>& summary = *all_summaries.begin();
+    return StrCat(name, "_", *summary);
+  }
+  return name.str();
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/naming_utils.h b/shardy/dialect/mpmd/transforms/export/naming_utils.h
new file mode 100644
index 0000000..cb98943
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/naming_utils.h
@@ -0,0 +1,102 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_NAMING_UTILS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_NAMING_UTILS_H_
+
+#include <cstdint>
+#include <optional>
+#include <string>
+#include <utility>
+
+#include "mlir/IR/Attributes.h"
+#include "mlir/Support/LLVM.h"
+
+namespace mlir::mpmd {
+
+// Returns an informative string for an array of UserOrigin attributes, suitable
+// for becoming a fragment function symbol name, by compressing together any
+// consecutive blocks that have been parsed with the same head and also have
+// consecutive counters. For example:
+//    ["f_1", "f_2", "f_3", "loss", "f_3"(1), "f_2"(1), "f_1"(1)]
+// will print as:
+//    "f_1:3.loss.f(1)_1:3"
+std::string GetInformativeFragmentName(ArrayRef<Attribute> origin);
+
+// Small utility to truncate a string up to max_length.
+std::string Truncate(StringRef str, int64_t max_length);
+
+// Returns a name for a fragment based on its metadata, i.e., its user origins,
+// stage id and call counter, when defined.
+//
+// The returned name has the following format:
+//   <name_from_origin>_<phase>
+//
+// Where `name_from_origin` includes a summary of all user origins:
+//   - "stage<stage_id>" if `stage_id` is defined.
+//   - "inferred" if the list of origins is empty.
+//   - or the most frequent name from the list of origins, followed by a summary
+//   of counters (e.g., block_1:3 for block 1, block 2, and block 3) followed by
+//   "..." if there are other names different from the most frequent one.
+//
+// `phase` is a summary of the phases from the list of origins:
+//   - "fwd" for each origin with transpose count 0.
+//   - "bwd" for each origin with transpose count 1.
+//   - "transpose<counter> for each origin with transpose count > 1.
+//
+// Note we don't include `call_counter` as part of the metadata used to generate
+// the name, as fragments are often reused across different calls.
+std::string GetFullNameFromMetadata(ArrayRef<Attribute> origins,
+                                    std::optional<int64_t> stage_id);
+
+// A call site is a pair of fragment name and optional call counter.
+using CallSite = std::pair<std::string, std::optional<uint32_t>>;
+
+using MeshToCallSites = DenseMap<StringRef, SmallVector<CallSite>>;
+
+// Given a map from mesh names to lists of `CallSite`s, returns a name for the
+// group of fragments that share the same code. The name is a best effort
+// attempt to summarize the call sites across all meshes.
+//
+// The returned name has the following format:
+//   <name>_<call_counter_summary>
+//
+// Where `name` is the name of the first (in lexicographic order) call site,
+// computed from each fragment's metadata, and `call_counter_summary` is a
+// summary of the call counters across all meshes, or empty if there is no call
+// counter or if the call counters are inconsistent across meshes.
+//
+// Example:
+//   - stage2_fwd_calls0to1
+//   - stage2_fwd_calls1from0
+//
+// The call counter summary of each mesh is computed as follows:
+//   - If there's a single call site, then we simply return "_call{counter}" or
+// empty string otherwise.
+//   - Otherwise, we check if the different call-sites are consistent with each
+// other, assuming the different call sites result from a (forward or backward)
+// loop over the same fragment (as it would happen with microbatching).
+// In particular, if the different call sites have different names, or are not
+// contiguously numbered, or they are not in ascending or descending order
+// (based on a program order), then they most likely don't belong to the same
+// loop. In these cases, we return an empty string. But when they are indeed
+// consistent, then we return: "_calls{min_counter}to{max_counter}" for the
+// ascending case, and "_calls{max_counter}from{min_counter}" for the descending
+// case.
+std::string GetCallSitesSummaryName(const MeshToCallSites& mesh_call_sites);
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_NAMING_UTILS_H_
diff --git a/shardy/dialect/mpmd/transforms/export/naming_utils_test.cc b/shardy/dialect/mpmd/transforms/export/naming_utils_test.cc
new file mode 100644
index 0000000..3af5f71
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/naming_utils_test.cc
@@ -0,0 +1,361 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/export/naming_utils.h"
+
+#include <cstdint>
+#include <optional>
+#include <string>
+
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include <gmock/gmock.h>
+#include <gtest/gtest.h>
+
+namespace mlir::mpmd {
+
+namespace {
+
+using ::testing::EndsWith;
+
+UserOriginAttr GetUserOrigin(MLIRContext* ctx, StringRef name,
+                             int64_t transpose_count) {
+  return UserOriginAttr::get(ctx, StringAttr::get(ctx, name), transpose_count);
+}
+
+TEST(InformativeFragmentNameTests, EmptyOrigin) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({}), "inferred");
+}
+
+TEST(InformativeFragmentNameTests, SingleOriginUnsplittable) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "f_25_g", 0)}),
+            "f_25_g");
+}
+
+TEST(InformativeFragmentNameTests, SingleOriginUnsplittableTransposed) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "f_25_g", 3)}),
+            R"origin(f_25_g(3))origin");
+}
+
+TEST(InformativeFragmentNameTests, SingleOriginSplittable) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "f_2", 0)}), "f_2");
+}
+
+TEST(InformativeFragmentNameTests, SingleOriginSplittableTransposed) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "f_2", 1)}),
+            R"origin(f(1)_2)origin");
+}
+
+TEST(InformativeFragmentNameTests, ConsecutiveBlocks) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "block_0", 0),
+                                        GetUserOrigin(&ctx, "block_1", 0),
+                                        GetUserOrigin(&ctx, "block_2", 0)}),
+            "block_0:2");
+}
+
+TEST(InformativeFragmentNameTests, ConsecutiveBlocksTransposed) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "block_2", 1),
+                                        GetUserOrigin(&ctx, "block_1", 1),
+                                        GetUserOrigin(&ctx, "block_0", 1)}),
+            R"origin(block(1)_0:2)origin");
+}
+
+TEST(InformativeFragmentNameTests, ConsecutiveBlocksBroken) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "block_0", 0),
+                                        GetUserOrigin(&ctx, "block_2", 0),
+                                        GetUserOrigin(&ctx, "block_3", 0)}),
+            "block_0.block_2:3");
+}
+
+TEST(InformativeFragmentNameTests, ConsecutiveBlocksBrokenByTranspose) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "block_0", 0),
+                                        GetUserOrigin(&ctx, "block_1", 1),
+                                        GetUserOrigin(&ctx, "block_2", 1)}),
+            R"origin(block_0.block(1)_1:2)origin");
+}
+
+TEST(InformativeFragmentNameTests, ConsecutiveBlocksBrokenAtEnd) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "block_0", 0),
+                                        GetUserOrigin(&ctx, "block_1", 0),
+                                        GetUserOrigin(&ctx, "loss", 0)}),
+            "block_0:1.loss");
+}
+
+TEST(InformativeFragmentNameTests, ConsecutiveBlocksBrokenAtStart) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "encoder", 0),
+                                        GetUserOrigin(&ctx, "block_0", 0),
+                                        GetUserOrigin(&ctx, "block_1", 0)}),
+            "encoder.block_0:1");
+}
+
+TEST(InformativeFragmentNameTests, ConsecutiveBlocksBrokenAtEndByTranspose) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetInformativeFragmentName({GetUserOrigin(&ctx, "block_0", 0),
+                                        GetUserOrigin(&ctx, "block_1", 0),
+                                        GetUserOrigin(&ctx, "block_2", 0),
+                                        GetUserOrigin(&ctx, "block_2", 1)}),
+            R"origin(block_0:2.block(1)_2)origin");
+}
+
+TEST(InformativeFragmentNameTests, ConsecutiveBlocksBrokenAtMiddle) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(
+      GetInformativeFragmentName(
+          {GetUserOrigin(&ctx, "block_0", 0), GetUserOrigin(&ctx, "block_1", 0),
+           GetUserOrigin(&ctx, "loss", 0), GetUserOrigin(&ctx, "block_1", 1),
+           GetUserOrigin(&ctx, "block_0", 1)}),
+      R"origin(block_0:1.loss.block(1)_0:1)origin");
+}
+
+TEST(TruncateTest, SmallString) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(Truncate("abcdefg", 32), "abcdefg");
+}
+
+TEST(TruncateTest, BigString) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  std::string truncated =
+      Truncate("12345678123456781234567812345678andsomemore", 32);
+  EXPECT_EQ(truncated.size(), 32);
+  EXPECT_THAT(truncated, EndsWith("<...>"));
+  EXPECT_EQ(truncated, "123456781234567812345678123<...>");
+}
+
+TEST(GetFullNameFromMetadataTests, EmptyOriginStage) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata({}, std::nullopt), "inferred");
+}
+
+TEST(GetFullNameFromMetadataTests, EmptyOriginWithStage) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata({}, 3), "stage3");
+}
+
+TEST(GetFullNameFromMetadataTests, SingleOriginWithoutStage) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(
+      GetFullNameFromMetadata({GetUserOrigin(&ctx, "foo", 0)}, std::nullopt),
+      "foo_fwd");
+}
+
+TEST(GetFullNameFromMetadataTests, SingleOriginWithMultipleTransposes) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(
+      GetFullNameFromMetadata({GetUserOrigin(&ctx, "bar", 5)}, std::nullopt),
+      "bar_transpose5");
+}
+
+TEST(GetFullNameFromMetadataTests,
+     ManyOriginWithCommonNamesButSameBlockCounter) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata({GetUserOrigin(&ctx, "block_6", 0),
+                                     GetUserOrigin(&ctx, "block_6", 0),
+                                     GetUserOrigin(&ctx, "block_6", 0)},
+                                    std::nullopt),
+            "block_6_fwd");
+}
+
+TEST(GetFullNameFromMetadataTests,
+     ManyOriginWithCommonNamesAndDifferentBlockCounters) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata({GetUserOrigin(&ctx, "block_6", 0),
+                                     GetUserOrigin(&ctx, "block_7", 0),
+                                     GetUserOrigin(&ctx, "block_8", 0)},
+                                    std::nullopt),
+            "block_6:8_fwd");
+}
+
+TEST(GetFullNameFromMetadataTests,
+     ManyOriginWithCommonNamesWithReverseBlockCountersButFwd) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata({GetUserOrigin(&ctx, "block_6", 0),
+                                     GetUserOrigin(&ctx, "block_5", 0),
+                                     GetUserOrigin(&ctx, "block_4", 0)},
+                                    std::nullopt),
+            "block_4:6_fwd");
+}
+
+TEST(GetFullNameFromMetadataTests,
+     ManyOriginWithCommonNamesAssumesContiguousBlockCounters) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata({GetUserOrigin(&ctx, "block_6", 0),
+                                     GetUserOrigin(&ctx, "block_4", 0)},
+                                    std::nullopt),
+            "block_4:6_fwd");
+}
+
+TEST(GetFullNameFromMetadataTests, ManyOriginWithCommonNamesAndBwd) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata({GetUserOrigin(&ctx, "block_6", 1),
+                                     GetUserOrigin(&ctx, "block_5", 1),
+                                     GetUserOrigin(&ctx, "block_4", 1)},
+                                    std::nullopt),
+            "block_6:4_bwd");
+}
+
+TEST(GetFullNameFromMetadataTests, PickMostPopularNameandDropLeastPopular) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(
+      GetFullNameFromMetadata(
+          {GetUserOrigin(&ctx, "block_6", 1), GetUserOrigin(&ctx, "block_5", 1),
+           GetUserOrigin(&ctx, "block_4", 1), GetUserOrigin(&ctx, "scan", 1)},
+          std::nullopt),
+      "block_6:4..._bwd");
+}
+
+TEST(GetFullNameFromMetadataTests, PickMostPopularNameandDropLeastPopular2) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(
+      GetFullNameFromMetadata(
+          {GetUserOrigin(&ctx, "scan_6", 1), GetUserOrigin(&ctx, "scan_5", 1),
+           GetUserOrigin(&ctx, "scan_4", 1), GetUserOrigin(&ctx, "block", 1)},
+          std::nullopt),
+      "scan_6:4..._bwd");
+}
+
+TEST(GetFullNameFromMetadataTests, MixedFwdAndBwd) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  // The block numbers aren't reversed as this fragment isn't completely bwd
+  // (i.e. the scan origin has transpose count = 0).
+  EXPECT_EQ(GetFullNameFromMetadata({GetUserOrigin(&ctx, "block_6", 1),
+                                     GetUserOrigin(&ctx, "block_5", 1),
+                                     GetUserOrigin(&ctx, "scan", 0)},
+                                    std::nullopt),
+            "block_5:6..._fwd_bwd");
+}
+
+TEST(GetFullNameFromMetadataTests, SameNameFwdAndBwd) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata(
+                {GetUserOrigin(&ctx, "foo", 0), GetUserOrigin(&ctx, "foo", 1)},
+                std::nullopt),
+            "foo_fwd_bwd");
+}
+
+TEST(GetFullNameFromMetadataTests, DifferentNamesFwdAndBwd) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  EXPECT_EQ(GetFullNameFromMetadata(
+                {GetUserOrigin(&ctx, "foo", 0), GetUserOrigin(&ctx, "bar", 1)},
+                std::nullopt),
+            "bar..._fwd_bwd");
+}
+
+TEST(GetCallSitesSummaryNameTests, SingleCallSiteWithCallCounter) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  MeshToCallSites mesh_call_sites = {{"mesh1", {{"foo", 0}}}};
+  EXPECT_EQ(GetCallSitesSummaryName(mesh_call_sites), "foo_call0");
+}
+
+TEST(GetCallSitesSummaryNameTests, SingleCallSiteWithoutCallCounter) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  MeshToCallSites mesh_call_sites = {{"mesh1", {{"foo", std::nullopt}}}};
+  EXPECT_EQ(GetCallSitesSummaryName(mesh_call_sites), "foo");
+}
+
+TEST(GetCallSitesSummaryNameTests,
+     ManyCallsInSingleMeshWithAscendingAndContiguousCallCounter) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  MeshToCallSites mesh_call_sites = {{"mesh1", {{"x", 0}, {"x", 1}, {"x", 2}}}};
+  EXPECT_EQ(GetCallSitesSummaryName(mesh_call_sites), "x_calls0to2");
+}
+
+TEST(GetCallSitesSummaryNameTests,
+     ManyCallsInSingleMeshWithDescendingAndContiguousCallCounter) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  MeshToCallSites mesh_call_sites = {{"mesh1", {{"x", 2}, {"x", 1}, {"x", 0}}}};
+  EXPECT_EQ(GetCallSitesSummaryName(mesh_call_sites), "x_calls2from0");
+}
+
+TEST(GetCallSitesSummaryNameTests, ManyCallsInSingleMeshWithInconsistentNames) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  MeshToCallSites mesh_call_sites = {{"mesh1", {{"x", 0}, {"y", 1}, {"z", 2}}}};
+  EXPECT_EQ(GetCallSitesSummaryName(mesh_call_sites), "x");
+}
+
+TEST(GetCallSitesSummaryNameTests,
+     ManyCallsInSingleMeshWithDescendingButNotContiguousCallCounter) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  MeshToCallSites mesh_call_sites = {{"mesh1", {{"x", 3}, {"x", 1}}}};
+  EXPECT_EQ(GetCallSitesSummaryName(mesh_call_sites), "x");
+}
+
+TEST(GetCallSitesSummaryNameTests, MultipleMeshesWithConsistentCallCounters) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  MeshToCallSites mesh_call_sites = {{"mesh1", {{"x", 0}, {"x", 1}, {"x", 2}}},
+                                     {"mesh2", {{"y", 0}, {"y", 1}, {"y", 2}}}};
+  EXPECT_EQ(GetCallSitesSummaryName(mesh_call_sites), "x_calls0to2");
+}
+
+TEST(GetCallSitesSummaryNameTests, MultipleMeshesWithInconsistentCallCounters) {
+  MLIRContext ctx;
+  ctx.loadDialect<MpmdDialect>();
+  MeshToCallSites mesh_call_sites = {
+      {"mesh1", {{"x", 0}, {"x", 1}, {"x", 2}}},
+      {"mesh2", {{"y", 0}, {"y", std::nullopt}, {"y", 3}}}};
+  EXPECT_EQ(GetCallSitesSummaryName(mesh_call_sites), "x");
+}
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/passes.h b/shardy/dialect/mpmd/transforms/export/passes.h
new file mode 100644
index 0000000..52bdb39
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/passes.h
@@ -0,0 +1,60 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_PASSES_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_PASSES_H_
+
+// IWYU pragma: begin_keep
+
+#include <memory>
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/transforms/common/distributed_function_pass.h"
+
+// IWYU pragma: end_keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DECL
+#define GEN_PASS_REGISTRATION
+#include "shardy/dialect/mpmd/transforms/export/passes.h.inc"
+
+// Options for the export pipeline.
+struct ExportOptions {
+  // Whether to copy constants produced in one fragment to their consumers,
+  // possibly through transfers.
+  bool copyConstantsFromProducerToConsumer = false;
+  // Whether to do more aggressive fragment grouping, across meshes. This may
+  // not be desirable for heterogeneous systems.
+  bool groupFragmentsAcrossMeshes = true;
+  // Whether to apply the merge transfers optimization pass.
+  bool applyMergeTransfers = false;
+  // Whether to enable verbose logging.
+  bool verboseLogging = false;
+};
+
+// Adds the standard set of passes to export an MPMD program.
+void addExportPipeline(OpPassManager& pm, const ExportOptions& options = {});
+
+// Register the `-mpmd-export-pipeline`.
+void registerExportPipeline();
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_PASSES_H_
diff --git a/shardy/dialect/mpmd/transforms/export/passes.td b/shardy/dialect/mpmd/transforms/export/passes.td
new file mode 100644
index 0000000..0157e99
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/passes.td
@@ -0,0 +1,124 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+def LowerToFragmentCallsPass : Pass<"mpmd-lower-to-fragment-calls", "ModuleOp"> {
+  let summary = "Lowers MPMD fragments to fragment calls and functions.";
+  let description = [{
+    Replaces all fragments with fragment calls.
+
+    This pass creates a function for every group of fragments that have an
+    identical body and mesh shape, with the name of the first encountered
+    fragment in the group as the symbol name, and adds it to the symbol table.
+
+    The body of each function is extracted from the body of the first
+    encountered fragment in the respective group, and the mesh shape is taken
+    from the topology by the mesh name of that fragment. If the fragment has
+    argument attributes about input-output aliasing, they will be assigned to
+    the argument attributes of the lowered function.
+
+    Since functions must have unique names, this pass appends an index to the
+    name of all but the first function with the same original name, i.e., the
+    ith function with name "some_name" for i > 0 will have the name
+    "some_name_i".
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect", "mlir::sdy::SdyDialect"];
+
+  let options = [
+    Option<"groupAcrossMeshes", "group-across-meshes", "bool",
+           /*default=*/"false",
+           "Whether to do more aggressive fragment grouping, across meshes. "
+           "This may not be desirable for heterogeneous systems.">,
+    Option<"verboseLogging", "verbose-logging", "bool", /*default=*/"false",
+           "Whether to enable verbose logging">,
+  ];
+}
+
+def MarkAliasingAndDonationPass :
+        PassBase<"mpmd-mark-aliasing-and-donation", "DistributedFunctionPass"> {
+  let summary = "Marks each fragment with aliasing or donation information.";
+  let description = [{
+    Sets an `arg_attrs` attribute to Fragment ops when any of their inputs
+    can be aliased with an output or donated. Each input that can be aliased
+    will have a `tf.aliasing_output` attribute. Otherwise, a
+    `jax.buffer_donor = true` attribute. For example, `{arg_attrs =
+    \[{tf.aliasing_output = 0 : i32}, {jax.buffer_donor = true}, {}\]}` shows
+    the first input can be aliased with output 0, the second input can be
+    donated so that XLA will find an aliased output, and the third input can't
+    be aliased or donated.
+  }];
+}
+
+def MarkFragmentReservedMemoryPass :
+        PassBase<"mpmd-mark-fragment-reserved-memory", "DistributedFunctionPass"> {
+  let summary = "Mark each fragment with the amount of memory that needs to be "
+                "reserved for compilation.";
+  let description = [{
+    Assigns an `xla_tpu_user_reserved_hbm_bytes` attribute to each fragment
+    which will tell XLA how many bytes to keep around while compiling each
+    fragment. By keeping track of live tensors on a mesh, XLA will know of
+    actual minimum memory usage at the time of execution, and we can prevent
+    it from applying optimizations in the executable that would increase
+    memory usage beyond the device capacity.
+    NOTE: this pass assumes that fragments are executed in program order.
+  }];
+}
+
+def MarkInputOutputWithLayoutsPass :
+        PassBase<"mpmd-mark-input-output-with-layouts", "DistributedFunctionPass"> {
+  let summary = "Propagates layouts from func args/results to fragment "
+                "args/results.";
+  let description = [{
+    Propagates `mhlo.layout_mode` attributes from program inputs to fragments
+    that are consumers of program input, and propagate `mhlo.layout_mode`
+    attributes from program outputs to fragments that are output producers.
+    If a program argument is returned then `mhlo.layout_mode` is propagated
+    to/from program results. See `PropagateLayoutsForReturnedFuncArgs`'s
+    comment for a detailed description of the propagation logic.
+  }];
+}
+
+// TODO: b/374994155 - Consider using memory-kind field of mesh types instead.
+def MarkOffloadedInputOutputPass :
+        PassBase<"mpmd-mark-offloaded-input-output", "DistributedFunctionPass"> {
+  let summary = "Marks offloaded input and output values so the compiler knows "
+                "they are in host memory.";
+  let description = [{
+    Marks fragment args and results with attributes to identify which values
+    live in host memory, so that this information can be used by XLA. Also marks
+    the entrypoint func args and results so that Pathways can use this
+    information.
+  }];
+}
+
+def DelayInferredFragmentsPass :
+        PassBase<"mpmd-delay-inferred-fragments", "DistributedFunctionPass"> {
+  let summary = "Delays inferred fragments to be executed as late as possible.";
+  let description = [{
+     Moves inferred fragments to be executed as late as possible, i.e., right
+     before their first consumer.
+  }];
+}
+
+def DelayTransfersFromCpuPass :
+        PassBase<"mpmd-delay-transfers-from-cpu", "DistributedFunctionPass"> {
+  let summary = "Delays transfers from CPU to be executed as late as possible.";
+  let description = [{
+    Moves cpu-to-device transfer ops right before their first consumer. This
+    means postponing the allocation of memory for the transferred buffers, which
+    can be beneficial for HBM usage.
+  }];
+}
diff --git a/shardy/dialect/mpmd/transforms/export/reschedule_ops.cc b/shardy/dialect/mpmd/transforms/export/reschedule_ops.cc
new file mode 100644
index 0000000..10a6825
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/reschedule_ops.cc
@@ -0,0 +1,115 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <vector>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Analysis/Liveness.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/export/passes.h"  // IWYU pragma: keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_DELAYINFERREDFRAGMENTSPASS
+#define GEN_PASS_DEF_DELAYTRANSFERSFROMCPUPASS
+#include "shardy/dialect/mpmd/transforms/export/passes.h.inc"
+
+namespace {
+
+Operation* FindFirstUserInProgramOrder(Operation* op) {
+  if (op->use_empty()) {
+    return nullptr;
+  }
+  return *llvm::min_element(op->getUsers(), [](Operation* lhs, Operation* rhs) {
+    return lhs->isBeforeInBlock(rhs);
+  });
+}
+
+class DelayInferredFragmentsPass
+    : public impl::DelayInferredFragmentsPassBase<DelayInferredFragmentsPass> {
+  using DelayInferredFragmentsPassBase::DelayInferredFragmentsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp main_func) override {
+    if (!IsMpmdFunction(main_func) || !IsEntryPointFunction(main_func)) {
+      return;
+    }
+
+    IRRewriter rewriter(main_func.getContext());
+    Block& block = main_func.getBody().front();
+    // Iterate over all ops in reverse order, so that we guarantee we don't
+    // visit the same inferred fragment twice. Also to make sure we handle
+    // chains of inferred fragments correctly.
+    for (Operation& operation :
+         llvm::make_early_inc_range(llvm::reverse(block.getOperations()))) {
+      if (auto fragment = dyn_cast<FragmentOp>(&operation)) {
+        if (fragment.isUserFragment()) {
+          continue;
+        }
+
+        if (Operation* first_user = FindFirstUserInProgramOrder(fragment)) {
+          rewriter.moveOpBefore(fragment, first_user);
+        }
+      }
+    }
+  }
+};
+
+bool IsOnCpuMesh(Value value) {
+  return cast<MeshTensorType>(value.getType())
+      .getMeshName()
+      .ends_with(kCpuMeshSuffix);
+}
+
+
+class DelayTransfersFromCpuPass
+    : public impl::DelayTransfersFromCpuPassBase<DelayTransfersFromCpuPass> {
+  using DelayTransfersFromCpuPassBase::DelayTransfersFromCpuPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp main_func) override {
+    if (!IsMpmdFunction(main_func) || !IsEntryPointFunction(main_func)) {
+      return;
+    }
+
+    IRRewriter rewriter(main_func.getContext());
+    Block& block = main_func.getBody().front();
+
+    // Copy all transfers to a vector so that rewriting the module
+    // doesn't affect the iteration order.
+    std::vector<TransferOp> transfers(block.getOps<TransferOp>().begin(),
+                                      block.getOps<TransferOp>().end());
+    for (auto transfer : transfers) {
+      auto operand_type = cast<MeshTensorType>(transfer.getOperand().getType());
+      if (IsOnCpuMesh(transfer.getOperand()) ||
+      operand_type.isOnHost()) {
+        if (auto first_user = FindFirstUserInProgramOrder(transfer)) {
+          rewriter.moveOpBefore(transfer, first_user);
+        }
+      }
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/test/BUILD b/shardy/dialect/mpmd/transforms/export/test/BUILD
new file mode 100644
index 0000000..677f6aa
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/BUILD
@@ -0,0 +1,21 @@
+# Lit tests for the MPMD export passes.
+
+load("//shardy:lit.bzl", "glob_lit_tests")
+
+package(default_visibility = ["//visibility:public"])
+
+filegroup(
+    name = "test_data",
+    testonly = True,
+    data = [
+        "//shardy/tools:mpmd_opt",
+        "@llvm-project//llvm:FileCheck",
+    ],
+)
+
+glob_lit_tests(
+    name = "all_tests",
+    data = [":test_data"],
+    driver = "@llvm-project//mlir:run_lit.sh",
+    test_file_exts = ["mlir"],
+)
diff --git a/shardy/dialect/mpmd/transforms/export/test/delay_inferred_fragments.mlir b/shardy/dialect/mpmd/transforms/export/test/delay_inferred_fragments.mlir
new file mode 100644
index 0000000..4e4654b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/delay_inferred_fragments.mlir
@@ -0,0 +1,88 @@
+// RUN: mpmd_opt %s -mpmd-delay-inferred-fragments 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+
+// CHECK-LABEL: func @user_fragment_is_not_delayed
+func.func @user_fragment_is_not_delayed(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK: mpmd.fragment<mesh="m1", origin=["g"]>
+  %f = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %g = mpmd.fragment<mesh="m1", origin=["g"]> (%arg1) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  func.return %f : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @inferred_fragment_is_delayed_to_before_first_consumer
+func.func @inferred_fragment_is_delayed_to_before_first_consumer(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["g"]>
+  // CHECK: mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f"]>
+  // CHECK: return
+  %inf = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %g = mpmd.fragment<mesh="m1", origin=["g"]> (%arg1) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %first_consumer = mpmd.fragment<mesh="m1", origin=["f"]> (%inf) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  func.return %inf : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @multiple_inferred_same_consumer
+func.func @multiple_inferred_same_consumer(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+  // Since we visit the ops backwards, the `add` fragment must appear
+  // before the `multiply` fragment as it's moved first.
+  // CHECK: mpmd.fragment<mesh="m1", origin=["g"]>
+  // CHECK: mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-NEXT: add
+  // CHECK: mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-NEXT: multiply
+  // CHECK: return
+  %inf1 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %0 = stablehlo.multiply %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %0 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %inf2 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %0 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %g = mpmd.fragment<mesh="m1", origin=["g"]> (%arg1) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %first_consumer = mpmd.fragment<mesh="m1", origin=["g"]> (%inf1, %inf2) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+  func.return %first_consumer : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @chain_of_inferred_is_moved
+func.func @chain_of_inferred_is_moved(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+  -> !mesh_1_tensor attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["g"]>
+  // CHECK: mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK: mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK: return
+  %inf1 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %inf2 = mpmd.fragment<mesh="m1", origin=[]> (%inf1) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  %g = mpmd.fragment<mesh="m1", origin=["g"]> (%arg1) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+  func.return %inf2 : !mesh_1_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/export/test/delay_transfers_from_cpu.mlir b/shardy/dialect/mpmd/transforms/export/test/delay_transfers_from_cpu.mlir
new file mode 100644
index 0000000..f594c92
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/delay_transfers_from_cpu.mlir
@@ -0,0 +1,121 @@
+// RUN: mpmd_opt %s -mpmd-delay-transfers-from-cpu 2>&1 | FileCheck %s
+
+!mesh_tensor = !mpmd.mesh_tensor<"m", tensor<4x8xf32>>
+!pinned_host_mesh_tensor = !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">
+!host_mesh_tensor = !mpmd.mesh_tensor<"m/cpu", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @transfer_of_arg_is_delayed
+func.func @transfer_of_arg_is_delayed(%arg0: !mesh_tensor, %arg1: !host_mesh_tensor)
+  -> (!mesh_tensor, !mesh_tensor) attributes {
+    "topology"=#mpmd.topology<<"m": <["x"=2, "y"=2]>>, <"m/cpu": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.fragment
+  // CHECK: mpmd.transfer
+  // This transfer isn't needed until later and it's operand lives on host. So,
+  // we will delay it.
+  %t = mpmd.transfer %arg1 : (!host_mesh_tensor) -> !mesh_tensor
+  %f = mpmd.fragment<mesh="m", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  func.return %f, %t : !mesh_tensor, !mesh_tensor
+}
+
+// CHECK-LABEL: func @interleave_transfers_with_fragments
+func.func @interleave_transfers_with_fragments(%arg0: !host_mesh_tensor, %arg1: !host_mesh_tensor)
+  -> (!mesh_tensor, !mesh_tensor) attributes {
+    "topology"=#mpmd.topology<<"m": <["x"=2, "y"=2]>>, <"m/cpu": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.transfer %arg0
+  // CHECK: mpmd.fragment<mesh="m", origin=["f0"]>
+  // CHECK: mpmd.transfer %arg1
+  // CHECK: mpmd.fragment<mesh="m", origin=["f1"]>
+  %t0 = mpmd.transfer %arg0 : (!host_mesh_tensor) -> !mesh_tensor
+  %t1 = mpmd.transfer %arg1 : (!host_mesh_tensor) -> !mesh_tensor
+  %f0 = mpmd.fragment<mesh="m", origin=["f0"]> (%t0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  %f1 = mpmd.fragment<mesh="m", origin=["f1"]> (%t1) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  func.return %f0, %f1 : !mesh_tensor, !mesh_tensor
+}
+
+// We shouldn't see this pattern in the wild (at least for now). But there's no
+// reason not to support it and it might be useful in the future.
+// CHECK-LABEL: func @transfer_of_produced_value_is_delayed
+func.func @transfer_of_produced_value_is_delayed(%arg0: !mesh_tensor, %arg1: !host_mesh_tensor)
+  -> (!mesh_tensor, !mesh_tensor) attributes {
+    "topology"=#mpmd.topology<<"m": <["x"=2, "y"=2]>>, <"m/cpu": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.fragment
+  // CHECK: mpmd.fragment
+  // CHECK: mpmd.transfer
+  %x = mpmd.fragment<mesh="m/cpu", origin=["f"]> (%arg1) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!host_mesh_tensor) -> !host_mesh_tensor
+  // This transfer isn't needed until later and it's operand lives on host. So,
+  // we will delay it.
+  %t = mpmd.transfer %x : (!host_mesh_tensor) -> !mesh_tensor
+  %f = mpmd.fragment<mesh="m", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  func.return %f, %t : !mesh_tensor, !mesh_tensor
+}
+
+// CHECK-LABEL: func @transfer_without_users_is_not_delayed
+func.func @transfer_without_users_is_not_delayed(%arg0: !host_mesh_tensor)
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=2, "y"=2]>>, <"m/cpu": <["x"=2, "y"=2]>>>}
+{
+  // CHECK: mpmd.transfer
+  // CHECK: return
+  %t = mpmd.transfer %arg0 : (!host_mesh_tensor) -> !mesh_tensor
+  func.return
+}
+
+// Typo in the mesh name means this is not a host mesh.
+!typo_host_mesh_tensor = !mpmd.mesh_tensor<"m/cpu_", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @transfer_of_arg_is_not_delayed_because_typo
+func.func @transfer_of_arg_is_not_delayed_because_typo(%arg0: !mesh_tensor, %arg1: !typo_host_mesh_tensor)
+  -> (!mesh_tensor, !mesh_tensor) attributes {
+    "topology"=#mpmd.topology<<"m": <["x"=2, "y"=2]>>, <"m/cpu_": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.transfer
+  // CHECK-NEXT: mpmd.fragment
+  // This transfer isn't needed until later and it's operand lives on host. So,
+  // we will delay it.
+  %t = mpmd.transfer %arg1 : (!typo_host_mesh_tensor) -> !mesh_tensor
+  %f = mpmd.fragment<mesh="m", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  func.return %f, %t : !mesh_tensor, !mesh_tensor
+}
+
+// CHECK-LABEL: func @transfer_of_arg_pinned_to_host_is_delayed
+func.func @transfer_of_arg_pinned_to_host_is_delayed(%arg0: !mesh_tensor, %arg1: !pinned_host_mesh_tensor)
+  -> (!mesh_tensor, !mesh_tensor) attributes {
+    "topology"=#mpmd.topology<<"m": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.fragment
+  // CHECK: mpmd.transfer
+  // This transfer isn't needed until later and it's operand is pinned to host.
+  // So, we will delay it.
+  %t = mpmd.transfer %arg1 : (!pinned_host_mesh_tensor) -> !mesh_tensor
+  %f = mpmd.fragment<mesh="m", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  func.return %f, %t : !mesh_tensor, !mesh_tensor
+}
+
+func.func @transfer_of_produced_value_pinned_to_host_is_delayed(%arg0: !mesh_tensor, %arg1: !pinned_host_mesh_tensor)
+  -> (!mesh_tensor, !mesh_tensor) attributes {
+    "topology"=#mpmd.topology<<"m": <["x"=2, "y"=2]>>>} {
+  // CHECK: mpmd.fragment
+  // CHECK: mpmd.fragment
+  // CHECK: mpmd.transfer
+  %x = mpmd.fragment<mesh="m", origin=["f"]> (%arg1) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!pinned_host_mesh_tensor) -> !pinned_host_mesh_tensor
+  // This transfer isn't needed until later and it's operand is pinned to host.
+  // So, we will delay it.
+  %t = mpmd.transfer %x : (!pinned_host_mesh_tensor) -> !mesh_tensor
+  %f = mpmd.fragment<mesh="m", origin=["f"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  func.return %f, %t : !mesh_tensor, !mesh_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/export/test/export_pipeline.mlir b/shardy/dialect/mpmd/transforms/export/test/export_pipeline.mlir
new file mode 100644
index 0000000..2d992f5
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/export_pipeline.mlir
@@ -0,0 +1,19 @@
+// RUN: mpmd_opt %s -mpmd-export-pipeline 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func.func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 0: i32}, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+// CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[CALLEE:.*]](%arg0, %arg1)
+// CHECK: func.func @[[CALLEE]](%arg0: tensor<4x8xf32> {tf.aliasing_output = 0 : i32}, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {mesh_shape = #sdy.mesh<["x"=2]>, xla_tpu_user_reserved_hbm_bytes = 0 : i64}
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    mpmd.return %0 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  func.return %0 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls.mlir b/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls.mlir
new file mode 100644
index 0000000..71eb9b9
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls.mlir
@@ -0,0 +1,509 @@
+// RUN: mpmd_opt %s -mpmd-lower-to-fragment-calls='group-across-meshes=false' -split-input-file 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+    -> (!mesh_1_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>
+    >} {
+  // CHECK-NEXT: %[[FRAGMENT_CALL_0:.*]] = mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0, %arg1) {remat}
+  // NB: just setting the remat flag on to see it preserved in the first (but only the first!) fragment call.
+  %f0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) {xla_tpu_user_reserved_hbm_bytes = 256 : i64, remat}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+
+  // CHECK-NEXT: %[[FRAGMENT_CALL_1:.*]] = mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT1:.*]](%arg0, %arg1)
+  %f1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) {xla_tpu_user_reserved_hbm_bytes = 128 : i64}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %13 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  func.return %f1: !mesh_1_tensor
+}
+
+// CHECK:       func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=4]>, xla_tpu_user_reserved_hbm_bytes = 256 : i64} {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg0, %arg1
+// CHECK-NEXT:    return %[[ADD]]
+// CHECK-NEXT:  }
+
+
+// CHECK:       func @[[FRAGMENT1]](%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=4]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:    %[[MUL:.*]] = stablehlo.multiply %arg0, %arg1
+// CHECK-NEXT:    return %[[MUL]]
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+    -> (!mesh_1_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=2, "y"=2]>>,
+      <"m3": <["x"=4]>>
+    >} {
+  // CHECK-NEXT: %[[FRAGMENT_CALL_0:.*]] = mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0, %arg1)
+  %f0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+  // This fragment has the same body and mesh as the fragment `%f0`,
+  // therefore the two fragments will call the same function.
+  // CHECK-NEXT: %[[FRAGMENT_CALL_2:.*]] = mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0, %arg1)
+  %f1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  func.return %f1 : !mesh_1_tensor
+}
+
+// CHECK:       func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=4]>, xla_tpu_user_reserved_hbm_bytes = 256 : i64} {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg0, %arg1
+// CHECK-NEXT:    return %[[ADD]]
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+    -> (!mesh_2_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=2, "y"=2]>>,
+      <"m3": <["x"=4]>>
+    >} {
+
+  // CHECK-NEXT: %[[FRAGMENT_CALL_0:.*]] = mpmd.fragment_call<mesh="m1", origin=["f0"]> @[[FRAGMENT0:.*]](%arg0, %arg1)
+  %f0 = mpmd.fragment<mesh="m1", origin=["f0"]> (%arg0, %arg1) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  // CHECK-NEXT: %[[TRANSFER_1:.*]] = mpmd.transfer %[[FRAGMENT_CALL_0:.*]]
+  %transfer1 = mpmd.transfer %f0 : (!mesh_1_tensor) -> !mesh_2_tensor
+
+  // This fragment has the same body as the fragment `%f0` fragment but a
+  // different mesh shape, therefore the two fragments won't call the same
+  // function.
+  // CHECK-NEXT: %[[FRAGMENT_CALL_3:.*]] = mpmd.fragment_call<mesh="m2", origin=["f3"]> @[[FRAGMENT2:.*]](%[[TRANSFER_1]], %[[TRANSFER_1]])
+  %f4 = mpmd.fragment<mesh="m2", origin=["f3"]> (%transfer1, %transfer1) {xla_tpu_user_reserved_hbm_bytes = 384 : i64}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_2_tensor, !mesh_2_tensor) -> !mesh_2_tensor
+
+  func.return %f4: !mesh_2_tensor
+}
+
+// CHECK:       func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=4]>, xla_tpu_user_reserved_hbm_bytes = 256 : i64} {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg0, %arg1
+// CHECK-NEXT:    return %[[ADD]]
+// CHECK-NEXT:  }
+
+// CHECK:       func @[[FRAGMENT2]](%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=2]>, xla_tpu_user_reserved_hbm_bytes = 384 : i64} {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg0, %arg1
+// CHECK-NEXT:    return %[[ADD]]
+// CHECK-NEXT:  }
+
+// -----
+
+
+!mesh_2_tensor_dist_x = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_2_tensor_dist_x)
+    -> (!mesh_2_tensor_dist_x, !mesh_2_tensor_dist_x) attributes {
+    "topology"=#mpmd.topology<
+      <"m2": <["x"=2, "y"=2]>>
+    >} {
+  // CHECK-NEXT: %[[FRAGMENT_CALL_4:.*]]:2 = mpmd.fragment_call<mesh="m2", origin=["f4"]> @[[FRAGMENT3:.*]](%arg0)
+  %f5:2 = mpmd.fragment<mesh="m2", origin=["f4"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg2: tensor<4x8xf32>) {
+    %13 = stablehlo.subtract %arg2, %arg2 : tensor<4x8xf32>
+    %14 = stablehlo.divide %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %14, %13 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_2_tensor_dist_x) -> (!mesh_2_tensor_dist_x, !mesh_2_tensor_dist_x)
+
+  // This fragment has the same body as the fragment `%f5` above up to the
+  // terminator, which has a different order of operands, therefore the two
+  // fragments won't call the same function.
+  // CHECK-NEXT: %[[FRAGMENT_CALL_5:.*]]:2 = mpmd.fragment_call<mesh="m2", origin=["f4"]> @[[FRAGMENT4:.*]](%arg0)
+  %f6:2 = mpmd.fragment<mesh="m2", origin=["f4"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 128 : i64}
+    (%arg2: tensor<4x8xf32>) {
+    %13 = stablehlo.subtract %arg2, %arg2 : tensor<4x8xf32>
+    %14 = stablehlo.divide %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %13, %14 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_2_tensor_dist_x) -> (!mesh_2_tensor_dist_x, !mesh_2_tensor_dist_x)
+
+  func.return %f6#0, %f6#1 : !mesh_2_tensor_dist_x, !mesh_2_tensor_dist_x
+
+}
+
+// CHECK:       func @[[FRAGMENT3]](%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=2]>, xla_tpu_user_reserved_hbm_bytes = 256 : i64} {
+// CHECK-NEXT:    %[[SUBTRACT:.*]] = stablehlo.subtract %arg0, %arg0
+// CHECK-NEXT:    %[[DIVIDE:.*]] = stablehlo.divide %arg0, %arg0
+// CHECK-NEXT:    return %[[DIVIDE]], %[[SUBTRACT]]
+// CHECK-NEXT:  }
+
+// CHECK:       func @[[FRAGMENT4]](%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=2]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:    %[[SUBTRACT:.*]] = stablehlo.subtract %arg0, %arg0
+// CHECK-NEXT:    %[[DIVIDE:.*]] = stablehlo.divide %arg0, %arg0
+// CHECK-NEXT:    return %[[SUBTRACT]], %[[DIVIDE]]
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_2_tensor_dist_x = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_2_tensor_dist_x)
+    -> (!mesh_2_tensor_dist_x, !mesh_2_tensor_dist_x) attributes {
+    "topology"=#mpmd.topology<
+      <"m2": <["x"=2, "y"=2]>>
+    >} {
+
+  // CHECK-NEXT: %[[FRAGMENT_CALL_5:.*]]:2 = mpmd.fragment_call<mesh="m2", origin=["f4"]> @[[FRAGMENT4:.*]](%arg0)
+  %f6:2 = mpmd.fragment<mesh="m2", origin=["f4"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 128 : i64}
+    (%arg2: tensor<4x8xf32>) {
+    %13 = stablehlo.subtract %arg2, %arg2 : tensor<4x8xf32>
+    %14 = stablehlo.divide %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %13, %14 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_2_tensor_dist_x) -> (!mesh_2_tensor_dist_x, !mesh_2_tensor_dist_x)
+
+  // CHECK-NEXT: %[[TRANSFER_2:.*]] = mpmd.transfer %[[FRAGMENT_CALL_5]]#0
+  %transfer3 = mpmd.transfer %f6#0 : (!mesh_2_tensor_dist_x) -> !mesh_2_tensor_dist_x
+
+  // This fragment has the same body as the second fragment `%f6` above but
+  // different operand/result mesh types. Since the outer types aren't relevant
+  // for the called function, the two fragments will call the same function.
+  // CHECK-NEXT: %[[FRAGMENT_CALL_6:.*]]:2 = mpmd.fragment_call<mesh="m2", origin=["f4"]> @[[FRAGMENT4:.*]](%[[TRANSFER_2]])
+  %f7:2 = mpmd.fragment<mesh="m2", origin=["f4"]> (%transfer3) {xla_tpu_user_reserved_hbm_bytes = 0 : i64}
+    (%arg2: tensor<4x8xf32>) {
+    %13 = stablehlo.subtract %arg2, %arg2 : tensor<4x8xf32>
+    %14 = stablehlo.divide %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %13, %14 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_2_tensor_dist_x) -> (!mesh_2_tensor_dist_x, !mesh_2_tensor_dist_x)
+
+   func.return %f7#0, %f7#1 : !mesh_2_tensor_dist_x, !mesh_2_tensor_dist_x
+}
+
+// CHECK:       func @[[FRAGMENT4]](%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=2, "y"=2]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:    %[[SUBTRACT:.*]] = stablehlo.subtract %arg0, %arg0
+// CHECK-NEXT:    %[[DIVIDE:.*]] = stablehlo.divide %arg0, %arg0
+// CHECK-NEXT:    return %[[SUBTRACT]], %[[DIVIDE]]
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!mesh_2_tensor_dist_x = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+!mesh_2_tensor_dist_y = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"y"}, {?}]>>
+!mesh_3_tensor = !mpmd.mesh_tensor<"m3", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+    -> (!mesh_3_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=2, "y"=2]>>,
+      <"m3": <["x"=4]>>
+    >} {
+
+  // CHECK-NEXT: %[[FRAGMENT_CALL_0:.*]] = mpmd.fragment_call<mesh="m1", origin=["f0"]> @[[FRAGMENT0:.*]](%arg0, %arg1)
+  %f0 = mpmd.fragment<mesh="m1", origin=["f0"]> (%arg0, %arg1) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  // CHECK-NEXT: %[[TRANSFER_3:.*]] = mpmd.transfer %arg0
+  %transfer4 = mpmd.transfer %arg0 : (!mesh_1_tensor) -> !mesh_3_tensor
+
+  // This fragment has the same body as the fragment `%f0` above but is
+  // assigned to a different mesh, therefore the two fragments won't call the
+  // same function.
+  // CHECK-NEXT: %[[FRAGMENT_CALL_7:.*]] = mpmd.fragment_call<mesh="m3", origin=["f5"]> @[[FRAGMENT5:.*]](%[[TRANSFER_3]], %[[TRANSFER_3]])
+  %f8 = mpmd.fragment<mesh="m3", origin=["f5"]> (%transfer4, %transfer4) {xla_tpu_user_reserved_hbm_bytes = 0 : i64}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_3_tensor, !mesh_3_tensor) -> !mesh_3_tensor
+
+  // This fragment has the same body as the fragment `%f8` but an additional
+  // unused block argument, therefore the two fragments won't call the same
+  // function.
+  // CHECK-NEXT: %[[FRAGMENT_CALL_8:.*]] = mpmd.fragment_call<mesh="m3", origin=["f5"]> @[[FRAGMENT6:.*]](%[[FRAGMENT_CALL_7]], %[[FRAGMENT_CALL_7]], %[[FRAGMENT_CALL_7]])
+  %f9 = mpmd.fragment<mesh="m3", origin=["f5"]> (%f8, %f8, %f8) {xla_tpu_user_reserved_hbm_bytes = 128 : i64}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mesh_3_tensor, !mesh_3_tensor, !mesh_3_tensor) -> !mesh_3_tensor
+
+  // CHECK-NEXT: return %[[FRAGMENT_CALL_8]]
+  func.return %f9 : !mesh_3_tensor
+}
+
+// CHECK:       func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=4]>, xla_tpu_user_reserved_hbm_bytes = 256 : i64} {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg0, %arg1
+// CHECK-NEXT:    return %[[ADD]]
+// CHECK-NEXT:  }
+
+// CHECK:       func @[[FRAGMENT5]](%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=4]>, xla_tpu_user_reserved_hbm_bytes = 0 : i64} {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg0, %arg1 : tensor<4x8xf32>
+// CHECK-NEXT:    return %[[ADD]] : tensor<4x8xf32>
+// CHECK-NEXT:  }
+
+// CHECK:       func @[[FRAGMENT6]](%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:      attributes {mesh_shape = #sdy.mesh<["x"=4]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg0, %arg1
+// CHECK-NEXT:    return %[[ADD]]
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>
+    >} {
+  // This fragment and the next fragment are the same except for the arg_attr attributes. So there should be two fragment calls to different functions.
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0)
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {arg_attrs = [{tf.aliasing_output = 0 : i32}], xla_tpu_user_reserved_hbm_bytes = 128 : i64} (%arg2: tensor<4x8xf32>) {
+    %1 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT1:.*]](%arg0)
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 128 : i64} (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// This fragment call function has the tf.aliasing_output arg attribute.
+// CHECK: func @[[FRAGMENT0]](%arg0: tensor<4x8xf32> {tf.aliasing_output = 0 : i32})
+// CHECK-SAME: attributes {mesh_shape = #sdy.mesh<["x"=2]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:  %0 = stablehlo.abs %arg0 : tensor<4x8xf32>
+// CHECK-NEXT:  return %0 : tensor<4x8xf32>
+// CHECK-NEXT:  }
+
+// This fragment call function does not have the tf.aliasing_output arg attribute.
+// CHECK: func @[[FRAGMENT1]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME: attributes {mesh_shape = #sdy.mesh<["x"=2]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:    %0 = stablehlo.abs %arg0 : tensor<4x8xf32>
+// CHECK-NEXT:  return %0 : tensor<4x8xf32>
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>
+    >} {
+  // This fragment and the next fragment are the same except for the res_attr attributes.
+  // So there should be two fragment calls to different functions.
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0)
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {res_attrs = [{mhlo.memory_kind = "pinned_host"}], xla_tpu_user_reserved_hbm_bytes = 0 : i64} (%arg2: tensor<4x8xf32>) {
+    %1 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT1:.*]](%arg0)
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 0 : i64} (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// This fragment call function has the mhlo.memory_kind res attribute.
+// CHECK: func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32> {mhlo.memory_kind = "pinned_host"})
+// This fragment call function does not have the tf.aliasing_output arg attribute.
+// CHECK: func @[[FRAGMENT1]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>
+    >} {
+  // This fragment and the next fragment are exactly the same. So there should be two fragment calls to the same function.
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0)
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {arg_attrs = [{tf.aliasing_output = 0 : i32}], xla_tpu_user_reserved_hbm_bytes = 128 : i64} (%arg2: tensor<4x8xf32>) {
+    %1 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0)
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {arg_attrs = [{tf.aliasing_output = 0 : i32}], xla_tpu_user_reserved_hbm_bytes = 128 : i64} (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// This fragment call has the tf.aliasing_output arg attribute.
+// CHECK: func @[[FRAGMENT0]](%arg0: tensor<4x8xf32> {tf.aliasing_output = 0 : i32}) -> tensor<4x8xf32> attributes {mesh_shape = #sdy.mesh<["x"=2]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:    %0 = stablehlo.abs %arg0 : tensor<4x8xf32>
+// CHECK-NEXT:   return %0 : tensor<4x8xf32>
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>
+    >} {
+  // This fragment and the next fragment are exactly the same. So there should
+  // be two fragment calls to the same function. One of the calls is annotated
+  // with `mpmd.is_gspmd_partitioned` and the other isn't.
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0) {mpmd.is_gspmd_partitioned
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {mpmd.is_gspmd_partitioned, xla_tpu_user_reserved_hbm_bytes = 0 : i64} (%arg2: tensor<4x8xf32>) {
+    %1 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0) :
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 0 : i64} (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK: func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {mesh_shape = #sdy.mesh<["x"=2]>, xla_tpu_user_reserved_hbm_bytes = 0 : i64} {
+// CHECK-NEXT:    %0 = stablehlo.abs %arg0 : tensor<4x8xf32>
+// CHECK-NEXT:   return %0 : tensor<4x8xf32>
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @no_reserved_memory
+func.func @no_reserved_memory(%arg0: !mesh_tensor)
+  -> (!mesh_tensor) attributes {"topology"=#mpmd.topology< <"m1": <["x"=2]>>>} {
+
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0) :
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> (!mesh_tensor)
+
+  func.return %0 : !mesh_tensor
+}
+
+// CHECK: func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {mesh_shape = #sdy.mesh<["x"=2]>} {
+// CHECK-NEXT:   return %arg0 : tensor<4x8xf32>
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_tensor)
+  -> (!mesh_tensor) attributes {"topology"=#mpmd.topology< <"m1": <["x"=2]>>>} {
+
+  // Only the FIRST fragment has a xla_tpu_user_reserved_hbm_bytes annotation.
+  // hbm_bytes gets attached to fragment function.
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0) :
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 128 : i64} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> (!mesh_tensor)
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0) :
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> (!mesh_tensor)
+
+  func.return %2 : !mesh_tensor
+}
+
+// CHECK: func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {mesh_shape = #sdy.mesh<["x"=2]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:   return %arg0 : tensor<4x8xf32>
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_tensor)
+  -> (!mesh_tensor) attributes {"topology"=#mpmd.topology< <"m1": <["x"=2]>>>} {
+
+  // Only the SECOND fragment has a xla_tpu_user_reserved_hbm_bytes annotation.
+  // hbm_bytes gets attached to fragment function.
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0) :
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)  (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> (!mesh_tensor)
+  // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0) :
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 128 : i64} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> (!mesh_tensor)
+
+  func.return %2 : !mesh_tensor
+}
+
+// CHECK: func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {mesh_shape = #sdy.mesh<["x"=2]>, xla_tpu_user_reserved_hbm_bytes = 128 : i64} {
+// CHECK-NEXT:   return %arg0 : tensor<4x8xf32>
+// CHECK-NEXT:  }
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+
+module attributes {mpmd.sdy_lowered} {
+  sdy.mesh @mesh = <["x"=2]>
+
+  // CHECK-LABEL: func @sdy_partitioned
+  func.func @sdy_partitioned(%arg0: !mesh_tensor) -> (!mesh_tensor)
+      attributes {"topology"=#mpmd.topology< <"m1": <["x"=2]>>>} {
+    // CHECK: mpmd.fragment_call<mesh="m1", origin=["f1"]>
+    %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      mpmd.return %arg2 : tensor<4x8xf32>
+    } : (!mesh_tensor) -> (!mesh_tensor)
+
+    func.return %0 : !mesh_tensor
+  }
+}
+
+// CHECK: func.func @[[FRAGMENT0:.*]](%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>}) ->
+// CHECK-SAME: (tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>}) attributes {mesh_shape = #sdy.mesh<["x"=2]>}
diff --git a/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls_group_across_meshes.mlir b/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls_group_across_meshes.mlir
new file mode 100644
index 0000000..fd26926
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls_group_across_meshes.mlir
@@ -0,0 +1,75 @@
+// RUN: mpmd_opt %s -mpmd-lower-to-fragment-calls='group-across-meshes=true' -split-input-file 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor)
+    -> (!mesh_2_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=2,"y"=2]>>
+    >} {
+  // Two identical fragment but with different mesh shapes.
+  // CHECK-NEXT: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT0:.*]](%arg0)
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.sine %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor) -> !mesh_2_tensor
+
+  // CHECK-NEXT: mpmd.fragment_call<mesh="m2", origin=["f2"]> @[[FRAGMENT1:.*]](%[[TRANSFER]])
+  %2 = mpmd.fragment<mesh="m2", origin=["f2"]> (%1) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.sine %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_2_tensor) -> !mesh_2_tensor
+
+  func.return %2: !mesh_2_tensor
+}
+
+// CHECK:       func @[[FRAGMENT0]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:  mesh_shape = #sdy.mesh<["x"=4]>
+
+
+// CHECK:       func @[[FRAGMENT1]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-SAME:  mesh_shape = #sdy.mesh<["x"=2, "y"=2]>
+
+// -----
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+func.func @main(%arg0: !mesh_1_tensor)
+    -> (!mesh_2_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["y"=4]>>
+    >} {
+  // Two identical fragment on different meshes with identical shapes.
+  // CHECK-NEXT: mpmd.fragment_call<mesh="m1", origin=["f1"]> @[[FRAGMENT:.*]](%arg0)
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.sine %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor) -> !mesh_1_tensor
+
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor) -> !mesh_2_tensor
+
+  // CHECK-NEXT: mpmd.fragment_call<mesh="m2", origin=["f2"]> @[[FRAGMENT:.*]](%[[TRANSFER]])
+  %2 = mpmd.fragment<mesh="m2", origin=["f2"]> (%1) {xla_tpu_user_reserved_hbm_bytes = 256 : i64}
+    (%arg1: tensor<4x8xf32>) {
+    %3 = stablehlo.sine %arg1 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_2_tensor) -> !mesh_2_tensor
+
+  func.return %2: !mesh_2_tensor
+}
+
+// CHECK:       func @[[FRAGMENT]](%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NOT:   func
diff --git a/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls_naming.mlir b/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls_naming.mlir
new file mode 100644
index 0000000..60cf687
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/lower_to_fragment_calls_naming.mlir
@@ -0,0 +1,63 @@
+// RUN: mpmd_opt %s -mpmd-lower-to-fragment-calls -split-input-file 2>&1 | FileCheck %s
+
+!mesh_tensor = !mpmd.mesh_tensor<"m", tensor<4x8xf32>>
+// CHECK-LABEL: func @guid_printing
+func.func @guid_printing(%arg0: !mesh_tensor) -> !mesh_tensor
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=4]>>>} {
+  // CHECK: mpmd.fragment_call<mesh="m", origin=["block"]> @p0_block_fwd.main
+  %0 = mpmd.fragment<mesh="m", origin=["block"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64} (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  return %0 : !mesh_tensor
+}
+
+// -----
+!mesh_tensor = !mpmd.mesh_tensor<"m", tensor<4x8xf32>>
+// CHECK-LABEL: func @one_guid_multiple_call_sites
+func.func @one_guid_multiple_call_sites(%arg0: !mesh_tensor) -> (!mesh_tensor, !mesh_tensor)
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=4]>>>} {
+  // CHECK: mpmd.fragment_call<mesh="m", origin=["foo"]> @p0_foo_fwd.main
+  %0 = mpmd.fragment<mesh="m", origin=["foo"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64} (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  // This is reusing the same fragment call as above.
+  // CHECK: mpmd.fragment_call<mesh="m", origin=["bar"]> @p0_foo_fwd.main
+  %1 = mpmd.fragment<mesh="m", origin=["bar"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64} (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  return %0, %1 : !mesh_tensor, !mesh_tensor
+}
+
+// -----
+!mesh_tensor = !mpmd.mesh_tensor<"m", tensor<4x8xf32>>
+// CHECK-LABEL: func @multiple_guids
+func.func @multiple_guids(%arg0: !mesh_tensor) -> (!mesh_tensor, !mesh_tensor)
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=4]>>>} {
+  // CHECK: mpmd.fragment_call<mesh="m", origin=["foo"]> @p0_foo_fwd.main
+  %0 = mpmd.fragment<mesh="m", origin=["foo"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64} (%arg1: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  // CHECK: mpmd.fragment_call<mesh="m", origin=["bar"(1)]> @p1_bar_bwd.main
+  %1 = mpmd.fragment<mesh="m", origin=["bar"(1)]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64} (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  return %0, %1 : !mesh_tensor, !mesh_tensor
+}
+
+// -----
+
+!mesh_tensor = !mpmd.mesh_tensor<"m", tensor<4x8xf32>>
+
+// CHECK-LABEL: module @jit_test_module
+module @jit_test_module {
+// CHECK-LABEL: func @custom_module_name
+func.func @custom_module_name(%arg0: !mesh_tensor) -> !mesh_tensor
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=4]>>>} {
+  // CHECK: mpmd.fragment_call<mesh="m", origin=["block"]> @p0_block_fwd.test_module
+  %0 = mpmd.fragment<mesh="m", origin=["block"]> (%arg0) {xla_tpu_user_reserved_hbm_bytes = 256 : i64} (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_tensor) -> !mesh_tensor
+  return %0 : !mesh_tensor
+}
+}
diff --git a/shardy/dialect/mpmd/transforms/export/test/mark_aliasing_and_donation.mlir b/shardy/dialect/mpmd/transforms/export/test/mark_aliasing_and_donation.mlir
new file mode 100644
index 0000000..0946656
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/mark_aliasing_and_donation.mlir
@@ -0,0 +1,234 @@
+// RUN: mpmd_opt %s -mpmd-mark-aliasing-and-donation 2>&1 | FileCheck --implicit-check-not arg_attrs %s
+
+
+// These tests verify only the arg_attrs attribute and assume that the structure of the IR remain
+// unchanged aside from additional attributes.
+
+// The `--implicit-check-not arg_attrs` on FileCheck means that the text "arg_attrs"
+// is only allowed when explicitly specified in a CHECK.
+
+
+!mesh_1_tensor_1 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+!mesh_1_tensor_2 = !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+!mesh_1_tensor_3 = !mpmd.mesh_tensor<"m1", tensor<4x16xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+
+!mesh_2_tensor_1 = !mpmd.mesh_tensor<"m2", tensor<4x16xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+!mesh_2_tensor_2 = !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>
+!mesh_2_tensor_3 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+
+// CHECK-LABEL: func @mark_aliasing_for_input_with_and_without_aliasing(
+func.func @mark_aliasing_for_input_with_and_without_aliasing(
+  %arg0: !mesh_1_tensor_1 {tf.aliasing_output = 1 : i32},
+  %arg1: !mesh_1_tensor_2,
+  %arg2: !mesh_2_tensor_2 {tf.aliasing_output = 0 : i32}
+) -> (
+  !mesh_2_tensor_2 ,
+  !mesh_2_tensor_3
+) attributes {topology = #mpmd.topology<<"m1" : <["x"=2, "y"=4]>>, <"m2" : <["x"=2, "z"=3]>>>} {
+  // %arg0 is marked by the user with tf.aliasing_output but it does not match
+  // the op result type so it can't be aliased. Instead, it is donated.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: {arg_attrs = [{jax.buffer_donor = true}, {}]}
+  %0 = mpmd.fragment<mesh="m1", origin=["f0"]> (%arg0, %arg1) (%arg3: tensor<4x8xf32>, %arg4: tensor<8x16xf32>) {
+    %4 = "stablehlo.dot"(%arg3, %arg4) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    mpmd.return %4 : tensor<4x16xf32>
+  } : (!mesh_1_tensor_1, !mesh_1_tensor_2) -> !mesh_1_tensor_3
+  // %arg2 is marked by the user with tf.aliasing_output and it matches the op result type so it can be aliased.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: {arg_attrs = [{tf.aliasing_output = 0 : i32}]}
+  %1 = mpmd.fragment<mesh="m2", origin=["f1"]> (%arg2) (%arg3: tensor<16x8xf32>) {
+    %4 = stablehlo.add %arg3, %arg3 : tensor<16x8xf32>
+    mpmd.return %4 : tensor<16x8xf32>
+  } : (!mesh_2_tensor_2 ) -> !mesh_2_tensor_2
+  %2 = mpmd.transfer %0 : (!mesh_1_tensor_3) -> !mesh_2_tensor_1
+  // First input type does not match with any output type, so it is donated
+  // instead of aliased.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: {arg_attrs = [{jax.buffer_donor = true}, {}]}
+  %3 = mpmd.fragment<mesh="m2", origin=["f2"]> (%2, %1) (%arg3: tensor<4x16xf32>, %arg4: tensor<16x8xf32>) {
+    %4 = "stablehlo.dot"(%arg3, %arg4) : (tensor<4x16xf32>, tensor<16x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_1, !mesh_2_tensor_2 ) -> !mesh_2_tensor_3
+  return %1, %3 : !mesh_2_tensor_2 , !mesh_2_tensor_3
+}
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-LABEL: func.func @can_only_alias_to_one_output
+func.func @can_only_alias_to_one_output(%arg0: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 1 : i32}, %arg1: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 1 : i32})
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // Both %arg0 and %arg1 can donate to the output but there is only one output to donate to.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: {arg_attrs = [{tf.aliasing_output = 0 : i32}, {jax.buffer_donor = true}]}
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    %1 = stablehlo.abs %arg3: tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  func.return %0: !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func.func @alias_to_multiple_output
+func.func @alias_to_multiple_output(%arg0: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 1 : i32}, %arg1: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 0 : i32})
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // Both input can be aliased.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: {arg_attrs = [{tf.aliasing_output = 0 : i32}, {tf.aliasing_output = 1 : i32}]}
+  %0, %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    %1 = stablehlo.abs %arg3: tensor<4x8xf32>
+    mpmd.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32,!mesh_1_tensor_4_8_f32)
+  func.return %0, %1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func.func @should_not_alias_offloaded_values
+func.func @should_not_alias_offloaded_values(
+  %arg0: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 1 : i32},
+  %arg1: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 0 : i32})
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // Only the last input and the last output can be aliased, because the other
+  // values are on host.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.memory_kind = "pinned_host"}, {tf.aliasing_output = 1 : i32}]
+  %0, %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+    {arg_attrs = [{mhlo.memory_kind = "pinned_host"}, {}], res_attrs = [{mhlo.memory_kind = "pinned_host"}, {}]}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    %1 = stablehlo.abs %arg3: tensor<4x8xf32>
+    mpmd.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32,!mesh_1_tensor_4_8_f32)
+  func.return %0, %1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+
+// CHECK-LABEL: func.func @should_not_alias_on_different_layout
+func.func @should_not_alias_on_different_layout(
+  %arg0: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 0 : i32},
+  %arg1: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 1 : i32})
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // Only arg1 can be aliased to result1, because the layout of arg0 and
+  // result0 are different.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: {arg_attrs = [{jax.buffer_donor = true, mhlo.layout_mode = "abc"},
+  // CHECK-SAME: {tf.aliasing_output = 1 : i32}], res_attrs = [{mhlo.layout_mode = "xyz"}, {}]}
+  %0, %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+    {arg_attrs = [{mhlo.layout_mode = "abc"}, {}], res_attrs = [{mhlo.layout_mode = "xyz"}, {}]}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    %1 = stablehlo.abs %arg3: tensor<4x8xf32>
+    mpmd.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32,!mesh_1_tensor_4_8_f32)
+  func.return %0, %1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func.func @should_not_alias_block_argument_if_not_marked_in_arg_attribute
+func.func @should_not_alias_block_argument_if_not_marked_in_arg_attribute(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // %arg0 can be aliased but the user did not mark the block argument for aliasing.
+  // CHECK: mpmd.fragment
+  %0, %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    %1 = stablehlo.abs %arg3: tensor<4x8xf32>
+    mpmd.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32,!mesh_1_tensor_4_8_f32)
+  func.return %0, %1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func.func @one_argument_used_in_multiple_fragment_should_only_alias_in_last
+func.func @one_argument_used_in_multiple_fragment_should_only_alias_in_last(%arg0: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 1 : i32}, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK: mpmd.fragment
+  %0, %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %0 = stablehlo.subtract %arg2, %arg3: tensor<4x8xf32>
+    %1 = stablehlo.add %0, %arg3: tensor<4x8xf32>
+    mpmd.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32,!mesh_1_tensor_4_8_f32)
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: {arg_attrs = [{tf.aliasing_output = 0 : i32}, {}]}
+  %2, %3 = mpmd.fragment<mesh="m1", origin=["f2"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    %5 = stablehlo.abs %arg3: tensor<4x8xf32>
+    mpmd.return %4, %5 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32,!mesh_1_tensor_4_8_f32)
+  func.return %0, %3 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func.func @transfer_op_operand_cannot_be_aliased
+func.func @transfer_op_operand_cannot_be_aliased(%arg0: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 1 : i32}, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+ %0 = mpmd.transfer %arg0 : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+ %1 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+ // %0 can be aliased with output 0 but since it is used by a transfer op so it won't be aliased.
+ // CHECK: mpmd.fragment
+ %4 = mpmd.fragment<mesh="m1", origin=["f1"]> (%0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %2 = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    %3 = stablehlo.abs %arg3: tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  func.return %1, %4 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func.func @transfer_result_can_be_aliased
+func.func @transfer_result_can_be_aliased (%arg0: !mesh_2_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#topology} {
+  // CHECK: %[[TRANSFER_RESULT:.*]] = mpmd.transfer %arg0
+  %param = mpmd.transfer %arg0 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"]>
+  // CHECK-SAME: (%[[TRANSFER_RESULT]]) {arg_attrs = [{tf.aliasing_output = 0 : i32}]}
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%param) (%arg2: tensor<4x8xf32>) {
+    %add = stablehlo.add %arg2, %arg2: tensor<4x8xf32>
+    mpmd.return %add : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  func.return %0 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func.func @alias_intemediates_in_chain_of_fragments
+func.func @alias_intemediates_in_chain_of_fragments(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#topology} {
+  // CHECK: %[[FIRST_FRAGMENT_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]>
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %initial_gradient_accumulate = stablehlo.constant dense<0.000000e+00> : tensor<4x8xf32>
+    %add = stablehlo.add %arg2, %initial_gradient_accumulate: tensor<4x8xf32>
+    mpmd.return %add : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f2"]>
+  // CHECK-SAME: (%arg0, %[[FIRST_FRAGMENT_RESULT]]) {arg_attrs = [{}, {tf.aliasing_output = 0 : i32}]}
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%arg0, %0) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %add = stablehlo.add %arg2, %arg3: tensor<4x8xf32>
+    mpmd.return %add : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32)
+  func.return %arg0, %1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/export/test/mark_fragment_reserved_memory.mlir b/shardy/dialect/mpmd/transforms/export/test/mark_fragment_reserved_memory.mlir
new file mode 100644
index 0000000..53cb6cf
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/mark_fragment_reserved_memory.mlir
@@ -0,0 +1,333 @@
+// RUN: mpmd_opt %s -mpmd-mark-fragment-reserved-memory 2>&1 | FileCheck %s
+
+// NOTE:
+// - mesh_1_tensor and mesh_2_tensor is 128 bytes.
+// - mesh_1_tensor_dist_x is 32 bytes.
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_1_tensor_dist_x = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @single_mesh
+func.func @single_mesh(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+    -> (!mesh_1_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>>} {
+
+  // Fragment only takes inputs from the function, no intermediates, so 0.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 0
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  // Fragment takes one input from the function and one intermediates, so 128 due to
+  // %arg0.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  // Fragment takes two intermediates, so 256 due to %arg0 and %arg1
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 256
+  %2 = mpmd.fragment<mesh="m1", origin=["f3"]> (%0, %1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  // Fragment takes an input and a intermediate %2. Note %0's and %1's last use
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %3 = mpmd.fragment<mesh="m1", origin=["f4"]> (%arg0, %2)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  func.return %3 : !mesh_1_tensor
+}
+
+// Make sure we don't subtract twice from live memory usage if a fragment takes
+// two of the same inputs.
+// CHECK-LABEL: func @duplicate_input
+func.func @duplicate_input(%arg0: !mesh_1_tensor)
+    -> (!mesh_1_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>>} {
+
+  // Fragment only takes inputs from the function, no intermediates, so 0.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 0
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  func.return %0 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @offloaded_value
+func.func @offloaded_value(%arg0: !mesh_1_tensor, %arg1: !mesh_1_tensor)
+    -> (!mesh_1_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>>} {
+
+  // Fragment only takes inputs from the function, no intermediates, so 0.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 0
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+    {res_attrs = [{mhlo.memory_kind = "pinned_host"}, {}]}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    mpmd.return %arg2, %arg3 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor)
+
+  // Fragment takes one input from the function and one intermediates, so
+  // 128 due to %arg0. The unused intermediate is on the host so it's ignored.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%0#1, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  // Fragment takes one input from the function and one intermediates, so
+  // 128 due to %arg0. %0#1 had its last use removed so it's not tracked
+  // anymore.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %2 = mpmd.fragment<mesh="m1", origin=["f3"]> (%0#0, %1, %arg1)
+    {arg_attrs = [{mhlo.memory_kind = "pinned_host"}, {}, {}]}
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %4 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  func.return %2 : !mesh_1_tensor
+}
+
+// Same test as `@single_mesh` but now with some tensors existing on other
+// meshes.
+// CHECK-LABEL: func @multiple_meshes
+func.func @multiple_meshes(%arg0: !mesh_1_tensor, %arg1: !mesh_2_tensor)
+    -> (!mesh_2_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>, <"m2": <["x"=2]>>>} {
+
+  %0 = mpmd.transfer %arg0 : (!mesh_1_tensor) -> !mesh_2_tensor
+  %1 = mpmd.transfer %arg1 : (!mesh_2_tensor) -> !mesh_1_tensor
+
+  // On m2, only %arg1 and %0 exist, which are inputs to f1, so 0 bytes.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 0
+  %2 = mpmd.fragment<mesh="m2", origin=["f1"]> (%0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %8 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %8 : tensor<4x8xf32>
+  } : (!mesh_2_tensor, !mesh_2_tensor) -> !mesh_2_tensor
+
+  %3 = mpmd.transfer %2 : (!mesh_2_tensor) -> !mesh_1_tensor
+
+  // On m1, %3, %1 and %arg0 are still alive, so 128 bytes for %1.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %4 = mpmd.fragment<mesh="m1", origin=["f2"]> (%3, %arg0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %8 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %8 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  // On m1, %4, %1 and %arg0 are still alive, %3 is dead now, so 128 bytes.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %5 = mpmd.fragment<mesh="m1", origin=["f3"]> (%4, %1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %8 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %8 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  %6 = mpmd.transfer %5 : (!mesh_1_tensor) -> !mesh_2_tensor
+
+  // On m2, %6 and %arg1 are still alive, %0 is dead now, so 0 bytes.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 0
+  %7 = mpmd.fragment<mesh="m2", origin=["f4"]> (%6, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %8 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %8 : tensor<4x8xf32>
+  } : (!mesh_2_tensor, !mesh_2_tensor) -> !mesh_2_tensor
+
+  func.return %7 : !mesh_2_tensor
+}
+
+// Tests that the pass accounts for the per device local shape, not global
+// shape, if the tensor is distributed.
+// CHECK-LABEL: func @distributed_tensor
+func.func @distributed_tensor(%arg0: !mesh_1_tensor)
+    -> (!mesh_1_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>>} {
+
+  %0 = mpmd.transfer %arg0 : (!mesh_1_tensor) -> !mesh_1_tensor_dist_x
+
+  // Fragment takes all live values, so 0.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 0
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %arg3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor_dist_x) -> !mesh_1_tensor
+
+  // %1 and %0 are alive still, so 128+32=160 bytes.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 160
+  %2 = mpmd.fragment<mesh="m1", origin=["f2"]> (%arg0, %arg0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  // Only input is unused. The input is still considered alive because it
+  // has not been donated to the program, so 128 bytes.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %3 = mpmd.fragment<mesh="m1", origin=["f3"]> (%2, %1, %0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg4 : tensor<4x8xf32>
+    %5 = stablehlo.add %arg3, %4 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor, !mesh_1_tensor_dist_x) -> !mesh_1_tensor
+
+  func.return %3 : !mesh_1_tensor
+}
+
+// Test that verifies the unused output of a fragment is not accounted for in
+// the live buffers.
+// CHECK-LABEL: func.func @unused_fragment_result_is_not_counted
+func.func @unused_fragment_result_is_not_counted(
+    %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+    %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+    %arg2: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+    %arg3: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+      -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+          !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+      attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>} {
+
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 256
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) (%arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg4, %arg5 : tensor<4x8xf32>
+    %11 = stablehlo.abs %arg5 : tensor<4x8xf32>
+    mpmd.return %11 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 256
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg2, %arg3) (%arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+    %14 = stablehlo.add %arg4, %arg5 : tensor<4x8xf32>
+    %15 = stablehlo.abs %arg5 : tensor<4x8xf32>
+    mpmd.return %15 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+  return %1, %arg3 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// Test that verifies a donated program argument is not accounted for after its
+// last use. The test verifies both jax.buffer_donor and tf.aliasing_output
+// attributes.
+// CHECK-LABEL: func.func @donated_program_arg_is_not_counted_after_last_use
+func.func @donated_program_arg_is_not_counted_after_last_use(
+    %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+    %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {jax.buffer_donor = true},
+    %arg2: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {tf.aliasing_output = 0 : i32},
+    %arg3: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+      -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+          !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+      attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>} {
+
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 256
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg1, %arg0) (%arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg4, %arg5 : tensor<4x8xf32>
+    %11 = stablehlo.abs %arg5 : tensor<4x8xf32>
+    mpmd.return %11 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+  // %arg3 and %arg0 are still alive. %arg1 is donated and not used anymore so
+  // it's not accounted for.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %1 = mpmd.fragment<mesh="m1", origin=["f2"]> (%arg2, %arg3) (%arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+    %14 = stablehlo.add %arg4, %arg5 : tensor<4x8xf32>
+    %15 = stablehlo.abs %arg5 : tensor<4x8xf32>
+    mpmd.return %15 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+  // %arg3, %arg0, and %1 are still alive.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 384
+  %2 = mpmd.fragment<mesh="m1", origin=["f3"]> (%arg3, %arg3) (%arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+    %14 = stablehlo.add %arg4, %arg5 : tensor<4x8xf32>
+    %15 = stablehlo.abs %arg5 : tensor<4x8xf32>
+    mpmd.return %15 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+  return %1, %2 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// Test that verifies args on hosts or donated args are not accounted for.
+// CHECK-LABEL: func @offloaded_or_unused_donated_args_are_not_counted
+func.func @offloaded_or_unused_donated_args_are_not_counted(
+    %arg0: !mesh_1_tensor {mhlo.memory_kind = "pinned_host"},
+    %arg1: !mesh_1_tensor,
+    %arg2: !mesh_1_tensor,
+    %arg3: !mesh_1_tensor {jax.buffer_donor = true})
+      -> (!mesh_1_tensor, !mesh_1_tensor)
+        attributes {"topology"=#mpmd.topology<<"m1": <["x"=4]>>>} {
+
+  // The program arguments that are on the host or donated and not be accounted
+  // for.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 0
+  %0:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg1, %arg2)
+    (%arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+    mpmd.return %arg4, %arg5 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor)
+
+  // Account for %0#0 and %arg1 which are alive until the end of the program.
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 256
+  %1:2 = mpmd.fragment<mesh="m1", origin=["f2"]> (%arg0, %arg2)
+    {arg_attrs = [{mhlo.memory_kind = "pinned_host"}, {}]}
+    (%arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+    mpmd.return %arg4, %arg5 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor)
+
+  func.return %0#0, %1#0 : !mesh_1_tensor, !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @unused_input_not_donated
+func.func @unused_input_not_donated(%arg0: !mesh_1_tensor, %unused_arg1: !mesh_1_tensor)
+    -> (!mesh_1_tensor) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>>} {
+
+  // CHECK: mpmd.fragment
+  // CHECK-SAME: xla_tpu_user_reserved_hbm_bytes = 128
+  %0 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor, !mesh_1_tensor) -> !mesh_1_tensor
+
+  func.return %0 : !mesh_1_tensor
+}
+
diff --git a/shardy/dialect/mpmd/transforms/export/test/mark_input_output_with_layouts.mlir b/shardy/dialect/mpmd/transforms/export/test/mark_input_output_with_layouts.mlir
new file mode 100644
index 0000000..42ebe7c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/mark_input_output_with_layouts.mlir
@@ -0,0 +1,253 @@
+// RUN: mpmd_opt %s -mpmd-mark-input-output-with-layouts -split-input-file -verify-diagnostics 2>&1 | FileCheck %s
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+!m2_16x16 = !mpmd.mesh_tensor<"m2", tensor<16x16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8,"y"=8]>>>
+
+// CHECK-LABEL: func @func_default_layout_to_frag_layout(%arg0: {{.*}} {mhlo.layout_mode = "auto"}) ->
+// CHECK-SAME:    {mhlo.layout_mode = "auto"})
+func.func @func_default_layout_to_frag_layout(
+  %func_arg: !m1_16 {mhlo.layout_mode = "auto"}) ->
+  (!m1_16 {mhlo.layout_mode = "auto"}) attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.layout_mode = "auto"}], res_attrs = [{mhlo.layout_mode = "auto"}]}
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    mpmd.return %arg0 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f : !m1_16
+}
+
+// CHECK-LABEL: func @func_mix_of_no_layout_and_auto_layout_to_frag(%arg0: {{.*}}, %arg1: {{.*}} {mhlo.layout_mode = "auto"}) ->
+// CHECK-SAME:    (!mpmd.mesh_tensor<"m1", tensor<16xf32>>, !mpmd.mesh_tensor<"m1", tensor<16xf32>> {mhlo.layout_mode = "auto"})
+func.func @func_mix_of_no_layout_and_auto_layout_to_frag(
+  %func_arg0: !m1_16, %func_arg1: !m1_16 {mhlo.layout_mode = "auto"}) ->
+  (!m1_16, !m1_16 {mhlo.layout_mode = "auto"}) attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{}, {mhlo.layout_mode = "auto"}], res_attrs = [{}, {}, {mhlo.layout_mode = "auto"}]}
+  %f:3 = mpmd.fragment<mesh="m1", origin=[]> (%func_arg0, %func_arg1)
+    (%arg0: tensor<16xf32>, %arg1: tensor<16xf32>) {
+    mpmd.return %arg0, %arg0, %arg1 : tensor<16xf32>, tensor<16xf32>, tensor<16xf32>
+  } : (!m1_16, !m1_16) -> (!m1_16, !m1_16, !m1_16)
+
+  func.return %f#0, %f#2 : !m1_16, !m1_16
+}
+
+// Verifies that the func arg and res layouts are refined before they are
+// propagated to the fragment.
+
+// CHECK-LABEL: func @layouts_propagated_between_func_args_and_func_res_before_frag(%arg0: {{.*}} {mhlo.layout_mode = "{0}"}, %arg1: {{.*}}) ->
+// CHECK-SAME:    (!mpmd.mesh_tensor<"m1", tensor<16xf32>> {mhlo.layout_mode = "auto"}, !mpmd.mesh_tensor<"m1", tensor<16xf32>>, !mpmd.mesh_tensor<"m1", tensor<16xf32>> {mhlo.layout_mode = "{0}"})
+func.func @layouts_propagated_between_func_args_and_func_res_before_frag(
+  %func_arg0: !m1_16 {mhlo.layout_mode = "auto"},
+  %func_arg1: !m1_16 {mhlo.layout_mode = "default"})
+    -> (!m1_16 {mhlo.layout_mode = "auto"},
+        !m1_16 {mhlo.layout_mode = "default"},
+        !m1_16 {mhlo.layout_mode = "{0}"})
+       attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.layout_mode = "{0}"}, {}], res_attrs = [{mhlo.layout_mode = "auto"}, {}]}
+  %f:2 = mpmd.fragment<mesh="m1", origin=[]> (%func_arg0, %func_arg1)
+    (%arg0: tensor<16xf32>, %arg1: tensor<16xf32>) {
+    mpmd.return %arg0, %arg1 : tensor<16xf32>, tensor<16xf32>
+  } : (!m1_16, !m1_16) -> (!m1_16, !m1_16)
+
+  func.return %f#0, %f#1, %func_arg0: !m1_16, !m1_16, !m1_16
+}
+
+// CHECK-LABEL: func @func_arg_multiple_users_and_func_res_multiple_producers
+// CHECK-SAME:    (%arg0: {{.*}} {mhlo.layout_mode = "auto"})
+func.func @func_arg_multiple_users_and_func_res_multiple_producers(
+  %func_arg: !m1_16 {mhlo.layout_mode = "auto"}) ->
+  (!m1_16 {mhlo.layout_mode = "auto"}, !m1_16 {mhlo.layout_mode = "auto"})
+  attributes {topology=#topology} {
+
+  // CHECK: fragment{{.*}} origin=["f"]
+  // CHECK-SAME: {arg_attrs = [{mhlo.layout_mode = "auto"}], res_attrs = [{mhlo.layout_mode = "auto"}]}
+  %f1 = mpmd.fragment<mesh="m1", origin=["f"]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    mpmd.return %arg0 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  // CHECK: fragment{{.*}} origin=["g"]
+  // CHECK-SAME: {arg_attrs = [{mhlo.layout_mode = "auto"}], res_attrs = [{mhlo.layout_mode = "auto"}]}
+  %f2 = mpmd.fragment<mesh="m1", origin=["g"]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    mpmd.return %arg0 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f1, %f2 : !m1_16, !m1_16
+}
+
+// CHECK-LABEL: func @two_args_w_custom_layouts_to_two_frags(%arg0: {{.*}} {mhlo.layout_mode = "{0, 1}"}, %arg1: {{.*}} {mhlo.layout_mode = "{1, 0}"}) ->
+// CHECK-SAME:    {mhlo.layout_mode = "{0, 1}"}
+// CHECK-SAME:    {mhlo.layout_mode = "{1, 0}"})
+func.func @two_args_w_custom_layouts_to_two_frags(
+    %func_arg_0: !m2_16x16 {mhlo.layout_mode = "{0, 1}"},
+    %func_arg_1: !m2_16x16 {mhlo.layout_mode = "{1, 0}"})
+    -> (!m2_16x16 {mhlo.layout_mode = "{0, 1}"},
+        !m2_16x16 {mhlo.layout_mode = "{1, 0}"})
+        attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.layout_mode = "{0, 1}"}], res_attrs = [{mhlo.layout_mode = "{0, 1}"}]}
+  %f1 = mpmd.fragment<mesh="m2", origin=[]> (%func_arg_0)
+    (%arg0: tensor<16x16xf32>) {
+    %8 = stablehlo.add %arg0, %arg0 : tensor<16x16xf32>
+    mpmd.return %8 : tensor<16x16xf32>
+  } : (!m2_16x16) -> !m2_16x16
+
+  // CHECK: fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.layout_mode = "{0, 1}"}, {mhlo.layout_mode = "{1, 0}"}], res_attrs = [{mhlo.layout_mode = "{1, 0}"}]}
+  %f2 = mpmd.fragment<mesh="m2", origin=[]> (%func_arg_0, %func_arg_1)
+    (%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) {
+    %8 = stablehlo.add %arg0, %arg1 : tensor<16x16xf32>
+    mpmd.return %8 : tensor<16x16xf32>
+  } : (!m2_16x16, !m2_16x16) -> !m2_16x16
+
+  func.return %f1, %f2 : !m2_16x16, !m2_16x16
+}
+
+// CHECK-LABEL: func @no_op_func_arg_returned(%arg0: {{.*}} {mhlo.layout_mode = "auto"}) ->
+// CHECK-SAME:    {mhlo.layout_mode = "auto"})
+func.func @no_op_func_arg_returned(%func_arg: !m1_16 {mhlo.layout_mode = "auto"})
+    -> (!m1_16 {mhlo.layout_mode = "auto"})
+  attributes {topology=#topology} {
+
+  func.return %func_arg : !m1_16
+}
+
+// -----
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>>
+
+// CHECK-LABEL: func @args_w_layouts_propagated_to_return(%arg0: {{.*}}, %arg1: {{.*}} {mhlo.layout_mode = "{0}"}, %arg2: {{.*}}, %arg3: {{.*}}) ->
+// CHECK-SAME:    {mhlo.layout_mode = "auto"}
+// CHECK-SAME:    {mhlo.layout_mode = "{0}"}
+// CHECK-NOT:     {mhlo.layout_mode = "default"}
+func.func @args_w_layouts_propagated_to_return(
+  %arg0: !m1_16 {mhlo.layout_mode = "auto"},
+  %arg1: !m1_16 {mhlo.layout_mode = "{0}"},
+  %arg2: !m1_16,
+  %arg3: !m1_16 {mhlo.layout_mode = "default"})
+    -> (!m1_16 {mhlo.layout_mode = "auto"}, !m1_16 {mhlo.layout_mode = "auto"},
+        !m1_16, !m1_16 {mhlo.layout_mode = "auto"})
+        attributes {topology=#topology} {
+  func.return %arg0, %arg1, %arg2, %arg3 : !m1_16, !m1_16, !m1_16, !m1_16
+}
+
+// -----
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>>
+
+// CHECK-LABEL: func @args_w_layouts_propagated_from_return(%arg0: {{.*}} {mhlo.layout_mode = "{0}"}, %arg1: {{.*}}, %arg2: {{.*}}) ->
+// CHECK-SAME:    {mhlo.layout_mode = "{0}"}
+// CHECK-NOT: {mhlo.layout_mode = "default"}
+func.func @args_w_layouts_propagated_from_return(
+  %arg0: !m1_16 {mhlo.layout_mode = "auto"},
+  %arg1: !m1_16,
+  %arg2: !m1_16 {mhlo.layout_mode = "auto"})
+    -> (!m1_16 {mhlo.layout_mode = "{0}"},
+        !m1_16, !m1_16 {mhlo.layout_mode = "default"})
+    attributes {topology=#topology} {
+  func.return %arg0, %arg1, %arg2 : !m1_16, !m1_16, !m1_16
+}
+
+// -----
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>>
+
+// CHECK-LABEL: func @convert_returned_auto_layouts(%arg0: {{.*}}, %arg1: {{.*}} {mhlo.layout_mode = "{0}"}) ->
+// CHECK-SAME:    {mhlo.layout_mode = "{0}"})
+// CHECK-NOT: {mhlo.layout_mode = "default"}
+func.func @convert_returned_auto_layouts(
+  %arg0: !m1_16 {mhlo.layout_mode = "auto"},
+  %arg1: !m1_16 {mhlo.layout_mode = "{0}"})
+    -> (!m1_16 {mhlo.layout_mode = "default"},
+        !m1_16 {mhlo.layout_mode = "auto"})
+    attributes {topology=#topology} {
+  func.return %arg0, %arg1 : !m1_16, !m1_16
+}
+
+// -----
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>>
+
+// CHECK-LABEL: func @refine_both_arg_and_res_auto_layouts_from_default_result_layout(%arg0: {{.*}}) ->
+// CHECK-NOT: {mhlo.layout_mode = "default"}
+func.func @refine_both_arg_and_res_auto_layouts_from_default_result_layout(
+  %arg0: !m1_16 {mhlo.layout_mode = "auto"})
+    -> (!m1_16 {mhlo.layout_mode = "auto"},
+        !m1_16 {mhlo.layout_mode = "default"})
+    attributes {topology=#topology} {
+  func.return %arg0, %arg0 : !m1_16, !m1_16
+}
+
+// -----
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>>
+
+// CHECK-LABEL: func @refine_func_return_layout_from_fragment_result(%arg0: {{.*}}) ->
+// CHECK-NOT: {mhlo.layout_mode = "default"}
+func.func @refine_func_return_layout_from_fragment_result(%func_arg: !m1_16) ->
+  (!m1_16 {mhlo.layout_mode = "auto"}, !m1_16 {mhlo.layout_mode = "default"})
+  attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{}], res_attrs = [{}]}
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    mpmd.return %arg0 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f, %f : !m1_16, !m1_16
+}
+
+// -----
+
+!m1_16x16 = !mpmd.mesh_tensor<"m1", tensor<16x16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8,"y"=8]>>>
+
+// expected-error@+1 {{Arg #0 is returned as result #0, but with incompatible layouts: "{0, 1}" vs. "{1, 0}"}}
+func.func @error_return_func_return_with_custom_layout(
+    %arg0: !m1_16x16 {mhlo.layout_mode = "{0, 1}"})
+      -> (!m1_16x16 {mhlo.layout_mode = "{1, 0}"})
+      attributes {topology=#topology} {
+  func.return %arg0 : !m1_16x16
+}
+
+// -----
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>>
+
+// expected-error@+1 {{Arg #0 is returned as result #1 and result #0, but with incompatible layouts: "{0}" vs. "default"}}
+func.func @error_incompatible_func_return_layouts(
+  %arg0: !m1_16 {mhlo.layout_mode = "auto"})
+    -> (!m1_16 {mhlo.layout_mode = "default"},
+        !m1_16 {mhlo.layout_mode = "{0}"})
+    attributes {topology=#topology} {
+  func.return %arg0, %arg0 : !m1_16, !m1_16
+}
+
+// -----
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>>
+
+func.func @error_return_twice_with_incompatible_layouts(%func_arg: !m1_16) ->
+  (!m1_16 {mhlo.layout_mode = "{0}"}, !m1_16 {mhlo.layout_mode = "default"})
+  attributes {topology=#topology} {
+
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    mpmd.return %arg0 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  // expected-error@+1 {{Result #0 is also returned as result #1, but with incompatible layouts: "{0}" vs. "default"}}
+  func.return %f, %f : !m1_16, !m1_16
+}
diff --git a/shardy/dialect/mpmd/transforms/export/test/mark_offloaded_input_output.mlir b/shardy/dialect/mpmd/transforms/export/test/mark_offloaded_input_output.mlir
new file mode 100644
index 0000000..ba10359
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/mark_offloaded_input_output.mlir
@@ -0,0 +1,359 @@
+// RUN: mpmd_opt %s -mpmd-mark-offloaded-input-output 2>&1 | FileCheck -implicit-check-not=mhlo.memory_kind %s
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+!m1_4x4 = !mpmd.mesh_tensor<"m1", tensor<4x4xf32>>
+!m1_16x16x16 = !mpmd.mesh_tensor<"m1", tensor<16x16x16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+// CHECK-LABEL: func @simple(%arg0: {{.*}} {mhlo.memory_kind = "pinned_host"}) ->
+// CHECK-SAME:    {mhlo.memory_kind = "pinned_host"})
+func.func @simple(%func_arg: !m1_16 {mhlo.memory_kind = "pinned_host"}) ->
+  (!m1_16 {mhlo.memory_kind = "pinned_host"})
+  attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.memory_kind = "pinned_host"}], res_attrs = [{mhlo.memory_kind = "pinned_host"}]}
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    %8 = stablehlo.add %7, %7 : tensor<16xf32>
+
+    %9 = stablehlo.custom_call @annotate_device_placement(%8) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "pinned_host"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+    mpmd.return %9 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f : !m1_16
+}
+
+// CHECK-LABEL: func @with_optimization_barrier(%arg0
+// CHECK-SAME:    %arg1: {{.*}} {mhlo.memory_kind = "pinned_host"}) ->
+func.func @with_optimization_barrier(%func_arg0: !m1_16,
+  %func_arg1: !m1_16 {mhlo.memory_kind = "pinned_host"}) -> !m1_16
+  attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{}, {mhlo.memory_kind = "pinned_host"}], res_attrs = [{}]}
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg0, %func_arg1)
+    (%arg0: tensor<16xf32>, %arg1: tensor<16xf32>) {
+    %0:2 = stablehlo.optimization_barrier %arg0, %arg1 : tensor<16xf32>, tensor<16xf32>
+    %1:2 = stablehlo.optimization_barrier %0#0, %0#1 : tensor<16xf32>, tensor<16xf32>
+
+    %7 = stablehlo.custom_call @annotate_device_placement(%1#1) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    %8 = stablehlo.add %7, %7 : tensor<16xf32>
+    mpmd.return %8 : tensor<16xf32>
+  } : (!m1_16, !m1_16) -> !m1_16
+
+  func.return %f : !m1_16
+}
+
+// CHECK-LABEL: func @in_while_loop_with_update
+// CHECK-SAME:  -> ({{.*}}mesh_tensor{{.*}}mesh_tensor{{.*}} {mhlo.memory_kind = "pinned_host"})
+func.func @in_while_loop_with_update(%func_arg: !m1_16x16x16) -> (!m1_16x16x16, !m1_16x16x16 {mhlo.memory_kind = "pinned_host"})
+  attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{}], res_attrs = [{}, {mhlo.memory_kind = "pinned_host"}]}
+  %f:2 = mpmd.fragment<mesh="m1", origin=[]> (%func_arg) (%arg0: tensor<16x16x16xf32>) {
+    %0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
+    %1 = stablehlo.constant dense<1> : tensor<i32>
+    %5 = stablehlo.constant dense<0.000000e+00> : tensor<16x16x16xf32>
+    %6:4 = stablehlo.while(%iterArg = %1, %iterArg_0 = %0, %iterArg_1 = %5, %iterArg_2 = %5)
+      : tensor<i32>, tensor<f32>, tensor<16x16x16xf32>, tensor<16x16x16xf32>
+      cond {
+      %7 = stablehlo.compare  LT, %iterArg, %iterArg,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>
+      stablehlo.return %7 : tensor<i1>
+    } do {
+      %10 = stablehlo.dynamic_slice %iterArg_2, %iterArg, %iterArg, %iterArg, sizes = [1, 16, 16] : (tensor<16x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<1x16x16xf32>
+      %11 = stablehlo.reshape %10 : (tensor<1x16x16xf32>) -> tensor<16x16xf32>
+      %13 = stablehlo.broadcast_in_dim %iterArg_0, dims = [] : (tensor<f32>) -> tensor<16x16xf32>
+      %14 = stablehlo.add %11, %13 : tensor<16x16xf32>
+      %15 = stablehlo.sine %14 : tensor<16x16xf32>
+      %16 = stablehlo.cosine %14 : tensor<16x16xf32>
+      %17 = stablehlo.custom_call @annotate_device_placement(%16) {backend_config = "", has_side_effect = true, mhlo.frontend_attributes = {_xla_buffer_placement = "pinned_host"}} : (tensor<16x16xf32>) -> tensor<16x16xf32>
+      %18 = stablehlo.reshape %15 : (tensor<16x16xf32>) -> tensor<1x16x16xf32>
+      %19 = stablehlo.dynamic_update_slice %iterArg_1, %18, %iterArg, %iterArg, %iterArg : (tensor<16x16x16xf32>, tensor<1x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<16x16x16xf32>
+      %20 = stablehlo.reshape %17 : (tensor<16x16xf32>) -> tensor<1x16x16xf32>
+      %21 = stablehlo.dynamic_update_slice %iterArg_2, %20, %iterArg, %iterArg, %iterArg : (tensor<16x16x16xf32>, tensor<1x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<16x16x16xf32>
+      stablehlo.return %iterArg, %iterArg_0, %19, %21 : tensor<i32>, tensor<f32>, tensor<16x16x16xf32>, tensor<16x16x16xf32>
+    }
+    %cc = stablehlo.custom_call @Sharding(%6#3) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    %cc1 = stablehlo.custom_call @SPMDFullToShardShape(%cc) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    %cc2 = stablehlo.custom_call @SPMDShardToFullShape(%cc1) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    mpmd.return %6#2, %cc2 : tensor<16x16x16xf32>, tensor<16x16x16xf32>
+  } : (!m1_16x16x16) -> (!m1_16x16x16, !m1_16x16x16)
+  func.return %f#0, %f#1: !m1_16x16x16, !m1_16x16x16
+}
+
+// CHECK-LABEL: func @in_while_loop_with_slice(%arg0: {{.*}} {mhlo.memory_kind = "pinned_host"})
+func.func @in_while_loop_with_slice(
+    %func_arg: !m1_16x16x16 {mhlo.memory_kind = "pinned_host"}
+  ) -> !m1_16x16x16 attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.memory_kind = "pinned_host"}], res_attrs = [{}]}
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg)
+    (%arg0: tensor<16x16x16xf32>) {
+    %0 = stablehlo.constant dense<1> : tensor<i32>
+    %3 = stablehlo.constant dense<0.000000e+00> : tensor<16x16x16xf32>
+    %cc = stablehlo.custom_call @Sharding(%arg0) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    %cc1 = stablehlo.custom_call @SPMDFullToShardShape(%cc) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    %cc2 = stablehlo.custom_call @SPMDShardToFullShape(%cc1) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    %4:3 = stablehlo.while(%iterArg = %0, %iterArg_0 = %3, %iterArg_5 = %cc2) : tensor<i32>, tensor<16x16x16xf32>, tensor<16x16x16xf32>
+      cond {
+      %5 = stablehlo.compare  LT, %iterArg, %iterArg,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>
+      stablehlo.return %5 : tensor<i1>
+    } do {
+      %10 = stablehlo.dynamic_slice %iterArg_0, %iterArg, %iterArg, %iterArg, sizes = [1, 16, 16] : (tensor<16x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<1x16x16xf32>
+      %11 = stablehlo.reshape %10 : (tensor<1x16x16xf32>) -> tensor<16x16xf32>
+      %12 = stablehlo.dynamic_slice %iterArg_5, %iterArg, %iterArg, %iterArg, sizes = [1, 16, 16] : (tensor<16x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<1x16x16xf32>
+      %13 = stablehlo.reshape %12 : (tensor<1x16x16xf32>) -> tensor<16x16xf32>
+      %14:2 = stablehlo.optimization_barrier %13, %11 : tensor<16x16xf32>, tensor<16x16xf32>
+      %15 = stablehlo.custom_call @annotate_device_placement(%14#0) {backend_config = "", has_side_effect = true, mhlo.frontend_attributes = {_xla_buffer_placement = "device"}} : (tensor<16x16xf32>) -> tensor<16x16xf32>
+      %17 = stablehlo.reshape %15 : (tensor<16x16xf32>) -> tensor<1x16x16xf32>
+      %18 = stablehlo.dynamic_update_slice %iterArg_0, %17, %iterArg, %iterArg, %iterArg : (tensor<16x16x16xf32>, tensor<1x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<16x16x16xf32>
+      stablehlo.return %iterArg, %18, %iterArg_0 : tensor<i32>, tensor<16x16x16xf32>, tensor<16x16x16xf32>
+    }
+    mpmd.return %4#1 : tensor<16x16x16xf32>
+  } : (!m1_16x16x16) -> !m1_16x16x16
+  return %f : !m1_16x16x16
+}
+
+// CHECK-LABEL: func @place_device_with_incompatible_reshape_and_custom_call(
+// CHECK-NOT:     mhlo.memory_kind
+func.func @place_device_with_incompatible_reshape_and_custom_call(%func_arg0: !m1_16, %func_arg1: !m1_16) -> (!m1_4x4, !m1_16)
+  attributes {topology=#topology} {
+  %f:2 = mpmd.fragment<mesh="m1", origin=[]> (%func_arg0, %func_arg1)
+    (%arg0: tensor<16xf32>, %arg1: tensor<16xf32>) {
+    %0 = stablehlo.reshape %arg0 : (tensor<16xf32>) -> tensor<4x4xf32>
+    %6 = stablehlo.custom_call @annotate_device_placement(%0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<4x4xf32>) -> tensor<4x4xf32>
+
+    %cc = stablehlo.custom_call @Something(%arg1) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16xf32>) -> tensor<16xf32>
+    %7 = stablehlo.custom_call @annotate_device_placement(%cc) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+    mpmd.return %6, %7: tensor<4x4xf32>, tensor<16xf32>
+  }  : (!m1_16, !m1_16) -> (!m1_4x4, !m1_16)
+  func.return %f#0, %f#1 : !m1_4x4, !m1_16
+}
+
+// CHECK-LABEL: func @place_host_with_incompatible_reshape_and_custom_call(
+// CHECK-NOT:     mhlo.memory_kind
+func.func @place_host_with_incompatible_reshape_and_custom_call(%func_arg0: !m1_16, %func_arg1: !m1_16) -> (!m1_4x4, !m1_16)
+  attributes {topology=#topology} {
+  %f:2 = mpmd.fragment<mesh="m1", origin=[]> (%func_arg0, %func_arg1)
+    (%arg0: tensor<16xf32>, %arg1: tensor<16xf32>) {
+    %6 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "pinned_host"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+    %0 = stablehlo.reshape %6 : (tensor<16xf32>) -> tensor<4x4xf32>
+
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg1) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "pinned_host"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+    %cc = stablehlo.custom_call @Something(%7) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16xf32>) -> tensor<16xf32>
+    mpmd.return %0, %cc: tensor<4x4xf32>, tensor<16xf32>
+  }  : (!m1_16, !m1_16) -> (!m1_4x4, !m1_16)
+  func.return %f#0, %f#1 : !m1_4x4, !m1_16
+}
+
+// CHECK-LABEL: func @func_arg_multiple_matching_users
+// CHECK-SAME:    (%arg0: {{.*}} {mhlo.memory_kind = "pinned_host"})
+func.func @func_arg_multiple_matching_users(
+  %func_arg: !m1_16 {mhlo.memory_kind = "pinned_host"}) -> (!m1_16, !m1_16)
+  attributes {topology=#topology} {
+
+  // CHECK: fragment{{.*}} origin=["f"]
+  // CHECK-SAME: {arg_attrs = [{mhlo.memory_kind = "pinned_host"}]
+  %f1 = mpmd.fragment<mesh="m1", origin=["f"]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    mpmd.return %7 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  // CHECK: fragment{{.*}} origin=["g"]
+  // CHECK-SAME: {arg_attrs = [{mhlo.memory_kind = "pinned_host"}]
+  %f2 = mpmd.fragment<mesh="m1", origin=["g"]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    mpmd.return %7 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+  func.return %f1, %f2 : !m1_16, !m1_16
+}
+
+// CHECK-LABEL: func @func_arg_match
+// CHECK-SAME:    (%arg0: {{.*}} {mhlo.memory_kind = "pinned_host"})
+func.func @func_arg_match(%func_arg: !m1_16 {mhlo.memory_kind = "pinned_host"}) -> !m1_16
+  attributes {topology=#topology} {
+
+  // CHECK: fragment{{.*}} origin=["f"]
+  // CHECK-SAME: {arg_attrs = [{mhlo.memory_kind = "pinned_host"}]
+  %f1 = mpmd.fragment<mesh="m1", origin=["f"]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    mpmd.return %7 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f1 : !m1_16
+}
+
+
+// CHECK-LABEL: func @func_result_match
+// CHECK-SAME:  -> ({{.*}} {mhlo.memory_kind = "pinned_host"})
+func.func @func_result_match(%func_arg: !m1_16) -> (!m1_16 {mhlo.memory_kind = "pinned_host"})
+  attributes {topology=#topology} {
+
+  // CHECK: fragment{{.*}} origin=["f"]
+  // CHECK-SAME: {arg_attrs = [{}], res_attrs = [{mhlo.memory_kind = "pinned_host"}]}
+  %f1 = mpmd.fragment<mesh="m1", origin=["f"]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "pinned_host"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    mpmd.return %7 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f1 : !m1_16
+}
+
+// CHECK-LABEL: func @func_arg_returned_match(%arg0: {{.*}} {mhlo.memory_kind = "pinned_host"}) ->
+// CHECK-SAME:    {mhlo.memory_kind = "pinned_host"})
+func.func @func_arg_returned_match(%func_arg: !m1_16 {mhlo.memory_kind = "pinned_host"})
+    -> (!m1_16 {mhlo.memory_kind = "pinned_host"})
+  attributes {topology=#topology} {
+
+  func.return %func_arg : !m1_16
+}
+
+// CHECK-LABEL: func @noop_if_no_non_return_user(
+// CHECK-NOT:     mhlo.memory_kind
+func.func @noop_if_no_non_return_user(%func_arg0: !m1_16, %func_arg1: !m1_16) -> (!m1_4x4, !m1_16)
+  attributes {topology=#topology} {
+
+  %f:2 = mpmd.fragment<mesh="m1", origin=[]> (%func_arg0, %func_arg1)
+    (%arg0: tensor<16xf32>, %arg1: tensor<16xf32>) {
+    %0 = stablehlo.reshape %arg0 : (tensor<16xf32>) -> tensor<4x4xf32>
+    mpmd.return %0, %arg1: tensor<4x4xf32>, tensor<16xf32>
+  }  : (!m1_16, !m1_16) -> (!m1_4x4, !m1_16)
+
+  func.return %f#0, %f#1 : !m1_4x4, !m1_16
+}
+
+// CHECK-LABEL: func @no_mismatch_between_func_device_and_frag_missing(%arg0: {{.*}} {mhlo.memory_kind = "device"}) ->
+// CHECK-SAME:    {mhlo.memory_kind = "device"})
+func.func @no_mismatch_between_func_device_and_frag_missing(%func_arg: !m1_16 {mhlo.memory_kind = "device"}) ->
+  (!m1_16 {mhlo.memory_kind = "device"})
+  attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{}], res_attrs = [{}]}
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    mpmd.return %arg0 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f : !m1_16
+}
+
+// CHECK-LABEL: func @no_mismatch_between_func_missing_and_frag_device(
+// CHECK-NOT:    {mhlo.memory_kind = "device"}
+func.func @no_mismatch_between_func_missing_and_frag_device(%func_arg: !m1_16 ) -> !m1_16
+  attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.memory_kind = "device"}], res_attrs = [{mhlo.memory_kind = "device"}]}
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg)
+    {arg_attrs = [{mhlo.memory_kind = "device"}], res_attrs = [{mhlo.memory_kind = "device"}]}
+    (%arg0: tensor<16xf32>) {
+    mpmd.return %arg0 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f : !m1_16
+}
+
+// CHECK-LABEL: func @simple_compute_on_host(%arg0: {{.*}} {mhlo.memory_kind = "pinned_host"}) ->
+// CHECK-SAME:    {mhlo.memory_kind = "pinned_host"})
+func.func @simple_compute_on_host(%func_arg: !m1_16 {mhlo.memory_kind = "pinned_host"}) ->
+  (!m1_16 {mhlo.memory_kind = "pinned_host"})
+  attributes {topology=#topology} {
+
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{mhlo.memory_kind = "pinned_host"}], res_attrs = [{mhlo.memory_kind = "pinned_host"}]}
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    %8 = stablehlo.add %arg0, %arg0 {
+      mhlo.frontend_attributes = {_xla_compute_type = "host"}
+    } : tensor<16xf32>
+    mpmd.return %8 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f : !m1_16
+}
+
+// CHECK-LABEL: func @propagate_memory_kind_with_broadcast
+// CHECK-SAME:  -> ({{.*}}mesh_tensor{{.*}}mesh_tensor{{.*}} {mhlo.memory_kind = "pinned_host"})
+func.func @propagate_memory_kind_with_broadcast(%func_arg: !m1_16x16x16) -> (!m1_16x16x16, !m1_16x16x16 {mhlo.memory_kind = "pinned_host"})
+  attributes {topology=#topology} {
+  // CHECK-NEXT: fragment
+  // CHECK-SAME: {arg_attrs = [{}], res_attrs = [{}, {mhlo.memory_kind = "pinned_host"}]}
+  %f:2 = mpmd.fragment<mesh="m1", origin=[]> (%func_arg) (%arg0: tensor<16x16x16xf32>) {
+    %0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
+    %1 = stablehlo.constant dense<1> : tensor<i32>
+    %5 = stablehlo.constant dense<0.000000e+00> : tensor<16x16x16xf32>
+    %6:4 = stablehlo.while(%iterArg = %1, %iterArg_0 = %0, %iterArg_1 = %5, %iterArg_2 = %5)
+      : tensor<i32>, tensor<f32>, tensor<16x16x16xf32>, tensor<16x16x16xf32>
+      cond {
+      %7 = stablehlo.compare  LT, %iterArg, %iterArg,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>
+      stablehlo.return %7 : tensor<i1>
+    } do {
+      %10 = stablehlo.dynamic_slice %iterArg_2, %iterArg, %iterArg, %iterArg, sizes = [1, 16, 16] : (tensor<16x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<1x16x16xf32>
+      %11 = stablehlo.reshape %10 : (tensor<1x16x16xf32>) -> tensor<16x16xf32>
+      %13 = stablehlo.broadcast_in_dim %iterArg_0, dims = [] : (tensor<f32>) -> tensor<16x16xf32>
+      %14 = stablehlo.add %11, %13 : tensor<16x16xf32>
+      %15 = stablehlo.sine %14 : tensor<16x16xf32>
+      %16 = stablehlo.cosine %14 : tensor<16x16xf32>
+      %17 = stablehlo.custom_call @annotate_device_placement(%16) {backend_config = "", has_side_effect = true, mhlo.frontend_attributes = {_xla_buffer_placement = "pinned_host"}} : (tensor<16x16xf32>) -> tensor<16x16xf32>
+      %18 = stablehlo.broadcast_in_dim %15, dims = [1, 2] : (tensor<16x16xf32>) -> tensor<1x16x16xf32>
+      %19 = stablehlo.dynamic_update_slice %iterArg_1, %18, %iterArg, %iterArg, %iterArg : (tensor<16x16x16xf32>, tensor<1x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<16x16x16xf32>
+      %20 = stablehlo.reshape %17 : (tensor<16x16xf32>) -> tensor<1x16x16xf32>
+      %21 = stablehlo.dynamic_update_slice %iterArg_2, %20, %iterArg, %iterArg, %iterArg : (tensor<16x16x16xf32>, tensor<1x16x16xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<16x16x16xf32>
+      stablehlo.return %iterArg, %iterArg_0, %19, %21 : tensor<i32>, tensor<f32>, tensor<16x16x16xf32>, tensor<16x16x16xf32>
+    }
+    %cc = stablehlo.custom_call @Sharding(%6#3) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    %cc1 = stablehlo.custom_call @SPMDFullToShardShape(%cc) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    %cc2 = stablehlo.custom_call @SPMDShardToFullShape(%cc1) {backend_config = "", mhlo.sharding = "{replicated}"} : (tensor<16x16x16xf32>) -> tensor<16x16x16xf32>
+    mpmd.return %6#2, %cc2 : tensor<16x16x16xf32>, tensor<16x16x16xf32>
+  } : (!m1_16x16x16) -> (!m1_16x16x16, !m1_16x16x16)
+  func.return %f#0, %f#1: !m1_16x16x16, !m1_16x16x16
+}
+
diff --git a/shardy/dialect/mpmd/transforms/export/test/mark_offloaded_input_output_failure.mlir b/shardy/dialect/mpmd/transforms/export/test/mark_offloaded_input_output_failure.mlir
new file mode 100644
index 0000000..6ecb909
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/test/mark_offloaded_input_output_failure.mlir
@@ -0,0 +1,111 @@
+// RUN: mpmd_opt %s -mpmd-mark-offloaded-input-output -split-input-file -verify-diagnostics 2>&1
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+
+// expected-error@+1 {{Memory kind mismatch between users of arg 0: <<NULL ATTRIBUTE>> vs "pinned_host"}}
+func.func @func_arg_multiple_different_fragment_users(%func_arg: !m1_16) -> (!m1_16, !m1_16)
+  attributes {topology=#topology} {
+
+  %f1 = mpmd.fragment<mesh="m1", origin=["f"]> (%func_arg)
+    {arg_attrs = [{mhlo.memory_kind = "pinned_host"}], res_attrs = [{}]}
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    mpmd.return %7 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  %f2 = mpmd.fragment<mesh="m1", origin=["g"]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    mpmd.return %arg0 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+  func.return %f1, %f2 : !m1_16, !m1_16
+}
+
+// -----
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+// expected-error@+1 {{Memory kind mismatch between users of arg 0: <<NULL ATTRIBUTE>> vs "pinned_host"}}
+func.func @func_arg_multiple_different_users(%func_arg: !m1_16) -> (!m1_16, !m1_16)
+  attributes {topology=#topology} {
+
+  %f1 = mpmd.fragment<mesh="m1", origin=["f"]> (%func_arg)
+    {arg_attrs = [{mhlo.memory_kind = "pinned_host"}], res_attrs = [{}]}
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    mpmd.return %7 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  // Note that we don't support the transferring of offloaded tensors right now.
+  %f2 = mpmd.transfer %func_arg : (!m1_16) -> !m1_16
+  func.return %f1, %f2 : !m1_16, !m1_16
+}
+
+// -----
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+// expected-error@+1 {{Memory kind mismatch between arg 0 and users: "device" vs "pinned_host"}}
+func.func @func_arg_mismatch(%func_arg: !m1_16 {mhlo.memory_kind = "device"}) -> !m1_16
+  attributes {topology=#topology} {
+
+  %f1 = mpmd.fragment<mesh="m1", origin=["f"]> (%func_arg)
+    {arg_attrs = [{mhlo.memory_kind = "pinned_host"}], res_attrs = [{}]}
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "device"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    mpmd.return %7 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+  func.return %f1 : !m1_16
+}
+
+// -----
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+func.func @func_result_mismatch(%func_arg: !m1_16) -> (!m1_16 {mhlo.memory_kind = "device"})
+  attributes {topology=#topology} {
+
+  %f1 = mpmd.fragment<mesh="m1", origin=["f"]> (%func_arg)
+    (%arg0: tensor<16xf32>) {
+    %7 = stablehlo.custom_call @annotate_device_placement(%arg0) {
+        backend_config = "", has_side_effect = true,
+        mhlo.frontend_attributes = {_xla_buffer_placement = "pinned_host"}
+      } : (tensor<16xf32>) -> tensor<16xf32>
+
+    mpmd.return %7 : tensor<16xf32>
+  } : (!m1_16) -> !m1_16
+
+// expected-error@+1 {{Memory kind mismatch between result 0 and defining op: "device" vs "pinned_host"}}
+  func.return %f1 : !m1_16
+}
+
+// -----
+
+!m1_16 = !mpmd.mesh_tensor<"m1", tensor<16xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+// expected-error@+1 {{Memory kind mismatch between arg 0 and users: "pinned_host" vs "device"}}
+func.func @func_arg_returned_mismatch(%func_arg: !m1_16 {mhlo.memory_kind = "pinned_host"})
+    -> (!m1_16 {mhlo.memory_kind = "device"})
+  attributes {topology=#topology} {
+// expected-error@+1 {{Memory kind mismatch between result 0 and defining op: "device" vs "pinned_host"}}
+  func.return %func_arg : !m1_16
+}
+
diff --git a/shardy/dialect/mpmd/transforms/export/utils.cc b/shardy/dialect/mpmd/transforms/export/utils.cc
new file mode 100644
index 0000000..0b5610b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/utils.cc
@@ -0,0 +1,68 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/export/utils.h"
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+
+namespace mlir::mpmd {
+
+using ::mlir::func::FuncOp;
+
+using OperandLastUseMap = DenseMap<Operation*, SmallVector<unsigned int>>;
+
+DenseSet<BlockArgument> GetAliasedBlockArguments(FuncOp main_func) {
+  DenseSet<BlockArgument> aliased_block_args;
+  for (unsigned i = 0; i < main_func.getNumArguments(); ++i) {
+    if (main_func.getArgAttrOfType<IntegerAttr>(i, kAliasingAttrName)) {
+      aliased_block_args.insert(main_func.getArgument(i));
+    }
+  }
+  return aliased_block_args;
+}
+
+DenseSet<BlockArgument> GetDonatedBlockArguments(FuncOp main_func) {
+  DenseSet<BlockArgument> donated_block_args;
+  for (unsigned i = 0; i < main_func.getNumArguments(); ++i) {
+    auto donated_attr =
+        main_func.getArgAttrOfType<BoolAttr>(i, kBufferDonationAttrName);
+    if (donated_attr && donated_attr.getValue()) {
+      donated_block_args.insert(main_func.getArgument(i));
+    }
+  }
+  return donated_block_args;
+}
+
+OperandLastUseMap OperandsForDeletionMapping(FuncOp main_func) {
+  OperandLastUseMap last_use_map;
+
+  DenseMap<Value, OpOperand*> value_to_last_use;
+  for (Operation& op : main_func.getOps()) {
+    for (OpOperand& use : op.getOpOperands()) {
+      value_to_last_use[use.get()] = &use;
+    }
+  }
+
+  for (auto [_, use] : value_to_last_use) {
+    last_use_map[use->getOwner()].push_back(use->getOperandNumber());
+  }
+  return last_use_map;
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/export/utils.h b/shardy/dialect/mpmd/transforms/export/utils.h
new file mode 100644
index 0000000..fd893a2
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/utils.h
@@ -0,0 +1,105 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_UTILS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_UTILS_H_
+
+#include "mlir/Analysis/Liveness.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+
+namespace mlir::mpmd {
+
+// Name of attribute used by XLA to alias inputs to outputs. The attribute
+// is of type `I32IntegerAttr`, is attached to the input, and stores the
+// index of the output the input is aliased with.
+constexpr StringRef kAliasingAttrName = "tf.aliasing_output";
+// Name of attribute used by XLA to donate inputs. The attribute is of type
+// `BoolAttr`, is attached to the input, and is set to true if the input is
+// donated. This attribute is set in situations in which no aliasing was found
+// for the input, which can happen if we do not sufficient information to
+// compute the exact per-device byte size of the input. When this attribute is
+// set to true, XLA will try to alias the input with an output using correct
+// per-device byte size information.
+constexpr StringRef kBufferDonationAttrName = "jax.buffer_donor";
+
+// Name of attribute that will tell XLA how much memory to reserve while
+// compiling each fragment.
+constexpr StringRef kReservedHbmBytes = "xla_tpu_user_reserved_hbm_bytes";
+
+// Returns the set of user marked block arguments to be aliased.
+DenseSet<BlockArgument> GetAliasedBlockArguments(
+    func::FuncOp main_func);
+
+// Returns the set of user marked block arguments to donate.
+DenseSet<BlockArgument> GetDonatedBlockArguments(
+    func::FuncOp main_func);
+
+// Returns a map from a operation to a vector of OpOperand, such that each
+// OpOperand is used in the respective operation last. This is done by tracking
+// the last OpOperand use of a value in a map.
+//
+// For each operation with an OpOperand that is used for the last time, we
+// store a list of indices of which operand(s) are the last use. This will
+// then be used to figure out which operands can be deleted from the live
+// range.
+DenseMap<Operation*, SmallVector<unsigned int>>
+OperandsForDeletionMapping(func::FuncOp main_func);
+
+// Checks the arg attrs of the op to see if the arg is on the host.
+inline bool IsArgOnHost(Operation* op, int index) {
+  return GetArgAttr(op, index, kMemoryKindAttr) ==
+         StringAttr::get(op->getContext(), kMemoryKindPinnedHost);
+}
+
+// Checks the arg of the function is on the host.
+inline bool IsArgOnHost(func::FuncOp func, int index) {
+  return func.getArgAttrOfType<StringAttr>(index,
+                                                 kMemoryKindAttr) ==
+         StringAttr::get(func.getContext(), kMemoryKindPinnedHost);
+}
+
+// Checks the result attrs of the op to see if the result is on the host.
+inline bool IsResultOnHost(OpResult op_result) {
+  return GetResAttr(op_result.getOwner(),
+                                op_result.getResultNumber(),
+                                kMemoryKindAttr) ==
+         StringAttr::get(op_result.getContext(),
+                               kMemoryKindPinnedHost);
+}
+
+// Checks if the layout of the input and output match.
+inline bool IsInputOutputLayoutMatch(Operation* op, int input_index,
+                                     int output_index) {
+  return GetArgAttr(op, input_index, kLayoutModeAttr) ==
+         GetResAttr(op, output_index, kLayoutModeAttr);
+}
+
+// Checks if the arg of the function is donated or aliased.
+inline bool IsArgDonated(func::FuncOp func, int index) {
+  return func.getArgAttrOfType<StringAttr>(index, kAliasingAttrName) ||
+         func.getArgAttrOfType<BoolAttr>(index,
+                                               kBufferDonationAttrName) ==
+             BoolAttr::get(func.getContext(), true);
+}
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_EXPORT_UTILS_H_
diff --git a/shardy/dialect/mpmd/transforms/export/utils_test.cc b/shardy/dialect/mpmd/transforms/export/utils_test.cc
new file mode 100644
index 0000000..cacf6f7
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/export/utils_test.cc
@@ -0,0 +1,110 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/export/utils.h"
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OwningOpRef.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Parser/Parser.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/register.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include <gmock/gmock.h>
+#include <gtest/gtest.h>
+
+using ::mlir::func::FuncOp;
+using ::testing::Pair;
+using ::testing::SizeIs;
+using ::testing::UnorderedElementsAre;
+
+namespace mlir::mpmd {
+namespace {
+
+MATCHER(OperationIsAFragmentOp, "") { return isa<FragmentOp>(arg); }
+
+MATCHER(OperationIsAReturnOp, "") {
+  return isa<func::ReturnOp>(arg);
+}
+
+const char kProgramWithUserMarkedDonation[] = R"mlir(
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // %arg0 is marked to be donated by the user.
+  func.func @main(%arg0: !mesh_1_tensor_4_8_f32 {tf.aliasing_output = 1 : i32}, %arg1: !mesh_1_tensor_4_8_f32 {jax.buffer_donor= true}, %arg2: !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+      "topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>} {
+  // %arg0 and %arg1 are used in this mpmd.fragment op last.
+  %0, %1, %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1, %arg2) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg3, %arg4: tensor<4x8xf32>
+    %1 = stablehlo.abs %0: tensor<4x8xf32>
+    %2 = stablehlo.abs %arg5: tensor<4x8xf32>
+    mpmd.return %0, %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32,!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  // All three arguments are used in the return operation last.
+  func.return %0, %1, %arg2 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+  )mlir";
+
+TEST(GetAliasedBlockArguments, ShouldReturnCorrectBlockArgsToAlias) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgramWithUserMarkedDonation, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  DenseSet<BlockArgument> block_args_to_alias =
+      GetAliasedBlockArguments(func_op);
+
+  ASSERT_THAT(block_args_to_alias, SizeIs(1));
+  EXPECT_THAT(block_args_to_alias.begin()->getArgNumber(), 0);
+}
+
+TEST(GetDonatedBlockArguments, ShouldReturnCorrectBlockArgsToDonate) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgramWithUserMarkedDonation, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  DenseSet<BlockArgument> block_args_to_donate =
+      GetDonatedBlockArguments(func_op);
+
+  ASSERT_THAT(block_args_to_donate, SizeIs(1));
+  EXPECT_THAT(block_args_to_donate.begin()->getArgNumber(), 1);
+}
+
+TEST(OperandsForDeletionMapping,
+     ShouldReturnCorrectOperandsForDeletionMapping) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgramWithUserMarkedDonation, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  DenseMap<Operation*, SmallVector<unsigned int>> operands_last_used_in_op =
+      OperandsForDeletionMapping(func_op);
+
+  EXPECT_THAT(operands_last_used_in_op,
+              UnorderedElementsAre(
+                  Pair(OperationIsAFragmentOp(), UnorderedElementsAre(0, 1)),
+                  Pair(OperationIsAReturnOp(), UnorderedElementsAre(0, 1, 2))));
+}
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/BUILD b/shardy/dialect/mpmd/transforms/import/BUILD
new file mode 100644
index 0000000..f9c885e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/BUILD
@@ -0,0 +1,185 @@
+# The MPMD import passes and pipeline.
+
+# load("@rules_cc//cc:cc_library.bzl", "cc_library")
+# load("@rules_cc//cc:cc_test.bzl", "cc_test")
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library", "td_library")
+
+package(default_visibility = ["//visibility:public"])
+
+td_library(
+    name = "passes_td_files",
+    srcs = [
+        "passes.td",
+    ],
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+gentbl_cc_library(
+    name = "passes_inc",
+    tbl_outs = {
+        "passes.h.inc": [
+            "-gen-pass-decls",
+            "-name=MpmdImport",
+        ],
+        "g3doc/mpmd_import_passes.md": ["-gen-pass-doc"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "passes.td",
+    deps = [":passes_td_files"],
+)
+
+cc_library(
+    name = "passes",
+    srcs = [
+        "copy_topology_from_main.cc",
+        "enforce_equisharding.cc",
+        "import_pipeline.cc",
+        "infer_mesh_assignment.cc",
+        "infer_mesh_validation.cc",
+        "insert_nameless_clones_of_negligible_ops.cc",
+        "introduce_transfers.cc",
+        "map_input_output_to_mesh.cc",
+        "map_named_ops_to_mpmd_ops.cc",
+        "simplify_named_computations.cc",
+        "validate_named_ops_in_mpmd_func.cc",
+    ],
+    hdrs = [
+        "infer_mesh_assignment.h",
+        "passes.h",
+    ],
+    deps = [
+        ":mesh_assignment_map",
+        ":mesh_inference_origins",
+        ":mesh_inference_utils",
+        ":meshes_with_origins",
+        ":passes_inc",
+        ":sharding_constraints",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/transforms/common:distributed_function_pass",
+        "//shardy/dialect/mpmd/transforms/common:passes",
+        "//shardy/dialect/mpmd/transforms/common:simplify_region_op_base",
+        "//shardy/dialect/mpmd/transforms/common:utils",
+        "//shardy/dialect/sdy/ir:dialect",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Rewrite",
+        "@llvm-project//mlir:SideEffectInterfaces",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+        "@stablehlo//:stablehlo_ops",
+        "@stablehlo//:stablehlo_passes",
+        "@stablehlo//:stablehlo_passes_optimization",
+    ],
+)
+
+cc_library(
+    name = "mesh_assignment_map",
+    srcs = ["mesh_assignment_map.cc"],
+    hdrs = ["mesh_assignment_map.h"],
+    deps = ["@llvm-project//llvm:Support"],
+)
+
+cc_test(
+    name = "enforce_equisharding_test",
+    srcs = ["enforce_equisharding_test.cc"],
+    deps = [
+        ":passes",
+        ":sharding_constraints",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/ir:register",
+        "@com_google_googletest//:gtest_main",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Parser",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_library(
+    name = "sharding_constraints",
+    srcs = ["sharding_constraints.cc"],
+    hdrs = ["sharding_constraints.h"],
+    deps = ["@llvm-project//llvm:Support"],
+)
+
+cc_library(
+    name = "mesh_inference_utils",
+    srcs = ["mesh_inference_utils.cc"],
+    hdrs = ["mesh_inference_utils.h"],
+    deps = [
+        ":meshes_with_origins",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/ir:fragment_arg_res_attrs",
+        "//shardy/dialect/mpmd/transforms/common:utils",
+        "//shardy/dialect/sdy/ir:dialect",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_library(
+    name = "mesh_inference_origins",
+    srcs = ["mesh_inference_origins.cc"],
+    hdrs = ["mesh_inference_origins.h"],
+    deps = [
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_test(
+    name = "mesh_inference_utils_test",
+    srcs = ["mesh_inference_utils_test.cc"],
+    deps = [
+        ":mesh_inference_utils",
+        ":meshes_with_origins",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "@com_google_googletest//:gtest_main",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Parser",
+        "@llvm-project//mlir:Support",
+        "@stablehlo//:stablehlo_ops",
+    ],
+)
+
+cc_library(
+    name = "meshes_with_origins",
+    srcs = ["meshes_with_origins.cc"],
+    hdrs = ["meshes_with_origins.h"],
+    deps = [
+        ":mesh_inference_origins",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_test(
+    name = "meshes_with_origins_test",
+    srcs = ["meshes_with_origins_test.cc"],
+    deps = [
+        ":mesh_inference_origins",
+        ":meshes_with_origins",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "@com_google_googletest//:gtest_main",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
diff --git a/shardy/dialect/mpmd/transforms/import/copy_topology_from_main.cc b/shardy/dialect/mpmd/transforms/import/copy_topology_from_main.cc
new file mode 100644
index 0000000..76f3c2f
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/copy_topology_from_main.cc
@@ -0,0 +1,96 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/sdy/ir/dialect.h"
+
+using ::mlir::func::FuncOp;
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_COPYTOPOLOGYFROMMAINPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+// If the module doesn't have an `sdy.mesh`, constructs a global mesh containing
+// all the axes in the main function's topology. Assumes all meshes in the
+// topology are homogeneous.
+void MaybeConstructSdyMesh(ModuleOp module_op) {
+  if (!module_op.getOps<sdy::MeshOp>().empty()) {
+    return;
+  }
+
+  FuncOp main_func = GetMainFunction(module_op);
+  SDY_CHECK(HasHomogeneousTopology(main_func));
+
+  // Create a global mesh containing all the axes.
+  auto current_topology_attr =
+      main_func->getAttrOfType<TopologyAttr>(kTopologyAttr);
+  SDY_CHECK(current_topology_attr);
+  sdy::MeshAttr named_mesh =
+      current_topology_attr.getMeshes().front().getMesh();
+
+  MLIRContext* ctx = module_op->getContext();
+  SmallVector<sdy::MeshAxisAttr> sdy_axes;
+  sdy_axes.reserve(named_mesh.getAxes().size());
+  for (sdy::MeshAxisAttr mesh_axis : named_mesh.getAxes()) {
+    sdy_axes.push_back(
+        sdy::MeshAxisAttr::get(ctx, mesh_axis.getName(), mesh_axis.getSize()));
+  }
+
+  OpBuilder::atBlockBegin(module_op.getBody())
+      .create<sdy::MeshOp>(module_op.getLoc(), kGlobalMeshName,
+                           sdy::MeshAttr::get(ctx, sdy_axes));
+}
+
+class CopyTopologyFromMainPass
+    : public impl::CopyTopologyFromMainPassBase<CopyTopologyFromMainPass> {
+  using CopyTopologyFromMainPassBase::CopyTopologyFromMainPassBase;
+
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+
+    // TODO(b/428336749): remove gspmd specific logic when no longer needed.
+    MaybeConstructSdyMesh(module_op);
+
+    for (FuncOp func_op : module_op.getOps<FuncOp>()) {
+      auto main_topology_attr =
+          func_op->getAttrOfType<TopologyAttr>(kTopologyAttr);
+      if (!main_topology_attr) continue;
+
+      func_op.walk([main_topology_attr](CallOp call_op) {
+        FuncOp callee = GetCalleeFunc(call_op);
+        callee->setAttr(kTopologyAttr, main_topology_attr);
+        // Set the callee to private to mark that it's not an entry point
+        // function.
+        callee.setPrivate();
+      });
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/enforce_equisharding.cc b/shardy/dialect/mpmd/transforms/import/enforce_equisharding.cc
new file mode 100644
index 0000000..e462920
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/enforce_equisharding.cc
@@ -0,0 +1,65 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/import/sharding_constraints.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_ENFORCEEQUISHARDINGPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+// Enforces input-output equisharding constraints for MPMD functions by
+// introducing TransferOps when necessary.
+class EnforceEquishardingPass
+    : public impl::EnforceEquishardingPassBase<EnforceEquishardingPass> {
+  using EnforceEquishardingPassBase::EnforceEquishardingPassBase;
+
+  void runOnOperation() final {
+    func::FuncOp func_op = getOperation();
+    if (!IsMpmdFunction(func_op) || !IsEntryPointFunction(func_op)) {
+      return;
+    }
+    IRRewriter rewriter(func_op.getContext());
+    FunctionType func_type = func_op.getFunctionType();
+    auto func_ret = cast<func::ReturnOp>(func_op.front().getTerminator());
+
+    rewriter.setInsertionPoint(func_ret);
+    for (const InputOutputEquishardingConstraint& constraint : constraints) {
+      Type output_mesh_type = func_type.getResult(constraint.output_index);
+      Type input_mesh_type = func_type.getInput(constraint.input_index);
+      if (input_mesh_type != output_mesh_type) {
+        Value new_operand = rewriter.create<TransferOp>(
+            func_ret->getLoc(), input_mesh_type,
+            func_ret->getOperand(constraint.output_index));
+        func_ret->setOperand(constraint.output_index, new_operand);
+      }
+    }
+    UpdateFunctionType(func_op);
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/enforce_equisharding_test.cc b/shardy/dialect/mpmd/transforms/import/enforce_equisharding_test.cc
new file mode 100644
index 0000000..27ed70c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/enforce_equisharding_test.cc
@@ -0,0 +1,192 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <string>
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OwningOpRef.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Parser/Parser.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/register.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"
+#include "shardy/dialect/mpmd/transforms/import/sharding_constraints.h"
+#include <gmock/gmock.h>
+#include <gtest/gtest.h>
+
+namespace mlir::mpmd {
+namespace {
+
+using ::mlir::func::FuncOp;
+
+MATCHER(IsTransferOp, "") { return isa<TransferOp>(arg.getDefiningOp()); }
+
+void enforceEquishardingConstraints(
+    ModuleOp module,
+    SmallVector<InputOutputEquishardingConstraint> constraints) {
+  PassManager pm(module->getContext());
+  pm.enableVerifier();
+  pm.addNestedPass<FuncOp>(createEnforceEquishardingPass(
+      EnforceEquishardingPassOptions{constraints}));
+  SDY_CHECK(succeeded(pm.run(module)));
+}
+
+TEST(EnforceEquishardingConstraints, NoConstraints) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string kProgram = R"mlir(
+  func.func @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+      -> (!mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"y"}]>>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["y"=4]>>
+    >} {
+    %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+        -> !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+    func.return %0 : !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"y"}]>>
+  })mlir";
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp orig_fn = GetMainFunction(*module);
+  FunctionType orig_type = orig_fn.getFunctionType();
+
+  enforceEquishardingConstraints(*module, {});
+
+  EXPECT_EQ(orig_type, GetMainFunction(*module).getFunctionType());
+}
+
+TEST(EnforceEquishardingConstraints, AlreadySatisfied) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string kProgram = R"mlir(
+  func.func @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>,
+                  %arg1: !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"y"}]>>)
+      -> (!mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"y"}]>>,
+          !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["y"=4]>>
+    >} {
+    func.return %arg1, %arg0 : !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"y"}]>>,
+                               !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+  })mlir";
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp orig_fn = GetMainFunction(*module);
+  FunctionType orig_type = orig_fn.getFunctionType();
+
+  enforceEquishardingConstraints(*module,
+                                 {InputOutputEquishardingConstraint(0, 1),
+                                  InputOutputEquishardingConstraint(1, 0)});
+
+  EXPECT_EQ(orig_type, GetMainFunction(*module).getFunctionType());
+}
+
+TEST(EnforceEquishardingConstraints, InsertTransfer) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string kProgram = R"mlir(
+  func.func @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"x"}]>>,
+                  %arg1: !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>,
+                  %arg2: !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+      -> (!mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"x"}]>>,
+          !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>,
+          !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=4]>>,
+      <"m2": <["x"=4]>>
+    >} {
+    %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"x"}]>>)
+        -> !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+    %1 = mpmd.fragment<mesh="m2", origin=["f"]> (%0) (%arg3: tensor<32x256xf32>) {
+      mpmd.return %arg3 : tensor<32x256xf32>
+    } : (!mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+        -> !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+
+    %2 = mpmd.transfer %arg1 : (!mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>)
+        -> !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"x"}]>>
+    %3 = mpmd.fragment<mesh="m1", origin=["f"]> (%2) (%arg3: tensor<32x256xf32>) {
+      mpmd.return %arg3 : tensor<32x256xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"x"}]>>)
+        -> !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"x"}]>>
+
+    func.return %3, %1, %arg2 : !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{?}, {"x"}]>>,
+                                   !mpmd.mesh_tensor<"m2", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>,
+                                   !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+  })mlir";
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  enforceEquishardingConstraints(*module,
+                                 {InputOutputEquishardingConstraint(0, 1),
+                                  InputOutputEquishardingConstraint(1, 0)});
+
+  FuncOp new_fn = GetMainFunction(*module);
+  FunctionType new_type = new_fn.getFunctionType();
+  EXPECT_EQ(new_type.getInput(0), new_type.getResult(1));
+  EXPECT_EQ(new_type.getInput(1), new_type.getResult(0));
+  EXPECT_EQ(new_type.getResult(2), new_type.getResult(2));
+  Operation* terminator = new_fn.getBody().front().getTerminator();
+  EXPECT_THAT(terminator->getOperand(0), IsTransferOp());
+  EXPECT_THAT(terminator->getOperand(1), IsTransferOp());
+}
+
+TEST(EnforceEquishardingConstraints, FunctionIsNotEntrypoint) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+
+  const std::string kProgram = R"mlir(
+  func.func private @f(%arg0: !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>,
+                  %arg1: !mpmd.mesh_tensor<"m2", tensor<32x256xf32>>)
+  -> (
+    !mpmd.mesh_tensor<"m2", tensor<32x256xf32>>,
+    !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+  ) attributes {"topology"=#mpmd.topology<<"m1": <["x"=4]>>, <"m2": <["x"=4]>>>} {
+    func.return %arg1, %arg0 : !mpmd.mesh_tensor<"m2", tensor<32x256xf32>>,
+                               !mpmd.mesh_tensor<"m1", tensor<32x256xf32>, sharding=<@mesh, [{"x"}, {?}]>>
+  })mlir";
+
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp orig_fn = dyn_cast_or_null<FuncOp>(module->lookupSymbol("f"));
+  SDY_CHECK(orig_fn);
+  FunctionType orig_type = orig_fn.getFunctionType();
+
+  // Does not enforce constraints because @f is not an entry-point function.
+  enforceEquishardingConstraints(*module,
+                                 {InputOutputEquishardingConstraint(0, 0),
+                                  InputOutputEquishardingConstraint(1, 1)});
+
+  EXPECT_EQ(
+      orig_type,
+      dyn_cast_or_null<FuncOp>(module->lookupSymbol("f")).getFunctionType());
+}
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/import_pipeline.cc b/shardy/dialect/mpmd/transforms/import/import_pipeline.cc
new file mode 100644
index 0000000..eba9c35
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/import_pipeline.cc
@@ -0,0 +1,177 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <utility>
+
+#include "llvm/Support/CommandLine.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Pass/PassRegistry.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/Passes.h"
+#include "shardy/dialect/mpmd/transforms/common/merge_fragments.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"
+#include "shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_assignment_map.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"
+#include "stablehlo/transforms/Passes.h"
+#include "stablehlo/transforms/optimization/Passes.h"
+
+namespace mlir::mpmd {
+
+using ::mlir::func::FuncOp;
+
+void addImportPipeline(OpPassManager& pm, ImportOptions options) {
+  // Add a few passes to make the module ready for MPMD partitioning.
+  pm.addNestedPass<FuncOp>(stablehlo::createChloLegalizeToStablehloPass());
+
+  pm.addPass(createInlinerPass());
+  pm.addNestedPass<FuncOp>(createCSEPass());
+
+  // Canonicalization / Target Independent Optimization needed for two things:
+  // 1. Aggressive constant folding: required for later fragment merging.
+  // 2. Flatten concat ops: needed for mesh inference as it doesn't support
+  //    nested concat ops.
+  pm.addNestedPass<FuncOp>(createCanonicalizerPass());
+
+  pm.addPass(createCopyTopologyFromMainPass());
+
+  // Unroll mpmd.for loops as they aren't yet supported by mesh inference.
+  // TODO(jupvfranco): postpone unrolling until after SPMD propagation.
+  pm.addNestedPass<FuncOp>(createUnrollForLoopsPass());
+  // After unrolling, we may have slice(stack(x1, ..., xn), index) in the code
+  // caused by for loops with enumeration of inputs. If x1 ... xn are large
+  // tensors, this could cause OOMs. However, given that index is a constant,
+  // we should be able to canonicalize this pattern away.
+  pm.addNestedPass<FuncOp>(createCanonicalizerPass());
+
+  // Don't fold anything that increases file size (iota -> cst)
+  stablehlo::StablehloTargetIndependentOptimizationPassOptions
+      stablehlo_optimization_options;
+  stablehlo_optimization_options.foldOpElementLimit = 0;
+  pm.addNestedPass<FuncOp>(
+      stablehlo::createStablehloTargetIndependentOptimizationPass(
+          stablehlo_optimization_options));
+  // Make sure the mpmd.calls include a call_counter, even if originated from a
+  // for_loop.
+  // TODO(jupvfranco): Consider using the unroll counter instead of the call
+  // counter in the scheduler pass.
+  pm.addNestedPass<FuncOp>(createFromUnrollToCallCounterPass());
+
+  // Sink negligible ops into call_ops. This is not strictly necessary, however
+  // it is advantageous for two main reasons: 1) it reduces in workload in mesh
+  // inference; and 2) the sunken ops can be immediately merged into fragments.
+  pm.addPass(createSinkNegligibleOpsIntoCallOpPass());
+
+  // This pass may leave unused outputs in named computations. Thus, we apply
+  // it before we simplify named_computations in order to remove those outputs.
+  pm.addNestedPass<FuncOp>(createInsertNamelessCloneOfNeglibleOpsPass());
+
+  // Needs to be applied before MPMD import passes as those replace named
+  // computations with fragments that are assigned to meshes, which can cause
+  // tensors to be assigned to the wrong mesh if they are just passed through a
+  // named computation without being used in it (which the simplify pass will
+  // eliminate).
+  pm.addNestedPass<FuncOp>(createSimplifyNamedComputationsPass());
+
+  // Map main function inputs and outputs to meshes.
+  MapInputOutputToMeshPassOptions map_in_out_options;
+  map_in_out_options.inputAssignment =
+      std::move(options.inputIndexToMeshAssignment);
+  map_in_out_options.outputAssignment =
+      std::move(options.outputIndexToMeshAssignment);
+  pm.addPass(createMapInputOutputToMeshPass(std::move(map_in_out_options)));
+
+  // Inline any mpmd op nested in a named_computation, checking that its mesh
+  // assignment matches that of the parent. This is needed as we only map names
+  // at the top level of the program (i.e., all fragments and transfers are at
+  // the top level of the functions).
+  pm.addNestedPass<FuncOp>(createInlineNestedUserExposedOpsPass(
+      InlineNestedUserExposedOpsPassOptions{options.nameToMeshAssignment}));
+
+  // Validate that all named ops are only nested in mpmd functions.
+  pm.addNestedPass<FuncOp>(createValidateNamedOpsInMpmdFuncPass());
+
+  // Map named computations and named tensors to fragments, assigns/unassigns.
+  pm.addNestedPass<FuncOp>(
+      createMapNamedOpsToMpmdOpsPass(MapNamedOpsToMpmdOpsPassOptions{
+          std::move(options.nameToMeshAssignment)}));
+
+  // Introduce transfer ops from unassign/assign ops.
+  pm.addPass(createIntroduceTransfersPass());
+
+  // Erase unused block arguments from functions that are target of mpmd.calls.
+  // We need to do this before mesh inference, which doesn't handle arguments
+  // that are used by the return op very well.
+  pm.addPass(createEraseUnusedCalleeBlockArgumentsPass());
+
+  // Run infer mesh assignment passes.
+  addInferMeshPipeline(pm, options.inputOutputConstraints,
+                       std::move(options.inferMeshOptions));
+
+  if (!options.mergeAfterScheduling) {
+    AddMergeInferredFragmentsPasses(
+        pm, options.absorbInferredFragmentsOnEntryPointFunction,
+        options.cloneInferredFragments);
+  }
+
+  // Enforce the user-specified input/output equi-assignment constraints.
+  pm.addNestedPass<FuncOp>(
+      createEnforceEquishardingPass(EnforceEquishardingPassOptions{
+          std::move(options.inputOutputConstraints)}));
+
+  // Simplify all the fragments. We never introduce identity fragments in our
+  // passes and any identity fragment that may have been created by a user
+  // would have been simplified away with `simplify-named-computation-ops`.
+  // Thus, we don't apply canonicalization again.
+  pm.addNestedPass<FuncOp>(createFragmentDedupPass());
+  pm.addNestedPass<FuncOp>(createFragmentDcePass());
+}
+
+namespace {
+
+struct ImportPipelineOptions
+    : public PassPipelineOptions<ImportPipelineOptions> {
+  Option<UserAssignmentMapOption> nameToMeshAssignment{
+      *this, "name-to-mesh-assignment",
+      llvm::cl::desc(
+          "Mapping between names (of computations and tensors) and mesh "
+          "names, and optionally stage ids."),
+      llvm::cl::init(UserAssignmentMapOption())};
+
+  Option<bool> mergeAfterScheduling{
+      *this, "merge-after-scheduling",
+      llvm::cl::desc(
+          "Whether to merge inferred fragments only after scheduling."),
+      llvm::cl::init(false)};
+};
+
+}  // namespace
+
+void registerImportPipeline() {
+  PassPipelineRegistration<ImportPipelineOptions>(
+      "mpmd-import-pipeline",
+      "Run the standard set of passes to import an MPMD program with a fixed "
+      "mesh assignment map.",
+      [](OpPassManager& pm, const ImportPipelineOptions& pipelineOptions) {
+        ImportOptions options;
+        options.nameToMeshAssignment = pipelineOptions.nameToMeshAssignment;
+        options.mergeAfterScheduling = pipelineOptions.mergeAfterScheduling;
+        addImportPipeline(pm, std::move(options));
+      });
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.cc b/shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.cc
new file mode 100644
index 0000000..85143a6
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.cc
@@ -0,0 +1,2726 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.h"
+
+#include <cstdint>
+#include <optional>
+#include <string>
+#include <string_view>
+#include <utility>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetOperations.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Block.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Dialect.h"
+#include "mlir/IR/IRMapping.h"
+#include "mlir/IR/Iterators.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Interfaces/SideEffectInterfaces.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Pass/PassRegistry.h"
+#include "mlir/Support/DebugStringHelper.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/RegionUtils.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_origins.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_utils.h"
+#include "shardy/dialect/mpmd/transforms/import/meshes_with_origins.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/import/sharding_constraints.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_INFERMESHPOPULATEUSESETPASS
+#define GEN_PASS_DEF_INFERMESHPOPULATESRCSETPASS
+#define GEN_PASS_DEF_INFERMESHASSIGNUSINGINPUTOUTPUTCONSTRAINTSPASS
+#define GEN_PASS_DEF_INFERMESHASSIGNMESHFORFUNCLEAVESPASS
+#define GEN_PASS_DEF_INFERMESHCONVERTREDUCEOPSPASS
+#define GEN_PASS_DEF_INFERMESHREWRITEUSINGANALYSISPASS
+#define GEN_PASS_DEF_INFERMESHFINALIZEPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+using ::llvm::DenseMap;
+using ::mlir::func::FuncOp;
+
+// The maximum number of iterations for use-set propagation of call-op chains.
+//
+// This is to prevent infinite loops in case of bugs. Arbitrarily chosen.
+constexpr int kMaxCallChainUseSetIterations = 10;
+
+// Returns success if either the mesh names of `assign_op` and `unassign_op`
+// match and `should_mesh_names_match` is true, or they don't match and
+// `should_mesh_names_match` is false.
+//
+// It's enough to check whether the mesh names match, since both assign and
+// unassign require their mesh tensor to be fully replicated.
+LogicalResult MatchIntraMeshAssignOfUnassign(AssignOp assign_op,
+                                             UnassignOp unassign_op,
+                                             PatternRewriter& rewriter,
+                                             bool should_mesh_names_match) {
+  StringRef unassign_mesh_name =
+      unassign_op.getTensor().getType().getMeshName();
+  StringRef assign_mesh_name = assign_op.getResult().getType().getMeshName();
+  bool mesh_names_match = unassign_mesh_name == assign_mesh_name;
+  if (mesh_names_match != should_mesh_names_match) {
+    return rewriter.notifyMatchFailure(assign_op, [&](Diagnostic& diag) {
+      diag << "Expected the mesh name of UnassignOp to "
+           << (should_mesh_names_match ? "" : "not ")
+           << "match that of the AssignOp: \"" << unassign_mesh_name
+           << "\" != \"" << assign_mesh_name << "\"";
+    });
+  }
+
+  return success();
+}
+
+// Verifies that all of the following constraints hold:
+//
+// - All the inputs and outputs of `func_op` are mesh tensors.
+// - All non-mpmd ops are nested within fragments.
+// - There are no assign or unassign ops.
+//
+// Note that we call emitError on the op that breaks the constraint, which will
+// be useful for debugging.
+//
+// The errors here should be caught earlier by validation, but this acts as a
+// sanity check in case something slips through.
+LogicalResult VerifyMeshAssignment(FuncOp func_op) {
+  FunctionType func_type = func_op.getFunctionType();
+
+  bool has_error = false;
+  for (auto [index, input_type] : llvm::enumerate(func_type.getInputs())) {
+    if (!isa<MeshTensorType>(input_type)) {
+      func_op.emitError("function input ")
+          << index << " is not a MeshTensorType, but a " << input_type;
+      has_error = true;
+    }
+  }
+  for (auto [index, output_type] : llvm::enumerate(func_type.getResults())) {
+    if (!isa<MeshTensorType>(output_type)) {
+      func_op.emitError("function output ")
+          << index << " is not a MeshTensorType, but a " << output_type;
+      has_error = true;
+    }
+  }
+
+  func_op.walk([&has_error](Operation* op) {
+    if (auto fragment = dyn_cast<FragmentOp>(op)) {
+      // Skip fragment ops and their regions for efficiency, as they are already
+      // assigned.
+      return WalkResult::skip();
+    }
+    if (isa<AssignOp, UnassignOp, BroadcastOp>(op)) {
+      has_error = true;
+      op->emitError(
+          "assigns, unassigns or broadcasts are not allowed after mesh "
+          "inference.");
+    }
+    if (IsMeshlessOp(op)) {
+      has_error = true;
+      op->emitError("no more meshless ops are expected at this point.");
+    }
+
+    if (ClearUseSetAndSrcSet(op)) {
+      SDY_LOG(WARNING) << "Use set or src set still present on op "
+                       << std::string_view(op->getName().getStringRef())
+                       << " after mesh inference. This shouldn't happen.";
+    }
+
+    return WalkResult::advance();
+  });
+
+  return failure(has_error);
+}
+
+// Eliminates no-op assign of unassign when the mesh tensors of both are the
+// same.
+//
+// In symbols:
+//   assign (unassign %v m) m ~> %v
+//
+// Note that the unassign can have additional users, in which case it won't be
+// erased following this rewrite and will trigger other patterns.
+class AssignOfUnassignSameMeshPattern final
+    : public OpRewritePattern<AssignOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(AssignOp op,
+                                PatternRewriter& rewriter) const override {
+    auto unassign_op = op.getTensor().getDefiningOp<UnassignOp>();
+    if (!unassign_op) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "Expected the operand of the AssignOp to be the result of an "
+                "UnassignOp";
+      });
+    }
+
+    if (failed(
+            MatchIntraMeshAssignOfUnassign(op, unassign_op, rewriter,
+                                           /*should_mesh_names_match=*/true))) {
+      return failure();
+    }
+
+    rewriter.replaceOp(op, unassign_op.getTensor());
+
+    return success();
+  }
+};
+
+// Replaces an assign of unassign between different meshes, where the operand of
+// the unassign is also used by a transfer with the same return type as the
+// assign and in the same or an enclosing block, with the result of that
+// transfer.
+//
+// If the transfer is after the assign in the same block, or after the ancestor
+// of the assign in the transfer's block, we move the transfer before it, so
+// that all users of the assign are after the transfer.
+//
+// In symbols:
+//
+//   Y = unassign<M1>(X)
+//   Z = assign<M2>(Y)
+//   op(Z)
+//   W = transfer<M1->M2>(X)
+//   other_user(Y)
+//   ~>
+//   Y = unassign<M1>(X)
+//   W = transfer<M1->M2>(X)
+//   op(W)
+//   other_user(Y)
+//
+// Note that the unassign can have additional users, in which case it won't be
+// erased following this rewrite and will trigger other patterns.
+class DedupAssignOfUnassignAndTransferPattern final
+    : public OpRewritePattern<AssignOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(AssignOp op,
+                                PatternRewriter& rewriter) const override {
+    auto unassign_op = op.getTensor().getDefiningOp<UnassignOp>();
+    if (!unassign_op) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "Expected the operand of the AssignOp the result of an "
+                "UnassignOp";
+      });
+    }
+
+    if (failed(MatchIntraMeshAssignOfUnassign(
+            op, unassign_op, rewriter,
+            /*should_mesh_names_match=*/false))) {
+      return failure();
+    }
+
+    auto users = unassign_op.getTensor().getUsers();
+    auto transfer_op_it = llvm::find_if(users, [&](Operation* user) {
+      auto transfer_op = dyn_cast<TransferOp>(user);
+      return transfer_op && transfer_op.getType() == op.getType() &&
+             HasAncestorInBlock(transfer_op->getBlock(), op);
+    });
+
+    if (transfer_op_it == users.end()) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "Expected the operand of the UnassignOp to be used by a "
+             << "TransferOp with the same return type as the AssignOp and in "
+             << "the same or an enclosing block";
+      });
+    }
+
+    auto transfer_op = cast<TransferOp>(*transfer_op_it);
+
+    // If the transfer is after the assign, we need to move the transfer before
+    // it, so that all uses of the assign are after the transfer.
+    Operation* ancestor_in_block =
+        GetAncestorInBlock(transfer_op->getBlock(), op);
+    if (ancestor_in_block->isBeforeInBlock(transfer_op)) {
+      transfer_op->moveBefore(ancestor_in_block);
+    }
+
+    rewriter.replaceOp(op, transfer_op.getResult());
+
+    return success();
+  }
+};
+
+// Replaces an assign of unassign between different meshes, where the operand of
+// the unassign is a block argument of a func op, with an inter-mesh
+// transfer.
+//
+// In symbols:
+//
+// func.func f(arg0: mesh_tensor<m1>) {
+//   x = unassign arg0
+//   y = assign x -> m2
+//   ...
+// }
+//
+// ~~>
+//
+// func.func f(arg0: mesh_tensor<m1>) {
+//   y = transfer arg0   m1 -> m2
+//   ...
+// }
+//
+// Note that the unassign can have additional users, in which case it won't be
+// erased following this rewrite and will trigger other patterns.
+class AssignOfUnassignFuncArgPattern final : public OpRewritePattern<AssignOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(AssignOp op,
+                                PatternRewriter& rewriter) const override {
+    auto unassign_op = op.getTensor().getDefiningOp<UnassignOp>();
+    if (!unassign_op) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "Expected the operand of the AssignOp to be the result of an "
+                "UnassignOp";
+      });
+    }
+
+    auto block_arg = dyn_cast<BlockArgument>(unassign_op.getTensor());
+    if (!block_arg || !isa<FuncOp>(block_arg.getOwner()->getParentOp()) ||
+        !IsEntryPointFunction(unassign_op->getParentOfType<FuncOp>())) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "Expected the operand of the unassign_op to be an argument of "
+             << "the entry point FuncOp";
+      });
+    }
+
+    if (failed(MatchIntraMeshAssignOfUnassign(
+            op, unassign_op, rewriter,
+            /*should_mesh_names_match=*/false))) {
+      return failure();
+    }
+
+    rewriter.replaceOpWithNewOp<TransferOp>(op, op.getType(),
+                                            unassign_op.getTensor());
+
+    return success();
+  }
+};
+
+// This pattern replaces assign(unassign(%v, m1), m2) ~~> transfer(%v, m1->m2).
+class AssignOfUnassignPattern final : public OpRewritePattern<AssignOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(AssignOp op,
+                                PatternRewriter& rewriter) const override {
+    auto unassign_op = op.getTensor().getDefiningOp<UnassignOp>();
+    if (!unassign_op) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "Expected the operand of the AssignOp to be the result of an "
+                "UnassignOp";
+      });
+    }
+
+    StringRef unassign_mesh_name =
+        unassign_op.getTensor().getType().getMeshName();
+    StringRef assign_mesh_name = op.getType().getMeshName();
+    if (unassign_mesh_name == assign_mesh_name) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "Expected the mesh name of UnassignOp to be different from the "
+                "AssignOp but got \""
+             << unassign_mesh_name << "\" == \"" << assign_mesh_name << "\"";
+      });
+    }
+
+    auto transfer = rewriter.replaceOpWithNewOp<TransferOp>(
+        op, op.getType(), unassign_op.getTensor());
+    // TODO: b/329221688 - Improve these logs to make it easier to debug.
+    SDY_VLOG(2) << "Created cross-mesh transfer "
+                << PrintOperationForLog(transfer);
+    return success();
+  }
+};
+
+FragmentOp CreateReduceFragment(ArrayRef<Value> mesh_tensors,
+                                StringRef mesh_name,
+                                ReductionType reduction_type,
+                                RewriterBase& rewriter) {
+  return FragmentOp::createMeshFragmentWithGlobalBody(
+      mesh_tensors.front().getLoc(), /*user_origin=*/{}, mesh_name,
+      mesh_tensors, mesh_tensors.front().getType(), rewriter,
+      [reduction_type](ArrayRef<Value> args, OpBuilder& block_builder) {
+        Value accumulator = args.front();
+        for (Value val : llvm::drop_begin(args)) {
+          accumulator =
+              CreateStablehloReduceOp(reduction_type, {accumulator, val},
+                                      val.getLoc(), block_builder)
+                  ->getResult(0);
+        }
+        return SmallVector<Value>({accumulator});
+      });
+}
+
+// This pattern lowers mpmd.reduce to reductions and transfers.
+// In symbols:
+//
+// %v = mpmd.reduce<R>(%v0,... %vn) use_set=m3
+//
+// ~~>
+//
+// %v_m1 = R(%v0, R(...)) # on mesh1
+// %v_m2 = R(...) # on mesh2
+// %v = R(transfer(%v_m1), transfer(%v_m2)) # on mesh3
+class LowerMpmdReducePattern final : public OpRewritePattern<ReduceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(ReduceOp mpmd_reduce,
+                                PatternRewriter& rewriter) const override {
+    rewriter.setInsertionPointAfter(mpmd_reduce);
+
+    // Group the mesh tensor operands by mesh.
+    DenseMap<StringRef, SmallVector<Value>> mesh_name_to_mesh_tensor_operands;
+    for (auto operand : mpmd_reduce.getTensors()) {
+      auto unassign = operand.getDefiningOp<UnassignOp>();
+      SDY_CHECK(unassign);
+      mesh_name_to_mesh_tensor_operands
+          [unassign.getTensor().getType().getMeshName()]
+              .push_back(unassign.getTensor());
+    }
+
+    // Create local reductions in each mesh, and keep track of the results.
+    SmallVector<Value> local_reductions;
+    local_reductions.reserve(mesh_name_to_mesh_tensor_operands.size());
+    for (const auto& [mesh_name, mesh_tensors] :
+         mesh_name_to_mesh_tensor_operands) {
+      if (mesh_tensors.size() == 1) {
+        // Nothing to reduce.
+        local_reductions.push_back(mesh_tensors.front());
+      } else {
+        local_reductions.push_back(
+            CreateReduceFragment(mesh_tensors, mesh_name,
+                                 mpmd_reduce.getReductionType(), rewriter)
+                .getResult(0));
+      }
+    }
+
+    // Group the assign users of the mpmd.reduce op by mesh.
+    DenseMap<StringRef, SmallVector<AssignOp>> mesh_name_to_assign_users;
+    for (Operation* user : mpmd_reduce->getUsers()) {
+      auto assign = dyn_cast<AssignOp>(user);
+      SDY_CHECK(assign);
+      mesh_name_to_assign_users[assign.getType().getMeshName()].push_back(
+          assign);
+    }
+
+    // For each destination mesh, transfer all the local reduce results to that
+    // mesh, and do another local reduce on that destination mesh.
+    for (StringRef user_mesh : GetUseMeshes(mpmd_reduce)) {
+      SmallVector<AssignOp> assign_users =
+          mesh_name_to_assign_users.lookup(user_mesh);
+      SDY_CHECK(!assign_users.empty())
+          << "Each mesh in the use_set should "
+             "correspond to at least one assign user.";
+      MeshTensorType user_type = assign_users.front().getType();
+      SmallVector<Value> transferred_intermediates;
+      for (Value reduced_val : local_reductions) {
+        if (reduced_val.getType() == user_type) {
+          transferred_intermediates.push_back(reduced_val);
+        } else {
+          transferred_intermediates.push_back(rewriter.create<TransferOp>(
+              reduced_val.getLoc(), user_type, reduced_val));
+        }
+      }
+
+      FragmentOp target_mesh_reduce =
+          CreateReduceFragment(transferred_intermediates, user_mesh,
+                               mpmd_reduce.getReductionType(), rewriter);
+      for (AssignOp user : assign_users) {
+        rewriter.replaceAllUsesWith(user, target_mesh_reduce.getResult(0));
+      }
+    }
+
+    return success();
+  }
+};
+
+WalkResult PopulateUseSet(Operation* op, OpBuilder& builder);
+
+// Populates the use_set of the callee func of the given call_op.
+// Returns true if the use_set of the callee func has changed.
+bool PopulateUseSetForCalleeFunc(CallOp call_op, FuncOp callee_func,
+                                 OpBuilder& builder) {
+  bool has_changed = false;
+  // Propagate through to the callee result.
+  for (OpResult call_result : call_op->getResults()) {
+    MeshesWithOrigins use_set =
+        GetResUseSet(callee_func, call_result.getResultNumber());
+
+    int original_num_uses = use_set.size();
+    UpdateTransitiveUses(call_result, use_set);
+    if (use_set.size() != original_num_uses) {
+      has_changed = true;
+      SetResUseSet(callee_func, call_result.getResultNumber(), use_set,
+                   builder);
+    }
+  }
+
+  // We only need to populate the use_set of the callee func again if the
+  // use_set of the results have changed, or if we've not populated it yet.
+  // Otherwise, the use_set of the body will be unchanged.
+  if (has_changed || !CallOpHasUseSetPopulated(call_op)) {
+    // Populate the func's use_set.
+    callee_func.walk<WalkOrder::PostOrder, ReverseIterator>(
+        [&builder](Operation* op) { return PopulateUseSet(op, builder); });
+    return true;
+  }
+  return false;
+}
+
+// This populates the use_set of a CallOp and its callee func. We populate the
+// CallOp as if it were a region op, since the behaviour of mesh inference
+// should be the same whether or not the CallOp is inlined. Treating it as a
+// region op gives similar behaviour as inlining: we populate the
+// use_set of the callee func whenever we see the CallOp. The main difference is
+// that the ops in the callee func are repeatedly processed (instead of
+// processing different ops). This can cause the inference to give problematic
+// results and we validate this in a separate pass. E.g.
+//
+// x1 = ...
+// c1 = call @f(x1)
+// assign c1 -> m1
+//
+// x2 = ...
+// c2 = call @f(x2)
+// assign c2 -> m2
+//
+// func f (%arg0) {
+//   %add = stablehlo.add %arg0, %arg0
+//   return %add
+// }
+//
+// So we need to propagate through each time we see the call to get the correct
+// use_set {m1,m2} for %add and also for %x1. But note that %x2 will have
+// use_set {m2} only. This means the use_set for %x2 is actually invalid since
+// it is used in an add which has use_set {m1,m2}. We allow this here but will
+// raise an error in a later validation pass, as we don't allow edges out of a
+// CallOp to have multiple entries in the use_set – in such a case, there's no
+// sensible mesh assignment for the func arg or return value: we don't allow
+// setting the signature of f to accept tensors from both m1 and m2.
+//
+// If the call op is in a call chain, we keep propagating the use_set
+// until a fixed point, as that will be required for the mesh assignments to be
+// valid. Note that this is a little bit like the ForOp.
+void PopulateUseSetForCallOp(CallOp call_op, OpBuilder& builder) {
+  FuncOp callee_func = GetCalleeFunc(call_op);
+
+  bool has_changed = PopulateUseSetForCalleeFunc(call_op, callee_func, builder);
+
+  // If the call op is in a call chain, we need to keep propagating the use_set
+  // until a fixed point.
+  if (IsCallOpInCallChain(call_op)) {
+    for (int i = 0; i < kMaxCallChainUseSetIterations && has_changed; ++i) {
+      has_changed = PopulateUseSetForCalleeFunc(call_op, callee_func, builder);
+    }
+  }
+}
+
+// This populates the use_set of a ForOp.
+void PopulateUseSetForForOp(ForOp for_op, OpBuilder& builder) {
+  for (BlockArgument arg : for_op.getRegion().getArguments()) {
+    MeshesWithOrigins use_set = GetUseSet(for_op, arg.getArgNumber());
+    UpdateTransitiveUses(arg, use_set);
+    SetUseSet(for_op, arg.getArgNumber(), use_set, builder);
+  }
+}
+
+// Populates the use set of the defining op of op_result inside the call_op
+// callee func.
+void UpdateUseSetForCallOpUsingOpResult(CallOp call_op, OpResult op_result,
+                                        OpBuilder& builder) {
+  FuncOp callee_func = GetCalleeFunc(call_op);
+  MeshesWithOrigins use_set =
+      GetResUseSet(callee_func, op_result.getResultNumber());
+  UpdateTransitiveUses(op_result, use_set);
+  SetResUseSet(callee_func, op_result.getResultNumber(), use_set, builder);
+}
+
+// This propagates the use_set from the return op through to the body of the
+// matching ForOp.
+void PopulateUseSetForForOpTerminator(ReturnOp return_op, OpBuilder& builder) {
+  // TODO(petebu): Attach use-sets to ForOp res attr to avoid multi-step
+  // propagation.
+  if (auto for_op = dyn_cast<ForOp>(return_op->getParentOp())) {
+    for (auto [for_result, for_return_operand] :
+         llvm::zip(for_op->getResults(), return_op->getOperands())) {
+      if (Operation* defining_op = for_return_operand.getDefiningOp()) {
+        if (auto call_op = dyn_cast<CallOp>(defining_op)) {
+          UpdateUseSetForCallOpUsingOpResult(call_op, for_result, builder);
+        } else {
+          MeshesWithOrigins use_set = GetUseSet(defining_op);
+          UpdateTransitiveUses(for_result, use_set);
+          SetUseSet(defining_op, use_set, builder);
+        }
+      }
+    }
+  }
+}
+
+// This populates the use_set of an op or func arg. If it is an AssignOp, it
+// initializes it. Otherwise, if it is an op needing assignment, it generates
+// the use_set from its users. Func args are a special case where having the
+// use_set helps for further analysis.
+//
+// Pre-condition: all users of `op` must have their use_sets populated.
+//
+// Hence, it must be used in conjunction with a post-order traversal of the
+// MLIR graph, so that all users are processed before the current op.
+WalkResult PopulateUseSet(Operation* op, OpBuilder& builder) {
+  if (auto assign_op = dyn_cast<AssignOp>(op)) {
+    SetUseSet(assign_op, MeshesWithOrigins(assign_op.getMeshWithOrigin()),
+              builder);
+  } else if (auto func = dyn_cast<FuncOp>(op)) {
+    for (BlockArgument arg : func.getArguments()) {
+      MeshesWithOrigins use_set = GetArgUseSet(func, arg.getArgNumber());
+      UpdateTransitiveUses(arg, use_set);
+      SetArgUseSet(func, arg.getArgNumber(), use_set, builder);
+    }
+  } else if (auto fragment = dyn_cast<FragmentOp>(op)) {
+    // Skip fragment ops and their regions for efficiency,
+    // as they are already assigned.
+    return WalkResult::skip();
+  } else if (auto call_op = dyn_cast<CallOp>(op)) {
+    PopulateUseSetForCallOp(call_op, builder);
+  } else if (auto for_op = dyn_cast<ForOp>(op)) {
+    PopulateUseSetForForOp(for_op, builder);
+  } else if (auto return_op = dyn_cast<ReturnOp>(op)) {
+    PopulateUseSetForForOpTerminator(return_op, builder);
+  } else if (IsMeshlessOp(op) || IsTerminalNodeInAnalysis(op) ||
+             isa<UnassignOp>(op)) {
+    MeshesWithOrigins use_set = GetUseSet(op);
+    UpdateTransitiveUses(op, use_set);
+    SetUseSet(op, use_set, builder);
+  }
+
+  return WalkResult::advance();
+}
+
+class InferMeshPopulateUseSetPass
+    : public impl::InferMeshPopulateUseSetPassBase<
+          InferMeshPopulateUseSetPass> {
+  using InferMeshPopulateUseSetPassBase::InferMeshPopulateUseSetPassBase;
+
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+    OpBuilder builder(&getContext());
+
+    for (FuncOp func_op : GetMpmdFunctions(module_op)) {
+      if (IsEntryPointFunction(func_op)) {
+        // Do a post-order traversal.
+        func_op.walk<WalkOrder::PostOrder, ReverseIterator>(
+            [&builder](Operation* op) { return PopulateUseSet(op, builder); });
+      }
+    }
+  }
+};
+
+// If `op` is the concat of a concat-reduce pair, then check that:
+// - the concat and reduction are on the same dim,
+// - the concat is on a dim of size 1 for all operands, and
+// - the concat is only used by the reduce.
+//
+// Returns the matched ReduceOp.
+stablehlo::ReduceOp MatchCrossMeshConcatReduce(Operation* op) {
+  if (!llvm::all_equal(op->getOperandTypes())) {
+    return nullptr;
+  }
+  auto concat = dyn_cast<stablehlo::ConcatenateOp>(op);
+  stablehlo::ReduceOp reduce_user =
+      op->hasOneUse() ? dyn_cast<stablehlo::ReduceOp>(*op->getUsers().begin())
+                      : nullptr;
+
+  if (concat && reduce_user && reduce_user.getInputs().size() == 1 &&
+      reduce_user.getDimensions().size() == 1) {
+    auto concat_operand_type =
+        dyn_cast<RankedTensorType>(op->getOperandTypes().front());
+    int64_t reduce_dim = reduce_user.getDimensions().front();
+    if (concat_operand_type && reduce_dim == concat.getDimension() &&
+        concat_operand_type.getDimSize(reduce_dim) == 1) {
+      return reduce_user;
+    }
+  }
+  return nullptr;
+}
+
+class InferMeshPopulateSrcSetPass
+    : public impl::InferMeshPopulateSrcSetPassBase<
+          InferMeshPopulateSrcSetPass> {
+  using InferMeshPopulateSrcSetPassBase::InferMeshPopulateSrcSetPassBase;
+
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+    OpBuilder builder(&getContext());
+
+    for (FuncOp func_op : GetMpmdFunctions(module_op)) {
+      if (IsEntryPointFunction(func_op)) {
+        func_op.walk<WalkOrder::PreOrder>(
+            [&](Operation* op) { return PopulateSrcSet(op, builder); });
+      }
+    }
+  }
+
+ private:
+  // Copies the use_set to the src_set, updating the origin to inferred_in,
+  // rather than copying the old one (e.g. "layer0"). We don't want to copy over
+  // the origins for the use_set, we want new origins to indicate that the
+  // meshes are inferred.
+  void AddUseSetToSrcSet(MLIRContext* context, MeshesWithOrigins& src_set,
+                         const MeshesWithOrigins& use_set) {
+    for (StringRef mesh_name : use_set.MeshNamesOrEmpty()) {
+      src_set.insert(MeshWithOriginsAttr::get(
+          context, mesh_name, OriginAttr::get(context, kInferredInputOrigin)));
+    }
+  }
+
+  // Initializes the src_set to the origin mesh of the UnassignOp and the
+  // destination meshes of TransferOp users of the UnassignOp operand.
+  //
+  // E.g. if we have
+  // x = fragment "m1" {...}
+  // y = unassign x: mesh_tensor<m1, ...>
+  // z = transfer x "m2" {...}
+  //
+  // Then y has src-set {"m1", "m2"}.
+  // And these are the only meshes that y can exist on:
+  //    - "m1" is ok because we can replace uses of "y" with "x"
+  //    - "m2" is ok because we can replace uses of "y" with "z"
+  //    - any other mesh is invalid: say that y is assigned to another mesh
+  //    "m3",
+  //      and x has no other transfer users, then a new transfer to m3 needs to
+  //      be created. But this is not allowed because we do not allow creation
+  //      of transfers (see src_set definition above).
+  //
+  // Note that the transfers must be direct users of x, i.e.
+  // `z = transfer x "m"`. Indirect transfers of x do not work because uses of x
+  // are either TransferOps, UnassignOps, or other ops. TransferOps and
+  // UnassignOps are discussed above. For ops acting on x (e.g. a FragmentOp),
+  // the result will be a different tensor (assuming it's non-trivial) and hence
+  // we cannot dedup into that op (without introducing logic around the inverse
+  // of ops).
+  //
+  // Note that x is a mesh tensor, so its users also cannot be an AssignOp nor
+  // can it be a meshless op. It could also be a FragmentOp but that is
+  // addressed above.
+  //
+  // We register the UnassignOp mesh first, as we prefer that mesh.
+  // E.g. in the following program, it is most natural to assign z to mesh m1.
+  //
+  // x = frag m1 {...}
+  // u = unassign(x)
+  // t = transfer(x) m1 -> m2
+  // y = frag(t) m2 {...}
+  // z = u + u
+  void InitializeSrcSet(UnassignOp op, OpBuilder& builder) {
+    MeshesWithOrigins src_set;
+    src_set.insert(op.getMeshWithOrigin());
+    for (Operation* user : op.getTensor().getUsers()) {
+      if (auto transfer = dyn_cast<TransferOp>(user)) {
+        src_set.insert(TransferMeshWithOrigin(transfer));
+      }
+    }
+
+    // When the unassign is on an entrypoint func arg, then we treat it like a
+    // func arg: i.e. we add all its user meshes (the use_set) to the src_set
+    // and allow transfers to be created on it.
+    if (isa<BlockArgument>(op.getTensor()) &&
+        IsEntryPointFunction(op->getParentOfType<FuncOp>())) {
+      MeshesWithOrigins use_set;
+      UpdateTransitiveUses(op.getResult(), use_set);
+      AddUseSetToSrcSet(op->getContext(), src_set, use_set);
+    }
+
+    SetSrcSet(op, src_set, builder);
+  }
+
+  // Initializes the src_set of a func arg to its use_set. This initialization
+  // restricts the src_set to a subset of all meshes. It leaves args with empty
+  // use_set untouched. We only want to restrict the mesh assignments of args
+  // that have explicit uses in a mesh.
+  //
+  // Pre-condition: use_set is populated.
+  //
+  // If func arg is used in mesh m1, then it will exist in mesh m1 and so the
+  // func arg does exist there and is part of the src_set. This is special logic
+  // for the func args, because we allow introduction of transfers on func args
+  // according to the meshes they are used in.
+  void InitializeSrcSet(FuncOp func, OpBuilder& builder) {
+    for (BlockArgument arg : func.getArguments()) {
+      // use_sets aren't populated for block args, so we compute it manually.
+      MeshesWithOrigins use_set = GetArgUseSet(func, arg.getArgNumber());
+      if (!use_set.empty()) {
+        MeshesWithOrigins src_set;
+        AddUseSetToSrcSet(func->getContext(), src_set, use_set);
+        SetSrcSet(func, arg.getArgNumber(), src_set, builder);
+      }
+    }
+  }
+
+  // Infers reduce ops and propagates src_set as a special case: the src_set of
+  // a reduce_op is the union of its operands, rather than the intersection.
+  //
+  // The inferring algorithm is split in two phases: annotating, and rewriting.
+  // This is to preserve our analysis-only phase of mesh inference for easier
+  // debugging.
+  //
+  // We infer a reduce op if:
+  // - the intersection of operand src_sets is empty OR one of the operands is a
+  // reduce.
+  // - the op is a standard element-wise reduction: e.g. add, mul, etc., or is a
+  // concat-reduce pair. See `ParseCrossMeshConcatReduce` for details.
+  // - all the operands have non-empty src_set.
+  //
+  // The src_set of the reduce op is the union of its operand src_sets, if the
+  // operand src_set is present (i.e. we ignore operands that could exist on any
+  // mesh). Such operands with missing src_sets are usually a special case which
+  // should not affect the src_set of the reduce.
+  //
+  // Pre-condition: `op` already has its src_set populated in the usual way.
+  //
+  // TODO: b/343174113 - support custom reductions.
+  // TODO: b/340565987 - consider allowing src_set of reductions to be the whole
+  // set, and tracking the srcs as a new annotation `reduce_set` if need be.
+  void InferReduceAndPropagateSrcSet(Operation* op, OpBuilder& builder) {
+    stablehlo::ReduceOp cross_mesh_reduce = MatchCrossMeshConcatReduce(op);
+    std::optional<ReductionType> reduction_type =
+        cross_mesh_reduce
+            ? ComputeReductionType(cross_mesh_reduce.getBody().front())
+            : GetReductionOpType(op);
+    if (!reduction_type) {
+      return;
+    }
+    auto reduction_attr = ReductionAttr::get(op->getContext(), *reduction_type);
+
+    MeshesWithOrigins all_srcs;
+    bool some_operand_is_reduce = false;
+    for (OpOperand& operand : op->getOpOperands()) {
+      MeshesWithOrigins operand_src_set = GetSrcSet(operand);
+      if (operand_src_set.empty()) {
+        return;
+      }
+
+      all_srcs.Union(operand_src_set);
+
+      if (Operation* operand_op = operand.get().getDefiningOp();
+          operand_op &&
+          operand_op->getAttr(kMpmdReduceAnnotation) == reduction_attr) {
+        some_operand_is_reduce = true;
+      }
+    }
+
+    if (GetSrcSet(op).empty() || some_operand_is_reduce) {
+      op->setAttr(kMpmdReduceAnnotation, reduction_attr);
+      if (cross_mesh_reduce) {
+        cross_mesh_reduce->setAttr(kMpmdReduceAnnotation, reduction_attr);
+      }
+      SetSrcSet(op, all_srcs, builder);
+    }
+  }
+
+  // Propagates src_sets forward from operands to the op itself, taking the
+  // intersection of the operand src_sets.
+  //
+  // Pre-condition: all operands of `op` must have their src_sets populated.
+  //
+  // This is indeed the src_set of the op, since the op and its operands (and
+  // results) must live on the same mesh.
+  //
+  // E.g. if we have
+  // x = op(w, s)
+  //
+  // where w has src_set {m1, m2} and s has src_set {m2, m3} then x has src_set
+  // {m2}. This is because `w,s,x` and the op must live on the same mesh
+  // (because we don't allow transfers to be created on intermediate values) and
+  // the only mesh they can all live on is m2.
+  //
+  // We prove by contradiction that x cannot live on m1. If x were to live on
+  // m1, then the op would live on m1 and so s (or a copy of s) must live on m1.
+  // But that's not possible since m1 not in src_set(s), so x cannot live on m1.
+  // And similarly, x cannot live on m3. So x has src_set {m2}.
+  void PropagateSrcSet(Operation* op, OpBuilder& builder) {
+    MeshesWithOrigins src_set = GetSrcSet(op);
+    for (OpOperand& operand : op->getOpOperands()) {
+      src_set.Intersect(GetSrcSet(operand));
+    }
+
+    if (src_set) {
+      SetSrcSet(op, src_set, builder);
+
+      InferReduceAndPropagateSrcSet(op, builder);
+    }
+  }
+
+  // This populates the src_set of a CallOp and its callee func. We populate the
+  // CallOp as if it were a region op, since the behaviour of mesh inference
+  // should be the same whether or not the CallOp is inlined. Treating it as a
+  // region op gives similar behaviour as inlining: we populate the
+  // src_set of the callee func whenever we see the CallOp. The main difference
+  // is that the ops in the callee func are repeatedly processed (instead of
+  // processing different ops). This can cause the inference to give problematic
+  // results and we validate this in a separate pass. See the tests and
+  // the docs of `PopulateUseSetForCallOp` for an example of this.
+  void PopulateSrcSetForCallOp(CallOp call_op, OpBuilder& builder) {
+    FuncOp callee_func = GetCalleeFunc(call_op);
+    // Propagate through to the callee func body.
+    for (OpOperand& call_operand : call_op->getOpOperands()) {
+      MeshesWithOrigins src_set = GetSrcSet(call_operand);
+      src_set.Intersect(
+          GetSrcSet(callee_func, call_operand.getOperandNumber()));
+      if (src_set) {
+        SetSrcSet(callee_func, call_operand.getOperandNumber(), src_set,
+                  builder);
+      }
+    }
+
+    // Populate the func's src_set.
+    // Walk only the body, not the func itself. We don't want to set the func
+    // args' src_set with the use_set, since the src_set should come from
+    // call operands.
+    callee_func.getBody().walk(
+        [&](Operation* op) { return PopulateSrcSet(op, builder); });
+  }
+
+  // This populates the src_set of a ForOp.
+  void PopulateSrcSetForForOp(ForOp for_op, OpBuilder& builder) {
+    for (OpOperand& for_operand : for_op.getOperation()->getOpOperands()) {
+      MeshesWithOrigins src_set = GetSrcSet(for_operand);
+      if (src_set) {
+        SetSrcSet(for_op, for_operand.getOperandNumber(), src_set, builder);
+      }
+    }
+  }
+
+  // This populates the src_set of an op. See the docs of the individual
+  // function calls for details on initialization and propagation.
+  //
+  // Pre-condition: all operands of `op` must have their src_sets populated.
+  //
+  // Hence, it must be used in conjunction with a pre-order traversal of the
+  // MLIR graph, so that all users are processed before the current op.
+  //
+  // TODO: b/340565987 - move to class so that we can access `infer_reductions`
+  // as a field instead of as an arg.
+  WalkResult PopulateSrcSet(Operation* op, OpBuilder& builder) {
+    if (auto unassign = dyn_cast<UnassignOp>(op)) {
+      InitializeSrcSet(unassign, builder);
+    } else if (auto func = dyn_cast<FuncOp>(op)) {
+      InitializeSrcSet(func, builder);
+    } else if (auto fragment = dyn_cast<FragmentOp>(op)) {
+      // Skip fragment ops and their regions for efficiency, as they are already
+      // assigned.
+      return WalkResult::skip();
+    } else if (auto call_op = dyn_cast<CallOp>(op)) {
+      PopulateSrcSetForCallOp(call_op, builder);
+    } else if (auto for_op = dyn_cast<ForOp>(op)) {
+      PopulateSrcSetForForOp(for_op, builder);
+    } else if (IsMeshlessOp(op) || isa<AssignOp>(op)) {
+      PropagateSrcSet(op, builder);
+      // Skip regions of these meshless ops (if they have a region). These
+      // regions will have the same mesh as the op itself.
+      return WalkResult::skip();
+    }
+
+    return WalkResult::advance();
+  }
+};
+
+// Prints a set of meshes {m1,m2,m3} as "m1,m2,m3".
+std::string GetPrintableString(MeshesWithOrigins meshes) {
+  return llvm::join(meshes.MeshNames().getArrayRef(), ",");
+}
+
+// Returns a mesh for assignment that is valid: i.e. which is in the operand's
+// src_set. It tries to pick from `preferred_meshes` where possible, and if not,
+// it picks the first mesh in the operand's src_set.
+std::optional<StringRef> GetMeshForAssignment(
+    OpOperand& operand, StringRef first_mesh_name,
+    const MeshesWithOrigins& preferred_meshes) {
+  MeshesWithOrigins src_set = GetSrcSet(operand);
+  if (src_set.empty()) {
+    return std::nullopt;
+  }
+
+  if (!src_set) {
+    return preferred_meshes.GetPrioritizedMeshName().value_or(first_mesh_name);
+  }
+
+  return *src_set.GetPrioritizedMeshName(preferred_meshes.MeshNamesOrEmpty());
+}
+
+// Returns the mesh for the main function's return operand which is compatible
+// with the `input_use_set`, preferring to pick a mesh from the output's use_set
+// over the src_set. Returns std::nullopt if there is no such mesh. We assume
+// here that `input_use_set` is the use_set of the input, where the pair (input,
+// output) is specified by the user to be constrained to the same mesh.
+//
+// If the input or output is already a MeshTensor (because of user-provided
+// assignment), then we try to use the mesh it is assigned to. If the output is
+// assigned, we can always assign the input (because the input can be assigned
+// anywhere). If the input is assigned, we may not be able to assign the output
+// without introducing a transfer. In this case, we leave it to the
+// `EnforceEquishardingPass` pass to introduce this transfer.
+//
+// If the input or output is already a MeshTensor (because of user-provided
+// assignment), then we try to use the mesh it is assigned to. If the output is
+// assigned, we can always assign the input (because the input can be assigned
+// anywhere). If the input is assigned, we may not be able to assign the output
+// without introducing a transfer. In this case, we leave it to the
+// `EnforceEquishardingPass` pass to introduce this transfer.
+std::optional<StringRef> GetMeshForInputOutputAssignment(
+    OpOperand& output_operand, FuncOp func, int64_t input_index) {
+  auto output_mesh_type =
+      dyn_cast<MeshTensorType>(output_operand.get().getType());
+  BlockArgument input_arg = func.getArgument(input_index);
+  auto input_mesh_type = dyn_cast<MeshTensorType>(input_arg.getType());
+
+  if (output_mesh_type) {
+    if (input_mesh_type) {
+      SDY_CHECK_EQ(std::string_view(input_mesh_type.getMeshName()),
+                   std::string_view(output_mesh_type.getMeshName()))
+          << "Constraint was given to map input " << input_index
+          << " to output " << output_operand.getOperandNumber()
+          << " but the user provided invalid meshes differ. This should be "
+             "caught in python validation.";
+    }
+    return output_mesh_type.getMeshName();
+  }
+
+  MeshesWithOrigins output_src_set = GetSrcSet(output_operand);
+  // Returns an empty set if the result is an argument.
+  MeshesWithOrigins output_use_set =
+      GetUseSet(output_operand.get().getDefiningOp());
+
+  MeshesWithOrigins input_use_set;
+  if (input_mesh_type) {
+    input_use_set.insert(MeshWithOriginsAttr::get(
+        func->getContext(), input_mesh_type.getMeshName()));
+  } else {
+    input_use_set = GetArgUseSet(func, input_index);
+    if (input_use_set.empty()) {
+      // This means the input is not used in any mesh. In these cases, the
+      // algorithm assigns it to any mesh, but we will have to guarantee that
+      // it's assigned to the same mesh as the output. We can do that without
+      // restricting the assignment to input's use_set.
+      if (!output_use_set.empty()) {
+        return *output_use_set.GetPrioritizedMeshName();
+      }
+      if (!output_src_set.empty()) {
+        return *output_src_set.GetPrioritizedMeshName();
+      }
+      return std::nullopt;
+    }
+  }
+
+  if (MeshesWithOrigins candidate_meshes =
+          GetUseSet(output_operand.get().getDefiningOp());
+      !candidate_meshes.empty()) {
+    candidate_meshes.Intersect(input_use_set);
+
+    if (!candidate_meshes.empty()) {
+      if (candidate_meshes.size() > 1) {
+        SDY_LOG(INFO)
+            << "Picking first mesh for func output "
+            << output_operand.getOperandNumber()
+            << " from the intersection of the input and outputs use_set: {"
+            << GetPrintableString(candidate_meshes) << "}";
+      }
+
+      return *candidate_meshes.GetPrioritizedMeshName();
+    }
+  }
+
+  MeshesWithOrigins candidate_meshes = input_use_set;
+  candidate_meshes.Intersect(GetSrcSet(output_operand));
+  if (!candidate_meshes.empty()) {
+    if (candidate_meshes.size() > 1) {
+      SDY_LOG(INFO)
+          << "Picking smallest mesh (in alphanum order) for func output "
+          << output_operand.getOperandNumber()
+          << " from the intersection of the input use_set and output src_set: {"
+          << GetPrintableString(candidate_meshes) << "}";
+    }
+    return *candidate_meshes.GetPrioritizedMeshName();
+  }
+
+  return std::nullopt;
+}
+
+void AssignInputAndOutputToMesh(FuncOp func, BlockArgument input_arg,
+                                OpOperand& return_operand, StringRef mesh_name,
+                                sdy::MeshAttr mesh_attr,
+                                RewriterBase& rewriter) {
+  // Assign the output to the mesh.
+  if (!isa<MeshTensorType>(return_operand.get().getType())) {
+    rewriter.setInsertionPoint(return_operand.getOwner());
+    return_operand.set(rewriter.create<AssignOp>(
+        GetResultInfoLoc(func, return_operand.getOperandNumber())
+            .value_or(return_operand.get().getLoc()),
+        return_operand.get(), mesh_name, mesh_attr, kIoConstraintOutputOrigin));
+  }
+
+  // Assign the input to the mesh.
+  if (!isa<MeshTensorType>(input_arg.getType())) {
+    rewriter.setInsertionPointAfterValue(input_arg);
+    input_arg.setType(MeshTensorType::getFullyReplicated(
+        input_arg.getContext(), mesh_name, mesh_attr,
+        cast<RankedTensorType>(input_arg.getType())));
+    auto unassign = rewriter.create<UnassignOp>(input_arg.getLoc(), input_arg,
+                                                kIoConstraintInputOrigin);
+    rewriter.replaceAllUsesExcept(input_arg, unassign, unassign);
+  }
+}
+
+sdy::MeshAttr GetMeshByName(
+    const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name, StringRef name) {
+  sdy::MeshAttr result = meshes_by_name.lookup(name);
+  SDY_CHECK(result) << "Mesh with name '" << name.str() << "' not found.";
+  return result;
+}
+
+class InferMeshAssignUsingInputOutputConstraintsPass
+    : public impl::InferMeshAssignUsingInputOutputConstraintsPassBase<
+          InferMeshAssignUsingInputOutputConstraintsPass> {
+  using InferMeshAssignUsingInputOutputConstraintsPassBase::
+      InferMeshAssignUsingInputOutputConstraintsPassBase;
+
+  void runOnOperation() final {
+    IRRewriter rewriter(&getContext());
+    for (FuncOp func_op : GetMpmdFunctions(getOperation())) {
+      if (IsEntryPointFunction(func_op)) {
+        runOnFunc(func_op, rewriter);
+      }
+    }
+  }
+
+  void runOnFunc(FuncOp func, RewriterBase& rewriter) {
+    DenseMap<StringRef, sdy::MeshAttr> meshes_by_name =
+        GetMeshesByName(GetTopologyMeshes(func));
+
+    Operation* return_op = func.front().getTerminator();
+    for (const InputOutputEquishardingConstraint& constraint : constraints) {
+      SDY_CHECK_LT(constraint.input_index, func.getNumArguments());
+      SDY_CHECK_LT(constraint.output_index, func.getNumResults());
+
+      OpOperand& return_operand =
+          return_op->getOpOperand(constraint.output_index);
+      if (std::optional<StringRef> mesh_name = GetMeshForInputOutputAssignment(
+              return_operand, func, constraint.input_index)) {
+        AssignInputAndOutputToMesh(
+            func, func.getArgument(constraint.input_index), return_operand,
+            *mesh_name, GetMeshByName(meshes_by_name, *mesh_name), rewriter);
+      } else if (verboseLogging) {
+        SDY_LOG(INFO)
+            << "No suitable mesh found for input-output constraint on input "
+            << constraint.input_index << " and output "
+            << constraint.output_index;
+      }
+    }
+
+    UpdateFunctionType(func);
+  }
+};
+
+class InferMeshAssignMeshForFuncLeavesPass
+    : public impl::InferMeshAssignMeshForFuncLeavesPassBase<
+          InferMeshAssignMeshForFuncLeavesPass> {
+  using InferMeshAssignMeshForFuncLeavesPassBase::
+      InferMeshAssignMeshForFuncLeavesPassBase;
+
+  void runOnOperation() final {
+    IRRewriter rewriter(&getContext());
+    for (FuncOp func_op : GetMpmdFunctions(getOperation())) {
+      ArrayRef<NamedMeshAttr> meshes = GetTopologyMeshes(func_op);
+      NamedMeshAttr first_mesh =
+          *llvm::min_element(meshes, [](NamedMeshAttr a, NamedMeshAttr b) {
+            return a.getName() < b.getName();
+          });
+      DenseMap<StringRef, sdy::MeshAttr> meshes_by_name =
+          GetMeshesByName(meshes);
+      if (IsEntryPointFunction(func_op)) {
+        runOnEntryPointFunc(func_op, first_mesh, meshes_by_name, rewriter);
+      } else {
+        runOnNonEntryPointFunc(func_op, first_mesh, meshes_by_name, rewriter);
+      }
+    }
+  }
+
+  void runOnEntryPointFunc(
+      FuncOp func, NamedMeshAttr first_mesh,
+      const llvm::DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+      RewriterBase& rewriter) {
+    // We can assign any mesh to the unused func inputs, and we arbitrarily
+    // pick the first mesh.
+    AssignUnusedFuncInputs(func, first_mesh);
+    AssignFuncOutputs(func, cast<func::ReturnOp>(func.front().getTerminator()),
+                      meshes_by_name, first_mesh.getName(), rewriter);
+    runOnFunc(func, first_mesh, meshes_by_name, rewriter);
+    UpdateFunctionType(func);
+  }
+
+  void runOnNonEntryPointFunc(
+      FuncOp func, NamedMeshAttr first_mesh,
+      const llvm::DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+      RewriterBase& rewriter) {
+    runOnFunc(func, first_mesh, meshes_by_name, rewriter);
+    // Needs to run after `runOnFunc`, because `runOnFunc` will clear the
+    // use_set, but these functions will set the use_set of unused func inputs
+    // and outputs.
+    AssignUnusedCalleeInputs(func, first_mesh.getName(), rewriter);
+    AssignUnusedCalleeOutputs(func, first_mesh.getName(), rewriter);
+  }
+
+  void runOnFunc(FuncOp func, NamedMeshAttr first_mesh,
+                 const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+                 RewriterBase& rewriter) {
+    // Applied to any function in the module.
+    func.getFunctionBody().walk<WalkOrder::PreOrder>(
+        [this, first_mesh_name = first_mesh.getName(), &meshes_by_name,
+         &rewriter](Operation* op) {
+          if (auto fragment = dyn_cast<FragmentOp>(op)) {
+            // Skip fragment ops and their regions for efficiency, as they are
+            // already assigned.
+            return WalkResult::skip();
+          }
+
+          if (IsMeshlessOp(op)) {
+            if (op->getNumResults() == 0) {
+              AssignResultlessOp(op, first_mesh_name, rewriter);
+            } else if (op->use_empty()) {
+              AssignUnusedOp(op, first_mesh_name, meshes_by_name, rewriter);
+            } else {
+              ClearUseSet(op);
+            }
+            return WalkResult::skip();
+          } else if (IsTerminalNodeInAnalysis(op)) {
+            AssignOperandsOfAnalysisTerminalNodes(op, meshes_by_name,
+                                                  first_mesh_name, rewriter);
+            ClearUseSet(op);
+            return WalkResult::skip();
+          } else if (sdy::inDialect<MpmdDialect>(op)) {
+            // Don't skip the body of other MPMD ops.
+            return WalkResult::advance();
+          }
+
+          return WalkResult::skip();
+        });
+    ClearUseSet(func);
+  }
+
+  // Picks a mesh assignment for each func output. This assigns an arbitrary
+  // mesh from the use_set if it exists, or otherwise from the src_set (note
+  // that the outputs may not have a use_set).
+  //
+  // The use_set could exist since an op result can be used in both a return op
+  // and other computations e.g. if we have
+  //
+  // x = some_op
+  // y = assign x "m1"
+  // return x, y
+  //
+  // then x is a func output and has use_set {m1}.
+  //
+  // We pick a mesh for the return value v by wrapping it in an AssignOp, i.e.
+  // we return AssignOp(v) instead of v, to make clear precisely which mesh it
+  // is assigned to. This is not possible with attributes on the ops, and while
+  // we could annotate the func result attrs, it would be harder to spot when
+  // debugging.
+  void AssignFuncOutputs(
+      FuncOp func, func::ReturnOp op,
+      const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+      StringRef first_mesh_name, OpBuilder& builder) {
+    builder.setInsertionPoint(op);
+    for (OpOperand& return_op_operand : op->getOpOperands()) {
+      Value return_operand = return_op_operand.get();
+      if (isa<MeshTensorType>(return_operand.getType())) {
+        continue;
+      }
+
+      std::optional<StringRef> mesh_name =
+          GetMeshForAssignment(return_op_operand, first_mesh_name,
+                               GetUseSet(return_operand.getDefiningOp()));
+      if (!mesh_name) {
+        // When --infer-transfers=true, we want to assign it to a
+        // mesh anyway. Otherwise, we want to emit an error.
+        if (!inferTransfers) {
+          op->emitError("Func output ") << return_op_operand.getOperandNumber()
+                                        << " has no mesh to be assigned to.";
+          IncErrorCountAndMaybeFail();
+        }
+        mesh_name = first_mesh_name;
+      }
+      return_op_operand.set(builder.create<AssignOp>(
+          GetResultInfoLoc(func, return_op_operand.getOperandNumber())
+              .value_or(return_operand.getLoc()),
+          return_operand, *mesh_name, GetMeshByName(meshes_by_name, *mesh_name),
+          kInferredOutputOrigin));
+    }
+  }
+
+  // Returns a mesh from the src_set. If the src_set is empty, returns
+  // default_mesh_name with a warning. Additionally, it returns a boolean value
+  // which is true if the src_set is empty.
+  std::pair<StringRef, bool> GetMeshFromSrcSet(MeshesWithOrigins src_set,
+                                               StringRef default_mesh_name) {
+    if (src_set) {
+      if (src_set.empty()) {
+        return {default_mesh_name, true};
+      }
+      return {*src_set.GetPrioritizedMeshName(), false};
+    }
+    return {default_mesh_name, false};
+  }
+
+  // Assigns a mesh to unused func outputs. These outputs could have a src_set,
+  // e.g. if they have a func input as operand, but they will not have a use_set
+  // since they have no uses.
+  //
+  // This only sets the use_set, it does not create assign ops. Assign ops will
+  // be created later during the rewrite.
+  void AssignUnusedCalleeOutputs(FuncOp callee, StringRef first_mesh_name,
+                                 OpBuilder& builder) {
+    for (MpmdDataflowEdge& edge : GetMpmdDataflowEdgesForFuncResults(callee)) {
+      // Filter away outputs with uses.
+      if (llvm::any_of(edge.targets, [](Value v) { return !v.use_empty(); })) {
+        continue;
+      }
+
+      OpOperand& return_op_operand = *edge.sources.front();
+      Value return_operand = return_op_operand.get();
+      if (isa<MeshTensorType>(return_operand.getType())) {
+        continue;
+      }
+
+      auto [mesh_name, inferred_name] =
+          GetMeshFromSrcSet(GetSrcSet(return_op_operand), first_mesh_name);
+      if (inferred_name && !inferTransfers) {
+        return_op_operand.getOwner()->emitError("Callee @")
+            << callee.getSymName() << " unused output "
+            << return_op_operand.getOperandNumber()
+            << " has no mesh to be assigned to.";
+        return IncErrorCountAndMaybeFail();
+      }
+      SetResUseSet(callee, return_op_operand.getOperandNumber(),
+                   MeshesWithOrigins(UnusedCalleeOutputMeshWithOrigin(
+                       callee->getContext(), mesh_name)),
+                   builder);
+    }
+  }
+
+  // Assigns a mesh to unused callee inputs. These inputs could have a src_set
+  // but will not have a use_set. So we pick a mesh from the src_set. This only
+  // sets the use_set, it does not create assign ops. Assign ops will be created
+  // later during the rewrite.
+  void AssignUnusedCalleeInputs(FuncOp callee, StringRef first_mesh_name,
+                                OpBuilder& builder) {
+    for (BlockArgument arg : callee.getArguments()) {
+      if (arg.use_empty() && isa<RankedTensorType>(arg.getType())) {
+        auto [mesh_name, inferred_name] = GetMeshFromSrcSet(
+            GetSrcSet(callee, arg.getArgNumber()), first_mesh_name);
+        if (inferred_name && !inferTransfers) {
+          emitError(arg.getLoc(), "Callee @")
+              << callee.getSymName() << " unused input " << arg.getArgNumber()
+              << " has no mesh to be assigned to.";
+          return IncErrorCountAndMaybeFail();
+        }
+        SetArgUseSet(callee, arg.getArgNumber(),
+                     MeshesWithOrigins(UnusedCalleeInputMeshWithOrigin(
+                         callee->getContext(), mesh_name)),
+                     builder);
+      }
+    }
+  }
+
+  // Assigns a mesh to unused intermediate ops, by creating an assign user.
+  // These ops could have a src_set, e.g. if they have a func input as operand.
+  // But they will not have a use_set since they have no uses.
+  //
+  // The ops may have no results, in which case we wrap them in a fragment since
+  // we cannot create an assign user for an op with no results.
+  //
+  // This relies on the pass pipeline not doing DCE, since the newly created
+  // assign ops will be unused and would otherwise be DCE-ed.
+  void AssignUnusedOp(Operation* op, StringRef default_mesh_name,
+                      const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+                      RewriterBase& rewriter) {
+    StringRef mesh_name;
+    if (MeshesWithOrigins src_set = GetSrcSet(op)) {
+      if (src_set.empty()) {
+        op->emitError("src_set must not be empty for this op.");
+        // In this case, we have to stop here, or otherwise we would crash
+        // below.
+        return signalPassFailure();
+      }
+      mesh_name = *src_set.GetPrioritizedMeshName();
+    } else {
+      // If there is no src_set attr, then the src_set is all meshes and so we
+      // pick an arbitrary mesh.
+      mesh_name = default_mesh_name;
+    }
+    rewriter.setInsertionPointAfter(op);
+    sdy::MeshAttr mesh = GetMeshByName(meshes_by_name, mesh_name);
+    for (Value res : op->getResults()) {
+      rewriter.create<AssignOp>(op->getLoc(), res, mesh_name, mesh,
+                                kInferredUnusedOrigin);
+    }
+
+    ClearUseSet(op);
+  }
+
+  // Like `AssignUnusedOp` but for ops that have no results. In this case, we
+  // wrap the op in a fragment, since the op has no results to add.
+  //
+  // If the op is not pure, then we clone it per mesh. This is important because
+  // non-pure ops may have side effects, and we want to keep them per-mesh. E.g.
+  // the sdy.sharding_group op needs to be cloned per mesh.
+  //
+  // TODO: b/397895929 - Improve how we handle non-pure meshless ops. Ideally,
+  // we should clone them according to the actual mesh assignment, rather than
+  // the src_set, which may not actually be the assignment.
+  void AssignResultlessOp(Operation* op, StringRef default_mesh_name,
+                          RewriterBase& rewriter) {
+    std::optional<SetVector<StringRef>> src_set = GetSrcMeshes(op);
+    if (src_set) {
+      if (src_set->empty()) {
+        op->emitError("src_set must not be empty for resultless ops.");
+        return signalPassFailure();
+      }
+    } else {
+      // If there is no src_set attr, then we at least assign it to a default
+      // mesh.
+      src_set = SetVector<StringRef>();
+      src_set->insert(default_mesh_name);
+    }
+    ClearUseSetAndSrcSet(op);
+    rewriter.setInsertionPointAfter(op);
+
+    for (StringRef mesh_name : src_set->getArrayRef()) {
+      WrapOpWithFragment(op, mesh_name, rewriter);
+      if (isPure(op)) {
+        // For pure ops, we only need to wrap it in a fragment once. But for
+        // non-pure ops, we need to keep them associated with each src.
+        break;
+      }
+    }
+    rewriter.eraseOp(op);
+  };
+
+  // Assigns a mesh to unused func inputs. These inputs will not have a src_set
+  // nor a use_set as they are unused. So we can assign any mesh.
+  void AssignUnusedFuncInputs(FuncOp func, NamedMeshAttr first_mesh) {
+    for (BlockArgument arg : func.getArguments()) {
+      if (auto type = dyn_cast<RankedTensorType>(arg.getType());
+          type && arg.use_empty()) {
+        SDY_LOG(INFO) << "Picking first mesh for unused func input "
+                      << arg.getArgNumber() << " from the topology, i.e. "
+                      << std::string_view(first_mesh.getName());
+        arg.setType(MeshTensorType::getFullyReplicated(
+            func->getContext(), first_mesh.getName(), first_mesh.getMesh(),
+            type));
+      }
+    }
+  }
+
+  // Treats the operands of analysis-terminal-nodes (e.g. mpmd.broadcast and
+  // mpmd.reduce) as leaves of the tree, and assigns them to a specific mesh.
+  void AssignOperandsOfAnalysisTerminalNodes(
+      Operation* op, const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+      StringRef first_mesh_name, OpBuilder& builder) {
+    builder.setInsertionPoint(op);
+
+    // By picking a mesh in the use_set, we can save transfers. In particular,
+    // if there are multiple meshes in the src_set and we know it will be needed
+    // in one of the meshes in the use_set, we might as well assign it to one of
+    // the meshes in the src_set that also appears in the use_set.
+    MeshesWithOrigins preferred_meshes = GetUseSet(op);
+    for (OpOperand& operand : op->getOpOperands()) {
+      std::optional<StringRef> mesh_name =
+          GetMeshForAssignment(operand, first_mesh_name, preferred_meshes);
+      if (!mesh_name) {
+        SDY_CHECK(inferTransfers)
+            << "This should only happen if we are inferring "
+               "transfers.";
+        // This happens if the src_set is empty. In this case, we will have to
+        // introduce transfers in the later pass, or error out.
+        mesh_name =
+            preferred_meshes.GetPrioritizedMeshName().value_or(first_mesh_name);
+      }
+      Value operand_val = operand.get();
+      AssignOp assign = builder.create<AssignOp>(
+          operand_val.getLoc(), operand_val, *mesh_name,
+          GetMeshByName(meshes_by_name, *mesh_name), TerminalNodesOrigin(op));
+      operand.set(builder.create<UnassignOp>(operand_val.getLoc(), assign));
+    }
+  }
+
+  void IncErrorCountAndMaybeFail() {
+    error_count_++;
+    if (error_count_ == max_errors_) {
+      return signalPassFailure();
+    }
+  }
+
+  LogicalResult initialize(MLIRContext* context) final {
+    error_count_ = 0;
+    max_errors_ = GetValidatedMaxErrors(errorLimit);
+    return success();
+  }
+
+  int error_count_ = 0;
+  int max_errors_ = 1;
+};
+
+// If `op` is the reduce of a cross-mesh concat-reduce pair, then replace the
+// concat-reduce pair with an mpmd.reduce and return true.
+void ConvertConcatReduceOp(Operation* op, RewriterBase& rewriter) {
+  if (isa<stablehlo::ConcatenateOp>(op)) {
+    // Conversion of the concat will happen together with the stablehlo.reduce.
+    return;
+  }
+  auto reduce = dyn_cast<stablehlo::ReduceOp>(op);
+  if (!reduce) {
+    return;
+  }
+  auto concat =
+      reduce.getInputs().front().getDefiningOp<stablehlo::ConcatenateOp>();
+  if (!concat) {
+    return;
+  }
+  rewriter.setInsertionPoint(reduce);
+
+  // Collapse the concat and reduce dim of the operands.
+  SmallVector<Value> reshaped_operands;
+  reshaped_operands.reserve(concat.getOperands().size());
+  for (Value operand : concat.getOperands()) {
+    auto reshape = rewriter.create<stablehlo::ReshapeOp>(
+        operand.getLoc(), reduce->getResultTypes().front(), operand);
+    if (operand.getDefiningOp()) {
+      reshape->setDiscardableAttrs(
+          operand.getDefiningOp()->getDiscardableAttrDictionary());
+    }
+    reshaped_operands.push_back(reshape);
+  }
+
+  rewriter.setInsertionPoint(reduce);
+  MeshesWithOrigins src_set = GetSrcSet(op);
+  auto mpmd_reduce = rewriter.replaceOpWithNewOp<ReduceOp>(
+      reduce, reshaped_operands,
+      op->getAttrOfType<ReductionAttr>(kMpmdReduceAnnotation));
+  if (src_set) {
+    SetSrcSet(mpmd_reduce, src_set, rewriter);
+  }
+  rewriter.eraseOp(concat);
+}
+
+// Converts the annotated op to an actual mpmd.reduce op, and also flattens any
+// reduce chains.
+//
+// Pre-condition: op is annotated to be an MPMD reduce.
+void ConvertAnnotatedReduceOp(Operation* op, RewriterBase& rewriter) {
+  if (isa<stablehlo::ConcatenateOp, stablehlo::ReduceOp>(op)) {
+    ConvertConcatReduceOp(op, rewriter);
+    return;
+  }
+
+  auto reduction_attr = op->getAttrOfType<ReductionAttr>(kMpmdReduceAnnotation);
+
+  SmallVector<Value> operands;
+  SmallVector<ReduceOp> operands_to_erase;
+  for (Value operand : op->getOperands()) {
+    if (auto operand_reduce = operand.getDefiningOp<ReduceOp>();
+        operand_reduce && reduction_attr == operand_reduce.getReduction()) {
+      operands.append(operand_reduce->getOperands().begin(),
+                      operand_reduce->getOperands().end());
+      if (operand_reduce->hasOneUse()) {
+        operands_to_erase.push_back(operand_reduce);
+      }
+    } else {
+      operands.push_back(operand);
+    }
+  }
+
+  rewriter.setInsertionPoint(op);
+  MeshesWithOrigins src_set = GetSrcSet(op);
+  auto reduce =
+      rewriter.replaceOpWithNewOp<ReduceOp>(op, operands, reduction_attr);
+  if (src_set) {
+    SetSrcSet(reduce, src_set, rewriter);
+  }
+  for (ReduceOp operand : operands_to_erase) {
+    rewriter.eraseOp(operand);
+  }
+}
+
+// Annotates the binary op with the attribute that indicates the reduction type
+// if the binary op's reduction type matches the target reduction type.
+// Rewrites
+//
+// x = add(w0, w1)
+//
+// ~~>
+//
+// x = add(w0, w1) {mpmd.reduce = #mpmd.reduce<add>}
+// The allowed binary ops are add, mul, max, min, and or.
+void AnnotateBinaryOp(Operation* op, ReductionType target_reduction_type) {
+  if (!op) {
+    return;
+  }
+
+  if (GetReductionOpType(op) == target_reduction_type) {
+    op->setAttr(kCanConvertToReduce, UnitAttr::get(op->getContext()));
+    // If the op has a valid reduction type, it's a binary op.
+    SDY_CHECK(op->getNumOperands() == 2 && op->getNumResults() == 1);
+    AnnotateBinaryOp(op->getOperand(0).getDefiningOp(), target_reduction_type);
+    AnnotateBinaryOp(op->getOperand(1).getDefiningOp(), target_reduction_type);
+  }
+}
+
+// Returns true if `op` is a unary operation.
+bool IsUnaryOperation(Operation* op) {
+  return op->getNumOperands() == 1 && op->getNumResults() == 1;
+}
+
+// Annotates `op` with the attribute that indicates the reduction type if it is
+// a binary op that is used by a mpmd.reduce<none> op.
+// Rewrites
+//
+// x = add(w0, w1)
+// y = mpmd.reduce<none>(x)
+//
+// ~~>
+//
+// x = add(w0, w1) {mpmd.can_convert_to_reduce}
+// y = mpmd.reduce<none>(x)
+// If any of w0 or w1 are from add ops, they will also be annotated recursively.
+//
+// This also annotates a concat-reduce pair if the concat is a cross-mesh
+// concat.
+//
+// x = stablehlo.concat(w0, w1)
+// y = stablehlo.reduce<add>(x)
+// z = mpmd.reduce<none>(y)
+//
+// ~~>
+// x = stablehlo.concat(w0, w1)
+// y = stablehlo.reduce<add>(x) {mpmd.can_convert_to_reduce}
+// z = mpmd.reduce<none>(y)
+//
+// This annotation skips any unary ops that uses the result of the binary op.
+// For example,
+//
+// x = add(w0, w1)
+// y = abs(x)
+// z = mpmd.reduce<none>(y)
+//
+// ~~>
+//
+// x = add(w0, w1) {mpmd.can_convert_to_reduce}
+// y = abs(x)
+// z = mpmd.reduce<none>(y)
+void AnnotateProducerOfNoneReduce(ReduceOp none_reduce) {
+  if (none_reduce.getReductionType() != ReductionType::kNone) {
+    return;
+  }
+  Operation* defining_op = none_reduce.getTensors().front().getDefiningOp();
+  if (!defining_op) {
+    return;
+  }
+
+  // Skip all the unary ops until we reach a binary op.
+  Operation* current_op = defining_op;
+  while (IsUnaryOperation(current_op)) {
+    current_op = current_op->getOperands().front().getDefiningOp();
+    if (!current_op) {
+      return;
+    }
+  }
+
+  std::optional<ReductionType> reduction_type = GetReductionOpType(current_op);
+  if (reduction_type.has_value()) {
+    AnnotateBinaryOp(current_op, *reduction_type);
+    return;
+  }
+
+  // Match the `mpmd.reduce<none>(stablehlo.reduce<max>(x, y))` pattern. If the
+  // concat is a cross-mesh concat, then we add the annotation to the reduce.
+  if (isa<stablehlo::ReduceOp>(current_op)) {
+    auto concat =
+        current_op->getOperand(0).getDefiningOp<stablehlo::ConcatenateOp>();
+    if (!concat) {
+      return;
+    }
+
+    stablehlo::ReduceOp cross_mesh_reduce = MatchCrossMeshConcatReduce(concat);
+    std::optional<ReductionType> reduction_type =
+        cross_mesh_reduce
+            ? ComputeReductionType(cross_mesh_reduce.getBody().front())
+            : GetReductionOpType(current_op);
+    if (!reduction_type) {
+      return;
+    }
+    current_op->setAttr(kCanConvertToReduce,
+                        UnitAttr::get(current_op->getContext()));
+  }
+}
+
+class InferMeshConvertReduceOpsPass
+    : public impl::InferMeshConvertReduceOpsPassBase<
+          InferMeshConvertReduceOpsPass> {
+  using InferMeshConvertReduceOpsPassBase::InferMeshConvertReduceOpsPassBase;
+
+  void runOnOperation() final {
+    FuncOp func = getOperation();
+    IRRewriter rewriter(&getContext());
+
+    // Annotate the binary ops that are used by a reduce<none> op.
+    // This annotation will allow us to introduce reduce ops. For example, an
+    // add op with the annotation will be converted to a reduce<add> op.
+    func.walk(
+        [&](ReduceOp reduce_op) { AnnotateProducerOfNoneReduce(reduce_op); });
+
+    // Walk the op before its region, so that we can delete any reduce-ops
+    // without needing to walk its body. Also, we can skip the body of fragment
+    // ops.
+    func.getFunctionBody().walk<WalkOrder::PreOrder>(
+        [&rewriter, this](Operation* op) {
+          if (auto fragment = dyn_cast<FragmentOp>(op)) {
+            return WalkResult::skip();
+          } else if (IsMeshlessOp(op)) {
+            if (IsMpmdReduceAnnotated(op) &&
+                (inferCrossMeshReductions || CanConvertToReduce(op))) {
+              ConvertAnnotatedReduceOp(op, rewriter);
+              return WalkResult::skip();
+            }
+          }
+          return WalkResult::advance();
+        });
+    // We no longer need the temporary can_convert_to_reduce attribute.
+    ClearCanConvertAttr(func);
+  }
+};
+
+// Attribute name to annotate where the current value was copied to, when
+// copying a value across meshes.
+inline constexpr StringRef kMpmdCopied = "mpmd.copies";
+
+// Assigns the results of `callee` to the use_set, duplicating results which are
+// assigned to multiple meshes. The original result will be annotated with an
+// attribute indicating the position of the clones. E.g.
+//
+// func.func mpmdf(...) -> (T {use_set=[m1,m2,m3]})
+// ~~>
+// func.func mpmdf(...) -> (T_m1 {mpmd.copies=[1,2]}, T_m2, T_m3)
+//
+// Note that this does not update the type of the func op itself. That is done
+// separately.
+void AssignCalleeFuncResultsUsingAnalysis(
+    FuncOp callee, const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+    RewriterBase& rewriter) {
+  auto return_op =
+      cast<func::ReturnOp>(callee.getBody().front().getTerminator());
+  rewriter.setInsertionPoint(return_op);
+
+  SmallVector<Value> new_operands(return_op.getOperands());
+  // Process the existing return operands, copying them as needed and adding
+  // them to the set of `new_operands`.
+  for (auto [res_num, return_val] : llvm::enumerate(return_op.getOperands())) {
+    if (isa<MeshTensorType>(return_val.getType())) {
+      continue;
+    }
+
+    SetVector<StringRef> mesh_names =
+        GetResUseSet(callee, res_num).MeshNamesOrEmpty();
+
+    // This can happen when a callee result is unused. We only handle this
+    // during the rewrite, because this requires the use set analysis to
+    // determine that it is in fact unused.
+    if (mesh_names.empty()) {
+      SDY_CHECK(return_val.getDefiningOp())
+          << "Callee should not have value chained through from arg to result "
+             "directly.";
+      StringRef mesh_name;
+      if (MeshesWithOrigins src_meshes = GetSrcSet(return_val.getDefiningOp());
+          src_meshes && !src_meshes.empty()) {
+        mesh_name = *src_meshes.GetPrioritizedMeshName();
+      } else {
+        mesh_name = *llvm::min_element(llvm::map_range(
+            meshes_by_name, [](const auto& it) { return it.first; }));
+      }
+      mesh_names.insert(mesh_name);
+      SDY_LOG(INFO) << "Callee @" << std::string_view(callee.getSymName())
+                    << " output " << res_num << " is unused. Assigning to mesh "
+                    << std::string_view(mesh_name);
+    }
+
+    if (mesh_names.size() > 1) {
+      // If the result is assigned to multiple meshes, annotate the result with
+      // the positions where it is copied to.
+      callee.setResultAttr(
+          res_num, kMpmdCopied,
+          rewriter.getDenseI64ArrayAttr(llvm::to_vector(llvm::seq<int64_t>(
+              new_operands.size(),
+              new_operands.size() + mesh_names.size() - 1))));
+    }
+
+    // Assign the results to the corresponding meshes. If a result has multiple
+    // meshes, we copy it such that each result corresponds to a single mesh.
+    for (auto [i, mesh_name] : llvm::enumerate(mesh_names.getArrayRef())) {
+      auto assign =
+          rewriter.create<AssignOp>(return_val.getLoc(), return_val, mesh_name,
+                                    GetMeshByName(meshes_by_name, mesh_name));
+      if (i == 0) {
+        new_operands[res_num] = assign;
+      } else {
+        new_operands.push_back(assign);
+      }
+    }
+  }
+
+  // Finally, update the return_op with the new operands.
+  return_op->setOperands(new_operands);
+}
+
+// Assigns the arguments of `callee` to according to the meshes of assign users,
+// duplicating arguments which are assigned to multiple meshes. The original
+// argument will be annotated with an attribute indicating the position of the
+// clones. E.g.
+//
+// func.func mpmdf(%arg0: T {use_set=[m1,m2,m3]}) -> ...
+// ~~>
+// func.func mpmdf(%arg0: T_m1 {mpmd.copies=[1,2]}, %arg1: T_m2, %arg2: T_m3) ->
+// ...
+//
+// Note that this does not update the type of the func op itself. That is done
+// separately.
+void AssignCalleeFuncArgsToAssignUsers(
+    FuncOp callee, const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+    RewriterBase& rewriter) {
+  Block& body = callee.front();
+  rewriter.setInsertionPointToStart(&body);
+  // Process the existing args, copying them as needed.
+  // Save the argument list to a vector, because we'll be rewriting the
+  // arguments.
+  for (auto [arg_num, arg] :
+       llvm::enumerate(llvm::to_vector(callee.getArguments()))) {
+    if (isa<MeshTensorType>(arg.getType())) {
+      continue;
+    }
+
+    SDY_CHECK(!arg.getUsers().empty())
+        << "Callee argument is unused. It should have been removed in the "
+           "mpmd-erase-unused-callee-block-arguments pass.";
+
+    // Group users by mesh_name, to avoid having to loop through the user list
+    // multiple times, which may be problematic since we will be updating the
+    // user list.
+    DenseMap<StringRef, SmallVector<AssignOp>> assign_users_by_mesh_name;
+    for (auto user : arg.getUsers()) {
+      if (auto assign_user = dyn_cast<AssignOp>(user)) {
+        assign_users_by_mesh_name[assign_user.getType().getMeshName()]
+            .push_back(assign_user);
+      }
+    }
+    SmallVector<StringRef> mesh_names = llvm::map_to_vector(
+        assign_users_by_mesh_name, [](const auto& it) { return it.first; });
+    llvm::sort(mesh_names);
+
+    SDY_CHECK(!mesh_names.empty()) << "No mesh names found.";
+    if (mesh_names.size() > 1) {
+      // If the arg is assigned to multiple meshes, annotate the arg with
+      // the positions where it is copied to.
+      callee.setArgAttr(
+          arg_num, kMpmdCopied,
+          rewriter.getDenseI64ArrayAttr(llvm::to_vector(llvm::seq<int64_t>(
+              body.getNumArguments(),
+              body.getNumArguments() + mesh_names.size() - 1))));
+    }
+
+    // Assign the args to the corresponding meshes. If a arg has multiple
+    // meshes, we copy it such that each arg corresponds to a single mesh.
+    // To preserve the arg type, we replace it users with an unassign.
+    auto local_type = cast<RankedTensorType>(arg.getType());
+    for (auto [i, mesh_name] : llvm::enumerate(mesh_names)) {
+      auto mesh_type = MeshTensorType::getFullyReplicated(
+          arg.getContext(), mesh_name, GetMeshByName(meshes_by_name, mesh_name),
+          local_type);
+      UnassignOp unassign_op;
+      if (i == 0) {
+        arg.setType(mesh_type);
+        unassign_op = rewriter.create<UnassignOp>(arg.getLoc(), arg);
+      } else {
+        unassign_op = rewriter.create<UnassignOp>(
+            arg.getLoc(), body.addArgument(mesh_type, arg.getLoc()));
+      }
+
+      if (auto users_it = assign_users_by_mesh_name.find(mesh_name);
+          users_it != assign_users_by_mesh_name.end()) {
+        for (auto assign_user : users_it->second) {
+          assign_user.setOperand(unassign_op);
+        }
+      }
+    }
+  }
+}
+
+// Rewrites the `call_op` using the updated information from its callee func.
+// I.e. it takes into account the results and arguments which have been copied
+// because they were used in multiple meshes, and creates assigns into the call,
+// and unassigns out of the call accordingly.
+//
+// Pre-condition: The callee should already be rewritten to have mesh types.
+// Also, the users of the callee should already be rewritten, i.e. assigned to a
+// mesh.
+void RewriteAccordingToUpdatedCallee(CallOp call_op, RewriterBase& rewriter) {
+  auto callee = GetCalleeFunc(call_op);
+  Block& call_body = callee.front();
+  rewriter.setInsertionPointAfter(call_op);
+
+  // Get assigned operands for the new call.
+  std::vector<Value> new_operands(call_body.getNumArguments());
+  for (auto [arg_num, operand] : llvm::enumerate(call_op.getOperands())) {
+    if (isa<MeshTensorType>(operand.getType())) {
+      new_operands[arg_num] = operand;
+      continue;
+    }
+    SDY_CHECK(isa<MeshTensorType>(call_body.getArgument(arg_num).getType()));
+    new_operands[arg_num] = rewriter.create<AssignOp>(
+        operand.getLoc(), call_body.getArgument(arg_num).getType(), operand);
+
+    if (auto copies =
+            callee.getArgAttrOfType<DenseI64ArrayAttr>(arg_num, kMpmdCopied)) {
+      for (int64_t cloned_arg_index : copies.asArrayRef()) {
+        SDY_CHECK(isa<MeshTensorType>(
+            call_body.getArgument(cloned_arg_index).getType()));
+        new_operands[cloned_arg_index] = rewriter.create<AssignOp>(
+            operand.getLoc(), call_body.getArgument(cloned_arg_index).getType(),
+            operand);
+      }
+    }
+  }
+
+  // Create the new call and copy attrs over.
+  auto new_call_op = rewriter.create<CallOp>(
+      call_op.getLoc(), call_body.getTerminator()->getOperandTypes(),
+      new_operands, call_op.getCalleeAttr());
+  new_call_op->setDiscardableAttrs(call_op->getDiscardableAttrDictionary());
+
+  // Replace uses of the original call with the new call's results.
+  for (OpResult res : call_op.getResults()) {
+    DenseMap<Type, int64_t> type_to_arg_num;
+    type_to_arg_num.try_emplace(
+        new_call_op.getResult(res.getResultNumber()).getType(),
+        res.getResultNumber());
+
+    if (auto copies = callee.getResultAttrOfType<DenseI64ArrayAttr>(
+            res.getResultNumber(), kMpmdCopied)) {
+      for (int64_t cloned_arg_num : copies.asArrayRef()) {
+        type_to_arg_num.try_emplace(
+            new_call_op.getResult(cloned_arg_num).getType(), cloned_arg_num);
+      }
+    }
+
+    // We need to save the user list, because we'll be rewriting the users.
+    for (Operation* user : llvm::to_vector(res.getUsers())) {
+      if (auto assign_user = dyn_cast<AssignOp>(user)) {
+        auto arg_num_it = type_to_arg_num.find(assign_user.getType());
+        SDY_CHECK(arg_num_it != type_to_arg_num.end())
+            << "Argument number for type " << debugString(assign_user.getType())
+            << " not found";
+        assign_user.setOperand(rewriter.create<UnassignOp>(
+            assign_user.getLoc(), new_call_op.getResult(arg_num_it->second)));
+      }
+    }
+  }
+  rewriter.eraseOp(call_op);
+}
+
+// Assigns the arguments of the entrypoint func to the first mesh of its
+// assign users in lexicographic order (i.e. default string sorting order).
+//
+// func.func main(%arg0: T) -> ...
+// {
+//    %0 = assign %arg0 -> m2
+//    %1 = assign %arg0 -> m1
+//    %2 = assign %arg0 -> m3
+//    return %0, %1, %2
+// }
+// ~~>
+// func.func main(%arg0: T_m1) ->
+// {
+//    %0 = unassign %arg0 -> T
+//    %1 = assign %0 -> m1
+//    %2 = assign %0 -> m3
+//    return %arg0, %1, %2
+// }
+//
+// Note that this does not update the type of the func op itself. That is done
+// separately.
+//
+// If an argument isn't a mesh tensor type yet, and a mesh tensor type needs to
+// be built, then we check for consistency in the memory kinds of the assign
+// users. If it succeeds, we return true. Otherwise, we return false.
+//
+// Pre-condition: The users of the args should either be mesh tensors, or only
+// used by AssignOps.
+bool AssignEntrypointFuncArgsToAssignUsers(FuncOp entrypoint_func,
+                                           RewriterBase& rewriter) {
+  Block& body = entrypoint_func.front();
+  rewriter.setInsertionPointToStart(&body);
+  for (auto arg : body.getArguments()) {
+    if (isa<MeshTensorType>(arg.getType())) {
+      continue;
+    }
+
+    // Find the memory kind, if any, that should be used for the mesh tensor.
+    llvm::SetVector<StringAttr> memory_kinds;
+    bool has_user_with_undefined_memory_kind = false;
+    for (auto user : arg.getUsers()) {
+      if (auto assign_user = dyn_cast<AssignOp>(user)) {
+        StringAttr user_memory_kind = assign_user.getType().getMemoryKind();
+        if (user_memory_kind) {
+          memory_kinds.insert(user_memory_kind);
+        } else {
+          has_user_with_undefined_memory_kind = true;
+        }
+      }
+    }
+    if (has_user_with_undefined_memory_kind && !memory_kinds.empty()) {
+      entrypoint_func.emitError()
+          << "Argument " << arg.getArgNumber()
+          << " has different memory kinds assigned to it. Found at least one "
+             "user with undefined memory kind and at least one user with a "
+             "memory kind.";
+      return false;
+    }
+
+    if (memory_kinds.size() > 1) {
+      // TODO: b/374994155 - Consider a different verification pass for memory
+      // kinds.
+      entrypoint_func.emitError()
+          << "Argument " << arg.getArgNumber()
+          << " has different memory kinds assigned to it.";
+      return false;
+    }
+
+    StringAttr memory_kind =
+        has_user_with_undefined_memory_kind ? nullptr : *memory_kinds.begin();
+
+    StringRef mesh_name =
+        *llvm::min_element(llvm::map_range(arg.getUsers(), [](Operation* user) {
+          return cast<AssignOp>(user).getType().getMeshName();
+        }));
+
+    arg.setType(MeshTensorType::get(arg.getContext(), mesh_name,
+                                    cast<RankedTensorType>(arg.getType()),
+                                    memory_kind));
+
+    UnassignOp unassign_op = rewriter.create<UnassignOp>(arg.getLoc(), arg);
+    rewriter.replaceAllUsesExcept(arg, unassign_op, unassign_op);
+  }
+  return true;
+}
+
+// Clones `op` into `consumer` and returns the cloned results.
+ValueRange AbsorbOpByCloning(FragmentOp consumer, Operation* op,
+                             IRMapping operand_mapping,
+                             RewriterBase& rewriter) {
+  rewriter.setInsertionPointToStart(consumer.getBody());
+  Operation* cloned_op = rewriter.clone(*op, operand_mapping);
+  return cloned_op->getResults();
+}
+
+// Inlines `op` into `consumer` and returns the inlined results, given a mapping
+// between operands/free variables of `op` and block arguments of consumer.
+// `has_free_vars` indicates whether `op` contains any free variables (which
+// would be included in `operand_mapping`).
+ValueRange AbsorbOpByInlining(FragmentOp consumer, Operation* op,
+                              IRMapping operand_or_freevar_mapping,
+                              bool has_free_vars, RewriterBase& rewriter) {
+  for (auto& [operand_or_freevar, new_value] :
+       operand_or_freevar_mapping.getValueMap()) {
+    // As we move the op to the body of a fragment and because fragments do
+    // not have free variables, we need to guarantee that op's free variables
+    // and operands will be replaced with the respective arguments of the
+    // fragment.
+    rewriter.replaceUsesWithIf(operand_or_freevar, new_value,
+                               [&](OpOperand& operand) {
+                                 if (!has_free_vars) {
+                                   // In this case, no need to visit the op's
+                                   // ancestor, which could go all the way to
+                                   // the module.
+                                   return op == operand.getOwner();
+                                 }
+                                 return op->isAncestor(operand.getOwner());
+                               });
+  }
+  rewriter.moveOpBefore(op, consumer.getBody(), consumer.getBody()->begin());
+  return op->getResults();
+}
+
+// Absorbs a meshless `op` into a consumer fragment.
+// This means the op will be cloned (if it has multiple users) or inlined (if it
+// has a single user) into the consumer fragment. Operands of the absorbed op
+// are appended to the consumer's operands.
+//
+// For example,
+//  ```
+//    x = op1(a, b)
+//    a = assign x
+//    fragment(a) (%arg) {
+//      y = op2(%arg)
+//      return y
+//    }
+//      ~~>
+//    a' = assign a
+//    b' = assign b
+//    fragment(a', b') (%arg0, %arg1) {
+//      y1 = op1(%arg0, %arg1)
+//      y2 = op2(y1)
+//      return y2
+//    }
+//  ```
+//
+// TODO: jupvfranco - we could optimize this code by checking if the op's
+// operands already exist in the consumer's operands. For now, we avoid this for
+// the sake of code simplicity.
+void AbsorbMeshlessProducer(FragmentOp consumer, Operation* op,
+                            bool op_used_by_consumer_only,
+                            RewriterBase& rewriter) {
+  rewriter.setInsertionPoint(consumer);
+  StringRef mesh_name = consumer.getMeshName();
+  sdy::MeshAttr mesh_attr = GetMeshOrFail(consumer);
+  Block* body = consumer.getBody();
+
+  // When inlining the op in a fragment, get its operands before inlining as
+  // they'll change during inlining and we still need them later on to extend
+  // the consumer fragment's operands.
+  // It isn't enough to use the operands of the op, as some control-flow ops
+  // (e.g., stablehlo.case) may use free variables.
+  llvm::SetVector<Value> op_operands_and_free_vars;
+  getUsedValuesDefinedAbove(op->getRegions(), op_operands_and_free_vars);
+  bool has_free_vars = !op_operands_and_free_vars.empty();
+  op_operands_and_free_vars.insert(op->operand_begin(), op->operand_end());
+
+  IRMapping mapping;
+  for (Value operand : op_operands_and_free_vars) {
+    mapping.map(operand,
+                body->insertArgument(body->getNumArguments(), operand.getType(),
+                                     operand.getLoc()));
+  }
+  ValueRange new_results =
+      op_used_by_consumer_only
+          ? AbsorbOpByInlining(consumer, op, mapping, has_free_vars, rewriter)
+          : AbsorbOpByCloning(consumer, op, mapping, rewriter);
+  BitVector erase_args(body->getNumArguments());
+  // Make sure that the body of consumer uses the op results directly instead
+  // of using the arguments respective of their uses.
+  // Note: when the op is inlined, old_result and new_result are the same.
+  for (auto [old_result, new_result] :
+       llvm::zip(op->getResults(), new_results)) {
+    for (Operation* user : old_result.getUsers()) {
+      AssignOp assign_op = cast<AssignOp>(user);
+      for (OpOperand& assign_use : assign_op->getUses()) {
+        if (assign_use.getOwner() == consumer) {
+          rewriter.replaceAllUsesWith(
+              body->getArgument(assign_use.getOperandNumber()), new_result);
+          erase_args.set(assign_use.getOperandNumber());
+        }
+      }
+    }
+  }
+
+  body->eraseArguments(erase_args);
+  std::vector<Value> new_consumer_operands;
+  for (auto index : erase_args.flip().set_bits()) {
+    // The number of arguments is the same or higher as we may have appended
+    // more arguments to the block of the fragment.
+    if (index < consumer.getNumOperands()) {
+      new_consumer_operands.push_back(consumer->getOperand(index));
+    }
+  }
+  rewriter.setInsertionPoint(consumer);
+  for (Value operand : op_operands_and_free_vars) {
+    new_consumer_operands.push_back(rewriter.create<AssignOp>(
+        operand.getLoc(),
+        MeshTensorType::getFullyReplicated(
+            operand.getContext(), mesh_name, mesh_attr,
+            cast<RankedTensorType>(operand.getType())),
+        operand));
+  }
+  consumer->setOperands(new_consumer_operands);
+}
+
+// Wraps the op in fragments based on the mesh types of its assign users. At
+// most we get one fragment per mesh type.
+void WrapBasedOnAssignUsers(Operation* op, RewriterBase& rewriter) {
+  // Use a set, instead of a vector, to avoid creating redundant fragments.
+  DenseSet<StringRef> user_mesh_types;
+  for (Operation* user : op->getUsers()) {
+    AssignOp assign_op = cast<AssignOp>(user);
+    user_mesh_types.insert(assign_op.getType().getMeshName());
+  }
+  for (StringRef mesh_name : user_mesh_types) {
+    WrapOpWithFragment(
+        op, mesh_name, rewriter,
+        /*should_replace_use=*/[&mesh_name](OpOperand& use) {
+          if (auto assign_user = dyn_cast<AssignOp>(use.getOwner())) {
+            return assign_user.getType().getMeshName() == mesh_name;
+          }
+          return false;
+        });
+  }
+}
+
+// Assigns the meshless `op` to a mesh either by being absorbed (by cloning or
+// inlining) or wrapped in a new fragment. In particular, if `op`:
+// - is used by a single fragment, then it is inlined into the fragment;
+// - is used by an op that is not a fragment (e.g., a transfer), then it is
+// wrapped in a fragment;
+// - is used by N fragments and N <= `max_clones`, or
+// the op has no operands, then it is cloned into each of its fragment users.
+//
+// Pre-condition: The meshless `op` is used by AssignOps only.
+void AssignOpBasedOnConsumers(Operation* op, const int max_clones,
+                              RewriterBase& rewriter) {
+  SDY_CHECK_GT(op->getNumResults(), 0)
+      << "All ops with no results should have been assigned by "
+         "now.";
+  ClearUseSetAndSrcSet(op);
+  DenseSet<FragmentOp> fragment_users;
+  bool has_non_fragment_user = false;
+  for (Operation* user : op->getUsers()) {
+    auto assign_op = dyn_cast<AssignOp>(user);
+    SDY_CHECK(assign_op) << "Expected user to be AssignOp, got: "
+                         << user->getName().getStringRef().str();
+    for (Operation* assign_user : assign_op->getUsers()) {
+      if (auto fragment_op = dyn_cast<FragmentOp>(assign_user)) {
+        fragment_users.insert(fragment_op);
+      } else {
+        has_non_fragment_user = true;
+      }
+    }
+  }
+
+  if (fragment_users.size() == 1 && !has_non_fragment_user) {
+    // Absorb by the op by inlining it.
+    DenseSet<Operation*> users_pre_inlining(op->getUsers().begin(),
+                                            op->getUsers().end());
+    AbsorbMeshlessProducer(*fragment_users.begin(), op,
+                           /*op_used_by_consumer_only=*/true, rewriter);
+    for (Operation* user : users_pre_inlining) {
+      rewriter.eraseOp(user);
+    }
+    return;
+  }
+
+  // When `fragment_users.empty` then there's no fragment to absorb meshless
+  // ops.
+  // When `has_non_fragment_user=true` we cannot clone as any non-fragment
+  // user of the assign ops will not be erased, meaning `op` cannot be
+  // erased either, as well as all its predecessors.
+  // When `op` has many users (> `kMaxClones`) we do not want it to be
+  // cloned as it could be the root of a large tree, i.e., we would
+  // replicate too much code, potentially slowing down this pass (and maybe
+  // others) significantly.
+  // In any other case, we clone `op` into each of its fragment users.
+  if (fragment_users.empty() || has_non_fragment_user ||
+      (fragment_users.size() > max_clones && op->getNumOperands() > 0)) {
+    WrapBasedOnAssignUsers(op, rewriter);
+  } else {
+    for (FragmentOp fragment_user : fragment_users) {
+      AbsorbMeshlessProducer(fragment_user, op,
+                             /*op_used_by_consumer_only=*/false, rewriter);
+    }
+  }
+  // All users of op must be unused at this point. The op was either: (a)
+  // wrapped in one or more fragments, and its assign users replaced with
+  // the respective fragments, or (b) cloned into each of its consumer
+  // fragments. This means that all the assign users may be safely erased.
+  llvm::SmallDenseSet<Operation*> users_to_erase(op->getUsers().begin(),
+                                                 op->getUsers().end());
+  for (Operation* user : users_to_erase) {
+    rewriter.eraseOp(user);
+  }
+  // And because we erase all its users, then the op is also unused.
+  // Note we need to get the previous op before erasing the current one.
+  rewriter.eraseOp(op);
+}
+
+// Rewrites the terminator of the given `for_op` to return mesh tensor types.
+// For each non-mesh tensor result, we create an AssignOp that assigns the
+// result to the mesh of the result's use_set.
+void RewriteForOpTerminator(
+    ForOp for_op, const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+    RewriterBase& rewriter) {
+  auto return_op =
+      dyn_cast<ReturnOp>(for_op.getRegion().front().getTerminator());
+  rewriter.setInsertionPoint(return_op);
+
+  SmallVector<Value> new_operands;
+  for (auto [res_num, return_val] : llvm::enumerate(return_op.getOperands())) {
+    if (isa<MeshTensorType>(return_val.getType())) {
+      continue;
+    }
+
+    SetVector<StringRef> mesh_names = GetUseMeshes(return_val.getDefiningOp());
+    // TODO(petebu): Handle unused result.
+    SDY_CHECK(!mesh_names.empty()) << "No mesh names found for return value";
+    // TODO(b/401476674): Handle multiple meshes.
+    SDY_CHECK_LE(mesh_names.size(), 1)
+        << "Multiple mesh names found for return value";
+
+    new_operands.push_back(rewriter.create<AssignOp>(
+        return_val.getLoc(), return_val, mesh_names[0],
+        GetMeshByName(meshes_by_name, mesh_names[0])));
+  }
+
+  return_op->setOperands(new_operands);
+}
+
+// Iterates over all ops of the function and (1) assigns every meshless op to a
+// mesh (by wrapping it or absorbing it into a fragment) and (2) rewrites any
+// call_op so that it can handle inputs/outputs used in multiple meshes and so
+// that it returns mesh tensor types. This is analogous to
+// WalkAndAbsorbMeshlessProducers, but for ForOp.
+void RewriteForOpBody(ForOp for_op,
+                      const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+                      const int max_clones, RewriterBase& rewriter) {
+  Block& block = *for_op.getBody();
+  for (Operation& operation :
+       llvm::make_early_inc_range(llvm::reverse(block.getOperations()))) {
+    Operation* op = &operation;
+    if (auto call_op = dyn_cast<CallOp>(op)) {
+      RewriteAccordingToUpdatedCallee(call_op, rewriter);
+    } else if (auto for_op = dyn_cast<ForOp>(op)) {
+      SDY_CHECK(false) << "Nested ForOp is not supported";
+    } else if (IsMeshlessOp(op)) {
+      AssignOpBasedOnConsumers(op, max_clones, rewriter);
+    }
+  }
+}
+
+// Rewrites the arguments of the given `for_op` to be mesh tensor types. For
+// each non-mesh tensor argument, we create an UnAssignOp on entry to the loop
+// body that converts the mesh tensor type to a local tensor type.
+void RewriteForOpArgsAndTypes(
+    ForOp for_op, const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+    RewriterBase& rewriter) {
+  Block& body = for_op.getRegion().front();
+  rewriter.setInsertionPointToStart(&body);
+
+  for (auto [arg_num, arg] :
+       llvm::enumerate(llvm::to_vector(for_op.getRegion().getArguments()))) {
+    if (isa<MeshTensorType>(arg.getType())) {
+      continue;
+    }
+
+    DenseMap<StringRef, SmallVector<AssignOp>> assign_users_by_mesh_name;
+    for (Operation* user : arg.getUsers()) {
+      if (auto assign_user = dyn_cast<AssignOp>(user)) {
+        assign_users_by_mesh_name[assign_user.getType().getMeshName()]
+            .push_back(assign_user);
+      }
+    }
+    SmallVector<StringRef> mesh_names = llvm::map_to_vector(
+        assign_users_by_mesh_name, [](const auto& it) { return it.first; });
+    llvm::sort(mesh_names);
+
+    // TODO(b/401476674): Handle multiple meshes.
+    SDY_CHECK_LE(mesh_names.size(), 1)
+        << "Multiple mesh names found for return value";
+
+    if (mesh_names.size() == 1) {
+      auto local_type = cast<RankedTensorType>(arg.getType());
+      auto mesh_type = MeshTensorType::getFullyReplicated(
+          arg.getContext(), mesh_names[0],
+          GetMeshByName(meshes_by_name, mesh_names[0]), local_type);
+      arg.setType(mesh_type);
+      UnassignOp unassign_op = rewriter.create<UnassignOp>(arg.getLoc(), arg);
+
+      if (auto users_it = assign_users_by_mesh_name.find(mesh_names[0]);
+          users_it != assign_users_by_mesh_name.end()) {
+        for (auto assign_user : users_it->second) {
+          assign_user.setOperand(unassign_op);
+        }
+      }
+    }
+  }
+
+  // Update ForOp result type.
+  for (auto [res, op_type] : llvm::zip(
+           for_op.getResults(),
+           for_op.getRegion().front().getTerminator()->getOperandTypes())) {
+    res.setType(op_type);
+  }
+}
+
+// Rewrites the operands of the given `for_op` to be mesh tensor types. For
+// each non-mesh tensor operand, we create an AssignOp that assigns the operand
+// to the mesh of the operand's use_set.
+void RewriteForOpOperands(ForOp for_op, RewriterBase& rewriter) {
+  Block& for_body = for_op.getRegion().front();
+  rewriter.setInsertionPoint(for_op);
+
+  std::vector<Value> new_operands(for_op.getNumOperands());
+  for (auto [arg_num, operand] : llvm::enumerate(for_op.getOperands())) {
+    if (isa<MeshTensorType>(operand.getType())) {
+      new_operands[arg_num] = operand;
+      continue;
+    }
+    new_operands[arg_num] = rewriter.create<AssignOp>(
+        operand.getLoc(), for_body.getArgument(arg_num).getType(), operand);
+  }
+
+  for_op->setOperands(new_operands);
+}
+
+// Rewrites the results of the given `for_op`. For each result, we create an
+// UnassignOp that converts the result to a local tensor type.
+void RewriteForOpResults(ForOp for_op, RewriterBase& rewriter) {
+  rewriter.setInsertionPointAfter(for_op);
+  for (OpResult res : for_op.getResults()) {
+    for (Operation* user : res.getUsers()) {
+      if (auto assign_user = dyn_cast<AssignOp>(user)) {
+        assign_user.setOperand(
+            rewriter.create<UnassignOp>(assign_user.getLoc(), res));
+      }
+    }
+  }
+  ClearUseSetAndSrcSet(for_op);
+}
+
+void RewriteForOp(ForOp for_op,
+                  const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+                  const int max_clones, RewriterBase& rewriter) {
+  RewriteForOpTerminator(for_op, meshes_by_name, rewriter);
+  RewriteForOpBody(for_op, meshes_by_name, max_clones, rewriter);
+  RewriteForOpArgsAndTypes(for_op, meshes_by_name, rewriter);
+  RewriteForOpOperands(for_op, rewriter);
+  RewriteForOpResults(for_op, rewriter);
+}
+
+// Iterates over all ops of the function and (1) assigns every meshless op to a
+// mesh (by wrapping it or absorbing it into a fragment) and (2) rewrites any
+// call_op so that it can handle inputs/outputs used in multiple meshes and so
+// that it returns mesh tensor types.
+void WalkAndAbsorbMeshlessProducers(
+    FuncOp func_op, const DenseMap<StringRef, sdy::MeshAttr>& meshes_by_name,
+    const int max_clones, RewriterBase& rewriter) {
+  Block& block = func_op.getBody().front();
+  for (Operation& operation :
+       llvm::make_early_inc_range(llvm::reverse(block.getOperations()))) {
+    Operation* op = &operation;
+    if (auto call_op = dyn_cast<CallOp>(op)) {
+      RewriteAccordingToUpdatedCallee(call_op, rewriter);
+    } else if (auto for_op = dyn_cast<ForOp>(op)) {
+      RewriteForOp(for_op, meshes_by_name, max_clones, rewriter);
+    } else if (IsMeshlessOp(op)) {
+      AssignOpBasedOnConsumers(op, max_clones, rewriter);
+    }
+  }
+}
+
+class BroadcastToTransfersPattern : public OpRewritePattern<BroadcastOp> {
+  using OpRewritePattern<BroadcastOp>::OpRewritePattern;
+
+ public:
+  LogicalResult matchAndRewrite(BroadcastOp op,
+                                PatternRewriter& rewriter) const override {
+    auto unassign_op = op.getTensor().getDefiningOp<UnassignOp>();
+    SDY_CHECK(unassign_op)
+        << "Expected the operand of the BroadcastOp to be the "
+           "result of an UnassignOp";
+    bool module_rewritten = false;
+    // We copy the users to a vector because we modify the list of users when
+    // replacing the op, which would cause a segfault otherwise.
+    SmallVector<Operation*> users(op->getUsers().begin(), op->getUsers().end());
+    for (Operation* user : users) {
+      if (auto assign_op = dyn_cast<AssignOp>(user)) {
+        if (assign_op.getType() != unassign_op.getTensor().getType()) {
+          rewriter.replaceOpWithNewOp<TransferOp>(
+              assign_op, assign_op.getType(), unassign_op.getTensor());
+        } else {
+          assign_op->setOperand(0, unassign_op.getResult());
+        }
+        module_rewritten = true;
+      }
+    }
+    return success(module_rewritten);
+  }
+};
+
+class InferMeshRewriteUsingAnalysisPass
+    : public impl::InferMeshRewriteUsingAnalysisPassBase<
+          InferMeshRewriteUsingAnalysisPass> {
+  using InferMeshRewriteUsingAnalysisPassBase::
+      InferMeshRewriteUsingAnalysisPassBase;
+
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+    MLIRContext* context = module_op->getContext();
+    IRRewriter rewriter(context);
+
+    llvm::SmallVector<func::FuncOp> mpmd_functions =
+        GetMpmdFunctions(module_op);
+
+    // Rewrite all the callees before rewriting the entrypoint functions, so
+    // that we can use the updated callee func info when we propagate through
+    // the mpmd.calls in the entrypoint function.
+    for (FuncOp func_op : mpmd_functions) {
+      if (!IsEntryPointFunction(func_op)) {
+        // Assigns meshless ops in `callee`s. This does not update the type
+        // of the func. We reserve that to after the calls to the callee have
+        // been updated. This only changes the `callee` and does not change
+        // calls to the callee.
+        DenseMap<StringRef, sdy::MeshAttr> meshes_by_name =
+            GetMeshesByName(GetTopologyMeshes(func_op));
+
+        // We assign results first, so that they can absorb meshless ops.
+        AssignCalleeFuncResultsUsingAnalysis(func_op, meshes_by_name, rewriter);
+        WalkAndAbsorbMeshlessProducers(func_op, meshes_by_name, maxClones,
+                                       rewriter);
+        AssignCalleeFuncArgsToAssignUsers(func_op, meshes_by_name, rewriter);
+      }
+    }
+    for (FuncOp func_op : mpmd_functions) {
+      if (IsEntryPointFunction(func_op)) {
+        DenseMap<StringRef, sdy::MeshAttr> meshes_by_name =
+            GetMeshesByName(GetTopologyMeshes(func_op));
+        WalkAndAbsorbMeshlessProducers(func_op, meshes_by_name, maxClones,
+                                       rewriter);
+      }
+    }
+    for (FuncOp func_op : mpmd_functions) {
+      UpdateFunctionType(func_op);
+    }
+  }
+};
+
+// TODO: b/359832656 - Use single walk instead of greedy rewriter.
+class InferMeshFinalizePass
+    : public impl::InferMeshFinalizePassBase<InferMeshFinalizePass> {
+  using InferMeshFinalizePassBase::InferMeshFinalizePassBase;
+
+  void runOnOperation() final {
+    ModuleOp module_op = getOperation();
+    MLIRContext* context = module_op->getContext();
+
+    IRRewriter rewriter(context);
+    for (FuncOp func_op : GetMpmdFunctions(module_op)) {
+      if (IsEntryPointFunction(func_op)) {
+        // TODO: joelwee - Move this into the RewriteUsingAnalysisPass, as that
+        // is the more natural place to handle the assignments.
+        if (!AssignEntrypointFuncArgsToAssignUsers(func_op, rewriter)) {
+          return signalPassFailure();
+        }
+      }
+    }
+
+    RewritePatternSet patterns(context);
+    patterns.add<AssignOfUnassignSameMeshPattern,
+                 DedupAssignOfUnassignAndTransferPattern,
+                 AssignOfUnassignFuncArgPattern, BroadcastToTransfersPattern,
+                 LowerMpmdReducePattern>(context);
+    if (inferTransfers) {
+      // Note that because we're doing this after running
+      // `RewriteUsingAnalysis`, the introduction of transfers is naive: they
+      // are created as early as possible: i.e. in the first place where
+      // `src_set` is not contained in `use_set`. E.g.
+      //
+      // x = unassign x_m0
+      // y = x + x
+      // z = y + y
+      // z_m1 = assign z -> m1
+      //
+      // ~~>
+      //
+      // x_m1 = transfer x_m0 -> m1
+      // z_m1 = frag m1 {
+      //   y = x + x
+      //   z = y + y
+      //   return z
+      // }
+      //
+      // To improve this further, we would need to do some form of min-cut
+      // analysis before the rewrite itself.
+      // TODO: b/329221688 - Use heuristics or a min-cut analysis to improve the
+      // creation of transfers.
+      patterns.add<AssignOfUnassignPattern>(context);
+    }
+    // This converges in 2 passes, where each pass completes the following:
+    // 1. Clean up AssignOps and UnassignOps.
+    // 2. Nothing (i.e. detect convergence).
+    //
+    // So in theory, we could do this with an MLIR walk if we refactored the
+    // cleanup patterns out of the RewritePattern classes.
+    GreedyRewriteConfig config;
+    config.enableFolding(false);
+    config.enableConstantCSE(false);
+    if (failed(applyPatternsGreedily(module_op, std::move(patterns), config))) {
+      return signalPassFailure();
+    }
+
+    for (FuncOp func_op : GetMpmdFunctions(module_op)) {
+      // Update function types once after all function arguments and return
+      // operands have been replaced.
+      UpdateFunctionType(func_op);
+      if (failed(VerifyMeshAssignment(func_op))) {
+        signalPassFailure();
+      }
+      ClearUseSetAndSrcSet(func_op);
+    }
+  }
+};
+
+}  // namespace
+
+void addInferMeshPipeline(
+    OpPassManager& pm,
+    SmallVector<InputOutputEquishardingConstraint> inputOutputConstraints,
+    InferMeshOptions options) {
+  // If infer_transfers is true, then we always infer cross_mesh reductions too,
+  // since infer_transfers is more permissive. Otherwise we may have dangling
+  // annotated mpmd.reduce ops.
+  if (options.inferTransfers) {
+    options.inferCrossMeshReductions = true;
+  }
+
+  pm.addPass(createInferMeshPopulateUseSetPass());
+  // We populate the src_set after we have the use_set info, as we want to use
+  // the use_set to constrain where the func inputs can be transferred to. Func
+  // inputs with a use_set will have their src_set constrained to the use_set.
+  // I.e. they are only assigned to meshes they are used in, as defined by the
+  // user (i.e. before any inference). If they are not used in any mesh, then
+  // they remain unconstrained, i.e. they can be assigned to any mesh. We
+  // populate the src_set to know which meshes we can assign our outputs to.
+  pm.addPass(createInferMeshPopulateSrcSetPass());
+
+  if (!inputOutputConstraints.empty()) {
+    // Use input_output_constraints to assign the func outputs and inputs,
+    // before making any other assignments.
+    InferMeshAssignUsingInputOutputConstraintsPassOptions constraints_options;
+    constraints_options.constraints = std::move(inputOutputConstraints);
+    constraints_options.verboseLogging = options.errorLimit == -1;
+    pm.addPass(createInferMeshAssignUsingInputOutputConstraintsPass(
+        std::move(constraints_options)));
+  }
+
+  // Convert annotated ops before assignment, which uses the converted reduce
+  // ops.
+  // If `options.inferCrossMeshReductions` is off and the user didn't explicitly
+  // tag an op as being a reduction (via mpmd.reduce), then this pass will be a
+  // noop, as no operation should be annotated. However, if the user did tag
+  // operations for reductions, then we want to apply this pass, even if the
+  // flag is off.
+  pm.addNestedPass<FuncOp>(createInferMeshConvertReduceOpsPass(
+      InferMeshConvertReduceOpsPassOptions{options.inferCrossMeshReductions}));
+
+  // Validate that every op can be assigned to some mesh.
+  if (!options.inferTransfers) {
+    pm.addPass(createInferMeshValidateSrcSetNotEmptyPass(
+        InferMeshValidateSrcSetNotEmptyPassOptions{options.errorLimit}));
+  }
+
+  // Assigning meshes to FuncOp leaves is enough, as every op in a FuncOp flows
+  // into some FuncOp leaf or AssignOp (which already has a mesh). After this
+  // pass, the leaf will have a use_set, which will be propagated when we run
+  // the PopulateUseSet pass again.
+  InferMeshAssignMeshForFuncLeavesPassOptions assign_mesh_for_leaves_options;
+  assign_mesh_for_leaves_options.inferTransfers = options.inferTransfers;
+  assign_mesh_for_leaves_options.errorLimit = options.errorLimit;
+  pm.addPass(createInferMeshAssignMeshForFuncLeavesPass(
+      assign_mesh_for_leaves_options));
+  pm.addPass(createInferMeshPopulateUseSetPass());
+
+  // Validate that no additional transfers are needed.
+  if (!options.inferTransfers) {
+    pm.addPass(createInferMeshValidateNoAdditionalTransfersNeededPass(
+        InferMeshValidateNoAdditionalTransfersNeededPassOptions{
+            options.errorLimit}));
+  }
+  // Finally, we rewrite the DAG to assign meshes using the analyses above.
+  //
+  // If `inferTransfers=true`, this will introduce new cross-mesh transfers.
+  // Note: currently the transfers are created as early as possible, which is
+  // likely suboptimal.
+  pm.addPass(createInferMeshRewriteUsingAnalysisPass(
+      InferMeshRewriteUsingAnalysisPassOptions{options.maxClones}));
+
+  pm.addPass(createInferMeshFinalizePass(
+      InferMeshFinalizePassOptions{options.inferTransfers}));
+}
+
+namespace {
+
+struct InferMeshPipelineOptions
+    : public PassPipelineOptions<InferMeshPipelineOptions> {
+  Option<bool> inferTransfers{
+      *this, "infer-transfers",
+      llvm::cl::desc(
+          "Whether to create transfers when needed, instead of erroring."),
+      llvm::cl::init(false)};
+
+  Option<bool> inferCrossMeshReductions{
+      *this, "infer-cross-mesh-reductions",
+      llvm::cl::desc("Whether to infer cross-mesh reductions."),
+      llvm::cl::init(false)};
+};
+
+}  // namespace
+
+void registerInferMeshPipeline() {
+  PassPipelineRegistration<InferMeshPipelineOptions>(
+      "mpmd-infer-mesh-pipeline", "Run the passes for mesh inference",
+      [](OpPassManager& pm, const InferMeshPipelineOptions& pipelineOptions) {
+        InferMeshOptions options;
+        options.inferTransfers = pipelineOptions.inferTransfers;
+        options.inferCrossMeshReductions =
+            pipelineOptions.inferCrossMeshReductions;
+        addInferMeshPipeline(pm, /*inputOutputConstraints=*/{}, options);
+      });
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.h b/shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.h
new file mode 100644
index 0000000..b908e8a
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.h
@@ -0,0 +1,57 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_INFER_MESH_ASSIGNMENT_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_INFER_MESH_ASSIGNMENT_H_
+
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/transforms/import/sharding_constraints.h"
+
+namespace mlir::mpmd {
+
+// Options for the infer mesh pipeline.
+struct InferMeshOptions {
+  // Whether to create transfers when needed, instead of erroring.
+  bool inferTransfers = false;
+  // Whether to infer cross-mesh reductions.
+  bool inferCrossMeshReductions = false;
+  // How many copies of a meshless operation we allow.
+  int maxClones = 1;
+  // The number of errors to emit. Set to -1 to emit all errors. Cannot be 0.
+  int errorLimit = 5;
+};
+
+// Infers the mesh assignments of non-mpmd ops that are not nested within a
+// fragment op. This uses an analysis to determine the mesh assignments. See
+// comments in the implementation for more details.
+//
+// These passes work to infer the meshes of all unassigned ops and func inputs
+// and outputs, with the restriction that we do not create any new transfers
+// except on func inputs.
+//
+// Between the analysis and rewrite, there shouldn't be any passes that do DCE.
+// E.g. no use of the greedy rewriter.
+void addInferMeshPipeline(
+    OpPassManager& pm,
+    SmallVector<InputOutputEquishardingConstraint> inputOutputConstraints,
+    InferMeshOptions options = {});
+
+// Register the `-mpmd-infer-mesh-pipeline`.
+void registerInferMeshPipeline();
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_INFER_MESH_ASSIGNMENT_H_
diff --git a/shardy/dialect/mpmd/transforms/import/infer_mesh_validation.cc b/shardy/dialect/mpmd/transforms/import/infer_mesh_validation.cc
new file mode 100644
index 0000000..c658f2f
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/infer_mesh_validation.cc
@@ -0,0 +1,905 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <limits>
+#include <optional>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetOperations.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Iterators.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_utils.h"
+#include "shardy/dialect/mpmd/transforms/import/meshes_with_origins.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_INFERMESHVALIDATESRCSETNOTEMPTYPASS
+#define GEN_PASS_DEF_INFERMESHVALIDATENOADDITIONALTRANSFERSNEEDEDPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+int GetValidatedMaxErrors(int error_limit) {
+  SDY_CHECK_NE(error_limit, 0)
+      << "mesh inference validation must not be disabled.";
+  return error_limit == -1 ? std::numeric_limits<int>::max() : error_limit;
+}
+
+namespace {
+
+using ::llvm::DenseMap;
+using ::mlir::func::FuncOp;
+
+inline constexpr StringRef kErrorMessageSeparator =
+    "\n---------------------------------------------------\n";
+
+enum class ValidationResult {
+  kOk,
+  kError,
+  kErrorButDontEmit,
+};
+
+StringRef SanitizeCallName(StringRef call_name) {
+  const StringRef mpmd_prefix = "shardy_mpmd";
+  if (call_name.starts_with(mpmd_prefix)) {
+    return call_name.drop_front(mpmd_prefix.size());
+  }
+  return call_name;
+}
+
+// Returns and caches the call ops for a given callee, since this is expensive
+// to compute. This lazily computes the call ops, since it is quite expensive
+// to do so, and we only need it for error reporting. I.e. we want to avoid
+// doing this computation in the case where there's no error.
+SmallVector<CallOp> GetMaybeCachedCallOps(
+    FuncOp callee,
+    DenseMap<StringRef, SmallVector<CallOp>>& lazy_call_ops_by_callee_) {
+  if (auto it = lazy_call_ops_by_callee_.find(callee.getSymName());
+      it != lazy_call_ops_by_callee_.end()) {
+    return it->second;
+  }
+
+  lazy_call_ops_by_callee_[callee.getSymName()] = GetCallOps(callee);
+  return lazy_call_ops_by_callee_[callee.getSymName()];
+}
+
+using LocToOrigins =
+    SmallVector<std::pair<Location, SmallVector<MeshWithOriginsAttr>>>;
+
+// Walk through the MLIR module and get the origins of the src_set of `value`,
+// with the MLIR location. All values in the src_set must arise from either
+// entrypoint func args or mpmd.unassign ops, so we just need to walk through.
+LocToOrigins GetSrcOriginsWithLocs(
+    ValueRange root,
+    DenseMap<StringRef, SmallVector<CallOp>>& lazy_call_ops_by_callee) {
+  LocToOrigins mesh_origins_with_locs;
+
+  std::vector<Value> worklist(root.begin(), root.end());
+  llvm::DenseSet<Value> visited;
+
+  while (!worklist.empty()) {
+    Value current_value = worklist.back();
+    worklist.pop_back();
+    if (visited.contains(current_value)) {
+      continue;
+    }
+    visited.insert(current_value);
+
+    // Case 1: BlockArgument
+    if (auto block_arg = dyn_cast<BlockArgument>(current_value)) {
+      Operation* parent_op = block_arg.getOwner()->getParentOp();
+      // Mesh inference only runs on values directly in func ops or for-ops.
+      // TODO: b/421069658 - Add support for block arguments of for-ops.
+      SDY_CHECK(!isa<ForOp>(parent_op)) << "Not supported yet.";
+      auto func_op = cast<FuncOp>(parent_op);
+
+      // Case 1a: BlockArgument of the entrypoint function.
+      if (IsEntryPointFunction(func_op)) {
+        mesh_origins_with_locs.emplace_back(
+            block_arg.getLoc(), GetSrcSet(func_op, block_arg.getArgNumber())
+                                    .ToArray(func_op.getContext()));
+        continue;
+      }
+
+      // Case 1b: BlockArgument of a mpmd.call.
+      SmallVector<CallOp> call_ops =
+          GetMaybeCachedCallOps(func_op, lazy_call_ops_by_callee);
+      for (CallOp call_op : call_ops) {
+        worklist.push_back(call_op.getOperand(block_arg.getArgNumber()));
+      }
+      continue;
+    }
+
+    // Case 2: The value is defined by an Operation
+    auto op_result = cast<OpResult>(current_value);
+    Operation* defining_op = op_result.getOwner();
+
+    // Case 2a: UnassignOp
+    if (auto unassign_op = dyn_cast<mpmd::UnassignOp>(defining_op)) {
+      MeshesWithOrigins src_set;
+      // Ensure that the first mesh and origin of the src_set is from the
+      // unassign op.
+      src_set.insert(unassign_op.getMeshWithOrigin());
+      src_set.Union(GetSrcSet(unassign_op));
+      mesh_origins_with_locs.emplace_back(
+          unassign_op.getLoc(), src_set.ToArray(defining_op->getContext()));
+      continue;
+    }
+
+    // Case 2b: Meshless op, so add to worklist.
+    if (IsMeshlessOp(defining_op)) {
+      for (Value operand : defining_op->getOperands()) {
+        worklist.push_back(operand);
+      }
+      continue;
+    }
+
+    // Case 2c: CallOp, so add to worklist.
+    if (auto call_op = dyn_cast<CallOp>(defining_op)) {
+      worklist.push_back(
+          GetCalleeFunc(call_op).front().getTerminator()->getOperand(
+              op_result.getResultNumber()));
+    }
+
+    // TODO: b/421069658 - Handle for ops.
+}
+
+  return mesh_origins_with_locs;
+}
+
+// Walk through the MLIR module and get the origins of the use_set of `value`,
+// with the MLIR location. All values in the use_set must arise from either
+// entrypoint func return values or mpmd.assign ops.
+LocToOrigins GetUseOriginsWithLocs(
+    ValueRange root,
+    DenseMap<StringRef, SmallVector<CallOp>>& lazy_call_ops_by_callee) {
+  LocToOrigins mesh_origins_with_locs;
+
+  std::vector<Value> worklist(root.begin(), root.end());
+  llvm::DenseSet<Value> visited;
+
+  while (!worklist.empty()) {
+    Value current_value = worklist.back();
+    worklist.pop_back();
+    if (visited.contains(current_value)) {
+      continue;
+    }
+    visited.insert(current_value);
+
+    for (OpOperand& use : current_value.getUses()) {
+      // Case 1: AssignOp
+      if (auto assign_op = dyn_cast<mpmd::AssignOp>(use.getOwner())) {
+        mesh_origins_with_locs.emplace_back(
+            assign_op.getLoc(),
+            SmallVector<MeshWithOriginsAttr>{assign_op.getMeshWithOrigin()});
+        continue;
+      }
+
+      // Case 2: The value is used by a meshless op.
+      if (IsMeshlessOp(use.getOwner())) {
+        for (Value res : use.getOwner()->getResults()) {
+          worklist.push_back(res);
+        }
+        continue;
+      }
+
+      // Case 3: Returned by a function.
+      if (auto return_op = dyn_cast<func::ReturnOp>(use.getOwner())) {
+        Operation* parent_op = return_op->getParentOp();
+        FuncOp func_op = cast<FuncOp>(parent_op);
+
+        // Case 3a: The value is returned by the entrypoint function.
+        if (IsEntryPointFunction(func_op)) {
+          mesh_origins_with_locs.emplace_back(
+              GetResultInfoLoc(func_op, use.getOperandNumber())
+                  .value_or(return_op->getLoc()),
+              SmallVector<MeshWithOriginsAttr>{});
+          continue;
+        }
+
+        // Case 3b: The value is returned by a mpmd.call.
+        for (CallOp call_op :
+             GetMaybeCachedCallOps(func_op, lazy_call_ops_by_callee)) {
+          worklist.push_back(call_op.getResult(use.getOperandNumber()));
+        }
+        continue;
+      }
+
+      // Case 4: Call Op
+      if (auto call_op = dyn_cast<CallOp>(use.getOwner())) {
+        worklist.push_back(
+            GetCalleeFunc(call_op).getArgument(use.getOperandNumber()));
+        continue;
+      }
+
+      // Case 5: For Op
+      // TODO: b/421069658 - Handle for ops.
+    }
+  }
+  return mesh_origins_with_locs;
+}
+
+// Prints mesh with origins in the format of
+// `mesh[origin1,origin2], mesh2[origin3]`.
+std::string PrintMeshWithOrigins(MeshWithOriginsAttr mesh_with_origins) {
+  std::string result;
+  llvm::raw_string_ostream str_stream(result);
+  str_stream << mesh_with_origins.getMeshName();
+  str_stream << "["
+             << llvm::join(llvm::map_range(mesh_with_origins.getOrigins(),
+                                           [](const OriginAttr& origin) {
+                                             return origin.getOriginLabel();
+                                           }),
+                           ",")
+             << "]";
+  return result;
+}
+
+std::string PrintMeshesWithOrigins(
+    ArrayRef<MeshWithOriginsAttr> meshes_with_origins) {
+  return llvm::join(llvm::map_range(meshes_with_origins, PrintMeshWithOrigins),
+                    ", ");
+}
+
+// Prints the loc to origins in the format of one of:
+// - `loc1 - Input <loc1>: mesh1[origin1], mesh2[origin3]`.
+// - `loc1 - Output <loc1>: mesh1[origin1], mesh2[origin3]`.
+// - `loc1 - named_computation "origin1": mesh1[origin1], mesh2[origin3]`.
+// - `loc1 - named_tensor "origin1": mesh1[origin1], mesh2[origin3]`.
+std::string PrintLocToOrigins(const LocToOrigins& loc_to_origins,
+                              StringRef loc_str_prefix = "",
+                              bool is_input = true) {
+  std::string result;
+  llvm::raw_string_ostream str_stream(result);
+
+  for (auto& [loc, meshes_with_origins] : loc_to_origins) {
+    std::string loc_str = PrintLocation(loc);
+    bool append_loc = true;
+    str_stream << "\n";
+    // TODO: b/396601755 - Avoid having to string check to detect named
+    // computation and named tensor.
+    if (llvm::StringRef(loc_str).contains("named_computation")) {
+      str_stream << loc_str_prefix << " - named_computation \""
+                 << meshes_with_origins[0].getOrigins()[0].getOriginLabel()
+                 << "\": ";
+
+    } else if (llvm::StringRef(loc_str).contains("named_tensor")) {
+      str_stream << loc_str_prefix << " - named_tensor \""
+                 << meshes_with_origins[0].getOrigins()[0].getOriginLabel()
+                 << "\": ";
+    } else {
+      if (is_input) {
+        str_stream << loc_str_prefix << " - Input " << loc_str << ": ";
+      } else {
+        str_stream << loc_str_prefix << " - Output " << loc_str << ": ";
+      }
+      append_loc = false;
+    }
+
+    str_stream << PrintMeshesWithOrigins(meshes_with_origins);
+
+    if (append_loc) {
+      str_stream << loc_str;
+    }
+  }
+  return result;
+}
+
+class InferMeshValidateSrcSetNotEmptyPass
+    : public impl::InferMeshValidateSrcSetNotEmptyPassBase<
+          InferMeshValidateSrcSetNotEmptyPass> {
+  using InferMeshValidateSrcSetNotEmptyPassBase::
+      InferMeshValidateSrcSetNotEmptyPassBase;
+
+  void runOnOperation() override {
+    error_count_ = 0;
+    const int max_errors = GetValidatedMaxErrors(errorLimit);
+
+    for (FuncOp func : GetMpmdFunctions(getOperation())) {
+      runOnFunc(func, max_errors);
+    }
+  }
+
+  void runOnFunc(FuncOp func, const int max_errors) {
+    // The walk is interrupted if we hit the limit number of errors we can emit.
+    func.walk([&](Operation* op) {
+      if (error_count_ == max_errors) {
+        return WalkResult::interrupt();
+      }
+      if (auto fragment = dyn_cast<FragmentOp>(op)) {
+        // Skip fragment ops and their regions for efficiency, as they are
+        // already assigned.
+        return WalkResult::skip();
+      }
+      if (IsMeshlessOp(op) && !ValidateMeshlessOpHasNonEmptySrcSet(op)) {
+        error_count_++;
+        return WalkResult::skip();
+      }
+      return WalkResult::advance();
+    });
+
+    for (int64_t arg_num = 0;
+         arg_num < func.getNumArguments() && error_count_ < max_errors;
+         ++arg_num) {
+      if (!ValidateMpmdCalleeArgHasNonEmptySrcSet(func, arg_num)) {
+        error_count_++;
+      }
+    }
+
+    // Catch the errors emitted if any.
+    if (error_count_ > 0) {
+      return signalPassFailure();
+    }
+  }
+
+  // Returns false for the first error spotted in an error chain. Otherwise
+  // returns true.
+  //
+  // If the op has an empty src_set, then we emit an error and return false.
+  // Also if the op should be a cross-mesh reduction but it is not, since in
+  // that case it would also have an empty src_set if we didn't infer the
+  // cross-mesh reduction.
+  bool ValidateMeshlessOpHasNonEmptySrcSet(Operation* op) {
+    std::optional<SetVector<StringRef>> src_set = GetSrcMeshes(op);
+    if (!src_set.has_value()) {
+      // src_set is not present means that the op can be assigned to any mesh,
+      // so any assignment is possible.
+      return true;
+    }
+
+    if (!src_set->empty() && !IsMpmdReduceAnnotated(op)) {
+      return true;
+    }
+
+    SDY_CHECK(!(IsMpmdReduceAnnotated(op) && src_set->empty()))
+        << "It should not be possible for an mpmd-reduce-annotated op to have "
+           "an empty src_set.";
+
+    if ((src_set->empty() && !IsFirstOpWithEmptySrcSet(op)) ||
+        (IsMpmdReduceAnnotated(op) && !IsFirstOpInReductionChain(op))) {
+      return true;
+    }
+    op->emitError(MeshlessOpEmptySrcSetError(op, IsMpmdReduceAnnotated(op)));
+    return false;
+  }
+
+  bool IsFirstOpWithEmptySrcSet(Operation* op) {
+    for (OpOperand& operand : op->getOpOperands()) {
+      std::optional<SetVector<StringRef>> operand_src_set =
+          GetSrcMeshes(operand);
+      if (operand_src_set && operand_src_set->empty()) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  bool IsFirstOpInReductionChain(Operation* op) {
+    for (Value operand : op->getOperands()) {
+      if (Operation* defining_op = operand.getDefiningOp();
+          defining_op && IsMpmdReduceAnnotated(defining_op)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  bool ValidateMpmdCalleeArgHasNonEmptySrcSet(FuncOp callee, int arg_num) {
+    std::optional<SetVector<StringRef>> src_set = GetSrcMeshes(callee, arg_num);
+    if (!src_set || !src_set->empty()) {
+      return true;
+    }
+
+    // Don't emit an error if the empty src_set is propagated from elsewhere.
+    if (!IsCalleeFirstOpWithEmptySrcSet(callee, arg_num)) {
+      return true;
+    }
+
+    emitError(callee->getLoc(), CalleeEmptySrcSetError(callee, arg_num));
+    return false;
+  }
+
+  bool IsCalleeFirstOpWithEmptySrcSet(FuncOp callee, int arg_num) {
+    for (CallOp call_op :
+         GetMaybeCachedCallOps(callee, lazy_call_ops_by_callee_)) {
+      std::optional<SetVector<StringRef>> operand_src_set =
+          GetSrcMeshes(call_op->getOpOperand(arg_num));
+      if (operand_src_set && operand_src_set->empty()) {
+        return false;
+      }
+    }
+
+    return true;
+  }
+
+  std::string MeshlessOpEmptySrcSetError(Operation* op,
+                                         bool is_cross_mesh_reduction_error) {
+    std::string error_str;
+    llvm::raw_string_ostream error_stream(error_str);
+    // Log errors for the op.
+    error_stream
+        << "Mesh assignment is not possible for op as its operands are on "
+           "conflicting meshes and thus we need to transfer some of the "
+           "operands. Add an explicit transfer to fix this.\n";
+
+    if (is_cross_mesh_reduction_error) {
+      error_stream << "To handle this automatically, set "
+                      "`mpmd_infer_cross_mesh_reductions` in the "
+                      "partitioning options.";
+    } else {
+      error_stream << "To handle this automatically, set "
+                      "`mpmd_infer_transfers` in the partitioning options.";
+    }
+    error_stream << kErrorMessageSeparator;
+    error_stream << "Op: \n\t" << PrintOperationForLog(op);
+
+    // Log the meshes and details of the op operands.
+    error_stream << "\n\nOperands with possibly conflicting meshes:\n";
+    for (OpOperand& operand : op->getOpOperands()) {
+      std::optional<SetVector<StringRef>> operand_src_set =
+          GetSrcMeshes(operand);
+      LocToOrigins loc_to_origins =
+          GetSrcOriginsWithLocs(operand.get(), lazy_call_ops_by_callee_);
+
+      error_stream << "\n - Operand " << operand.getOperandNumber();
+      if (operand_src_set) {
+        error_stream << " can be assigned to meshes {"
+                     << llvm::join(operand_src_set->getArrayRef(), ",")
+                     << "} which originates from:";
+      } else {
+        error_stream << " can be assigned to any mesh and originates from the "
+                        "intersections of:";
+      }
+      error_stream << PrintLocToOrigins(loc_to_origins,
+                                        /*loc_str_prefix=*/"  ");
+    }
+
+    auto use_meshes = GetUseMeshes(op);
+    if (!use_meshes.empty()) {
+      error_stream << "\n\nOp is used in meshes {"
+                   << llvm::join(use_meshes, ",")
+                   << "} which originates from the union of:";
+    } else {
+      error_stream << "\n\nOp is used in:";
+    }
+    error_stream << PrintLocToOrigins(
+        GetUseOriginsWithLocs(op->getResults(), lazy_call_ops_by_callee_),
+        /*loc_str_prefix=*/"", /*is_input=*/false);
+
+    error_stream << "\n\nOp stack trace:\n"
+                 << PrintStackTraceFromLoc(op->getLoc()) << "\n";
+    error_stream << kErrorMessageSeparator;
+    return error_str;
+  }
+
+  std::string CalleeEmptySrcSetError(FuncOp callee, int arg_num) {
+    std::string error_str;
+    llvm::raw_string_ostream error_stream(error_str);
+    // Log errors for the callee. Don't print the callee itself, since it is
+    // usually very verbose, so just print the name.
+    error_stream
+        << "Mesh assignment is not possible for arg" << arg_num
+        << " of mpmd.call \"" << SanitizeCallName(callee.getSymName()).str()
+        << "\" as its caller operands are on "
+           "conflicting meshes and thus we need to transfer some of the "
+           "operands. Add an explicit transfer to fix this.\n";
+
+    error_stream << "To handle this automatically, set "
+                    "`mpmd_infer_transfers` in the partitioning options.";
+
+    error_stream << kErrorMessageSeparator;
+
+    BlockArgument arg = callee.getArgument(arg_num);
+    error_stream
+        << "\nmpmd.call was called "
+        << GetMaybeCachedCallOps(callee, lazy_call_ops_by_callee_).size()
+        << " times and can be assigned to meshes: {"
+        << llvm::join(GetSrcSet(callee, arg_num).MeshNamesOrEmpty(), ",")
+        << "} which originates from the intersection of:";
+    error_stream << PrintLocToOrigins(
+        GetSrcOriginsWithLocs(arg, lazy_call_ops_by_callee_));
+
+    auto use_meshes = GetArgUseSet(callee, arg_num).MeshNamesOrEmpty();
+    if (!use_meshes.empty()) {
+      error_stream << "\n\nArg is used in meshes {"
+                   << llvm::join(use_meshes, ",")
+                   << "} which originates from the union of:";
+    } else {
+      error_stream << "\n\nArg is used in:";
+    }
+    error_stream << PrintLocToOrigins(
+        GetUseOriginsWithLocs(arg, lazy_call_ops_by_callee_));
+
+    error_stream << "\n\nmpmd.call Stack trace:\n"
+                 << PrintStackTraceFromLoc(callee.getLoc()) << "\n";
+
+    error_stream << "\n\nSample arg users:";
+    for (Operation* user : arg.getUsers()) {
+      error_stream << "\n - " << PrintOperationForLog(user)
+                   << PrintStackTraceFromLoc(user->getLoc());
+    }
+
+    error_stream << kErrorMessageSeparator;
+    return error_str;
+  }
+
+  DenseMap<StringRef, SmallVector<CallOp>> lazy_call_ops_by_callee_;
+  int error_count_ = 0;
+};
+
+// Attribute used to mark an op as visited by the validation pass, when
+// validation fails. This is used to avoid emitting the same error multiple
+// times.
+inline constexpr StringRef kVisitedFailure = "mpmd.visited_failure";
+
+void SetVisitedFailureAttr(Operation* op) {
+  op->setAttr(kVisitedFailure, UnitAttr::get(op->getContext()));
+}
+void SetVisitedFailureAttr(FuncOp op, int arg_num) {
+  op.setArgAttr(arg_num, kVisitedFailure, UnitAttr::get(op.getContext()));
+}
+bool IsFailureAndMeshlessOpVisited(Operation* op) {
+  if (IsMeshlessOp(op)) {
+    return op->hasAttr(kVisitedFailure);
+  }
+  return false;
+}
+bool IsFailureAndCalleeArgVisited(FuncOp op, int arg_num) {
+  return op.getArgAttr(arg_num, kVisitedFailure) != nullptr;
+}
+bool IsFailureAndOpVisited(OpOperand& use) {
+  if (auto call_op = dyn_cast<CallOp>(use.getOwner())) {
+    FuncOp callee_func = GetCalleeFunc(call_op);
+    return IsFailureAndCalleeArgVisited(callee_func, use.getOperandNumber());
+  }
+  return IsFailureAndMeshlessOpVisited(use.getOwner());
+}
+
+class InferMeshValidateNoAdditionalTransfersNeededPass
+    : public impl::InferMeshValidateNoAdditionalTransfersNeededPassBase<
+          InferMeshValidateNoAdditionalTransfersNeededPass> {
+  using InferMeshValidateNoAdditionalTransfersNeededPassBase::
+      InferMeshValidateNoAdditionalTransfersNeededPassBase;
+
+  void runOnOperation() override {
+    error_count_ = 0;
+    emitted_error_count_ = 0;
+    const int max_errors = GetValidatedMaxErrors(errorLimit);
+
+    SmallVector<FuncOp> mpmd_funcs = GetMpmdFunctions(getOperation());
+
+    // Run on non-entry point functions first, so that the call-ops are
+    // validated before the entry point functions.
+    for (FuncOp func : mpmd_funcs) {
+      if (!IsEntryPointFunction(func)) {
+        runOnFunc(func, max_errors);
+      }
+    }
+
+    for (FuncOp func : mpmd_funcs) {
+      if (IsEntryPointFunction(func)) {
+        runOnFunc(func, max_errors);
+      }
+    }
+  }
+
+  void runOnFunc(FuncOp func, const int max_errors) {
+    // The walk is interrupted if we hit the limit number of errors we can emit.
+    func.walk<WalkOrder::PreOrder, ReverseIterator>([&](Operation* op) {
+      if (error_count_ == max_errors) {
+        return WalkResult::interrupt();
+      }
+      if (auto fragment = dyn_cast<FragmentOp>(op)) {
+        // Skip fragment ops and their regions for efficiency, as they are
+        // already assigned.
+        return WalkResult::skip();
+      }
+      if (IsMeshlessOp(op) || isa<AssignOp>(op)) {
+        ValidationResult result = ValidateMeshlessOpDoesNotNeedTransfer(op);
+        if (ValidationResult::kOk != result) {
+          error_count_++;
+        }
+        if (ValidationResult::kError == result) {
+          emitted_error_count_++;
+        }
+      }
+      if (auto call_op = dyn_cast<CallOp>(op);
+          call_op && IsCallOpInCallChain(call_op)) {
+        ValidateCallOpChainUsesMatch(call_op, error_count_,
+                                     emitted_error_count_);
+      }
+      return WalkResult::advance();
+    });
+    for (int64_t arg_num = 0;
+         arg_num < func.getNumArguments() && emitted_error_count_ < max_errors;
+         ++arg_num) {
+      ValidationResult result =
+          ValidateCalleeArgDoesNotNeedTransfer(func, arg_num);
+      if (ValidationResult::kOk != result) {
+        error_count_++;
+      }
+      if (ValidationResult::kError == result) {
+        emitted_error_count_++;
+      }
+    }
+
+    // Catch the errors emitted if any.
+    if (error_count_ > 0) {
+      if (emitted_error_count_ == 0) {
+        SDY_LOG(ERROR)
+            << "No errors emitted, but error count is " << error_count_
+            << ". This means we've failed to emit an error and is a bug "
+               "in the error handling.";
+      }
+      return signalPassFailure();
+    }
+  }
+
+  ValidationResult ValidateCallOpChainUsesMatch(CallOp call_op,
+                                                int& error_count,
+                                                int& emitted_error_count) {
+    auto callee_func = GetCalleeFunc(call_op);
+    for (auto result : call_op->getOpResults()) {
+      for (OpOperand& use : result.getUses()) {
+        if (auto user_call_op = dyn_cast<CallOp>(use.getOwner());
+            !user_call_op || user_call_op.getCallee() != call_op.getCallee()) {
+          continue;
+        }
+
+        MeshesWithOrigins res_use_set =
+            GetResUseSet(callee_func, result.getResultNumber());
+        MeshesWithOrigins arg_use_set =
+            GetArgUseSet(callee_func, use.getOperandNumber());
+
+        if (!res_use_set.HasSameMeshes(arg_use_set)) {
+          emitError(callee_func->getLoc(),
+                    CallOpChainMismatchError(
+                        callee_func, use.getOperandNumber(),
+                        result.getResultNumber(), arg_use_set, res_use_set));
+          error_count++;
+          emitted_error_count++;
+        }
+      }
+    }
+    return ValidationResult::kOk;
+  }
+
+  std::string CallOpChainMismatchError(FuncOp callee, int arg_num, int res_num,
+                                       const MeshesWithOrigins& arg_use_set,
+                                       const MeshesWithOrigins& res_use_set) {
+    std::string error_str;
+    llvm::raw_string_ostream error_stream(error_str);
+
+    error_stream
+        << "Mesh assignment is not possible for mpmd.call \""
+        << SanitizeCallName(callee.getSymName()).str()
+        << "\" as it passes result " << res_num << " to arg " << arg_num
+        << " but they have mismatching mesh assignments: res: {"
+        << PrintMeshesWithOrigins(res_use_set.ToArray(callee.getContext()))
+        << "}, arg: {"
+        << PrintMeshesWithOrigins(arg_use_set.ToArray(callee.getContext()))
+        << "}. Please reach out if you see this.";
+
+    error_stream << kErrorMessageSeparator;
+
+    error_stream << "\n\nmpmd.call Stack trace:\n"
+                 << PrintStackTraceFromLoc(callee.getLoc()) << "\n";
+
+    return error_str;
+  }
+
+  // Validates that the necessary mesh assignments are possible
+  // without introducing any transfer ops, i.e. that the use_set is contained in
+  // the src_set.
+  //
+  // This must be used with a reverse iterator walk, as we want to visit the
+  // users before the op.
+  // To avoid emitting the same error multiple times, we mark the op as having
+  // failed validation if we emit an error.
+  // TODO(b/396601755): Return one error for all errors in an error chain.
+  ValidationResult ValidateMeshlessOpDoesNotNeedTransfer(Operation* op) {
+    std::optional<SetVector<StringRef>> src_set = GetSrcMeshes(op);
+    SetVector<StringRef> use_set = GetUseMeshes(op);
+
+    if (!src_set.has_value()) {
+      // src_set is not present means that the op can be assigned to any mesh,
+      // so any assignment is possible.
+      return ValidationResult::kOk;
+    }
+
+    // This should be caught by ValidateSrcSetNotEmpty.
+    SDY_CHECK(!src_set->empty())
+        << "This should have been caught by an earlier validation check. Reach "
+           "out if you see this.";
+
+    if (llvm::set_is_subset(use_set, *src_set)) {
+      return ValidationResult::kOk;
+    }
+
+    // Only emit error if none of the users have already been marked as having
+    // failed validation.
+    if (llvm::any_of(op->getUses(), IsFailureAndOpVisited)) {
+      SetVisitedFailureAttr(op);
+      return ValidationResult::kErrorButDontEmit;
+    }
+
+    op->emitError(
+        MeshlessOpError(op, use_set.getArrayRef(), src_set->getArrayRef()));
+    SetVisitedFailureAttr(op);
+    return ValidationResult::kError;
+  }
+
+  std::string MeshlessOpError(Operation* op, ArrayRef<StringRef> use_set,
+                              ArrayRef<StringRef> src_set) {
+    std::string error_str;
+    llvm::raw_string_ostream error_stream(error_str);
+
+    // Log errors for the op.
+    error_stream
+        << "Mesh assignment is not possible for op as it is used in {"
+        << llvm::join(use_set, ",") << "} but it can only be placed on {"
+        << llvm::join(src_set, ",")
+        << "} without introducing a transfer. Add an explicit transfer to fix "
+           "this.\n";
+
+    error_stream << "To handle this automatically, set "
+                    "`mpmd_infer_transfers` in the partitioning options.";
+    error_stream << kErrorMessageSeparator;
+    error_stream << "Op: \n\t" << PrintOperationForLog(op);
+
+    error_stream << "\n\nCan be assigned to meshes: {"
+                 << llvm::join(src_set, ",")
+                 << "} which originates from the intersection of:";
+    error_stream << PrintLocToOrigins(
+        GetSrcOriginsWithLocs(op->getOperands(), lazy_call_ops_by_callee_));
+
+    error_stream << "\n\nBut was used in meshes: {" << llvm::join(use_set, ",")
+                 << "} which originates from the union of:";
+    error_stream << PrintLocToOrigins(
+        GetUseOriginsWithLocs(op->getOperands(), lazy_call_ops_by_callee_),
+        /*loc_str_prefix=*/"", /*is_input=*/false);
+
+    error_stream << "\n\nOp stack trace:\n"
+                 << PrintStackTraceFromLoc(op->getLoc()) << "\n";
+
+    // Log the meshes and details of the op operands.
+    error_stream << kErrorMessageSeparator;
+    return error_str;
+  }
+
+  // Validates that the necessary mesh assignments are possible on CallOps
+  // without introducing any transfer ops, i.e. that the use_set is contained in
+  // the src_set.
+  //
+  // Returns `true` if validation succeeded (i.e., no error was emitted);
+  // `false` otherwise, in which case an error was emitted.
+  // TODO(b/396601755): Return one error for all errors in an error chain.
+  ValidationResult ValidateCalleeArgDoesNotNeedTransfer(FuncOp func,
+                                                        int arg_num) {
+    std::optional<SetVector<StringRef>> src_set = GetSrcMeshes(func, arg_num);
+    SetVector<StringRef> use_set =
+        GetArgUseSet(func, arg_num).MeshNamesOrEmpty();
+
+    if (!src_set.has_value()) {
+      // src_set is not present means that the op can be assigned to any mesh,
+      // so any assignment is possible.
+      return ValidationResult::kOk;
+    }
+
+    // This should be caught by ValidateSrcSetNotEmpty.
+    SDY_CHECK(!src_set->empty())
+        << "This should have been caught by an earlier validation check. Reach "
+           "out if you see this.";
+
+    if (llvm::set_is_subset(use_set, *src_set)) {
+      return ValidationResult::kOk;
+    }
+
+    SetVisitedFailureAttr(func, arg_num);
+
+    // Only emit error if none of the users have already been marked as having
+    // failed validation.
+    if (llvm::any_of(func.getArgument(arg_num).getUses(),
+                     IsFailureAndOpVisited)) {
+      return ValidationResult::kErrorButDontEmit;
+    }
+
+    emitError(func->getLoc(),
+              CalleeArgError(func, arg_num, use_set.getArrayRef(),
+                             src_set->getArrayRef()));
+
+    return ValidationResult::kError;
+  }
+
+  std::string CalleeArgError(FuncOp callee, int arg_num,
+                             ArrayRef<StringRef> use_set,
+                             ArrayRef<StringRef> src_set) {
+    std::string error_str;
+    llvm::raw_string_ostream error_stream(error_str);
+
+    // Log errors for the op.
+    error_stream << "Mesh assignment is not possible for arg" << arg_num
+                 << " of mpmd.call \""
+                 << SanitizeCallName(callee.getSymName()).str()
+                 << "\" as it is used in {" << llvm::join(use_set, ",")
+                 << "} but it can only be placed on {"
+                 << llvm::join(src_set, ",")
+                 << "} without introducing a transfer. Add an explicit "
+                    "transfer to fix this.\n";
+
+    error_stream << "To handle this automatically, set "
+                    "`mpmd_infer_transfers` in the partitioning options.";
+
+    error_stream << kErrorMessageSeparator;
+
+    BlockArgument arg = callee.getArgument(arg_num);
+    error_stream
+        << "\nmpmd.call was called "
+        << GetMaybeCachedCallOps(callee, lazy_call_ops_by_callee_).size()
+        << " times and can be assigned to meshes: {" << llvm::join(src_set, ",")
+        << "} which originates from the intersection of:";
+    error_stream << PrintLocToOrigins(
+        GetSrcOriginsWithLocs(arg, lazy_call_ops_by_callee_));
+
+    error_stream << "\n\nArg is used in meshes: {" << llvm::join(use_set, ",")
+                 << "} which originates from the union of:";
+    error_stream << PrintLocToOrigins(
+        GetUseOriginsWithLocs(arg, lazy_call_ops_by_callee_));
+
+    error_stream << "\n\nmpmd.call Stack trace:\n"
+                 << PrintStackTraceFromLoc(callee.getLoc()) << "\n";
+
+    error_stream << "\n\nSample arg users:";
+    for (Operation* user : arg.getUsers()) {
+      error_stream << "\n - " << PrintOperationForLog(user)
+                   << PrintStackTraceFromLoc(user->getLoc());
+    }
+
+    error_stream << kErrorMessageSeparator;
+
+    return error_str;
+  }
+
+  DenseMap<StringRef, SmallVector<CallOp>> lazy_call_ops_by_callee_;
+  int error_count_ = 0;
+  int emitted_error_count_ = 0;
+};
+
+}  // namespace
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/insert_nameless_clones_of_negligible_ops.cc b/shardy/dialect/mpmd/transforms/import/insert_nameless_clones_of_negligible_ops.cc
new file mode 100644
index 0000000..3754c29
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/insert_nameless_clones_of_negligible_ops.cc
@@ -0,0 +1,82 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_INSERTNAMELESSCLONEOFNEGLIBLEOPSPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+// Returns the producer of `fragment_result` if it is considered a negligible,
+// i.e., it has no operands and returns a single result.
+Operation* FindNegligibleProducer(OpResult fragment_result,
+                                  ReturnOp fragment_return) {
+  Value returned_value =
+      fragment_return.getOperand(fragment_result.getResultNumber());
+  if (Operation* producer = returned_value.getDefiningOp();
+      producer && producer->getNumOperands() == 0 &&
+      producer->getNumResults() == 1) {
+    return producer;
+  }
+  return nullptr;
+}
+
+class InsertNamelessCloneOfNeglibleOpsPass
+    : public impl::InsertNamelessCloneOfNeglibleOpsPassBase<
+          InsertNamelessCloneOfNeglibleOpsPass> {
+  using InsertNamelessCloneOfNeglibleOpsPassBase::
+      InsertNamelessCloneOfNeglibleOpsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) final {
+    IRRewriter rewriter(func_op.getContext());
+    func_op.walk([&rewriter](NamedComputationOp named_computation) {
+      rewriter.setInsertionPointAfter(named_computation);
+      Operation* terminator =
+          named_computation.getBody()->getTerminator();
+      for (OpResult result : named_computation->getResults()) {
+        if (Operation* producer =
+                FindNegligibleProducer(result, cast<ReturnOp>(terminator))) {
+          // We only replace uses that represent ops that need mesh assignment.
+          // A function's return will never be assigned to a mesh and therefore
+          // should not be affected. Moreover, doing so could cause problems
+          // when a named_computation produces a constant directly returned by
+          // the function (e.g., as observed in an init function), as they may
+          // have been explicitly assigned by the user with a named_computation,
+          // i.e., such assignment must be preserved.
+          rewriter.replaceUsesWithIf(
+              result, Clone(rewriter, *producer, {})->getResult(0),
+              [](OpOperand& use) {
+                return !isa<func::ReturnOp>(use.getOwner());
+              });
+        }
+      }
+    });
+  }
+};
+
+}  // namespace
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/introduce_transfers.cc b/shardy/dialect/mpmd/transforms/import/introduce_transfers.cc
new file mode 100644
index 0000000..4abc241
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/introduce_transfers.cc
@@ -0,0 +1,304 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Rewrite/FrozenRewritePatternSet.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+#include "stablehlo/dialect/StablehloOps.h"
+
+using ::mlir::func::FuncOp;
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_INTRODUCETRANSFERSPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+// Replaces the AssignOp of an UnassignOp with a TransferOp, or a noop if the
+// transfer is not needed. It reuses existing transfers if possible. For a given
+// value, this create at most one transfer of that value to a given mesh.
+class AssignOfUnassignPattern : public OpRewritePattern<AssignOp> {
+  using OpRewritePattern<AssignOp>::OpRewritePattern;
+
+ public:
+  LogicalResult matchAndRewrite(AssignOp op,
+                                PatternRewriter& rewriter) const override {
+    auto unassign_op = op.getTensor().getDefiningOp<UnassignOp>();
+    if (!unassign_op) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "Expected the operand of the AssignOp to be the result of an "
+                "UnassignOp";
+      });
+    }
+
+    Type target_type = op.getType();
+    TypedValue<MeshTensorType> op_to_transfer = unassign_op.getTensor();
+    if (op_to_transfer.getType() == target_type) {
+      // Noop transfer, so just replace with tensor.
+      rewriter.replaceOp(op, op_to_transfer);
+      return success();
+    }
+
+    auto existing_transfer_it =
+        llvm::find_if(op_to_transfer.getUsers(), [target_type](Operation* op) {
+          if (auto trf = DynCastInterMeshTransfer(op)) {
+            return trf.getType() == target_type;
+          }
+          return false;
+        });
+    if (existing_transfer_it != op_to_transfer.getUsers().end()) {
+      // We create the transfer at the user location, but we don't necessarily
+      // iterate the users in program order. So the transfer might be created on
+      // a later user, and comes after the current user, and we need to move it
+      // before the current user. Note that we could also create the transfer
+      // right after the transfer operand's defining op, but we don't for now
+      // to preserve existing semantics.
+      if (op->isBeforeInBlock(*existing_transfer_it)) {
+        existing_transfer_it->moveBefore(op);
+      }
+      rewriter.replaceOp(op, existing_transfer_it->getResults());
+    } else {
+      rewriter.replaceOpWithNewOp<TransferOp>(op, op.getType(),
+                                              unassign_op.getTensor());
+    }
+    return success();
+  }
+};
+
+// This checks if all `stablehlo.add` operands are UnassignOps, for additions of
+// two or more values. Because AddOp is a binary op, this means that we have to
+// check recursive adds. E.g., handle something like
+//
+// u_{i} = unassign(...)
+// x = stablehlo.add u_0, u_1
+// y = stablehlo.add x, u_2
+// z = stablehlo.add y, u_3
+//
+// which can also be expressed as (u_0 + u_1 + u_2 + u_3).
+//
+// For simplicity, we only handle the case where the entire addition chain has a
+// single user.
+//
+// This method is recursive, but the depth shouldn't be excessive since it is
+// unlikely that we have a cross-mesh reduction with many operands.
+LogicalResult IsAddOfUnassigns(stablehlo::AddOp add, Operation* user,
+                               PatternRewriter& rewriter) {
+  if (!add->hasOneUse()) {
+    return rewriter.notifyMatchFailure(user, [&](Diagnostic& diag) {
+      diag << "Expected AddOp to have only one user";
+    });
+  }
+
+  for (Value operand : add->getOperands()) {
+    if (auto unassign = operand.getDefiningOp<UnassignOp>()) {
+      continue;
+    }
+    if (auto nested_add = operand.getDefiningOp<stablehlo::AddOp>()) {
+      if (failed(IsAddOfUnassigns(nested_add, add, rewriter))) {
+        return failure();
+      }
+      continue;
+    }
+    return rewriter.notifyMatchFailure(user, [&](Diagnostic& diag) {
+      diag << "Expected all operands of " << user
+           << " to be an UnassignOp or AddOp";
+    });
+  }
+  return success();
+}
+
+// When we have a meshless addition between fragments, we want to assign the
+// addition to the consuming mesh and introduce a transfer. E.g.,
+//
+// x = unassign(x') from some_mesh1
+// y = unassign(y') from some_mesh2
+// z = x + y
+// z' = assign(z) to m1
+//
+// ~~>
+//
+// x = unassign(x')
+// y = unassign(y')
+// x'' = assign(x) to m1
+// y'' = assign(y) to m1
+// z' = fragment m1 {
+//    return x'' + y''
+// }
+//
+// More generally, we need to handle the case of adding more than two values:
+// assign(\sum{0..n} unassign(x_i)) ~~> \sum{0..n}(assign(unassign(x_i))).
+//
+// This pass does the assignment of the addition, and the transfer pattern will
+// create the transfer.
+//
+// We do this because in the backward pass of a model (e.g. via jax.grad),
+// a meshless add may appear in this way, and we want to transfer the operands
+// to the consuming mesh for the addition to be done.
+//
+// For simplicity, we handle only the case where the addition has a single user.
+//
+// Note: we could support other operators, but at present there's only a use
+// case for the AddOp.
+class PushAssignBackwardThroughAdd : public OpRewritePattern<AssignOp> {
+  using OpRewritePattern<AssignOp>::OpRewritePattern;
+
+ public:
+  LogicalResult matchAndRewrite(AssignOp assign,
+                                PatternRewriter& rewriter) const override {
+    auto meshless_add = assign.getTensor().getDefiningOp<stablehlo::AddOp>();
+    if (!meshless_add) {
+      return rewriter.notifyMatchFailure(assign, [&](Diagnostic& diag) {
+        diag << "Expected the operand of AssignOp to be an AddOp";
+      });
+    }
+    if (failed(IsAddOfUnassigns(meshless_add, assign, rewriter))) {
+      return failure();
+    }
+    WrapOpWithFragment(meshless_add, assign.getType().getMeshName(), rewriter);
+    return success();
+  }
+};
+
+// Returns the UnassignOp that defines the given value, or nullptr if the value
+// is not defined by an UnassignOp. Walks through CallOps to find the UnassignOp
+// in the callee, if necessary.
+UnassignOp FindUnassignOp(Value value) {
+  if (auto call = value.getDefiningOp<CallOp>()) {
+    FuncOp callee = GetCalleeFunc(call);
+    auto value_in_callee = callee.front().getTerminator()->getOperand(
+        cast<OpResult>(value).getResultNumber());
+    return value_in_callee.getDefiningOp<UnassignOp>();
+  }
+
+  return value.getDefiningOp<UnassignOp>();
+}
+
+// Returns true if the callee function with sources as its callers that use
+// arg can push an unassign op in the callee.
+bool ShouldPushInUnassignOp(ArrayRef<OpOperand*> sources, Value arg) {
+  SDY_CHECK(!sources.empty());
+
+  // If not all of the sources are from unassign op or the meshes are
+  // different we won't push in unassign op.
+  UnassignOp first_source_defining_op = FindUnassignOp(sources.front()->get());
+  if (!first_source_defining_op) {
+    return false;
+  }
+  StringRef first_source_mesh_name =
+      first_source_defining_op.getTensor().getType().getMeshName();
+
+  for (OpOperand* source : ArrayRef<OpOperand*>(sources).drop_front()) {
+    UnassignOp defining_op = FindUnassignOp(source->get());
+    if (!defining_op) {
+      return false;
+    }
+    if (defining_op.getTensor().getType().getMeshName() !=
+        first_source_mesh_name) {
+      return false;
+    }
+  }
+
+  // If the argument is used by any assign op in the callee, push in unassign
+  // op.
+  return llvm::any_of(arg.getUses(), [](const OpOperand& use) {
+    return isa<AssignOp>(use.getOwner());
+  });
+}
+
+// Pushes in unassign ops in the callee if possible.
+void MaybePushInUnassignOp(FuncOp callee, IRRewriter& rewriter) {
+  SmallVector<MpmdDataflowEdge> edges = GetMpmdDataflowEdgesForFuncArgs(callee);
+  // We go through each edge and check if the unassign op can be pushed into
+  // the callee. If so, we push them in and remove the unassign op in the
+  // caller if it is not used anymore.
+  // Each edge (sources, targets) pair is for one of the callee arguments.
+  // sources represents the inputs to the callee from different calls and
+  // targets represents the outputs of the callee.
+  for (auto& [sources, targets] : edges) {
+    BlockArgument callee_arg = cast<BlockArgument>(targets.front());
+    if (!ShouldPushInUnassignOp(sources, callee_arg)) {
+      continue;
+    }
+
+    rewriter.setInsertionPointAfterValue(callee_arg);
+    auto callee_assign = rewriter.create<AssignOp>(
+        callee_arg.getLoc(),
+        FindUnassignOp(sources.front()->get()).getTensor().getType(),
+        callee_arg);
+    auto callee_unassign =
+        rewriter.create<UnassignOp>(callee_arg.getLoc(), callee_assign);
+    rewriter.replaceAllUsesExcept(callee_arg, callee_unassign.getResult(),
+                                  callee_assign);
+  }
+}
+
+class IntroduceTransfersPass
+    : public impl::IntroduceTransfersPassBase<IntroduceTransfersPass> {
+  using IntroduceTransfersPassBase::IntroduceTransfersPassBase;
+
+  LogicalResult initialize(MLIRContext* context) final {
+    RewritePatternSet patternsInternal(context);
+    patternsInternal.add<AssignOfUnassignPattern, PushAssignBackwardThroughAdd>(
+        context);
+    patterns = std::move(patternsInternal);
+
+    return success();
+  }
+
+  void runOnOperation() final {
+    ModuleOp module = getOperation();
+    for (FuncOp func : GetMpmdFunctions(module)) {
+      if (!IsEntryPointFunction(func)) {
+        IRRewriter rewriter(func->getContext());
+        MaybePushInUnassignOp(func, rewriter);
+      }
+    }
+    GreedyRewriteConfig config =
+        GreedyRewriteConfig()
+            .setRegionSimplificationLevel(
+                mlir::GreedySimplifyRegionLevel::Disabled)
+            .enableFolding(false)
+            .enableConstantCSE(false);
+    if (failed(applyPatternsGreedily(module, patterns, config))) {
+      return signalPassFailure();
+    }
+  }
+
+ private:
+  FrozenRewritePatternSet patterns;
+};
+
+}  // namespace
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/map_input_output_to_mesh.cc b/shardy/dialect/mpmd/transforms/import/map_input_output_to_mesh.cc
new file mode 100644
index 0000000..f3aceab
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/map_input_output_to_mesh.cc
@@ -0,0 +1,169 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <optional>
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_origins.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_MAPINPUTOUTPUTTOMESHPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+void CleanUpMemoryKindAttributes(func::FuncOp func) {
+  for (auto [index, arg_type] : llvm::enumerate(func.getArgumentTypes())) {
+    StringAttr memory_kind_attr =
+        func.getArgAttrOfType<StringAttr>(index, kMemoryKindAttr);
+    if (!memory_kind_attr) {
+      continue;
+    }
+    if (auto mesh_tensor_type = dyn_cast<MeshTensorType>(arg_type)) {
+      if (!mesh_tensor_type.getMemoryKind()) {
+        // TODO: b/374994155 - For now, we just ignore this. But going forward,
+        // we should move the attribute to the type.
+        continue;
+      }
+      if (mesh_tensor_type.getMemoryKind() != memory_kind_attr) {
+        // TODO: b/374994155 - This should be an error once we unify input
+        //  assignment with input shardings.
+        SDY_LOG(WARNING) << "Memory kind attribute "
+                         << memory_kind_attr.getValue().str()
+                         << " on function argument " << index
+                         << " does not match the memory kind "
+                         << mesh_tensor_type.getMemoryKind().getValue().str()
+                         << " in its type.";
+      }
+      func.removeArgAttr(index, kMemoryKindAttr);
+    }
+  }
+  for (auto [index, result_type] : llvm::enumerate(func.getResultTypes())) {
+    StringAttr memory_kind_attr =
+        func.getResultAttrOfType<StringAttr>(index, kMemoryKindAttr);
+    if (!memory_kind_attr) {
+      continue;
+    }
+    if (auto mesh_tensor_type = dyn_cast<MeshTensorType>(result_type)) {
+      if (!mesh_tensor_type.getMemoryKind()) {
+        // TODO: b/374994155 - For now, we just ignore this. But going forward,
+        // we should move the attribute to the type.
+        continue;
+      }
+      if (mesh_tensor_type.getMemoryKind() != memory_kind_attr) {
+        // TODO: b/374994155 - This should be an error once we unify output
+        //  assignment with output shardings.
+        SDY_LOG(WARNING) << "Memory kind attribute "
+                         << memory_kind_attr.getValue().str()
+                         << " on function result " << index
+                         << " does not match the memory kind "
+                         << mesh_tensor_type.getMemoryKind().getValue().str()
+                         << " in its type.";
+      }
+      func.removeResultAttr(
+          index, StringAttr::get(func.getContext(), kMemoryKindAttr));
+    }
+  }
+}
+
+class MapInputOutputToMeshPass
+    : public impl::MapInputOutputToMeshPassBase<MapInputOutputToMeshPass> {
+  using MapInputOutputToMeshPassBase::MapInputOutputToMeshPassBase;
+
+ private:
+  MeshTensorType GetMeshTensorType(Value value, StringRef mesh_name) {
+    auto ranked_tensor_type = cast<RankedTensorType>(value.getType());
+    std::pair<StringRef, std::optional<StringRef>> mesh_name_and_memory_kind =
+        TryToExtractMemoryKindFromMeshName(mesh_name);
+    StringRef mesh_name_without_suffix = mesh_name_and_memory_kind.first;
+    StringAttr memory_kind = {};
+    if (mesh_name_and_memory_kind.second.has_value()) {
+      memory_kind = StringAttr::get(value.getContext(),
+                                    mesh_name_and_memory_kind.second.value());
+    }
+    return MeshTensorType::get(value.getContext(), mesh_name_without_suffix,
+                               ranked_tensor_type, memory_kind);
+  }
+
+  void runOnOperation() override {
+    ModuleOp module = getOperation();
+    for (func::FuncOp func : GetMpmdFunctions(module)) {
+      if (!IsEntryPointFunction(func)) {
+        continue;
+      }
+      IRRewriter rewriter(func->getContext());
+
+      // Go through the input index to mesh name map and assign the mesh to the
+      // corresponding argument. Note that the order of this iteration is not
+      // deterministic since llvm::DenseMap is unordered but the order does
+      // not matter here.
+      for (const auto& [input_index, mesh_name] : inputAssignment.value) {
+        SDY_CHECK_GE(input_index, 0) << "Input index must be non-negative.";
+        SDY_CHECK_LT(input_index, func.getNumArguments())
+            << "Input index out of bounds.";
+
+        // Assign the mesh to the argument.
+        Value arg = func.getArgument(input_index);
+        arg.setType(GetMeshTensorType(arg, mesh_name));
+
+        // Unassign the argument mesh before use.
+        rewriter.setInsertionPointAfterValue(arg);
+        auto unassign_op = rewriter.create<UnassignOp>(
+            arg.getLoc(), arg, /*origin=*/kUserInputOrigin);
+        rewriter.replaceAllUsesExcept(arg, unassign_op.getResult(),
+                                      unassign_op);
+      }
+
+      // Go through the output index to mesh name map and assign the mesh to the
+      // corresponding return value. Note that the order of this iteration is
+      // not deterministic since llvm::DenseMap is unordered but the order
+      // does not matter here because we only need the assigned ops to be
+      // present and any order is valid.
+      Operation* return_op = func.getBlocks().back().getTerminator();
+      for (const auto& [output_index, mesh_name] : outputAssignment.value) {
+        SDY_CHECK_GE(output_index, 0) << "Output index must be non-negative.";
+        SDY_CHECK_LT(output_index, func.getNumResults())
+            << "Output index must be less than the number of results.";
+
+        rewriter.setInsertionPoint(return_op);
+        Value output = return_op->getOperand(output_index);
+        auto assign_op = rewriter.create<AssignOp>(
+            GetResultInfoLoc(func, output_index).value_or(output.getLoc()),
+            GetMeshTensorType(output, mesh_name), output,
+            /*origin=*/kUserOutputOrigin);
+        return_op->setOperand(output_index, assign_op.getResult());
+      }
+      // Update the function signature.
+      UpdateFunctionType(func);
+      CleanUpMemoryKindAttributes(func);
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/map_named_ops_to_mpmd_ops.cc b/shardy/dialect/mpmd/transforms/import/map_named_ops_to_mpmd_ops.cc
new file mode 100644
index 0000000..f3ef35e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/map_named_ops_to_mpmd_ops.cc
@@ -0,0 +1,306 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <optional>
+#include <string_view>
+#include <utility>
+
+#include "llvm/ADT/StringRef.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/IRMapping.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_assignment_map.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/sdy/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_MAPNAMEDOPSTOMPMDOPSPASS
+#define GEN_PASS_DEF_INLINENESTEDUSEREXPOSEDOPSPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+void InlineNamedComputation(NamedComputationOp op, RewriterBase& rewriter) {
+  Block* block = op.getBody();
+  auto terminator = cast<ReturnOp>(block->getTerminator());
+  rewriter.inlineBlockBefore(block, op, op->getOperands());
+  rewriter.replaceOp(op, terminator->getOperands());
+  rewriter.eraseOp(terminator);
+}
+
+// Converts a named_computation to a fragment assigning it to a mesh (the first
+// value of `computation_assignment`) and to a stage (the second value of
+// `computation_assignment`) if defined.
+void MapNamedComputationToMesh(
+    NamedComputationOp named_computation_op,
+    const MeshStageAssignment& computation_assignment, IRRewriter& rewriter) {
+  rewriter.setInsertionPoint(named_computation_op);
+  std::pair<StringRef, std::optional<StringRef>> mesh_name_and_memory_kind =
+      TryToExtractMemoryKindFromMeshName(computation_assignment.first);
+  StringRef mesh_name = mesh_name_and_memory_kind.first;
+  if (mesh_name_and_memory_kind.second.has_value()) {
+    // TODO: b/374994155 - Support memory_kind assignment in named computations.
+    SDY_LOG(WARNING) << "Named computation "
+                     << std::string_view(named_computation_op.getName())
+                     << " has memory_kind="
+                     << std::string_view(
+                            mesh_name_and_memory_kind.second.value())
+                     << ", which will be ignored by Shardy:MPMD.";
+  }
+
+  sdy::MeshAttr mesh = GetMeshOrFail(named_computation_op, mesh_name);
+
+  // Create AssignOps for each operand of the named_computation. These will be
+  // the operands of the new FragmentOp.
+  SmallVector<Value> new_operands;
+  Location named_computation_loc = named_computation_op.getLoc();
+
+  IRMapping operand_to_assigned;
+  for (Value operand : named_computation_op.getOperands()) {
+    if (!operand_to_assigned.contains(operand)) {
+      auto assign_op = rewriter.create<AssignOp>(
+          named_computation_loc, operand, mesh_name, mesh,
+          /*origin=*/named_computation_op.getName());
+      operand_to_assigned.map(operand, assign_op.getResult());
+    }
+    // Reuse assign ops for repeated operands.
+    new_operands.push_back(operand_to_assigned.lookup(operand));
+  }
+
+  SmallVector<Type> result_types;
+  for (Value result : named_computation_op.getResults()) {
+    result_types.push_back(MeshTensorType::getFullyReplicated(
+        rewriter.getContext(), mesh_name, mesh,
+        cast<RankedTensorType>(result.getType())));
+  }
+
+  auto user_origin_attrs = ArrayAttr::get(
+      rewriter.getContext(), {named_computation_op.getOriginAttr()});
+
+  // Create a new fragment op replacing the named_computation op.
+  IntegerAttr stage_id = IntegerAttr();
+  if (computation_assignment.second.has_value()) {
+    stage_id = rewriter.getI64IntegerAttr(*computation_assignment.second);
+    SDY_CHECK(!user_origin_attrs.empty())
+        << "Inferred fragments cannot be assigned to stages.";
+  }
+  FragmentOp new_fragment = rewriter.create<FragmentOp>(
+      named_computation_loc, result_types, new_operands, user_origin_attrs,
+      mesh_name, stage_id);
+  new_fragment.getRegion().takeBody(named_computation_op.getRegion());
+
+  // Create UnassignOps for each result of the new fragment.
+  SmallVector<Value> new_results;
+  for (Value new_result : new_fragment.getResults()) {
+    new_results.push_back(
+        rewriter
+            .create<UnassignOp>(named_computation_loc, new_result,
+                                /*origin=*/named_computation_op.getName())
+            .getResult());
+  }
+  rewriter.replaceOp(named_computation_op, new_results);
+}
+
+std::optional<MeshStageAssignment> GetMeshStageAssignment(
+    NamedComputationOp op, const UserAssignmentMap& assignment_map) {
+  // We assume that `assignment_` contains a valid mapping between
+  // named_computations and mesh names, i.e., each key in the mapping
+  // is the name of an actual named_computation in the function being
+  // partitioned and each mesh name is part of the topology.
+  auto assignment_it = assignment_map.find(std::string_view(op.getName()));
+  if (assignment_it == assignment_map.end()) {
+    return std::nullopt;
+  }
+  return assignment_it->second;
+}
+
+std::optional<MeshTensorType> GetMeshTensorTypeFromAssignment(
+    NamedTensorOp op, const UserAssignmentMap& assignment_map) {
+  auto assignment_it = assignment_map.find(std::string_view(op.getName()));
+  if (assignment_it == assignment_map.end()) {
+    return std::nullopt;
+  }
+  // We ignore the stage assignment of any named tensor.
+  // StringRef mesh_name = assignment_it->second.first;
+  std::pair<StringRef, std::optional<StringRef>> mesh_name_and_memory_kind =
+      TryToExtractMemoryKindFromMeshName(assignment_it->second.first);
+  StringRef mesh_name_without_memory_kind = mesh_name_and_memory_kind.first;
+  StringAttr memory_kind = {};
+  if (mesh_name_and_memory_kind.second.has_value()) {
+    memory_kind = StringAttr::get(op.getContext(),
+                                  mesh_name_and_memory_kind.second.value());
+  }
+
+  std::optional<int64_t> stage_id = assignment_it->second.second;
+  if (stage_id.has_value()) {
+    SDY_VLOG(2) << "Named tensor " << std::string_view(op.getName())
+                << " was assigned to stage " << *stage_id
+                << " but this will be ignored by Shardy:MPMD.";
+  }
+  return MeshTensorType::get(op.getContext(), mesh_name_without_memory_kind,
+                             cast<RankedTensorType>(op.getTensor().getType()),
+                             memory_kind);
+}
+
+// Returns whether it succeeded in converting the named_tensor.
+bool MapNamedTensorToUnassignOfAssign(NamedTensorOp named_tensor_op,
+                                      IRRewriter& rewriter,
+                                      const UserAssignmentMap& assignment) {
+  rewriter.setInsertionPoint(named_tensor_op);
+  std::optional<MeshTensorType> mesh_tensor =
+      GetMeshTensorTypeFromAssignment(named_tensor_op, assignment);
+  if (mesh_tensor.has_value()) {
+    auto assign_op = rewriter.create<AssignOp>(
+        named_tensor_op.getLoc(), *mesh_tensor, named_tensor_op.getTensor(),
+        /*origin=*/named_tensor_op.getNameAttr());
+    rewriter.replaceOpWithNewOp<UnassignOp>(
+        named_tensor_op, assign_op.getResult(),
+        /*origin=*/named_tensor_op.getNameAttr());
+  } else {
+    // No references to this name in `assignment_`, thus it's unused, so
+    // remove it.
+    rewriter.replaceOp(named_tensor_op, named_tensor_op.getOperand());
+  }
+  return true;
+}
+
+class InlineNestedUserExposedOpsPass
+    : public impl::InlineNestedUserExposedOpsPassBase<
+          InlineNestedUserExposedOpsPass> {
+  using InlineNestedUserExposedOpsPassBase::
+      InlineNestedUserExposedOpsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    IRRewriter rewriter(func_op.getContext());
+    bool pass_must_signal_failure = false;
+
+    // 1. Inline any named_computation, named tensor, broadcast and reduce ops
+    // that is nested in a named_computation, checking that its mesh assignment
+    // matches that of the parent.
+    func_op.getBody().walk([&](Operation* op) {
+      auto parent = op->getParentOfType<NamedComputationOp>();
+      if (!parent) {
+        return WalkResult::advance();
+      }
+      std::optional<MeshStageAssignment> parent_mesh_assignment =
+          GetMeshStageAssignment(parent, assignment.value);
+      if (auto named_computation = dyn_cast<NamedComputationOp>(op)) {
+        std::optional<MeshStageAssignment> op_assignment =
+            GetMeshStageAssignment(named_computation, assignment.value);
+        if (op_assignment.has_value() &&
+            op_assignment != parent_mesh_assignment) {
+          named_computation.emitError("NamedComputation '")
+              << named_computation.getName()
+              << "' is nested in a NamedComputation '" << parent.getName()
+              << "' which has a different mesh or stage assignment.";
+          pass_must_signal_failure = true;
+        };
+        rewriter.setInsertionPoint(named_computation);
+        InlineNamedComputation(named_computation, rewriter);
+        return WalkResult::advance();
+      }
+      if (auto named_tensor = dyn_cast<NamedTensorOp>(op)) {
+        std::optional<MeshTensorType> mesh_tensor =
+            GetMeshTensorTypeFromAssignment(named_tensor, assignment.value);
+        if (mesh_tensor.has_value() && mesh_tensor->getMemoryKind()) {
+          SDY_LOG(WARNING) << "Named tensor "
+                           << std::string_view(named_tensor.getName())
+                           << " has memory_kind="
+                           << std::string_view(
+                                  mesh_tensor->getMemoryKind().strref())
+                           << ", which will be ignored by Shardy:MPMD has it's "
+                              "nested in a NamedComputation.";
+        }
+        if (mesh_tensor.has_value() &&
+            mesh_tensor->getMeshName() != parent_mesh_assignment->first) {
+          named_tensor.emitError("NamedTensor '")
+              << named_tensor.getName() << "' is nested in a NamedComputation '"
+              << parent.getName() << "' which has a different mesh assignment.";
+          pass_must_signal_failure = true;
+        }
+      }
+      if (isa<NamedTensorOp, BroadcastOp, ReduceOp>(op)) {
+        rewriter.replaceAllUsesWith(op->getResult(0), op->getOperand(0));
+        rewriter.eraseOp(op);
+      }
+      return WalkResult::advance();
+    });
+
+    if (pass_must_signal_failure) {
+      return signalPassFailure();
+    }
+  }
+};
+
+class MapNamedOpsToMpmdOpsPass
+    : public impl::MapNamedOpsToMpmdOpsPassBase<MapNamedOpsToMpmdOpsPass> {
+  using MapNamedOpsToMpmdOpsPassBase::MapNamedOpsToMpmdOpsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    IRRewriter rewriter(func_op.getContext());
+    bool pass_must_signal_failure = false;
+
+    func_op.getBody().walk([&](Operation* op) {
+      if (auto named_computation = dyn_cast<NamedComputationOp>(op)) {
+        if (std::optional<MeshStageAssignment> op_assignment =
+                GetMeshStageAssignment(named_computation, assignment.value)) {
+          MapNamedComputationToMesh(named_computation, *op_assignment,
+                                    rewriter);
+        } else {
+          named_computation.emitError("Top-level NamedComputation '")
+              << named_computation.getName()
+              << "' is not assigned to a mesh in the user-defined "
+                 "named-to-mesh "
+                 "assignment.";
+          pass_must_signal_failure = true;
+        }
+        // No need to visit the body of the named_computation.
+        return WalkResult::skip();
+      }
+
+      if (auto named_tensor = dyn_cast<NamedTensorOp>(op)) {
+        if (!MapNamedTensorToUnassignOfAssign(named_tensor, rewriter,
+                                              assignment.value)) {
+          pass_must_signal_failure = true;
+        }
+      }
+      return WalkResult::advance();
+    });
+
+    if (pass_must_signal_failure) {
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/mesh_assignment_map.cc b/shardy/dialect/mpmd/transforms/import/mesh_assignment_map.cc
new file mode 100644
index 0000000..41e23ba
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/mesh_assignment_map.cc
@@ -0,0 +1,172 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/import/mesh_assignment_map.h"
+
+#include <cstddef>
+#include <cstdint>
+#include <optional>
+#include <string>
+#include <tuple>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace mlir::mpmd {
+
+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                              const UserAssignmentMapOption& option) {
+  llvm::interleaveComma(option.value, os, [&](const auto& entry) {
+    auto [mesh, stage] = entry.second;
+    os << entry.first << "@" << mesh;
+    if (stage) {
+      os << "/" << *stage;
+    }
+  });
+  return os;
+}
+
+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                              const IndexedAssignmentMapOption& option) {
+  llvm::interleaveComma(option.value, os, [&](const auto& entry) {
+    os << entry.first << "@" << entry.second;
+  });
+  return os;
+}
+
+IndexedAssignmentMap ConvertMeshVectorToMap(
+    const std::vector<std::optional<std::string>>& meshes) {
+  IndexedAssignmentMap index_to_mesh_map;
+  for (auto [index, optionalMesh] : llvm::enumerate(meshes)) {
+    if (optionalMesh) {
+      index_to_mesh_map.try_emplace(index, *optionalMesh);
+    }
+  }
+  return index_to_mesh_map;
+}
+
+}  // namespace mlir::mpmd
+
+namespace llvm::cl {
+
+using ::mlir::mpmd::IndexedAssignmentMap;
+using ::mlir::mpmd::IndexedAssignmentMapOption;
+using ::mlir::mpmd::UserAssignmentMap;
+using ::mlir::mpmd::UserAssignmentMapOption;
+
+//===----------------------------------------------------------------------===//
+// UserAssignmentMapOption
+//===----------------------------------------------------------------------===//
+
+template class basic_parser<UserAssignmentMapOption>;
+
+bool parser<UserAssignmentMapOption>::parse(Option& opt, StringRef,
+                                            StringRef arg,
+                                            UserAssignmentMapOption& value) {
+  UserAssignmentMap& assignment = value.value;
+  StringRef cur;
+  while (!arg.empty()) {
+    // This allows a redundant comma as the last character.
+    std::tie(cur, arg) = arg.split(',');
+    cur = cur.trim();
+    size_t name_separator_index = cur.find('@');
+    if (name_separator_index == StringRef::npos) {
+      return opt.error(
+          "Assignment must contain '@' denoting a name assigned to a mesh (and "
+          "stage), got: " +
+          cur);
+    }
+    StringRef name = cur.take_front(name_separator_index);
+    StringRef target = cur.drop_front(name_separator_index + 1);
+    size_t mesh_separator_index = target.find('/');
+    if (mesh_separator_index == StringRef::npos) {
+      assignment[name.str()] = std::make_pair(target, std::nullopt);
+    } else {
+      StringRef stage_str = target.drop_front(mesh_separator_index + 1);
+      int64_t stage;
+      if (stage_str.getAsInteger(10, stage)) {
+        return opt.error("Failed to parse stage number: " + stage_str);
+      }
+      assignment[name.str()] =
+          std::make_pair(target.take_front(mesh_separator_index), stage);
+    }
+  }
+  return false;
+}
+
+void parser<UserAssignmentMapOption>::printOptionDiff(
+    const Option& opt, const UserAssignmentMapOption& value,
+    const OptVal& defaultValue, size_t globalWidth) const {
+  printOptionName(opt, globalWidth);
+  outs() << "= " << value;
+  if (defaultValue.hasValue()) {
+    outs().indent(2) << " (default: " << defaultValue.getValue() << ")";
+  }
+  outs() << "\n";
+}
+
+void parser<UserAssignmentMapOption>::anchor() {}
+
+//===----------------------------------------------------------------------===//
+// IndexedAssignmentMapOption
+//===----------------------------------------------------------------------===//
+
+template class basic_parser<IndexedAssignmentMapOption>;
+
+bool parser<IndexedAssignmentMapOption>::parse(
+    Option& opt, StringRef, StringRef arg, IndexedAssignmentMapOption& value) {
+  IndexedAssignmentMap& assignment = value.value;
+  StringRef cur;
+  while (!arg.empty()) {
+    // This allows a redundant comma as the last character.
+    std::tie(cur, arg) = arg.split(',');
+    cur = cur.trim();
+    size_t name_separator_index = cur.find('@');
+    if (name_separator_index == StringRef::npos) {
+      return opt.error(
+          "Assignment must contain '@' denoting a name assigned to a mesh, "
+          "got: " +
+          cur);
+    }
+    StringRef index_str = cur.take_front(name_separator_index);
+    StringRef target = cur.drop_front(name_separator_index + 1);
+    int64_t index;
+    if (index_str.getAsInteger(10, index)) {
+      return opt.error("Failed to parse index: " + index_str);
+    }
+
+    assignment[index] = target;
+  }
+  return false;
+}
+
+void parser<IndexedAssignmentMapOption>::printOptionDiff(
+    const Option& opt, const IndexedAssignmentMapOption& value,
+    const OptVal& defaultValue, size_t globalWidth) const {
+  printOptionName(opt, globalWidth);
+  outs() << "= " << value;
+  if (defaultValue.hasValue()) {
+    outs().indent(2) << " (default: " << defaultValue.getValue() << ")";
+  }
+  outs() << "\n";
+}
+
+void parser<IndexedAssignmentMapOption>::anchor() {}
+
+}  // namespace llvm::cl
diff --git a/shardy/dialect/mpmd/transforms/import/mesh_assignment_map.h b/shardy/dialect/mpmd/transforms/import/mesh_assignment_map.h
new file mode 100644
index 0000000..bdc0ef8
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/mesh_assignment_map.h
@@ -0,0 +1,117 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_ASSIGNMENT_MAP_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_ASSIGNMENT_MAP_H_
+
+#include <cstddef>
+#include <cstdint>
+#include <functional>
+#include <map>
+#include <optional>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace mlir::mpmd {
+
+using MeshStageAssignment = std::pair<std::string, std::optional<int64_t>>;
+
+// A user-defined mapping between names (of computations and tensors) and mesh
+// names and optionally stage ids. E.g., n -> {m, s} means that the name n is
+// assigned to mesh m, stage s; n -> {m, nullopt} means that n is assigned to
+// mesh m, but to no particular stage of that mesh.
+//
+// Note: `std::map` is used to ensure stable iteration order for the MPMD
+// compilation cache key. We replace the default comparator with `std::less<>`
+// so we can lookup using `std::string_view`.
+using UserAssignmentMap =
+    std::map<std::string, MeshStageAssignment, std::less<>>;
+
+// A `UserAssignmentMap` option with a custom parser/printer.
+struct UserAssignmentMapOption {
+  UserAssignmentMap value;
+};
+
+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                              const UserAssignmentMapOption& option);
+
+// A user-defined mapping between function input/output indices and mesh names.
+// E.g., i -> m means that the ith input/output  is assigned to mesh m.
+//
+// Note: we use a map instead of a vector here because it's used in two ways:
+// 1. In testing, we need to support passing a comma separated list of mappings
+//    in the format "index@mesh_name".
+// 2. In production, we need to support passing a vector of
+//    `std::optional<std::string>` from the Python code.
+// It is possible to convert the vector to a map but not vice versa.
+using IndexedAssignmentMap = llvm::DenseMap<int, std::string>;
+
+// A `IndexedAssignmentMap` option with a custom parser/printer.
+struct IndexedAssignmentMapOption {
+  IndexedAssignmentMap value;
+};
+
+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                              const IndexedAssignmentMapOption& option);
+
+// Converts a vector of optional mesh names to a map of index (in the vector) to
+// mesh name.
+IndexedAssignmentMap ConvertMeshVectorToMap(
+    const std::vector<std::optional<std::string>>& meshes);
+
+}  // namespace mlir::mpmd
+
+namespace llvm::cl {
+
+extern template class basic_parser<mlir::mpmd::UserAssignmentMapOption>;
+extern template class basic_parser<mlir::mpmd::IndexedAssignmentMapOption>;
+
+template <>
+class parser<mlir::mpmd::UserAssignmentMapOption>
+    : public basic_parser<mlir::mpmd::UserAssignmentMapOption> {
+ public:
+  parser(Option& opt) : basic_parser(opt) {}
+  bool parse(Option& opt, StringRef argName, StringRef arg,
+             mlir::mpmd::UserAssignmentMapOption& value);
+  StringRef getValueName() const override { return "user-assignment-map"; }
+  void printOptionDiff(const Option& opt,
+                       const mlir::mpmd::UserAssignmentMapOption& value,
+                       const OptVal& defaultValue, size_t globalWidth) const;
+  void anchor() override;
+};
+
+template <>
+class parser<mlir::mpmd::IndexedAssignmentMapOption>
+    : public basic_parser<mlir::mpmd::IndexedAssignmentMapOption> {
+ public:
+  parser(Option& opt) : basic_parser(opt) {}
+  bool parse(Option& opt, StringRef argName, StringRef arg,
+             mlir::mpmd::IndexedAssignmentMapOption& value);
+  StringRef getValueName() const override { return "indexed-assignment-map"; }
+  void printOptionDiff(const Option& opt,
+                       const mlir::mpmd::IndexedAssignmentMapOption& value,
+                       const OptVal& defaultValue, size_t globalWidth) const;
+  void anchor() override;
+};
+
+}  // namespace llvm::cl
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_ASSIGNMENT_MAP_H_
diff --git a/shardy/dialect/mpmd/transforms/import/mesh_inference_origins.cc b/shardy/dialect/mpmd/transforms/import/mesh_inference_origins.cc
new file mode 100644
index 0000000..144ed7c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/mesh_inference_origins.cc
@@ -0,0 +1,65 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_origins.h"
+
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+StringRef TerminalNodesOrigin(Operation* op) {
+  if (auto mpmd_op = dyn_cast<mpmd::BroadcastOp>(op)) {
+    return kMpmdBroadcastOrigin;
+  }
+  if (auto mpmd_op = dyn_cast<mpmd::ReduceOp>(op)) {
+    return kMpmdReduceOrigin;
+  }
+
+  SDY_CHECK(false) << "Unexpected MPMD op: "
+                   << op->getName().getStringRef().str();
+}
+
+mpmd::MeshWithOriginsAttr UnusedCalleeInputMeshWithOrigin(MLIRContext* context,
+                                                          StringRef mesh_name) {
+  return mpmd::MeshWithOriginsAttr::get(
+      context, mesh_name,
+      mpmd::OriginAttr::get(context, kInferredUnusedCalleeInOrigin));
+}
+
+mpmd::MeshWithOriginsAttr UnusedCalleeOutputMeshWithOrigin(
+    MLIRContext* context, StringRef mesh_name) {
+  return mpmd::MeshWithOriginsAttr::get(
+      context, mesh_name,
+      mpmd::OriginAttr::get(context, kInferredUnusedCalleeOutOrigin));
+}
+
+mpmd::MeshWithOriginsAttr TransferMeshWithOrigin(mpmd::TransferOp transfer_op) {
+  return mpmd::MeshWithOriginsAttr::get(
+      transfer_op->getContext(), transfer_op.getType().getMeshName(),
+      mpmd::OriginAttr::get(transfer_op->getContext(), kTransferOrigin));
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/mesh_inference_origins.h b/shardy/dialect/mpmd/transforms/import/mesh_inference_origins.h
new file mode 100644
index 0000000..774e473
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/mesh_inference_origins.h
@@ -0,0 +1,77 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_INFERENCE_ORIGINS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_INFERENCE_ORIGINS_H_
+
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+// The following strings are the origin labels for AssignOps and UnassignOps.
+// They are used to indicate the source of the mesh assignment, and may be used
+// for merging.
+
+// Mesh assignment originates from the user. I.e. input_mesh_assignment or
+// in_sharding.
+inline constexpr StringRef kUserInputOrigin = "user_in";
+// Mesh assignment originates from the user. I.e. input_mesh_assignment or
+// in_sharding.inline constexpr StringRef kUserOutputOrigin = "user_out";
+inline constexpr StringRef kUserOutputOrigin = "user_out";
+// Mesh assignment originates from mesh inference where we assign the input
+// using our analysis.
+inline constexpr StringRef kInferredInputOrigin = "inferred_in";
+// Mesh assignment originates from mesh inference where we assign the output
+// using our analysis.
+inline constexpr StringRef kInferredOutputOrigin = "inferred_out";
+// Mesh assignment originates from mesh inference where we assign an unused op
+// using our analysis.
+inline constexpr StringRef kInferredUnusedOrigin = "inferred_unused";
+// Mesh assignment originates from mesh inference where we assign an unused
+// callee's input/output using our analysis.
+inline constexpr StringRef kInferredUnusedCalleeInOrigin =
+    "inferred_unused_callee_in";
+inline constexpr StringRef kInferredUnusedCalleeOutOrigin =
+    "inferred_unused_callee_out";
+// Mesh assignment originates from input/output constraints.
+inline constexpr StringRef kIoConstraintInputOrigin = "io_constraint_in";
+// Mesh assignment originates from input/output constraints.
+inline constexpr StringRef kIoConstraintOutputOrigin = "io_constraint_out";
+
+inline constexpr StringRef kTransferOrigin = "transfer";
+
+// Mesh assignment originates from broadcasting inputs.
+inline constexpr llvm::StringRef kBroadcastInputOrigin = "broadcast_in";
+// Mesh assignment originates from an mpmd.broadcast op.
+inline constexpr StringRef kMpmdBroadcastOrigin = "broadcast";
+// Mesh assignment originates from an mpmd.reduce op.
+inline constexpr StringRef kMpmdReduceOrigin = "reduce";
+
+// Origin string for terminal nodes (e.g. mpmd.broadcast and mpmd.reduce).
+StringRef TerminalNodesOrigin(Operation* op);
+
+MeshWithOriginsAttr UnusedCalleeInputMeshWithOrigin(MLIRContext* context,
+                                                    StringRef mesh_name);
+MeshWithOriginsAttr UnusedCalleeOutputMeshWithOrigin(MLIRContext* context,
+                                                     StringRef mesh_name);
+
+MeshWithOriginsAttr TransferMeshWithOrigin(TransferOp transfer_op);
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_INFERENCE_ORIGINS_H_
diff --git a/shardy/dialect/mpmd/transforms/import/mesh_inference_utils.cc b/shardy/dialect/mpmd/transforms/import/mesh_inference_utils.cc
new file mode 100644
index 0000000..64a6fca
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/mesh_inference_utils.cc
@@ -0,0 +1,388 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_utils.h"
+
+#include <optional>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringRef.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/AffineMap.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Dialect.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/fragment_arg_res_attrs.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/meshes_with_origins.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+
+namespace mlir::mpmd {
+
+using ::mlir::func::FuncOp;
+
+bool IsMeshlessOp(Operation* op) {
+  return op && !op->hasTrait<OpTrait::IsTerminator>() &&
+         !op->getParentOfType<FragmentOp>() &&
+         !sdy::inDialect<MpmdDialect>(op) && !isa<ModuleOp, FuncOp>(op);
+}
+
+MeshesWithOrigins GetUseSet(MeshesWithOriginsAttr origins) {
+  return MeshesWithOrigins::CreateUseSet(origins);
+}
+
+MeshesWithOrigins GetUseSet(Operation* op) {
+  if (!op) {
+    return MeshesWithOrigins::CreateUseSet({});
+  }
+  return GetUseSet(op->getAttrOfType<MeshesWithOriginsAttr>(kMpmdUseSet));
+}
+
+MeshesWithOrigins GetArgUseSet(FuncOp func, int arg) {
+  return GetUseSet(
+      func.getArgAttrOfType<MeshesWithOriginsAttr>(arg, kMpmdUseSet));
+}
+
+MeshesWithOrigins GetResUseSet(FuncOp func, int res) {
+  return GetUseSet(
+      func.getResultAttrOfType<MeshesWithOriginsAttr>(res, kMpmdUseSet));
+}
+
+MeshesWithOrigins GetUseSet(ForOp for_op, int arg_number) {
+  return GetUseSet(dyn_cast_if_present<MeshesWithOriginsAttr>(
+      GetArgAttr(for_op, arg_number, kMpmdUseSet)));
+}
+
+SetVector<StringRef> GetUseMeshes(Operation* op) {
+  return GetUseSet(op).MeshNamesOrEmpty();
+}
+
+void SetUseSet(Operation* op, MeshesWithOrigins use_set, OpBuilder& builder) {
+  if (!use_set.empty()) {
+    op->setAttr(kMpmdUseSet, use_set.ToAttr(builder));
+  }
+}
+
+void SetArgUseSet(FuncOp func, int arg_number, MeshesWithOrigins use_set,
+                  OpBuilder& builder) {
+  if (!use_set.empty()) {
+    func.setArgAttr(arg_number, kMpmdUseSet, use_set.ToAttr(builder));
+  }
+}
+
+void SetResUseSet(FuncOp func, int res_number, MeshesWithOrigins use_set,
+                  OpBuilder& builder) {
+  if (!use_set.empty()) {
+    func.setResultAttr(res_number, kMpmdUseSet, use_set.ToAttr(builder));
+  }
+}
+
+namespace {
+// ForOp has one arg attr per region arg (not per operand).
+SmallVector<Attribute> GetArgAttrsOrCreateDefault(ForOp op) {
+  return op->hasAttr(kArgAttrName)
+             ? llvm::to_vector(
+                   op->getAttrOfType<ArrayAttr>(kArgAttrName).getValue())
+             : SmallVector<Attribute>(
+                   op.getRegion().getNumArguments(),
+                   DictionaryAttr::get(op->getContext(), {}));
+}
+}  // namespace
+
+void SetUseSet(ForOp for_op, int arg_number, MeshesWithOrigins use_set,
+               OpBuilder& builder) {
+  if (!use_set.empty()) {
+    auto arg_attrs = GetArgAttrsOrCreateDefault(for_op);
+    InsertAttr(arg_attrs[arg_number], kMpmdUseSet,
+                           use_set.ToAttr(builder));
+    SetArgAttrs(for_op, arg_attrs);
+  }
+}
+
+// If the given `result` is the target of a mesh data flow edge, returns the
+// source of that edge, otherwise returns a nullptr.
+//
+// When an operand of an op flows into the result of the same or a parent op (in
+// case the op is a region terminator), and both ops can accept either local or
+// mesh tensors, we define a mesh data flow edge between the operand and the
+// result.
+OpOperand* GetMeshDataFlowSrc(OpResult result) {
+  if (!result) {
+    return nullptr;
+  }
+  Operation* op = result.getOwner();
+
+  if (auto call_op = dyn_cast<CallOp>(op)) {
+    return &GetCalleeFunc(call_op).front().getTerminator()->getOpOperand(
+        result.getResultNumber());
+  }
+  if (auto for_op = dyn_cast<ForOp>(op)) {
+    return &for_op.getRegion().front().getTerminator()->getOpOperand(
+        result.getResultNumber());
+  }
+
+  return nullptr;
+}
+
+MeshesWithOrigins GetSrcSet(MeshesWithOriginsAttr origins) {
+  return MeshesWithOrigins(origins);
+}
+
+MeshesWithOrigins GetSrcSet(Operation* op, StringRef set_attr_name) {
+  if (!op) {
+    return MeshesWithOrigins();
+  }
+  return GetSrcSet(op->getAttrOfType<MeshesWithOriginsAttr>(set_attr_name));
+}
+
+MeshesWithOrigins GetSrcSet(Operation* op) {
+  return GetSrcSet(op, kMpmdSrcSet);
+}
+
+MeshesWithOrigins GetSrcSet(FuncOp func, int arg_number) {
+  return GetSrcSet(dyn_cast_if_present<MeshesWithOriginsAttr>(
+      func.getArgAttr(arg_number, kMpmdSrcSet)));
+}
+
+MeshesWithOrigins GetSrcSet(ForOp for_op, int arg_number) {
+  return GetSrcSet(dyn_cast_if_present<MeshesWithOriginsAttr>(
+      GetArgAttr(for_op, arg_number, kMpmdSrcSet)));
+}
+
+MeshesWithOrigins GetSrcSet(OpOperand& op_operand) {
+  Value operand = op_operand.get();
+  // Pass through data flow ops.
+  while (auto target = dyn_cast<OpResult>(operand)) {
+    OpOperand* source = GetMeshDataFlowSrc(target);
+    if (!source) {
+      break;
+    }
+    operand = source->get();
+  }
+
+  if (Operation* defining_op = operand.getDefiningOp()) {
+    if (isa<AssignOp>(defining_op)) {
+      return MeshesWithOrigins();
+    }
+    return GetSrcSet(defining_op);
+  }
+
+  Operation* defining_op =
+      cast<BlockArgument>(operand).getOwner()->getParentOp();
+  if (auto func_op = dyn_cast<FuncOp>(defining_op)) {
+    return GetSrcSet(func_op,
+                     cast<BlockArgument>(operand).getArgNumber());
+  }
+  if (auto for_op = dyn_cast<ForOp>(defining_op)) {
+    return GetSrcSet(for_op, cast<BlockArgument>(operand).getArgNumber());
+  }
+
+  return MeshesWithOrigins();
+}
+
+std::optional<SetVector<StringRef>> GetSrcMeshes(Operation* op) {
+  return GetSrcSet(op).MaybeMeshNames();
+}
+
+std::optional<SetVector<StringRef>> GetSrcMeshes(FuncOp func, int arg_number) {
+  return GetSrcSet(func, arg_number).MaybeMeshNames();
+}
+
+std::optional<SetVector<StringRef>> GetSrcMeshes(OpOperand& op_operand) {
+  return GetSrcSet(op_operand).MaybeMeshNames();
+}
+
+void SetSrcSet(Operation* op, MeshesWithOrigins src_set, OpBuilder& builder) {
+  op->setAttr(kMpmdSrcSet, src_set.ToAttr(builder));
+}
+
+void SetSrcSet(FuncOp func, int arg_number, MeshesWithOrigins src_set,
+               OpBuilder& builder) {
+  func.setArgAttr(arg_number, kMpmdSrcSet, src_set.ToAttr(builder));
+}
+
+void SetSrcSet(ForOp for_op, int arg_number, MeshesWithOrigins src_set,
+               OpBuilder& builder) {
+  auto arg_attrs = GetArgAttrsOrCreateDefault(for_op);
+  InsertAttr(arg_attrs[arg_number], kMpmdSrcSet,
+                         src_set.ToAttr(builder));
+  SetArgAttrs(for_op, arg_attrs);
+}
+
+bool ClearUseSet(Operation* op) {
+  return op->removeAttr(kMpmdUseSet) != nullptr;
+}
+bool ClearUseSet(FuncOp func) {
+  bool removed_something = false;
+  for (BlockArgument arg : func.getArguments()) {
+    removed_something |=
+        func.removeArgAttr(arg.getArgNumber(), kMpmdUseSet) != nullptr;
+  }
+  for (OpResult res : func->getResults()) {
+    removed_something |=
+        func.removeResultAttr(res.getResultNumber(), kMpmdUseSet) != nullptr;
+  }
+
+  return removed_something;
+}
+
+bool ClearUseSetAndSrcSet(Operation* op) {
+  bool removed_something = false;
+  removed_something |= ClearUseSet(op);
+  removed_something |= op->removeAttr(kMpmdSrcSet) != nullptr;
+  return removed_something;
+}
+
+bool ClearUseSetAndSrcSet(FuncOp func) {
+  bool removed_something = false;
+  for (BlockArgument arg : func.getArguments()) {
+    removed_something |=
+        func.removeArgAttr(arg.getArgNumber(), kMpmdUseSet) != nullptr;
+    removed_something |=
+        func.removeArgAttr(arg.getArgNumber(), kMpmdSrcSet) != nullptr;
+  }
+
+  for (int res_number = 0; res_number < func.getNumResults(); ++res_number) {
+    removed_something |=
+        func.removeResultAttr(res_number, kMpmdUseSet) != nullptr;
+    removed_something |=
+        func.removeResultAttr(res_number, kMpmdSrcSet) != nullptr;
+  }
+
+  return removed_something;
+}
+
+bool ClearUseSetAndSrcSet(ForOp for_op) {
+  bool removed_something = false;
+  if (for_op->hasAttr(kArgAttrName)) {
+    auto arg_attrs =
+        for_op->getAttrOfType<ArrayAttr>(kArgAttrName).getValue();
+    SmallVector<Attribute> new_arg_attrs;
+    for (auto arg_attr : arg_attrs) {
+      removed_something |= RemoveAttr(arg_attr, kMpmdUseSet);
+      removed_something |= RemoveAttr(arg_attr, kMpmdSrcSet);
+      if (!cast<DictionaryAttr>(arg_attr).empty()) {
+        new_arg_attrs.push_back(arg_attr);
+      }
+    }
+    if (new_arg_attrs.empty()) {
+      for_op->removeAttr(kArgAttrName);
+    } else {
+      SetArgAttrs(for_op, new_arg_attrs);
+    }
+  }
+  return removed_something;
+}
+
+void ClearCanConvertAttr(FuncOp func) {
+  func->walk([](Operation* op) { op->removeAttr(kCanConvertToReduce); });
+}
+
+namespace {
+
+// Copies the meshes from `use` into `base_set`. If `use` is in `source_block`,
+// then we copy it directly from `use`. If it's in a nested-block (e.g. used in
+// a while-loop as a free variable), then we copy it over from the parent op in
+// the same block.
+void CopyMeshesFromUse(Block* source_block, OpOperand& use,
+                       MeshesWithOrigins& base_set) {
+  Operation* user = use.getOwner();
+
+  // TODO: b/341882915 - If we canonicalize chains of broadcasts, we might not
+  // need to special case the broadcast producers.
+  if (IsTerminalNodeInAnalysis(user) &&
+      !isa_and_present<BroadcastOp>(use.get().getDefiningOp())) {
+    return;
+  }
+
+  if (isa<UnassignOp>(user)) {
+    return;
+  }
+
+  if (Operation* ancestor = GetAncestorInBlock(source_block, user);
+      ancestor != user) {
+    base_set.Union(GetUseSet(ancestor));
+    return;
+  }
+
+  if (auto call_op = dyn_cast<CallOp>(use.getOwner())) {
+    base_set.Union(
+        GetArgUseSet(GetCalleeFunc(call_op), use.getOperandNumber()));
+    return;
+  }
+
+  if (auto func_return = dyn_cast<func::ReturnOp>(use.getOwner())) {
+    auto func = cast<FuncOp>(func_return->getParentOp());
+    if (!IsEntryPointFunction(func)) {
+      base_set.Union(GetResUseSet(func, use.getOperandNumber()));
+      return;
+    }
+  }
+
+  if (auto for_op = dyn_cast<ForOp>(use.getOwner())) {
+    base_set.Union(GetUseSet(for_op, use.getOperandNumber()));
+    return;
+  }
+
+  base_set.Union(GetUseSet(user));
+}
+
+}  // namespace
+
+void UpdateTransitiveUses(Operation* op, MeshesWithOrigins& base_set) {
+  for (OpOperand& use : op->getUses()) {
+    CopyMeshesFromUse(op->getBlock(), use, base_set);
+  }
+}
+
+void UpdateTransitiveUses(Value value, MeshesWithOrigins& base_set) {
+  for (OpOperand& use : value.getUses()) {
+    CopyMeshesFromUse(value.getParentBlock(), use, base_set);
+  }
+}
+
+bool IsTerminalNodeInAnalysis(Operation* op) {
+  return isa<BroadcastOp, ReduceOp>(op);
+}
+
+bool IsCallOpInCallChain(CallOp call_op) {
+  return llvm::any_of(call_op->getUsers(), [&](Operation* user) {
+    if (auto user_call_op = dyn_cast<CallOp>(user)) {
+      return user_call_op.getCallee() == call_op.getCallee();
+    }
+    return false;
+  });
+}
+
+bool CallOpHasUseSetPopulated(CallOp call_op) {
+  FuncOp callee_func = GetCalleeFunc(call_op);
+  return llvm::any_of(callee_func.getArguments(), [&](BlockArgument arg) {
+    return callee_func.getArgAttr(arg.getArgNumber(), kMpmdUseSet);
+  });
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/mesh_inference_utils.h b/shardy/dialect/mpmd/transforms/import/mesh_inference_utils.h
new file mode 100644
index 0000000..8516c2e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/mesh_inference_utils.h
@@ -0,0 +1,171 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_INFERENCE_UTILS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_INFERENCE_UTILS_H_
+
+#include <optional>
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Dialect.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/import/meshes_with_origins.h"
+
+namespace mlir::mpmd {
+
+// Attribute name representing a set of meshes that a meshless op is
+// transitively used in. This propagates backward from users to the op by union.
+// It tells us where the op must live: an op (or a copy of) must live on all
+// of its uses since we don't allow the introduction of transfers except on
+// function inputs and so the entire chain of ops must be in a single mesh (but
+// the chain could be duplicated across meshes).
+//
+// If the attribute is not present on an unassigned op, then the op is assumed
+// to have use_set = {} (i.e. the set is empty).
+//
+// See initialization passes below for details on initialization.
+inline constexpr StringRef kMpmdUseSet = "mpmd.use_set";
+
+// Attribute name representing a set of meshes that a meshless op can
+// exist on without introducing transfer ops. For unassign ops,
+// this is where the op/result actually lives. However, child ops are not
+// assigned a mesh yet, and so although the child op could be placed on a mesh,
+// it may not be. For example, in
+//
+// y = unassign x
+// z = transfer x m1 -> m2
+// yy = add y, y
+//
+// Then `add y, y` can live on both m1 and m2. But where it eventually lives on
+// depends on the use_set. Before assignment, yy doesn't live on m1 or m2 yet –
+// it could live on one of them, or both of them.
+//
+// This propagates forward from operands to the op by intersection, since an op
+// and its operands and results must live on the same mesh.
+//
+// If the attribute is not present on an unassigned op, then the op is assumed
+// to have src_set = {all meshes}.
+//
+// See initialization passes below for details on initialization.
+inline constexpr StringRef kMpmdSrcSet = "mpmd.src_set";
+
+// Temporary attribute name to store whether an op can be converted to a
+// mpmd.reduce from an analysis of the user-added mpmd.reduce<none>.
+inline constexpr StringRef kCanConvertToReduce = "mpmd.can_convert_to_reduce";
+
+// Temporary attribute name to store the inferred mpmd reduce info.
+inline constexpr StringRef kMpmdReduceAnnotation = "mpmd.reduce";
+
+// Returns true if the given `op` needs to be assigned to a mesh.
+bool IsMeshlessOp(Operation* op);
+
+// Returns the use_set. We return an empty use_set if it is missing, since they
+// are interchangeable.
+MeshesWithOrigins GetUseSet(Operation* op);
+MeshesWithOrigins GetArgUseSet(func::FuncOp func, int arg);
+MeshesWithOrigins GetResUseSet(func::FuncOp func, int res);
+MeshesWithOrigins GetUseSet(ForOp for_op, int arg_number);
+// Returns the use_set as a set of mesh names.
+llvm::SetVector<StringRef> GetUseMeshes(Operation* op);
+
+// Set the use_set attribute if the set is not empty. This is because the
+// absence of the attribute is the same as the empty list, and we do not want to
+// clutter the IR.
+void SetUseSet(Operation* op, MeshesWithOrigins use_set, OpBuilder& builder);
+void SetArgUseSet(func::FuncOp func, int arg, MeshesWithOrigins use_set,
+                  OpBuilder& builder);
+void SetResUseSet(func::FuncOp func, int res, MeshesWithOrigins use_set,
+                  OpBuilder& builder);
+void SetUseSet(ForOp for_op, int arg_number, MeshesWithOrigins use_set,
+               OpBuilder& builder);
+
+// Returns the src_set of an op or func arg.
+MeshesWithOrigins GetSrcSet(Operation* op);
+MeshesWithOrigins GetSrcSet(func::FuncOp func, int arg_number);
+// Returns the src_set as a set of mesh names.
+std::optional<llvm::SetVector<StringRef>> GetSrcMeshes(Operation* op);
+std::optional<llvm::SetVector<StringRef>> GetSrcMeshes(func::FuncOp func,
+                                                       int arg_number);
+MeshesWithOrigins GetSrcSet(ForOp for_op, int arg_number);
+
+// Returns the src_set of an operand, passing through data flow ops as required.
+MeshesWithOrigins GetSrcSet(OpOperand& op_operand);
+std::optional<llvm::SetVector<StringRef>> GetSrcMeshes(OpOperand& op_operand);
+
+// Set the src_set attribute. Unlike use_set, we set it even if it is empty,
+// because the empty src_set means that the op cannot live anywhere. Whereas
+// absence of the attribute means that the op can live on any mesh.
+void SetSrcSet(Operation* op, MeshesWithOrigins src_set, OpBuilder& builder);
+void SetSrcSet(func::FuncOp func, int arg_number, MeshesWithOrigins src_set,
+               OpBuilder& builder);
+void SetSrcSet(ForOp for_op, int arg_number, MeshesWithOrigins src_set,
+               OpBuilder& builder);
+
+// Removes the use_set attributes. Returns true if the attribute was removed.
+bool ClearUseSet(Operation* op);
+bool ClearUseSet(func::FuncOp func);
+
+// Removes the src_set and use_set attributes. Returns true if any attribute
+// was removed.
+bool ClearUseSetAndSrcSet(Operation* op);
+bool ClearUseSetAndSrcSet(func::FuncOp func);
+bool ClearUseSetAndSrcSet(ForOp for_op);
+
+// Removes the can_convert_to_reduce attribute.
+void ClearCanConvertAttr(func::FuncOp func);
+
+// Gets transitive uses of ops by combining the use_sets of users, updating
+// `base_set` in place. This assumes that all the users have the relevant
+// use_set data populated. It passes through data flow ops, treating them as
+// "transparent".
+void UpdateTransitiveUses(Operation* op, MeshesWithOrigins& base_set);
+void UpdateTransitiveUses(Value value, MeshesWithOrigins& base_set);
+
+// Returns if the op is a terminal node in the mesh analysis. I.e. it blocks
+// propagation of use/src analysis. The only such ops are: the mpmd.reduce and
+// mpmd.broadcast ops.
+bool IsTerminalNodeInAnalysis(Operation* op);
+
+inline bool IsMpmdReduceAnnotated(Operation* op) {
+  return op->hasAttr(kMpmdReduceAnnotation);
+}
+
+inline bool CanConvertToReduce(Operation* op) {
+  return op->hasAttr(kCanConvertToReduce);
+}
+
+// Returns true if the given `call_op` is in a call chain with itself. E.g.
+// we have a chain of calls like:
+//
+// %v = call @f(%v0)
+// %w = call @f(%v)
+bool IsCallOpInCallChain(CallOp call_op);
+
+// Returns true if the use_set of the callee func has been populated.
+// This checks if any of the callee func's args have the use_set attribute
+// populated as a proxy.
+bool CallOpHasUseSetPopulated(CallOp call_op);
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESH_INFERENCE_UTILS_H_
diff --git a/shardy/dialect/mpmd/transforms/import/mesh_inference_utils_test.cc b/shardy/dialect/mpmd/transforms/import/mesh_inference_utils_test.cc
new file mode 100644
index 0000000..263e343
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/mesh_inference_utils_test.cc
@@ -0,0 +1,198 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_utils.h"
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OwningOpRef.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Parser/Parser.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/import/meshes_with_origins.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include <gmock/gmock.h>
+#include <gtest/gtest.h>
+
+using ::mlir::func::FuncOp;
+using ::testing::UnorderedElementsAre;
+
+namespace mlir::mpmd {
+namespace {
+
+TEST(UpdateTransitiveUses, UpdatesForBlockArgs) {
+  constexpr StringRef kProgram = R"mlir(
+  func.func @main(%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>){
+    %1 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+    %2 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+    func.return %2 : tensor<4x8xf32>
+  })mlir";
+
+  MLIRContext context;
+  context.loadDialect<func::FuncDialect, stablehlo::StablehloDialect,
+                      MpmdDialect>();
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+
+  FuncOp main_func = dyn_cast<FuncOp>(module->lookupSymbol("main"));
+  BlockArgument block_arg = main_func.front().getArgument(0);
+  MeshesWithOrigins used_in_meshes;
+
+  UpdateTransitiveUses(block_arg, used_in_meshes);
+
+  EXPECT_THAT(used_in_meshes.MeshNamesOrEmpty(),
+              UnorderedElementsAre("m1", "m2"));
+}
+
+TEST(UpdateTransitiveUses, UpdatesForOperation) {
+  constexpr StringRef kProgram = R"mlir(
+  func.func @main(%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>){
+    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4x8xf32>
+    %1 = stablehlo.add %0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+    %2 = stablehlo.add %0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+    %3 = stablehlo.add %0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m3">} : tensor<4x8xf32>
+    func.return %3 : tensor<4x8xf32>
+  })mlir";
+
+  MLIRContext context;
+  context.loadDialect<func::FuncDialect, stablehlo::StablehloDialect,
+                      MpmdDialect>();
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+
+  FuncOp main_func = dyn_cast<FuncOp>(module->lookupSymbol("main"));
+  Operation* op = &*main_func.getOps().begin();
+  MeshesWithOrigins used_in_meshes;
+
+  UpdateTransitiveUses(op->getResult(0), used_in_meshes);
+
+  EXPECT_THAT(used_in_meshes.MeshNamesOrEmpty(),
+              UnorderedElementsAre("m1", "m2", "m3"));
+}
+
+TEST(UpdateTransitiveUses, UpdatesForValue) {
+  constexpr StringRef kProgram = R"mlir(
+  func.func @main(%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>){
+    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4x8xf32>
+    %1 = stablehlo.add %0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+    %2 = stablehlo.add %0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+    %3 = stablehlo.add %0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m3">} : tensor<4x8xf32>
+    func.return %3 : tensor<4x8xf32>
+  })mlir";
+
+  MLIRContext context;
+  context.loadDialect<func::FuncDialect, stablehlo::StablehloDialect,
+                      MpmdDialect>();
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+
+  FuncOp main_func = dyn_cast<FuncOp>(module->lookupSymbol("main"));
+  Value v0 = main_func.getOps().begin()->getResult(0);
+  MeshesWithOrigins used_in_meshes;
+
+  UpdateTransitiveUses(v0, used_in_meshes);
+
+  EXPECT_THAT(used_in_meshes.MeshNamesOrEmpty(),
+              UnorderedElementsAre("m1", "m2", "m3"));
+}
+
+TEST(UpdateTransitiveUses, UpdatesWhenUsedInRegionOps) {
+  constexpr StringRef kProgram = R"mlir(
+  func.func @main(%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>){
+    %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+    %1 = stablehlo.constant dense<1> : tensor<i1>
+    %region_op = stablehlo.while(%iterArg_0 = %1) : tensor<i1>
+    attributes {mpmd.use_set = #mpmd.meshes_with_origins<"m1">}
+    cond {
+      "stablehlo.return"(%iterArg_0) : (tensor<i1>) -> ()
+    } do {
+      %8 = stablehlo.add %arg0, %0 : tensor<4x8xf32>
+      "stablehlo.return"(%iterArg_0) : (tensor<i1>) -> ()
+    }
+    func.return %0 : tensor<4x8xf32>
+  })mlir";
+
+  MLIRContext context;
+  context.loadDialect<func::FuncDialect, stablehlo::StablehloDialect,
+                      MpmdDialect>();
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+
+  FuncOp main_func = dyn_cast<FuncOp>(module->lookupSymbol("main"));
+
+  BlockArgument block_arg = main_func.front().getArgument(0);
+  MeshesWithOrigins block_arg_meshes;
+  UpdateTransitiveUses(block_arg, block_arg_meshes);
+  EXPECT_THAT(block_arg_meshes.MeshNamesOrEmpty(), UnorderedElementsAre("m1"));
+
+  Operation* op = &*main_func.getOps().begin();
+  MeshesWithOrigins op_meshes;
+  UpdateTransitiveUses(op, op_meshes);
+  EXPECT_THAT(op_meshes.MeshNamesOrEmpty(), UnorderedElementsAre("m1"));
+
+  Value v0 = op->getResult(0);
+  MeshesWithOrigins v0_meshes;
+  UpdateTransitiveUses(v0, v0_meshes);
+  EXPECT_THAT(v0_meshes.MeshNamesOrEmpty(), UnorderedElementsAre("m1"));
+}
+
+TEST(UpdateTransitiveUses, UpdatesWhenUsedInCallOps) {
+  constexpr StringRef kProgram = R"mlir(
+  func.func @main(%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>){
+    %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+    %call:2 = mpmd.call @call(%arg0, %0) :
+      (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+    func.return %0 : tensor<4x8xf32>
+  }
+
+  func.func @call(
+    %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">},
+    %arg1: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">}) ->
+    (tensor<4x8xf32>){
+    %1 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+    %2 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    func.return %2 : tensor<4x8xf32>
+  })mlir";
+
+  MLIRContext context;
+  context.loadDialect<func::FuncDialect, stablehlo::StablehloDialect,
+                      MpmdDialect>();
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+
+  FuncOp main_func = dyn_cast<FuncOp>(module->lookupSymbol("main"));
+
+  BlockArgument block_arg = main_func.front().getArgument(0);
+  MeshesWithOrigins used_in_meshes;
+  UpdateTransitiveUses(block_arg, used_in_meshes);
+  EXPECT_THAT(used_in_meshes.MeshNamesOrEmpty(), UnorderedElementsAre("m1"));
+
+  Operation* op = &*main_func.getOps().begin();
+  MeshesWithOrigins op_meshes;
+  UpdateTransitiveUses(op, op_meshes);
+  EXPECT_THAT(op_meshes.MeshNamesOrEmpty(), UnorderedElementsAre("m2"));
+
+  Value v0 = op->getResult(0);
+  MeshesWithOrigins v0_meshes;
+  UpdateTransitiveUses(v0, v0_meshes);
+  EXPECT_THAT(v0_meshes.MeshNamesOrEmpty(), UnorderedElementsAre("m2"));
+}
+
+}  // namespace
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/meshes_with_origins.cc b/shardy/dialect/mpmd/transforms/import/meshes_with_origins.cc
new file mode 100644
index 0000000..3fdc894
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/meshes_with_origins.cc
@@ -0,0 +1,245 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/import/meshes_with_origins.h"
+
+#include <optional>
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetOperations.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_origins.h"
+
+namespace mlir::mpmd {
+
+void MeshesWithOrigins::Union(const MeshesWithOrigins& other) {
+  if (!other.has_meshes_specified()) {
+    return;
+  }
+  if (!has_meshes_specified()) {
+    mesh_to_origins_ = other.mesh_to_origins_;
+    return;
+  }
+
+  for (const auto& [mesh, origins] : *other.mesh_to_origins_) {
+    (*mesh_to_origins_)[mesh].insert_range(origins);
+  }
+
+  // There's no point adding the wildcard origins to every mesh for the Union,
+  // and it would just be noise. So we just keep track of the wildcard origins
+  // separately and don't add them to the mesh_to_origins_ map.
+  wildcard_origins_.insert_range(other.wildcard_origins_);
+}
+
+void MeshesWithOrigins::Intersect(const MeshesWithOrigins& other) {
+  if (!other.has_meshes_specified()) {
+    return;
+  }
+
+  if (!has_meshes_specified()) {
+    mesh_to_origins_ = other.mesh_to_origins_;
+    wildcard_origins_ = other.wildcard_origins_;
+    return;
+  }
+
+  SmallVector<StringRef> meshes_to_remove;
+
+  // For each mesh in our current set, we do the following:
+  // - If the mesh is in the current set, then we merge the origins.
+  // - If the mesh is not in the current set, then we use the wildcard mesh
+  //   (if it exists) or remove the mesh.
+  for (const auto& [mesh, origins] : *mesh_to_origins_) {
+    if (other.mesh_to_origins_->contains(mesh)) {
+      (*mesh_to_origins_)[mesh].insert_range(
+          other.mesh_to_origins_->lookup(mesh));
+    } else if (other.has_wildcard_mesh()) {
+      (*mesh_to_origins_)[mesh].insert_range(other.wildcard_origins_);
+    } else {
+      meshes_to_remove.push_back(mesh);
+    }
+  }
+
+  // Now we handle the case where the current set has a wildcard mesh, and add
+  // all the meshes from (other set - current set) to the current set.
+  if (has_wildcard_mesh()) {
+    // We need to add all the meshes from the other set.
+    for (const auto& [mesh, origins] : *other.mesh_to_origins_) {
+      if (!mesh_to_origins_->contains(mesh)) {
+        (*mesh_to_origins_)[mesh].insert_range(origins);
+        (*mesh_to_origins_)[mesh].insert_range(wildcard_origins_);
+      }
+    }
+  }
+
+  for (const auto& mesh : meshes_to_remove) {
+    mesh_to_origins_->erase(mesh);
+  }
+
+  // If both have wildcard mesh, then we join the origins.
+  // If neither has wildcard mesh, then it doesn't matter since both are
+  // empty.
+  // If only one has wildcard mesh, then we need to clear the wildcard
+  // origins.
+  if (has_wildcard_mesh() == other.has_wildcard_mesh()) {
+    wildcard_origins_.insert_range(other.wildcard_origins_);
+  } else {
+    wildcard_origins_.clear();
+  }
+}
+
+void MeshesWithOrigins::insert(MeshWithOriginsAttr origin) {
+  if (!mesh_to_origins_) {
+    mesh_to_origins_ = MeshToOrigins();
+  }
+
+  if (origin.getMeshName() == kWildcardMesh) {
+    SDY_CHECK(!origin.getOrigins().empty())
+        << "Wildcard mesh must have an origin.";
+    wildcard_origins_.insert_range(origin.getOrigins());
+  } else {
+    (*mesh_to_origins_)[origin.getMeshName()].insert_range(origin.getOrigins());
+  }
+}
+
+MeshesWithOriginsAttr MeshesWithOrigins::ToAttr(
+    OpBuilder& builder) const {
+  if (!has_meshes_specified()) {
+    return nullptr;
+  }
+
+  return MeshesWithOriginsAttr::get(builder.getContext(),
+                                    ToArray(builder.getContext()));
+}
+
+SmallVector<MeshWithOriginsAttr> MeshesWithOrigins::ToArray(
+    MLIRContext* context) const {
+  if (!has_meshes_specified()) {
+    return {};
+  }
+
+  SmallVector<MeshWithOriginsAttr> meshes_with_origins;
+  meshes_with_origins.reserve(mesh_to_origins_->size());
+  for (const auto& [mesh, origins] : *mesh_to_origins_) {
+    meshes_with_origins.push_back(
+        MeshWithOriginsAttr::get(context, mesh, origins.getArrayRef()));
+  }
+
+  if (has_wildcard_mesh()) {
+    meshes_with_origins.push_back(MeshWithOriginsAttr::get(
+        context, kWildcardMesh, wildcard_origins_.getArrayRef()));
+  }
+
+  return meshes_with_origins;
+}
+
+SetVector<StringRef> MeshesWithOrigins::MeshNames(
+    bool include_wildcard_mesh) const {
+  SDY_CHECK(has_meshes_specified()) << "MeshesWithOrigins is unspecified.";
+
+  SetVector<StringRef> mesh_names;
+  for (const auto& [mesh, origins] : *mesh_to_origins_) {
+    mesh_names.insert(mesh);
+  }
+
+  if (include_wildcard_mesh && has_wildcard_mesh()) {
+    mesh_names.insert(kWildcardMesh);
+  }
+
+  return mesh_names;
+}
+
+std::optional<SetVector<StringRef>> MeshesWithOrigins::MaybeMeshNames(
+    bool include_wildcard_mesh) const {
+  if (!has_meshes_specified()) {
+    return std::nullopt;
+  }
+  return MeshNames(include_wildcard_mesh);
+}
+
+SetVector<StringRef> MeshesWithOrigins::MeshNamesOrEmpty(
+    bool include_wildcard_mesh) const {
+  if (!has_meshes_specified()) {
+    return {};
+  }
+  return MeshNames(include_wildcard_mesh);
+}
+
+bool MeshesWithOrigins::HasSameMeshes(const MeshesWithOrigins& other) const {
+  auto meshes = MeshNamesOrEmpty(/*include_wildcard_mesh=*/true);
+  auto other_meshes = other.MeshNamesOrEmpty(/*include_wildcard_mesh=*/true);
+
+  return meshes.size() == other_meshes.size() &&
+         llvm::set_is_subset(meshes, other_meshes);
+}
+
+namespace {
+// Returns true if the origin is considered low priority.
+//
+// This is used for picking the best mesh to assign.
+bool LowPriorityOrigin(OriginAttr origin) {
+  return origin.getOriginLabel() == kBroadcastInputOrigin;
+}
+
+StringRef GetHighestPriorityMeshName(MeshToOrigins mesh_to_origins) {
+  SmallVector<StringRef> high_priority_mesh_names;
+  SmallVector<StringRef> low_priority_mesh_names;
+  for (const auto& [mesh, origins] : mesh_to_origins) {
+    if (none_of(origins, LowPriorityOrigin)) {
+      high_priority_mesh_names.push_back(mesh);
+    } else {
+      low_priority_mesh_names.push_back(mesh);
+    }
+  }
+
+  if (!high_priority_mesh_names.empty()) {
+    return *min_element(high_priority_mesh_names);
+  }
+
+  return *min_element(low_priority_mesh_names);
+}
+}  // namespace
+
+std::optional<StringRef> MeshesWithOrigins::GetPrioritizedMeshName(
+    const SetVector<StringRef>& preferred_mesh_names) const {
+  SDY_CHECK(has_meshes_specified())
+      << "GetPrioritizedMeshName is only allowed when meshes are specified.";
+  if (empty()) {
+    return std::nullopt;
+  }
+
+  // Get preferred meshes that are also in the current set.
+  MeshToOrigins valid_preferred_meshes;
+  for (const auto& mesh : preferred_mesh_names) {
+    if (mesh_to_origins_->contains(mesh)) {
+      valid_preferred_meshes.try_emplace(mesh, mesh_to_origins_->lookup(mesh));
+    } else if (has_wildcard_mesh()) {
+      valid_preferred_meshes.try_emplace(mesh, wildcard_origins_);
+    }
+  }
+
+  if (!valid_preferred_meshes.empty()) {
+    return GetHighestPriorityMeshName(valid_preferred_meshes);
+  }
+
+  return GetHighestPriorityMeshName(*mesh_to_origins_);
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/meshes_with_origins.h b/shardy/dialect/mpmd/transforms/import/meshes_with_origins.h
new file mode 100644
index 0000000..2715aa2
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/meshes_with_origins.h
@@ -0,0 +1,131 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESHES_WITH_ORIGINS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESHES_WITH_ORIGINS_H_
+
+#include <stdbool.h>
+
+#include <optional>
+
+#include "llvm/ADT/MapVector.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/Dialect.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+inline constexpr StringRef kWildcardMesh = "*";
+
+using MeshToOrigins = llvm::MapVector<StringRef, SetVector<OriginAttr>>;
+
+// Wrapper around MeshesWithOriginsAttr to facilitate merging.
+//
+// This is used for mesh inference, for propagation of use and src sets. If
+// `has_meshes_specified_` is false, then the set is considered to be all
+// meshes.
+class MeshesWithOrigins {
+ public:
+  MeshesWithOrigins() = default;
+
+  explicit MeshesWithOrigins(MeshesWithOriginsAttr meshes_with_origins) {
+    if (!meshes_with_origins) {
+      return;
+    }
+    mesh_to_origins_ = MeshToOrigins();
+    for (MeshWithOriginsAttr origin : meshes_with_origins.getValue()) {
+      insert(origin);
+    }
+
+    if (!wildcard_origins_.empty()) {
+      // For now, we only allow the wildcard mesh if there are also other meshes
+      // specified. This makes the logic simpler for the rest of the code.
+      SDY_CHECK(!mesh_to_origins_->empty())
+          << "Wildcard mesh should only be specified if there are also other "
+             "meshes specified.";
+    }
+  }
+
+  explicit MeshesWithOrigins(
+      mlir::mpmd::MeshWithOriginsAttr mesh_with_origins) {
+    SDY_CHECK(mesh_with_origins)
+        << "MeshesWithOrigins should not be initialized "
+           "with a null MeshWithOriginsAttr.";
+    SDY_CHECK(mesh_with_origins.getMeshName() != kWildcardMesh)
+        << "Wildcard origin not allowed for this constructor.";
+    insert(mesh_with_origins);
+  }
+
+  // Constructs a UseSet from a MeshesWithOriginsAttr. If the attr is empty,
+  // then meshes_with_origins_ will be initialized to an empty set to make clear
+  // that the set is empty.
+  static MeshesWithOrigins CreateUseSet(
+      MeshesWithOriginsAttr meshes_with_origins) {
+    MeshesWithOrigins use_set(meshes_with_origins);
+    if (!use_set.mesh_to_origins_) {
+      use_set.mesh_to_origins_ = MeshToOrigins();
+    }
+    return use_set;
+  }
+
+  // Takes the union of the meshes with origins from the other set. Ignores
+  // unspecified sets. If the current set is unspecified, it will be
+  // initialized to the other set.
+  void Union(const MeshesWithOrigins& other);
+
+  // Takes the intersection of the meshes with origins from the other set.
+  // Ignores unspecified sets. If the current set is unspecified, it will be
+  // initialized to the other set.
+  void Intersect(const MeshesWithOrigins& other);
+
+  MeshesWithOriginsAttr ToAttr(OpBuilder& builder) const;
+  SmallVector<MeshWithOriginsAttr> ToArray(MLIRContext* context) const;
+
+  SetVector<StringRef> MeshNames(bool include_wildcard_mesh = false) const;
+  std::optional<SetVector<StringRef>> MaybeMeshNames(
+      bool include_wildcard_mesh = false) const;
+  SetVector<StringRef> MeshNamesOrEmpty(
+      bool include_wildcard_mesh = false) const;
+  bool HasSameMeshes(const MeshesWithOrigins& other) const;
+
+  // Returns the prioritized mesh name. Useful for making mesh assignments.
+  // Assumes that meshes are specified.
+  // Prioritizes as follows (in order):
+  // 1. Prioritizes meshes from `preferred_mesh_names` if any.
+  // 2. Prioritizes meshes which are not low priority (see `LowPriorityOrigin`).
+  // 3. Prioritizes meshes based on lexicographic order of the mesh name.
+  std::optional<StringRef> GetPrioritizedMeshName(
+      const SetVector<StringRef>& preferred_mesh_names = {}) const;
+
+  void insert(MeshWithOriginsAttr origin);
+
+  int size() const { return mesh_to_origins_ ? mesh_to_origins_->size() : -1; }
+  bool empty() const { return mesh_to_origins_ && mesh_to_origins_->empty(); }
+  bool has_meshes_specified() const { return mesh_to_origins_.has_value(); }
+  bool has_wildcard_mesh() const { return !wildcard_origins_.empty(); }
+
+  explicit operator bool() const { return has_meshes_specified(); }
+
+ protected:
+  std::optional<MeshToOrigins> mesh_to_origins_;
+  SetVector<OriginAttr> wildcard_origins_;
+};
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_MESHES_WITH_ORIGINS_H_
diff --git a/shardy/dialect/mpmd/transforms/import/meshes_with_origins_test.cc b/shardy/dialect/mpmd/transforms/import/meshes_with_origins_test.cc
new file mode 100644
index 0000000..c7c8de8
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/meshes_with_origins_test.cc
@@ -0,0 +1,356 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/import/meshes_with_origins.h"
+
+#include <optional>
+#include <utility>
+
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/Support/DebugStringHelper.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_inference_origins.h"
+#include <gmock/gmock.h>
+#include <gtest/gtest.h>
+
+namespace mlir::mpmd {
+namespace {
+
+using ::testing::UnorderedElementsAre;
+
+MeshesWithOrigins GetMeshesWithOrigins(
+    MLIRContext& context,
+    ArrayRef<std::pair<StringRef, std::optional<StringRef>>> mesh_and_origins) {
+  SmallVector<MeshWithOriginsAttr> mesh_with_origins_attrs;
+  for (auto [mesh, origin] : mesh_and_origins) {
+    mesh_with_origins_attrs.push_back(MeshWithOriginsAttr::get(
+        &context, mesh,
+        origin ? ArrayRef<OriginAttr>(OriginAttr::get(&context, *origin))
+               : ArrayRef<OriginAttr>()));
+  }
+  return MeshesWithOrigins(
+      MeshesWithOriginsAttr::get(&context, mesh_with_origins_attrs));
+}
+
+MATCHER_P(MeshesWithOriginsAttrEq, expected, "") {
+  if (arg != expected) {
+    *result_listener << "\n Expected: \n\t" << debugString(expected)
+                     << "\n Actual: \n\t" << debugString(arg);
+    return false;
+  }
+  return true;
+}
+
+}  // namespace
+
+TEST(MeshesWithOrigins, BasicFunctionality) {
+  MLIRContext context;
+  context.loadDialect<MpmdDialect>();
+
+  MeshesWithOrigins m1;
+  EXPECT_EQ(m1.size(), -1);
+  EXPECT_FALSE(m1.empty());
+  EXPECT_FALSE(m1.has_meshes_specified());
+
+  MeshesWithOrigins m2 = GetMeshesWithOrigins(context, {{"m1", {}}});
+  EXPECT_EQ(m2.size(), 1);
+  EXPECT_FALSE(m2.empty());
+  EXPECT_TRUE(m2.has_meshes_specified());
+  EXPECT_THAT(m2.MeshNames(), UnorderedElementsAre("m1"));
+
+  MeshesWithOrigins m3 = GetMeshesWithOrigins(context, {});
+  EXPECT_EQ(m3.size(), 0);
+  EXPECT_TRUE(m3.empty());
+  EXPECT_TRUE(m3.has_meshes_specified());
+
+  MeshesWithOrigins m4 = MeshesWithOrigins::CreateUseSet({});
+  EXPECT_EQ(m4.size(), 0);
+  EXPECT_TRUE(m4.empty());
+  EXPECT_TRUE(m4.has_meshes_specified());
+}
+
+TEST(MeshesWithOrigins, Union) {
+  MLIRContext context;
+  OpBuilder builder(&context);
+  context.loadDialect<MpmdDialect>();
+
+  MeshesWithOrigins m1 = GetMeshesWithOrigins(context, {{"m1", {}}});
+  MeshesWithOrigins m2 = GetMeshesWithOrigins(context, {{"m1", "o1"}});
+  MeshesWithOrigins m3 = GetMeshesWithOrigins(context, {{"m2", {}}});
+
+  MeshesWithOrigins m4_wildcard =
+      GetMeshesWithOrigins(context, {{"m2", "o1"}, {kWildcardMesh, "o2"}});
+  MeshesWithOrigins m5_wildcard =
+      GetMeshesWithOrigins(context, {{"m2", "o1"}, {kWildcardMesh, "o3"}});
+
+  m1.Union(m2);
+  m1.Union(m3);
+  EXPECT_FALSE(m1.has_wildcard_mesh());
+  EXPECT_TRUE(m4_wildcard.has_wildcard_mesh());
+
+  m1.Union(m4_wildcard);
+  EXPECT_THAT(m1.MeshNames(/*include_wildcard_mesh=*/true),
+              UnorderedElementsAre("m1", "m2", "*"));
+  EXPECT_TRUE(m1.has_wildcard_mesh());
+
+  m1.Union(m5_wildcard);
+  EXPECT_THAT(m1.MeshNames(/*include_wildcard_mesh=*/true),
+              UnorderedElementsAre("m1", "m2", "*"));
+  EXPECT_THAT(
+      m1.ToAttr(builder),
+      MeshesWithOriginsAttrEq(MeshesWithOriginsAttr::get(
+          &context,
+          {MeshWithOriginsAttr::get(&context, "m1",
+                                    {OriginAttr::get(&context, "o1")}),
+           MeshWithOriginsAttr::get(&context, "m2",
+                                    {OriginAttr::get(&context, "o1")}),
+           MeshWithOriginsAttr::get(&context, kWildcardMesh,
+                                    {OriginAttr::get(&context, "o2"),
+                                     OriginAttr::get(&context, "o3")})})));
+
+  MeshesWithOrigins m4;
+  EXPECT_FALSE(m4.has_meshes_specified());
+
+  MeshesWithOrigins m5;
+  m4.Union(m5);
+  EXPECT_FALSE(m5.has_meshes_specified());
+
+  m4.Union(m1);
+  EXPECT_TRUE(m4.has_meshes_specified());
+}
+
+TEST(MeshesWithOrigins, Intersect) {
+  MLIRContext context;
+  context.loadDialect<MpmdDialect>();
+
+  MeshesWithOrigins m1 = GetMeshesWithOrigins(context, {{"m1", {}}});
+  MeshesWithOrigins m2 = GetMeshesWithOrigins(context, {{"m1", "o1"}});
+  MeshesWithOrigins m3 = GetMeshesWithOrigins(context, {{"m2", {}}});
+
+  m1.Intersect(m2);
+  EXPECT_THAT(m1.MeshNames(), UnorderedElementsAre("m1"));
+
+  m1.Intersect(m3);
+  EXPECT_TRUE(m1.empty());
+
+  MeshesWithOrigins m4;
+  EXPECT_FALSE(m4.has_meshes_specified());
+
+  MeshesWithOrigins m5;
+  m4.Intersect(m5);
+  EXPECT_FALSE(m5.has_meshes_specified());
+
+  m4.Intersect(m2);
+  EXPECT_TRUE(m4.has_meshes_specified());
+  EXPECT_THAT(m4.MeshNames(), UnorderedElementsAre("m1"));
+}
+
+TEST(MeshesWithOrigins, IntersectWithWildcard) {
+  MLIRContext context;
+  OpBuilder builder(&context);
+  context.loadDialect<MpmdDialect>();
+
+  MeshesWithOrigins m1 = GetMeshesWithOrigins(context, {{"m1", {}}});
+  MeshesWithOrigins m2_wildcard =
+      GetMeshesWithOrigins(context, {{"m1", "o1"}, {kWildcardMesh, "o3"}});
+  MeshesWithOrigins m3_wildcard =
+      GetMeshesWithOrigins(context, {{"m2", "o2"}, {kWildcardMesh, "o4"}});
+
+  m1.Intersect(m2_wildcard);
+  EXPECT_THAT(m1.MeshNames(/*include_wildcard_mesh=*/true),
+              UnorderedElementsAre("m1"));
+
+  m1.Intersect(m3_wildcard);
+  EXPECT_THAT(m1.MeshNames(/*include_wildcard_mesh=*/true),
+              UnorderedElementsAre("m1"));
+
+  // Intersect two wildcard sets.
+  m2_wildcard.Intersect(m3_wildcard);
+  EXPECT_THAT(m2_wildcard.MeshNames(/*include_wildcard_mesh=*/true),
+              UnorderedElementsAre("m1", "m2", "*"));
+  EXPECT_THAT(
+      m2_wildcard.ToAttr(builder),
+      MeshesWithOriginsAttrEq(MeshesWithOriginsAttr::get(
+          &context,
+          {MeshWithOriginsAttr::get(&context, "m1",
+                                    {OriginAttr::get(&context, "o1"),
+                                     OriginAttr::get(&context, "o4")}),
+           MeshWithOriginsAttr::get(&context, "m2",
+                                    {OriginAttr::get(&context, "o2"),
+                                     OriginAttr::get(&context, "o3")}),
+           MeshWithOriginsAttr::get(&context, kWildcardMesh,
+                                    {OriginAttr::get(&context, "o3"),
+                                     OriginAttr::get(&context, "o4")})})));
+
+  // Intersect with an empty set.
+  MeshesWithOrigins m4;
+  EXPECT_TRUE(m4.MeshNamesOrEmpty(/*include_wildcard_mesh=*/true).empty());
+  m4.Intersect(m3_wildcard);
+  EXPECT_THAT(
+      m4.ToAttr(builder),
+      MeshesWithOriginsAttrEq(MeshesWithOriginsAttr::get(
+          &context,
+          {MeshWithOriginsAttr::get(&context, "m2",
+                                    {OriginAttr::get(&context, "o2")}),
+           MeshWithOriginsAttr::get(&context, kWildcardMesh,
+                                    {OriginAttr::get(&context, "o4")})})));
+}
+
+TEST(MeshesWithOrigins, Insert) {
+  MLIRContext context;
+  context.loadDialect<MpmdDialect>();
+  MeshesWithOrigins m;
+  EXPECT_FALSE(m.has_meshes_specified());
+
+  m.insert(MeshWithOriginsAttr::get(&context, "m1", {}));
+  EXPECT_TRUE(m.has_meshes_specified());
+  EXPECT_THAT(m.MeshNames(), UnorderedElementsAre("m1"));
+
+  m.insert(MeshWithOriginsAttr::get(&context, "m2", {}));
+  EXPECT_THAT(m.MeshNames(), UnorderedElementsAre("m1", "m2"));
+
+  m.insert(MeshWithOriginsAttr::get(&context, kWildcardMesh,
+                                    OriginAttr::get(&context, "o1")));
+  EXPECT_THAT(m.MeshNames(/*include_wildcard_mesh=*/true),
+              UnorderedElementsAre("m1", "m2", "*"));
+
+  m.insert(MeshWithOriginsAttr::get(&context, kWildcardMesh,
+                                    OriginAttr::get(&context, "o2")));
+  OpBuilder builder(&context);
+  EXPECT_THAT(m.ToAttr(builder),
+              MeshesWithOriginsAttrEq(MeshesWithOriginsAttr::get(
+                  &context, {MeshWithOriginsAttr::get(&context, "m1", {}),
+                             MeshWithOriginsAttr::get(&context, "m2", {}),
+                             MeshWithOriginsAttr::get(
+                                 &context, kWildcardMesh,
+                                 {OriginAttr::get(&context, "o1"),
+                                  OriginAttr::get(&context, "o2")})})));
+}
+
+TEST(MeshesWithOrigins, ToAttr) {
+  MLIRContext context;
+  context.loadDialect<MpmdDialect>();
+  MeshesWithOrigins m1 =
+      GetMeshesWithOrigins(context, {{"m1", "o1"}, {"m2", "o2"}});
+  OpBuilder builder(&context);
+  MeshesWithOriginsAttr attr = m1.ToAttr(builder);
+  EXPECT_EQ(attr.size(), 2);
+  EXPECT_EQ(
+      attr,
+      MeshesWithOriginsAttr::get(
+          &context, {MeshWithOriginsAttr::get(
+                         &context, "m1", {OriginAttr::get(&context, "o1")}),
+                     MeshWithOriginsAttr::get(
+                         &context, "m2", {OriginAttr::get(&context, "o2")})}));
+
+  MeshesWithOrigins m2;
+  EXPECT_EQ(m2.ToAttr(builder), nullptr);
+
+  MeshesWithOrigins m3 =
+      GetMeshesWithOrigins(context, {{"m1", "o1"}, {kWildcardMesh, "o2"}});
+  EXPECT_THAT(
+      m3.ToAttr(builder),
+      MeshesWithOriginsAttrEq(MeshesWithOriginsAttr::get(
+          &context,
+          {MeshWithOriginsAttr::get(&context, "m1",
+                                    {OriginAttr::get(&context, "o1")}),
+           MeshWithOriginsAttr::get(&context, kWildcardMesh,
+                                    {OriginAttr::get(&context, "o2")})})));
+}
+
+TEST(MeshesWithOrigins, HasSameMeshes) {
+  MLIRContext context;
+  context.loadDialect<MpmdDialect>();
+
+  MeshesWithOrigins m1 =
+      GetMeshesWithOrigins(context, {{"m1", "o1"}, {"m2", "o2"}});
+  MeshesWithOrigins m2 =
+      GetMeshesWithOrigins(context, {{"m1", {}}, {"m2", {}}});
+  MeshesWithOrigins m3 = GetMeshesWithOrigins(context, {{"m1", {}}});
+
+  // Ignores origin and order
+  EXPECT_TRUE(m1.HasSameMeshes(m2));
+  EXPECT_TRUE(m2.HasSameMeshes(m1));
+  EXPECT_FALSE(m1.HasSameMeshes(m3));
+  EXPECT_FALSE(m3.HasSameMeshes(m1));
+
+  MeshesWithOrigins m4;
+  MeshesWithOrigins m5;
+  EXPECT_TRUE(m4.HasSameMeshes(m5));
+  EXPECT_FALSE(m4.HasSameMeshes(m1));
+  EXPECT_FALSE(m1.HasSameMeshes(m4));
+
+  MeshesWithOrigins m6_wildcard = GetMeshesWithOrigins(
+      context, {{"m1", {}}, {"m2", {}}, {kWildcardMesh, "o2"}});
+  MeshesWithOrigins m7_wildcard = GetMeshesWithOrigins(
+      context, {{"m1", {}}, {"m2", "o1"}, {kWildcardMesh, "o3"}});
+  EXPECT_TRUE(m6_wildcard.HasSameMeshes(m7_wildcard));
+  EXPECT_FALSE(m6_wildcard.HasSameMeshes(m1));
+  EXPECT_FALSE(m6_wildcard.HasSameMeshes(m2));
+}
+
+TEST(MeshesWithOrigins, GetPrioritizedMeshName) {
+  MLIRContext context;
+  context.loadDialect<MpmdDialect>();
+  MeshesWithOrigins m1 =
+      GetMeshesWithOrigins(context, {{"m2", "o1"}, {"m1", "o2"}});
+  EXPECT_EQ(m1.GetPrioritizedMeshName(), "m1");
+
+  MeshesWithOrigins m2 = MeshesWithOrigins::CreateUseSet({});
+  EXPECT_EQ(m2.GetPrioritizedMeshName(), std::nullopt);
+
+  SetVector<StringRef> preferred_mesh_names;
+  preferred_mesh_names.insert("m3");
+  EXPECT_EQ(m1.GetPrioritizedMeshName(preferred_mesh_names), "m1");
+  preferred_mesh_names.insert("m2");
+  EXPECT_EQ(m1.GetPrioritizedMeshName(preferred_mesh_names), "m2");
+}
+
+TEST(MeshesWithOrigins, GetPrioritizedMeshNameHandlingLowPriorityOrigins) {
+  MLIRContext context;
+  context.loadDialect<MpmdDialect>();
+
+  StringRef low_priority_origin = kBroadcastInputOrigin;
+  MeshesWithOrigins m =
+      GetMeshesWithOrigins(context, {{"m1", low_priority_origin},
+                                     {"m2", "o2"},
+                                     {"m3", low_priority_origin},
+                                     {"m4", "o4"},
+                                     {kWildcardMesh, low_priority_origin}});
+  EXPECT_EQ(m.GetPrioritizedMeshName(), "m2");
+
+  SetVector<StringRef> preferred_mesh_names;
+  preferred_mesh_names.insert("non_existent_mesh_2");
+  EXPECT_EQ(m.GetPrioritizedMeshName(preferred_mesh_names),
+            "non_existent_mesh_2");
+  preferred_mesh_names.insert("non_existent_mesh_1");
+  EXPECT_EQ(m.GetPrioritizedMeshName(preferred_mesh_names),
+            "non_existent_mesh_1");
+
+  preferred_mesh_names.insert("m3");
+  EXPECT_EQ(m.GetPrioritizedMeshName(preferred_mesh_names), "m3");
+
+  preferred_mesh_names.insert("m1");
+  EXPECT_EQ(m.GetPrioritizedMeshName(preferred_mesh_names), "m1");
+
+  preferred_mesh_names.insert("m4");
+  EXPECT_EQ(m.GetPrioritizedMeshName(preferred_mesh_names), "m4");
+  preferred_mesh_names.insert("m2");
+  EXPECT_EQ(m.GetPrioritizedMeshName(preferred_mesh_names), "m2");
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/passes.h b/shardy/dialect/mpmd/transforms/import/passes.h
new file mode 100644
index 0000000..bdb1a49
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/passes.h
@@ -0,0 +1,78 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_PASSES_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_PASSES_H_
+
+// IWYU pragma: begin_keep
+
+#include <memory>
+
+#include "llvm/Support/CommandLine.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/transforms/common/distributed_function_pass.h"
+#include "shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.h"
+#include "shardy/dialect/mpmd/transforms/import/mesh_assignment_map.h"
+#include "shardy/dialect/mpmd/transforms/import/sharding_constraints.h"
+
+// IWYU pragma: end_keep
+
+namespace mlir::mpmd {
+
+// Returns the maximum number of errors to emit when applying a single
+// validation pass in mesh inference given `error_limit`. If set to -1, emit all
+// errors. Cannot be 0. When set to -1, this also enables other verbose logging.
+int GetValidatedMaxErrors(int error_limit);
+
+// Options for the import pipeline.
+struct ImportOptions {
+  // Mapping between names (of computations and tensors) and mesh names, and
+  // optionally stage ids
+  UserAssignmentMapOption nameToMeshAssignment;
+  // Mapping between function input indices and assigned mesh names.
+  IndexedAssignmentMapOption inputIndexToMeshAssignment;
+  // Mapping between function output indices and assigned mesh names.
+  IndexedAssignmentMapOption outputIndexToMeshAssignment;
+  // Constraints enforcing inputs and outputs to be assigned to the same mesh.
+  SmallVector<InputOutputEquishardingConstraint> inputOutputConstraints;
+  // Whether to merge inferred fragments only after scheduling.
+  bool mergeAfterScheduling = false;
+  // Whether to absorb inferred fragments into user-defined fragments on
+  // entry-point functions.
+  bool absorbInferredFragmentsOnEntryPointFunction = false;
+  // Whether to clone inferred fragments when merging.
+  bool cloneInferredFragments = false;
+  // Infer mesh pipeline options.
+  InferMeshOptions inferMeshOptions;
+};
+
+// Adds the standard set of passes to import an MPMD program with a fixed mesh
+// assignment map.
+void addImportPipeline(OpPassManager& pm, ImportOptions options);
+
+// Register the `-mpmd-import-pipeline`.
+void registerImportPipeline();
+
+#define GEN_PASS_DECL
+#define GEN_PASS_REGISTRATION
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_PASSES_H_
diff --git a/shardy/dialect/mpmd/transforms/import/passes.td b/shardy/dialect/mpmd/transforms/import/passes.td
new file mode 100644
index 0000000..8a689ca
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/passes.td
@@ -0,0 +1,422 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+def Mpmd_UserAssignmentMapOption :
+    Option<"assignment", "assignment", "UserAssignmentMapOption",
+           /*default=*/"UserAssignmentMapOption()",
+           "Mapping between names (of computations and tensors) and mesh "
+           "names, and optionally stage ids. E.g., 'n0@m0,n1@m1' defines that "
+           "names n0 and n1 will be assigned to meshes m0 and m1, "
+           "respectively. Alternatively 'n0@m0/0,n1@m1/1' means that these "
+           "names are also assigned to the stages 0 and 1.">;
+
+def Mpmd_InputOutputEquishardingConstraintsOption :
+      ListOption<"constraints", "constraints",
+                 "InputOutputEquishardingConstraint",
+                 "A list of constraint, each enforcing that an input and "
+                 "output should be assigned to the same mesh.">;
+
+def Mpmd_InferTransfersOption :
+    Option<"inferTransfers", "infer-transfers", "bool", /*default=*/"false",
+           "Whether to create transfers when needed, instead of erroring.">;
+
+def Mpmd_ErrorLimitOption :
+    Option<"errorLimit", "error-limit", "int", /*default=*/"5",
+           "The number of errors to emit. Set to -1 to emit all errors. Cannot "
+           "be 0.">;
+
+def CopyTopologyFromMainPass : Pass<"mpmd-copy-topology-from-main", "ModuleOp"> {
+  let summary = "Copies the topology from the main function to functions "
+                "referred by mpmd.call.";
+  let description = [{
+    Copies the topology attribute from the main function to any function
+    referred by a mpmd.call. This also sets the mpmd.call callee to private
+    visibility, to avoid being mistaken for an entry point function.
+  }];
+  let dependentDialects = ["mlir::sdy::SdyDialect"];
+}
+
+// TODO(jupvfranco): Rename to EnforceEquiassignmentPass.
+def EnforceEquishardingPass :
+    Pass<"mpmd-enforce-input-output-equisharding", "func::FuncOp"> {
+  let summary = "Enforces equisharding constraints for MPMD functions.";
+  let description = [{
+    Enforces input-output equisharding constraints for MPMD functions by
+    introducing TransferOps when necessary.
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Mpmd_InputOutputEquishardingConstraintsOption
+  ];
+}
+
+def MapNamedOpsToMpmdOpsPass :
+    PassBase<"mpmd-map-named-ops-to-mpmd-ops", "DistributedFunctionPass"> {
+  let summary = "Assigns meshes to user defined operations.";
+  let description = [{
+    Creates a pass optionally assigning mpmd.named_tensor to
+    Assign(Unassign(%v)) (depends if there is an entry in `assignment`), and to
+    map each named_computation to a mesh, using a user-defined mapping between
+    named_computations and mesh names. This means replacing each
+    named_computation with a Fragment and creating AssignOps for the operands
+    and UnassignOps for the results of these Fragments. The now introduced
+    pattern Assign(Unassign(%v)) is rewritten into a Transfer(%v).
+    No named_computation/named_tensor ops will exist after this pass.
+
+    Requires: all named_computations and named_tensors to live at the top-level
+    of the function.
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Mpmd_UserAssignmentMapOption
+  ];
+}
+
+def MapInputOutputToMeshPass :
+    Pass<"mpmd-map-input-output-to-mesh", "ModuleOp"> {
+  let summary = "Assigns meshes to function inputs and output.";
+  let description = [{
+    Creates a pass that maps function inputs/outputs to meshes given a
+    user-defined mesh assignment.
+
+    For the input arguments, this pass:
+    1. Casts the input tensors that should be put on a mesh to a mesh
+       tensor.
+    2. Updates the function signature.
+    3. Adds mpmd.unassign before the tensor is used.
+
+    For the output arguments, this pass adds mpmd.assign before the tensor
+    is returned and updates the function signature.
+
+    Requires: Each input/output index is valid and each mapped mesh
+    is a valid mesh in the topology.
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Option<"inputAssignment", "input-assignment", "IndexedAssignmentMapOption",
+           /*default=*/"IndexedAssignmentMapOption()",
+           "Mapping between function input indices and assigned mesh names."
+           "E.g., '0@m0,1@m1' defines that input with index 0 will be assigned "
+           "to mesh m0 and input with index 1 will be assigned to mesh m1.">,
+    Option<"outputAssignment", "output-assignment", "IndexedAssignmentMapOption",
+           /*default=*/"IndexedAssignmentMapOption()",
+           "Mapping between function output indices and assigned mesh names."
+           "E.g., '0@m0,1@m1' defines that output with index 0 will be "
+           "assigned to mesh m0 and output with index 1 will be assigned to "
+           "mesh m1.">,
+  ];
+}
+
+def InlineNestedUserExposedOpsPass :
+    PassBase<"mpmd-inline-nested-user-exposed-ops", "DistributedFunctionPass"> {
+  let summary = "Inlines any user-exposed mpmd op nested in a named_computation.";
+  let description = [{
+    Inlines any named_computation, named_tensor, broadcast and reduce op that is
+    nested in a named_computation, checking that its mesh assignment (when
+    defined) matches that of the parent.
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Mpmd_UserAssignmentMapOption
+  ];
+}
+
+def IntroduceTransfersPass : Pass<"mpmd-introduce-transfers", "ModuleOp"> {
+  let summary = "Creates data transfers based on user mesh assignments.";
+  let description = [{
+    Creates a pass that introduces transfer operations based on user mesh
+    assignments. This includes:
+
+    1. Push in UnassignOp to mpmd calls if the result of UnassignOp is later
+       assigned in the callee.
+    2. Replaces the AssignOp of an UnassignOp with a TransferOp.
+    3. Assign the addition to the consuming mesh and introduce a transfer if
+       there is a meshless addition between fragments.
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+}
+
+def InsertNamelessCloneOfNeglibleOpsPass :
+    PassBase<"mpmd-insert-nameless-clone-of-negligible-ops",
+             "DistributedFunctionPass"> {
+  let summary = "Clones negligible ops outside of named computations.";
+  let description = [{
+    Clones negligible operations, i.e., single result, zero operand operations,
+    outside named computations whenever they are used by the computation's
+    return op, replacing the named_computation's result with the clone. This is
+    needed because if such results are used by named computations assigned to
+    different meshes, this could cause a mesh inference conflict. By applying
+    this pass, we allow mesh inference to clone these negligible ops.
+
+    This pass does NOT change the named computation at all.
+  }];
+}
+
+// TODO: b/430024003 - Replace with simpler dedup and dce passes.
+def SimplifyNamedComputationsPass :
+    PassBase<"mpmd-simplify-named-computations", "DistributedFunctionPass"> {
+  let summary = "Simplifies the inputs and outputs of named computation ops.";
+  let description = [{
+    Simplifies each named computation independently. In particular, it:
+    - deduplicates results, and their corresponding return values;
+    - deduplicates operands, and their corresponding block arguments;
+    - removes results whose corresponding return operand is a block
+      argument of the op;
+    - removes operands whose corresponding block argument has no more uses (or
+      didn't have any to begin with); and
+    - removes results that are unused.
+    - replaces the pattern `arg -> stablehlo.optimization_barrier -> return`
+      within a named computation with the pattern `arg -> return`, allowing
+      further simplification.
+  }];
+}
+
+def ValidateNamedOpsInMpmdFuncPass :
+    Pass<"mpmd-validate-named-ops-in-mpmd-func", "func::FuncOp"> {
+  let summary = "Validates that named ops are only nested in mpmd functions.";
+  let description = [{
+    Validates that NamedComputationOp and NamedTensorOp are only nested in
+    mpmd functions, i.e., functions with a topology attr.
+  }];
+}
+
+//===----------------------------------------------------------------------===//
+// Start of - Infer Mesh assignment passes
+//===----------------------------------------------------------------------===//
+
+def InferMeshPopulateUseSetPass :
+    Pass<"mpmd-infer-mesh-populate-use-set", "ModuleOp"> {
+  let summary = "Initializes the use_set for AssignOps and propagates the use_set.";
+  let description = [{
+    This pass initializes the use_set and propagates it backwards, populating the
+    graph with the use_set info.
+
+    Initialization: the use_set of an AssignOp is set to the mesh that it
+    assigns to.
+
+    Propagation: use_sets propagate backwards from users to the op itself, taking
+    the union of users. An op's use_set is the union of its users' use_sets by
+    definition, since the use_set is the set of transitive uses.
+  }];
+}
+
+def InferMeshPopulateSrcSetPass :
+    Pass<"mpmd-infer-mesh-populate-src-set", "ModuleOp"> {
+  let summary = "Initializes the src_set for UnassignOps and func args and "
+                "propagates the src_set.";
+  let description = [{
+    This pass initializes the src_set and propagates it, populating the graph
+    with the src_set info.
+
+    Pre-condition: for func args to have src_sets, the use_set must be populated.
+
+    Initialization: the src_set of an UnassignOp is set to the mesh that it
+    assigns to. The src_set of a func arg is set to its use_set.
+
+    Propagation: src_sets propagate forwards from operands to the op itself,
+    taking the intersection of operands. See `PropagateSrcSet` for details.
+  }];
+}
+
+def InferMeshAssignUsingInputOutputConstraintsPass :
+    Pass<"mpmd-infer-mesh-assign-using-input-output-constraints", "ModuleOp"> {
+  let summary = "Assigns a mesh to the inputs and outputs according to the "
+                "input-output assignment constraints.";
+  let description = [{
+    This pass uses the input-output equi-assignment constraints to assign both
+    the input and output to the same mesh.
+
+    Note that this guarantees the input is on the same mesh, regardless of
+    whether it is subsequently transferred to other meshes. But this means that
+    we should not run `populate-src-set` after this pass.
+
+    *Requires:*
+      * for any input `i` of the entry-point function that may be part of an
+      equi-assignment constraint: `i` has a MeshTensorType, OR a well-defined
+      use-set.
+      * for any output `o` of the entry-point function that may be part of an
+      equi-assignment constraint: `o` has type MeshTensorType, OR well-defined
+      src- and use- sets.
+
+    Where a well-defined use-set of a value includes all the meshes the value is
+    (transitively) assigned to, via mpmd.assign ops and no other mesh. A
+    well-defined src-set includes all the meshes where the tensor is allowed to
+    live and no other mesh..
+
+    Although this only runs on entry point functions, we make this a module op
+    pass because this requires all passes on existing functions to complete
+    before it runs. E.g. if we make this an `EntryPointFunctionPass` then the
+    pass manager might run this pass before validation completes on the non-entry
+    point functions.
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Option<"verboseLogging", "verbose-logging", "bool", /*default=*/"false",
+           "Whether to enable verbose logging">,
+    Mpmd_InputOutputEquishardingConstraintsOption
+  ];
+}
+
+def InferMeshAssignMeshForFuncLeavesPass :
+    Pass<"mpmd-infer-mesh-assign-mesh-func-leaves", "ModuleOp"> {
+  let summary = "Assigns a mesh to each unused computation, function output, "
+                "and function input using the use_set and src_set analysis.";
+  let description = [{
+    This pass assigns meshes to the func body leaves (i.e., the results of unused
+    computations, unused function arguments, and function outputs) by creating
+    AssignOps or changing the type, using the use_set and src_set information.
+
+    We also treat certain intermediate values as leaves, for the sake of
+    analysis. Namely: the operands of an mpmd.reduce and mpmd.broadcast are
+    treated as leaves and an assign-unassign pair will be created on them.
+
+    This assignment will clear the use_set of all non-leaf ops, as the previously
+    annotated uses will be stale since inferring reduce ops will change the
+    use_set of some values: the initial use_set propagation is unaware of reduce
+    ops, but now that we've inferred reduce ops, the propagation will be
+    different.
+
+    This pass will fail, emitting errors, if use- and src-sets aren't correctly
+    populated for leaf ops.
+
+    Pre-condition: every op has a non-empty src-set, or we infer transfers.
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Mpmd_InferTransfersOption,
+    Mpmd_ErrorLimitOption
+  ];
+}
+
+def InferMeshConvertReduceOpsPass :
+    Pass<"mpmd-infer-mesh-convert-reduce-ops", "func::FuncOp"> {
+  let summary = "Converts annotated reduce ops to mpmd.reduce ops and flattens "
+                "chains of reduce ops.";
+  let description = [{
+    Converts the annotated reduce ops to mpmd.reduce ops, and also flattens
+    chains of these reduce ops.
+
+    In symbols:
+
+    x = add(w0, w1) {mpmd.reduce = #mpmd.reduce<add>}
+    y = add(x, w2) {mpmd.reduce = #mpmd.reduce<add>}
+      ~~>
+    r = mpmd.reduce<add>(w0,w1,w2)
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Option<"inferCrossMeshReductions", "infer-cross-mesh-reductions", "bool",
+           /*default=*/"false",
+           "Whether to infer cross-mesh reductions. Will be enabled by default "
+           "once stable.">
+  ];
+}
+
+def InferMeshRewriteUsingAnalysisPass :
+    Pass<"mpmd-infer-mesh-rewrite-using-analysis", "ModuleOp"> {
+  let summary = "Rewrites ops according to the use_set.";
+  let description = [{
+    This pass assigns meshless ops by wrapping them in fragments, using the
+    use_set and src_set analyses.
+
+    It also removes the use_set and src_set attributes as part of clean up, since
+    the analyses are no longer needed after this.
+
+    Pre-condition: every op has a use_set, i.e. analysis is complete.
+    Pre-condition: every argument of a non-entry point function is used at least
+    by one non-terminator op.
+
+    TODO: jupvfranco - consider renaming this pass given that it doesn't depend
+    on the analysis so much anymore.
+  }];
+  let dependentDialects = [
+    "mlir::mpmd::MpmdDialect",
+    // This pass creates stablehlo reduction ops.
+    "mlir::stablehlo::StablehloDialect"
+  ];
+
+  let options = [
+    Option<"maxClones", "max-clones", "int", /*default=*/"1",
+           "How many copies of a meshless operation we allow. Setting it to 1 "
+           "means we never clone the op.">
+  ];
+}
+
+def InferMeshFinalizePass :
+    Pass<"mpmd-infer-mesh-finalize", "ModuleOp"> {
+  let summary = "Applies final clean up after patterns mesh inference.";
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Mpmd_InferTransfersOption
+  ];
+}
+
+def InferMeshValidateSrcSetNotEmptyPass :
+    Pass<"mpmd-infer-mesh-validate-src-set-not-empty", "ModuleOp"> {
+  let summary = "Validates that every meshless op has a non-empty src_set.";
+  let description = [{
+    This pass validates all meshless ops, checking that the op can be assigned
+    somewhere. I.e. for meshless ops which aren't func ops, it errors when the
+    src_set is empty on an op or if it was inferred to be a cross-mesh reduction
+    but it is not converted. This is a prerequisite for func leaves assignment.
+
+    For func ops, it suffices to check the above conditions for the func args,
+    since the func returns either meshless ops, or block args.
+
+    This needs to be a pass on the module level, since in the case of an error
+    on the callee, we want to print the callers.
+
+    Pre-condition: cross-mesh reductions should be converted to reduce ops before
+    this pass is run.
+  }];
+
+  let options = [
+    Mpmd_ErrorLimitOption
+  ];
+}
+
+def InferMeshValidateNoAdditionalTransfersNeededPass :
+    Pass<"mpmd-infer-mesh-validate-no-additional-transfers-needed", "ModuleOp"> {
+  let summary = "Validates no additional transfers are needed for mesh assignment.";
+  let description = [{
+    This pass validates that mesh assignment is possible for all meshless ops
+    without introducing any additional transfers.
+
+    For meshless ops which aren't func ops, it errors when:
+    1. use_set is not contained in the src_set for a given op, i.e. a transfer
+    is needed.
+
+    For func ops, it suffices to check the above conditions for the func args,
+    since the func returns either meshless ops, or block args.
+  }];
+
+  let options = [
+    Mpmd_ErrorLimitOption
+  ];
+}
+
+//===----------------------------------------------------------------------===//
+// End of - Infer Mesh assignment passes
+//===----------------------------------------------------------------------===//
diff --git a/shardy/dialect/mpmd/transforms/import/sharding_constraints.cc b/shardy/dialect/mpmd/transforms/import/sharding_constraints.cc
new file mode 100644
index 0000000..49b71a5
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/sharding_constraints.cc
@@ -0,0 +1,52 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/import/sharding_constraints.h"
+
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace mlir::mpmd {
+
+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                        const InputOutputEquishardingConstraint& constraint) {
+  return os << constraint.input_index << ":" << constraint.output_index;
+}
+
+}  // namespace mlir::mpmd
+
+namespace llvm::cl {
+using ::mlir::mpmd::InputOutputEquishardingConstraint;
+
+template class basic_parser<InputOutputEquishardingConstraint>;
+
+bool parser<InputOutputEquishardingConstraint>::parse(
+    Option& opt, StringRef, StringRef arg,
+    InputOutputEquishardingConstraint& value) {
+  auto [input_index_str, output_index_str] = arg.split(':');
+  auto& [input_index, output_index] = value;
+  if (input_index_str.getAsInteger(10, input_index)) {
+    return opt.error("Failed to parse input index: " + input_index_str);
+  }
+  if (output_index_str.getAsInteger(10, output_index)) {
+    return opt.error("Failed to parse output index: " + output_index_str);
+  }
+  return false;
+}
+
+void parser<InputOutputEquishardingConstraint>::anchor() {}
+
+}  // namespace llvm::cl
diff --git a/shardy/dialect/mpmd/transforms/import/sharding_constraints.h b/shardy/dialect/mpmd/transforms/import/sharding_constraints.h
new file mode 100644
index 0000000..21ddaf1
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/sharding_constraints.h
@@ -0,0 +1,69 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_SHARDING_CONSTRAINTS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_SHARDING_CONSTRAINTS_H_
+
+#include <cstdint>
+
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace mlir::mpmd {
+
+// A constraint enforcing that an input and an output (of the same global shape)
+// should be partitioned the same way.
+struct InputOutputEquishardingConstraint {
+  InputOutputEquishardingConstraint(): input_index(-1), output_index(-1) {};
+  InputOutputEquishardingConstraint(int64_t input_index, int64_t output_index)
+      : input_index(input_index), output_index(output_index) {}
+
+  int64_t input_index;
+  int64_t output_index;
+
+  bool operator==(const InputOutputEquishardingConstraint& other) const {
+    return input_index == other.input_index &&
+           output_index == other.output_index;
+  }
+};
+
+llvm::raw_ostream& operator<<(
+    llvm::raw_ostream& os, const InputOutputEquishardingConstraint& constraint);
+
+}  // namespace mlir::mpmd
+
+namespace llvm::cl {
+
+using ::mlir::mpmd::InputOutputEquishardingConstraint;
+
+extern template class basic_parser<InputOutputEquishardingConstraint>;
+
+template <>
+class parser<InputOutputEquishardingConstraint>
+    : public basic_parser<InputOutputEquishardingConstraint> {
+ public:
+  parser(Option& opt) : basic_parser(opt) {}
+  bool parse(Option& opt, StringRef argName, StringRef arg,
+             InputOutputEquishardingConstraint& value);
+  StringRef getValueName() const override {
+    return "input-output-equisharding-constraint";
+  }
+  void anchor() override;
+};
+
+}  // namespace llvm::cl
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_IMPORT_SHARDING_CONSTRAINTS_H_
diff --git a/shardy/dialect/mpmd/transforms/import/simplify_named_computations.cc b/shardy/dialect/mpmd/transforms/import/simplify_named_computations.cc
new file mode 100644
index 0000000..76ad60c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/simplify_named_computations.cc
@@ -0,0 +1,176 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <utility>
+
+#include "llvm/ADT/BitVector.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Block.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Rewrite/FrozenRewritePatternSet.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/transforms/common/simplify_region_op_base.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_SIMPLIFYNAMEDCOMPUTATIONSPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+using ::mlir::stablehlo::OptimizationBarrierOp;
+
+class SimplifyNamedComputationPattern
+    : public SimplifyRegionOpPatternBase<NamedComputationOp> {
+  using SimplifyRegionOpPatternBase::SimplifyRegionOpPatternBase;
+
+  NamedComputationOp createNewOp(NamedComputationOp op,
+                                 PatternRewriter& rewriter,
+                                 TypeRange result_types, ValueRange operands,
+                                 BitVector erased_results) const override {
+    return rewriter.create<NamedComputationOp>(op.getLoc(), result_types,
+                                               operands, op.getOriginAttr());
+  }
+};
+
+// This pattern replaces the pattern within a NamedComputationOp
+// `arg -> OptBarrier -> return` with the pattern `arg -> return`, allowing
+// further simplification of the NamedComputation.
+//
+// This may happen when jax.remat is used and unrelated args are thus captured
+// in the NamedComputation.
+//
+// It is generally safe to apply, since between programs, the OptBarrier is a
+// no-op, although we may end up merging with other ops which may need the
+// OptBarrier. But we assume that only values that are actually used are what we
+// need the OptBarrier on.
+class SimplifyOptimizationBarrier
+    : public OpRewritePattern<OptimizationBarrierOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+ public:
+  LogicalResult matchAndRewrite(OptimizationBarrierOp op,
+                                PatternRewriter& rewriter) const override {
+    auto named_comp_parent = dyn_cast<NamedComputationOp>(op->getParentOp());
+    if (!named_comp_parent) {
+      return rewriter.notifyMatchFailure(
+          op, [&](Diagnostic& diag) { diag << "Not in a named computation"; });
+    }
+
+    llvm::BitVector kept_operands(op->getNumOperands());
+    for (auto [idx, operand] : llvm::enumerate(op->getOperands())) {
+      if (isa<BlockArgument>(operand)) {
+        OpResult res = op->getResult(idx);
+        // We could extend this to handle multiple uses, but it's not clear
+        // that's needed.
+        if (res.hasOneUse() && isa<ReturnOp>(*res.getUsers().begin())) {
+          rewriter.replaceAllUsesWith(res, operand);
+          continue;
+        }
+      }
+      kept_operands.set(idx);
+    }
+
+    if (kept_operands.count() == op->getNumOperands()) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "All operands are kept, nothing to do.";
+      });
+    }
+
+    llvm::SmallVector<Value> new_operands;
+    for (auto [idx, operand] : llvm::enumerate(op->getOperands())) {
+      if (kept_operands.test(idx)) {
+        new_operands.push_back(operand);
+      }
+    }
+    // Replace opt_barrier op with a new one with the kept operands.
+    auto opt_barrier =
+        rewriter.create<OptimizationBarrierOp>(op.getLoc(), new_operands);
+
+    int opt_barrier_idx = 0;
+    for (auto [idx, operand] : llvm::enumerate(op->getOperands())) {
+      if (!kept_operands.test(idx)) {
+        continue;
+      }
+      rewriter.replaceAllUsesWith(op->getResult(idx),
+                                  opt_barrier->getResult(opt_barrier_idx));
+      opt_barrier_idx++;
+    }
+
+    rewriter.eraseOp(op);
+    return success();
+  }
+};
+
+class SimplifyNamedComputationsPass
+    : public impl::SimplifyNamedComputationsPassBase<
+          SimplifyNamedComputationsPass> {
+  using SimplifyNamedComputationsPassBase::SimplifyNamedComputationsPassBase;
+
+  LogicalResult initialize(MLIRContext* context) final {
+    {
+      RewritePatternSet patternsInternal(context);
+      patternsInternal.add<SimplifyOptimizationBarrier>(context);
+      opt_barrier_patterns = std::move(patternsInternal);
+    }
+
+    {
+      RewritePatternSet patternsInternal(context);
+      patternsInternal.add<SimplifyNamedComputationPattern>(context);
+      named_computation_patterns = std::move(patternsInternal);
+    }
+
+    return success();
+  }
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    IRRewriter rewriter(func_op.getContext());
+
+    // NOTE: We may possibly want to do this in JAXPR. Apply these passes first
+    // to avoid repeated simplification.
+    GreedyRewriteConfig config;
+    config.setRegionSimplificationLevel(GreedySimplifyRegionLevel::Disabled)
+        .enableFolding(false)
+        .enableConstantCSE(false);
+    if (failed(applyPatternsGreedily(func_op, opt_barrier_patterns, config))) {
+      return signalPassFailure();
+    }
+
+    config.setRegionSimplificationLevel(GreedySimplifyRegionLevel::Normal);
+    if (failed(applyPatternsGreedily(func_op, named_computation_patterns,
+                                     config))) {
+      return signalPassFailure();
+    }
+  }
+
+  FrozenRewritePatternSet opt_barrier_patterns;
+  FrozenRewritePatternSet named_computation_patterns;
+};
+
+}  // namespace
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/import/test/BUILD b/shardy/dialect/mpmd/transforms/import/test/BUILD
new file mode 100644
index 0000000..956593e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/BUILD
@@ -0,0 +1,21 @@
+# Lit tests for the MPMD import passes.
+
+load("//shardy:lit.bzl", "glob_lit_tests")
+
+package(default_visibility = ["//visibility:public"])
+
+filegroup(
+    name = "test_data",
+    testonly = True,
+    data = [
+        "//shardy/tools:mpmd_opt",
+        "@llvm-project//llvm:FileCheck",
+    ],
+)
+
+glob_lit_tests(
+    name = "all_tests",
+    data = [":test_data"],
+    driver = "@llvm-project//mlir:run_lit.sh",
+    test_file_exts = ["mlir"],
+)
diff --git a/shardy/dialect/mpmd/transforms/import/test/call_op_set_topology.mlir b/shardy/dialect/mpmd/transforms/import/test/call_op_set_topology.mlir
new file mode 100644
index 0000000..2a83989
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/call_op_set_topology.mlir
@@ -0,0 +1,40 @@
+// RUN: mpmd_opt %s -mpmd-copy-topology-from-main 2>&1 | FileCheck %s
+
+// CHECK-LABEL: sdy.mesh @mesh = <["a"=4, "b"=2]>
+#topology = #mpmd.topology<<"mesh1": <["a"=4, "b"=2]>>, <"mesh2": <["a"=4, "b"=2]>>>
+
+// CHECK-LABEL: func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<5xf32>, tensor<5xf32>, tensor<5xf32>) attributes {topology = #mpmd.topology<<"mesh1" : <["a"=4, "b"=2]>>, <"mesh2" : <["a"=4, "b"=2]>>>}
+func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<5xf32>, tensor<5xf32>, tensor<5xf32>) attributes {
+  topology=#topology
+}{
+  %0 = mpmd.call @shardy_mpmd_f(%arg0, %arg1) : (tensor<5xf32>, tensor<5xf32>) -> tensor<5xf32>
+  %1 = mpmd.call @shardy_mpmd_g(%arg0, %arg1) : (tensor<5xf32>, tensor<5xf32>) -> tensor<5xf32>
+  %2 = call @shardy_mpmd_i(%arg0, %arg1) : (tensor<5xf32>, tensor<5xf32>) -> tensor<5xf32>
+  return %0, %1, %2 : tensor<5xf32>, tensor<5xf32>, tensor<5xf32>
+}
+
+// CHECK-LABEL: func.func private @shardy_mpmd_f(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> tensor<5xf32> attributes {topology = #mpmd.topology<<"mesh1" : <["a"=4, "b"=2]>>, <"mesh2" : <["a"=4, "b"=2]>>>}
+func.func @shardy_mpmd_f(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> tensor<5xf32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<5xf32>
+  return %0 : tensor<5xf32>
+}
+
+// CHECK-LABEL: func.func private @shardy_mpmd_g(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> tensor<5xf32> attributes {topology = #mpmd.topology<<"mesh1" : <["a"=4, "b"=2]>>, <"mesh2" : <["a"=4, "b"=2]>>>}
+func.func private @shardy_mpmd_g(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> tensor<5xf32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<5xf32>
+  return %0 : tensor<5xf32>
+}
+
+// Not referred by a call_op, so it doesn't get a topology or have visibility changed.
+// CHECK-LABEL: func.func @shardy_mpmd_h(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> tensor<5xf32> {
+func.func @shardy_mpmd_h(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> tensor<5xf32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<5xf32>
+  return %0 : tensor<5xf32>
+}
+
+// Referred by a func.call_op, so it doesn't get a topology or have visibility changed.
+// CHECK-LABEL: func.func @shardy_mpmd_i(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> tensor<5xf32> {
+func.func @shardy_mpmd_i(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> tensor<5xf32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<5xf32>
+  return %0 : tensor<5xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/convert_none_reduce.mlir b/shardy/dialect/mpmd/transforms/import/test/convert_none_reduce.mlir
new file mode 100644
index 0000000..fe4c1e6
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/convert_none_reduce.mlir
@@ -0,0 +1,96 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-convert-reduce-ops 2>&1 | FileCheck --implicit-check-not mpmd.reduce %s
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>>
+
+// CHECK-LABEL: func @convert_none_reduce_to_actual_reduce
+func.func @convert_none_reduce_to_actual_reduce(%arg0: tensor<4x8xf32> , %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  // CHECK: %[[REDUCE:.*]] = mpmd.reduce<max>  %arg0, %arg1
+  // CHECK-NEXT: %[[REDUCE_NONE:.*]] = mpmd.reduce<none>  %[[REDUCE]]
+  // CHECK-NEXT: return %[[REDUCE_NONE]]
+  %0 = stablehlo.maximum %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<max>} : tensor<4x8xf32>
+  %1 = mpmd.reduce<none> %0 : (tensor<4x8xf32>) -> (tensor<4x8xf32>)
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @cannot_convert_none_reduce_to_actual_reduce_if_reduction_type_not_supported
+func.func @cannot_convert_none_reduce_to_actual_reduce_if_reduction_type_not_supported(%arg0: tensor<4x8xf32> , %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  // CHECK: %[[SUBTRACT:.*]] = stablehlo.subtract %arg0, %arg1
+  // CHECK: %[[REDUCE:.*]] = mpmd.reduce<none> %[[SUBTRACT]]
+  // CHECK: return %[[REDUCE]]
+  %0 = stablehlo.subtract %arg0, %arg1 : tensor<4x8xf32>
+  %1 = mpmd.reduce<none> %0 : (tensor<4x8xf32>) -> (tensor<4x8xf32>)
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @do_not_delete_single_none_reduce
+func.func @do_not_delete_single_none_reduce(%arg0: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  // CHECK: %[[REDUCE:.*]] = mpmd.reduce<none> %arg0
+  %0 = mpmd.reduce<none> %arg0 : (tensor<4x8xf32>) -> (tensor<4x8xf32>)
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @chain_of_reduces
+// CHECK: %[[REDUCE:.*]] = mpmd.reduce<max> %arg0, %arg1, %arg1
+// CHECK-NEXT: %[[REDUCE_NONE:.*]] = mpmd.reduce<none>  %[[REDUCE]]
+// CHECK-NEXT: return %[[REDUCE_NONE]]
+func.func @chain_of_reduces(%arg0: tensor<4x8xf32> , %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %0 = stablehlo.maximum %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<max>} : tensor<4x8xf32>
+  %1 = stablehlo.maximum %0, %arg1 {mpmd.reduce = #mpmd.reduction<max>} : tensor<4x8xf32>
+  %2 = mpmd.reduce<none> %1 : (tensor<4x8xf32>) -> (tensor<4x8xf32>)
+  func.return %2 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @chain_of_reduces_partially_fold_if_different_type
+// CHECK: %[[MIN:.*]] = stablehlo.minimum %arg0, %arg1
+// CHECK: %[[REDUCE:.*]] = mpmd.reduce<max> %[[MIN]], %[[MIN]], %[[MIN]], %arg1
+// CHECK-NEXT: %[[REDUCE_NONE:.*]] = mpmd.reduce<none>  %[[REDUCE]]
+// CHECK-NEXT: return %[[REDUCE_NONE]], %[[MIN]]
+func.func @chain_of_reduces_partially_fold_if_different_type(%arg0: tensor<4x8xf32> , %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {topology=#topology} {
+  %0 = stablehlo.minimum %arg0, %arg1: tensor<4x8xf32>
+  %another_max = stablehlo.maximum %0, %arg1 {mpmd.reduce = #mpmd.reduction<max>} : tensor<4x8xf32>
+  %max = stablehlo.maximum %0, %another_max {mpmd.reduce = #mpmd.reduction<max>} : tensor<4x8xf32>
+  %1 = stablehlo.maximum %0, %max {mpmd.reduce = #mpmd.reduction<max>} : tensor<4x8xf32>
+  %2 = mpmd.reduce<none> %1 : (tensor<4x8xf32>) -> (tensor<4x8xf32>)
+  func.return %2, %0 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @push_through_unary_ops
+func.func @push_through_unary_ops(%arg0: tensor<4x8xf32> , %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+// CHECK:  %[[REDUCE:.*]] = mpmd.reduce<max>  %arg0, %arg1 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK:  %[[SQRT:.*]] = stablehlo.sqrt %[[REDUCE]] : tensor<4x8xf32>
+// CHECK:  %[[ABS:.*]] = stablehlo.abs %[[SQRT]] : tensor<4x8xf32>
+// CHECK:  %[[REDUCE_NONE:.*]] = mpmd.reduce<none>  %[[ABS]] : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  %0 = stablehlo.maximum %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<max>}: tensor<4x8xf32>
+  %sqrt = stablehlo.sqrt %0 : tensor<4x8xf32>
+  %abs = stablehlo.abs %sqrt : tensor<4x8xf32>
+  %1 = mpmd.reduce<none> %abs : (tensor<4x8xf32>) -> (tensor<4x8xf32>)
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @concat_reduce
+func.func @concat_reduce(%arg0: tensor<4x1x8xf32>, %arg1: tensor<4x1x8xf32>)
+// CHECK-NEXT: stablehlo.constant
+// CHECK-NEXT: %[[R0:.*]] = stablehlo.reshape %arg0 : (tensor<4x1x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[R1:.*]] = stablehlo.reshape %arg1 : (tensor<4x1x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[R2:.*]] = stablehlo.reshape %arg0 : (tensor<4x1x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[R3:.*]] = stablehlo.reshape %arg1 : (tensor<4x1x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[MAX:.*]] = mpmd.reduce<max> %[[R0]], %[[R1]], %[[R2]], %[[R3]] :
+// CHECK-SAME:     (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[REDUCE_NONE:.*]] = mpmd.reduce<none> %[[MAX]] : (tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: return %[[REDUCE_NONE]]
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %init = stablehlo.constant dense<1.0> : tensor<f32>
+  %concat = "stablehlo.concatenate"(%arg0, %arg1, %arg0, %arg1) <{dimension = 1 : i64}> :
+    (tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>) -> tensor<4x4x8xf32>
+  %reduce = stablehlo.reduce(%concat init: %init) applies stablehlo.maximum across dimensions = [1]
+    {mpmd.reduce = #mpmd.reduction<max>} :
+    (tensor<4x4x8xf32>, tensor<f32>) -> tensor<4x8xf32>
+  %reduce_none = mpmd.reduce<none> %reduce : (tensor<4x8xf32>) -> (tensor<4x8xf32>)
+  func.return %reduce_none : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/import_pipeline.mlir b/shardy/dialect/mpmd/transforms/import/test/import_pipeline.mlir
new file mode 100644
index 0000000..33520eb
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/import_pipeline.mlir
@@ -0,0 +1,110 @@
+// RUN: mpmd_opt %s -mpmd-import-pipeline='name-to-mesh-assignment=f1@m1,f2@m2' -split-input-file 2>&1 | FileCheck %s
+
+// CHECK-LABEL: sdy.mesh @mesh = <["x"=2]>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+
+// named_computation_duplicate_args_simplified
+// CHECK-LABEL: func @main
+func.func @main(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#topology} {
+// CHECK-NEXT: %[[FRAG:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg1
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-NEXT: return %[[FRAG]]
+  %1 = mpmd.named_computation<"f1"> (%arg0, %arg0) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// -----
+
+// CHECK-LABEL: sdy.mesh @mesh = <["x"=2]>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+
+// named_computation_duplicate_and_noop_results_simplified
+// CHECK-LABEL: func @main
+func.func @main(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+    -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) attributes {
+    "topology"=#topology} {
+// CHECK-NEXT: %[[SIMPLIFIED_FRAG:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK:     %[[HELPER_FRAG:.*]]:3 = mpmd.fragment<mesh="m2", origin=["f2"]>
+// CHECK:     return %[[HELPER_FRAG]]#0, %[[HELPER_FRAG]]#1, %[[HELPER_FRAG]]#2
+  %1:3 = mpmd.named_computation<"f1"> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %10, %10, %arg3 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  // This named_computation f2 will not be simplified, but makes it easier to
+  // see that f1 is simplified.
+  %2:3 = mpmd.named_computation<"f2"> (%1#0, %1#1, %1#2) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    %11 = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+    %12 = stablehlo.add %arg4, %arg4 : tensor<4x8xf32>
+    mpmd.return %10, %11, %12 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+
+  func.return %2#0, %2#1, %2#2 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// -----
+
+// CHECK-LABEL: sdy.mesh @mesh = <["x"=2]>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+
+// noop_named_computation_args_removed_before_mesh_assignment
+// CHECK-LABEL: func @main
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+func.func @main(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+    -> (tensor<4x8xf32>, tensor<4x8xf32>)  attributes {
+    "topology"=#topology} {
+// CHECK-NEXT: %[[FRAG1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD1:.*]] = stablehlo.add %arg2, %arg2
+// CHECK-NEXT:   mpmd.return %[[ADD1]]
+// CHECK-NEXT: }
+// CHECK-NEXT: %[[FRAG2:.*]] = mpmd.fragment<mesh="m2", origin=["f2"]> (%arg1) (%arg2: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD2:.*]] = stablehlo.add %arg2, %arg2
+// CHECK-NEXT:   mpmd.return %[[ADD2]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[FRAG1]], %[[FRAG2]]
+  %0:2 = mpmd.named_computation<"f1"> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %2, %arg3 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %1 = mpmd.named_computation<"f2"> (%0#1) (%arg2: tensor<4x8xf32>) {
+    %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %2 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0#0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// -----
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+
+// mesh_inference_succeeds_through_returned_callee_arg
+// CHECK-LABEL: func.func public @main
+func.func public @main(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>) attributes {topology=#topology}
+{
+  // Callee arg in position 1 is returned and unused, and inference handles this
+  // fine.
+  %2:2 = mpmd.call @f(%arg0, %arg1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+
+  %1 = mpmd.named_computation<"f1"> (%2#0) (%arg3: tensor<3x5xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<3x5xf32>
+    mpmd.return %10 : tensor<3x5xf32>
+  } : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %1 : tensor<3x5xf32>
+}
+
+func.func private @f(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>) {
+  %0 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  return %0, %arg1 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+// No error.
diff --git a/shardy/dialect/mpmd/transforms/import/test/import_with_validation_errors.mlir b/shardy/dialect/mpmd/transforms/import/test/import_with_validation_errors.mlir
new file mode 100644
index 0000000..071461f
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/import_with_validation_errors.mlir
@@ -0,0 +1,27 @@
+// RUN: mpmd_opt %s -mpmd-import-pipeline -verify-diagnostics
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+func.func public @main(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32>) attributes {topology=#topology} {
+  %c = stablehlo.constant dense<1> : tensor<i32>
+  %c_0 = stablehlo.constant dense<10> : tensor<i32>
+  %0:2 = stablehlo.while(%iterArg = %arg0, %iterArg_1 = %arg1) : tensor<i32>, tensor<i32>
+     cond {
+      %1 = stablehlo.compare  LT, %iterArg, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>
+       %new_sum = func.call @None(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+      stablehlo.return %1 : tensor<i1>
+    } do {
+      %1 = stablehlo.add %iterArg, %c : tensor<i32>
+      %2 = stablehlo.add %iterArg_1, %1 : tensor<i32>
+      stablehlo.return %1, %2 : tensor<i32>, tensor<i32>
+  }
+  return %0#1 : tensor<i32>
+}
+
+func.func public @None(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // expected-error @below {{Named computations can only be nested in mpmd functions or mpmd ops.}}
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg3: tensor<i32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<i32>
+    mpmd.return %10 : tensor<i32>
+  } : (tensor<i32>) -> tensor<i32>
+  func.return %1 : tensor<i32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_mesh_for_func_op_leaves.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_mesh_for_func_op_leaves.mlir
new file mode 100644
index 0000000..9a4bd84
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_mesh_for_func_op_leaves.mlir
@@ -0,0 +1,456 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-assign-mesh-func-leaves='infer-transfers=true' 2>&1 | FileCheck --implicit-check-not use_set %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @unused_input(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+func.func @unused_input(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_without_src_set_or_use_set(%arg0: tensor<4x8xf32>)
+func.func @output_without_src_set_or_use_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // m1 is picked for the output because we pick the first mesh in the topology.
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %[[ADD]] {{.*}}mesh_tensor<"m1"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_with_src_set(%arg0: tensor<4x8xf32>)
+func.func @output_with_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %[[ADD]] {{.*}}mesh_tensor<"m3"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_with_src_set_multiple(%arg0: tensor<4x8xf32>)
+func.func @output_with_src_set_multiple(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // Both m2 and m3 are valid for the output, but m2 is picked because we pick
+  // the first mesh in the src_set since the use_set is empty.
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %[[ADD]] {{.*}}mesh_tensor<"m2"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_with_use_set(%arg0: tensor<4x8xf32>)
+func.func @output_with_use_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %[[ADD]] {{.*}}mesh_tensor<"m3"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_with_use_set_multiple(%arg0: tensor<4x8xf32>)
+func.func @output_with_use_set_multiple(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %[[ADD]] {{.*}}mesh_tensor<"m2"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m2", "m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_with_use_set_contained_in_src_set(%arg0: tensor<4x8xf32>
+func.func @output_with_use_set_contained_in_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %[[ADD]] {{.*}}mesh_tensor<"m3"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_with_use_set_equal_to_src_set(%arg0: tensor<4x8xf32>)
+func.func @output_with_use_set_equal_to_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %[[ADD]] {{.*}}mesh_tensor<"m2"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m3", "m2">, mpmd.use_set = #mpmd.meshes_with_origins<"m3", "m2">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_direct_from_input_no_src_set_or_use_set
+func.func @output_direct_from_input_no_src_set_or_use_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %arg0 {{.*}}mesh_tensor<"m1"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+
+// Test output_direct_from_input_with_src_set
+
+// It suffices to check with src_set, since if there's a use_set on func args,
+// there will be a src_set (this is covered by tests for that).
+// CHECK-LABEL: func @main
+func.func @main(%arg0: tensor<4x8xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m2", "m3">}) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %arg0 {{.*}}mesh_tensor<"m2"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @operand_returned_multiple_times()
+func.func @operand_returned_multiple_times() -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[CONST:.*]] = stablehlo.constant
+  // CHECK-NEXT: %[[ASSIGN_1:.*]] = mpmd.assign {origin = "inferred_out"} %[[CONST]] {{.*}}mesh_tensor<"m2"
+
+  // CHECK-NEXT: %[[ASSIGN_2:.*]] = mpmd.assign {origin = "inferred_out"} %[[CONST]] {{.*}}mesh_tensor<"m2"
+  // CHECK-NEXT: return %[[ASSIGN_1]], %[[ASSIGN_2]]
+
+  %0 = stablehlo.constant {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">} dense<1.0> : tensor<4x8xf32>
+  func.return %0, %0 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @operand_returned_multiple_times_and_assigned(%arg0: tensor<4x8xf32>)
+func.func @operand_returned_multiple_times_and_assigned(%arg0: tensor<4x8xf32>)
+   -> (tensor<4x8xf32>, !mesh_2_tensor_4_8_f32, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[CONST:.*]] = stablehlo.constant
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign %[[CONST]]
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %[[CONST]]
+
+  // CHECK-NEXT: %[[ASSIGN_1:.*]] = mpmd.assign {origin = "inferred_out"} %[[CONST]] {{.*}}mesh_tensor<"m2"
+
+  // CHECK-NEXT: %[[ASSIGN_2:.*]] = mpmd.assign {origin = "inferred_out"} %[[ADD]] {{.*}}mesh_tensor<"m3"
+  // CHECK-NEXT: return %[[ASSIGN_1]], %[[ASSIGN]], %[[ASSIGN_2]]
+
+  %0 = stablehlo.constant {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">} dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %2 = stablehlo.add %arg0, %0 {mpmd.src_set = #mpmd.meshes_with_origins<"m3">} : tensor<4x8xf32>
+  func.return %0, %1, %2 : tensor<4x8xf32>, !mesh_2_tensor_4_8_f32, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unused_computation_without_src_set(%arg0: tensor<4x8xf32>)
+func.func @unused_computation_without_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN_ADD:.*]] = mpmd.assign {origin = "inferred_unused"} %[[ADD]] {{.*}}mesh_tensor<"m1"
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %arg0 {{.*}}mesh_tensor<"m1"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  // Note we return arg0 and so %0 is unused
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unused_computation_with_src_set(%arg0: tensor<4x8xf32>)
+func.func @unused_computation_with_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // CHECK:      %[[ADD:.*]] = stablehlo.add
+  // CHECK-NEXT: %[[ASSIGN_ADD:.*]] = mpmd.assign {origin = "inferred_unused"} %[[ADD]] {{.*}}mesh_tensor<"m2"
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %arg0 {{.*}}mesh_tensor<"m1"
+  // CHECK-NEXT: return %[[ASSIGN]]
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">} : tensor<4x8xf32>
+  // Note we return arg0 and so %0 is unused
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_no_result_without_src_set(%arg0: tensor<4x8xf32>)
+func.func @op_with_no_result_without_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+// CHECK-NEXT:  %[[ASSIGN:.*]] = mpmd.assign %arg0
+// CHECK-NEXT:  mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN]]) (%arg1
+// CHECK-NEXT:    sdy.sharding_group %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+
+  // sdy.sharding_group has no results. So it gets wrapped in a fragment.
+  sdy.sharding_group %arg0 group_id=0 : tensor<4x8xf32>
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_no_result_with_src_set(%arg0: tensor<4x8xf32>)
+func.func @op_with_no_result_with_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+// CHECK-NEXT:  %[[ASSIGN:.*]] = mpmd.assign %arg0
+// CHECK-NEXT:  mpmd.fragment<mesh="m2", origin=[]> (%[[ASSIGN]]) (%arg1
+// CHECK-NEXT:    sdy.sharding_group %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+
+  // sdy.sharding_group has no results. So it gets wrapped in a fragment.
+  sdy.sharding_group %arg0 group_id=0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_no_result_with_multi_src_set(%arg0: tensor<4x8xf32>)
+func.func @op_with_no_result_with_multi_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+// CHECK-NEXT:  %[[ASSIGN_2:.*]] = mpmd.assign %arg0
+// CHECK-NEXT:  mpmd.fragment<mesh="m2", origin=[]> (%[[ASSIGN_2]]) (%arg1
+// CHECK-NEXT:    sdy.sharding_group %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[ASSIGN_3:.*]] = mpmd.assign %arg0
+// CHECK-NEXT:  mpmd.fragment<mesh="m3", origin=[]> (%[[ASSIGN_3]]) (%arg1
+// CHECK-NEXT:    sdy.sharding_group %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[ASSIGN:.*]] = mpmd.assign {origin = "inferred_out"} %arg0
+// CHECK-NEXT:  return %[[ASSIGN]]
+
+  // sdy.sharding_group has no results. So it gets wrapped in a fragment.
+  sdy.sharding_group %arg0 group_id=0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">} : tensor<4x8xf32>
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+
+// CHECK-LABEL: func @assign_broadcast_operand
+func.func @assign_broadcast_operand(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>  attributes {
+  "topology"=#mpmd.topology<
+    <"m0": <["x"=2]>>, <"m1": <["x"=2]>>
+  >}
+{
+  // CHECK-NEXT: %[[PROD:.*]] = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = {{.*}}"m1", "m0">}
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "broadcast"} %[[PROD]] {{.*}}m1{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign %[[ASSIGN]]
+  // CHECK-NEXT: %[[BCAST:.*]] = mpmd.broadcast %[[UNASSIGN]]
+  // CHECK-NEXT: assign {origin = "inferred_out"} %[[BCAST]]
+  %prod = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m0">} : tensor<4x8xf32>
+  // Although the op is not actually used in m1, we set the use_set attribute.
+  %0 = mpmd.broadcast {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %prod : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @assign_broadcast_operand
+func.func @assign_broadcast_operand_empty_intersection(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>  attributes {
+  "topology"=#mpmd.topology<
+    <"m0": <["x"=2]>>, <"m1": <["x"=2]>>
+  >}
+{
+  // CHECK-NEXT: %[[PROD:.*]] = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = {{.*}}"m0">}
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "broadcast"} %[[PROD]] {{.*}}m0{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign %[[ASSIGN]]
+  // CHECK-NEXT: %[[BCAST:.*]] = mpmd.broadcast %[[UNASSIGN]]
+  // CHECK-NEXT: assign {origin = "inferred_out"} %[[BCAST]] {{.*}}m1{{.*}}
+  %prod = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m0">} : tensor<4x8xf32>
+  // Although the op is not actually used in m1, we set the use_set attribute.
+  %0 = mpmd.broadcast {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %prod : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @assign_broadcast_empty_use_set
+func.func @assign_broadcast_empty_use_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>  attributes {
+  "topology"=#mpmd.topology<
+    <"m0": <["x"=2]>>, <"m1": <["x"=2]>>
+  >}
+{
+  // CHECK-NEXT: %[[PROD:.*]] = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = {{.*}}"m0">}
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "broadcast"} %[[PROD]] {{.*}}m0{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign %[[ASSIGN]]
+  // CHECK-NEXT: %[[BCAST:.*]] = mpmd.broadcast %[[UNASSIGN]]
+  %prod = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m0">} : tensor<4x8xf32>
+  %0 = mpmd.broadcast %prod : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @assign_reduce_operand
+func.func @assign_reduce_operand(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>  attributes {
+  "topology"=#mpmd.topology<
+    <"m0": <["x"=2]>>, <"m1": <["x"=2]>>
+  >}
+{
+  // CHECK-NEXT: %[[PROD:.*]] = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = {{.*}}"m1", "m0">}
+  // CHECK-NEXT: %[[PROD1:.*]] = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = {{.*}}"m1", "m0">}
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "reduce"} %[[PROD]] {{.*}}m1{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign %[[ASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN1:.*]] = mpmd.assign {origin = "reduce"} %[[PROD1]] {{.*}}m1{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN1:.*]] = mpmd.unassign %[[ASSIGN1]]
+  // CHECK-NEXT: %[[REDUCE:.*]] = mpmd.reduce<add> %[[UNASSIGN]], %[[UNASSIGN1]]
+  // CHECK-NEXT: assign {origin = "inferred_out"} %[[REDUCE]]
+  %prod = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m0">} : tensor<4x8xf32>
+  %prod1 = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m0">} : tensor<4x8xf32>
+  %0 = mpmd.reduce<add> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %prod, %prod1 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @assign_reduce_operand_empty_intersection
+func.func @assign_reduce_operand_empty_intersection(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>  attributes {
+  "topology"=#mpmd.topology<
+    <"m0": <["x"=2]>>, <"m1": <["x"=2]>>
+  >}
+{
+  // CHECK-NEXT: %[[PROD:.*]] = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = {{.*}}"m0">}
+  // CHECK-NEXT: %[[PROD1:.*]] = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = {{.*}}"m1">}
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "reduce"} %[[PROD]] {{.*}}m0{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign %[[ASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN1:.*]] = mpmd.assign {origin = "reduce"} %[[PROD1]] {{.*}}m1{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN1:.*]] = mpmd.unassign %[[ASSIGN1]]
+  // CHECK-NEXT: %[[REDUCE:.*]] = mpmd.reduce<add> %[[UNASSIGN]], %[[UNASSIGN1]]
+  // CHECK-NEXT: assign {origin = "inferred_out"} %[[REDUCE]] {{.*}}m1{{.*}}
+  %prod = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m0">} : tensor<4x8xf32>
+  %prod1 = stablehlo.multiply %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  %0 = mpmd.reduce<add> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %prod, %prod1 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func.func @call_op_unused_op(
+func.func @call_op_unused_op(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>}
+{
+  %0 = mpmd.call @call_op_unused_op_f(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK: func.func private @call_op_unused_op_f(
+func.func private @call_op_unused_op_f(%arg0: tensor<3x5xf32>)
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>}
+{
+  // The unused add op gets use_set = #mpmd.meshes_with_origins<"m1"> because "m1" is the default mesh
+  // (since it is the first mesh in the topology).
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg0
+  // CHECK-NEXT: %[[ASSIGN_ADD:.*]] = mpmd.assign {origin = "inferred_unused"} %[[ADD]] {{.*}}mesh_tensor<"m1"
+  %0 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  return %arg0 : tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func.func @call_op_unused_result(
+func.func @call_op_unused_result(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>}
+{
+  %0:2 = mpmd.call @call_op_unused_result_f(%arg0, %arg1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  return %0#0 : tensor<3x5xf32>
+}
+
+// CHECK:     func.func private @call_op_unused_result_f(
+// CHECK-SAME: %arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>
+// CHECK-SAME: -> (tensor<3x5xf32>, tensor<3x5xf32> {mpmd.use_set = {{.*}}"m1"["inferred_unused_callee_out"]>})
+func.func private @call_op_unused_result_f(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>}
+{
+  return %arg0, %arg1 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+// CHECK-LABEL: func.func @call_op_unused_input(
+func.func @call_op_unused_input(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>}
+{
+  %0 = mpmd.call @call_op_unused_input_f(%arg0, %arg1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %0 : tensor<3x5xf32>
+}
+
+// CHECK: func.func private @call_op_unused_input_f(
+// CHECK-SAME: %arg0: tensor<3x5xf32>,
+// CHECK-SAME: %arg1: tensor<3x5xf32> {mpmd.src_set = {{.*}}"m3", "m2">, mpmd.use_set = {{.*}}"m2"["inferred_unused_callee_in"]>}
+func.func private @call_op_unused_input_f(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"m3", "m2">})
+  -> tensor<3x5xf32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>}
+{
+  return %arg0 : tensor<3x5xf32>
+}
+
+// CHECK: func.func @broadcast_with_empty_src_set(
+func.func @broadcast_with_empty_src_set(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.constant {mpmd.src_set = #mpmd.meshes_with_origins<>} dense<1.0> : tensor<4x8xf32>
+  // CHECK: %[[ASSIGN:.*]] = mpmd.assign {{.*}}m2{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign %[[ASSIGN]]
+  // CHECK-NEXT: %[[BCAST:.*]] = mpmd.broadcast %[[UNASSIGN]]
+  %b = mpmd.broadcast {mpmd.use_set = #mpmd.meshes_with_origins<"m3", "m2">} %0 : tensor<4x8xf32>
+  func.return %b : tensor<4x8xf32>
+}
+
+// CHECK: func.func @broadcast_with_empty_src_set_and_empty_use_set(
+func.func @broadcast_with_empty_src_set_and_empty_use_set(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.constant {mpmd.src_set = #mpmd.meshes_with_origins<>} dense<1.0> : tensor<4x8xf32>
+  // CHECK: %[[ASSIGN:.*]] = mpmd.assign {{.*}}m1{{.*}}
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign %[[ASSIGN]]
+  // CHECK-NEXT: %[[BCAST:.*]] = mpmd.broadcast %[[UNASSIGN]]
+  %b = mpmd.broadcast %0 : tensor<4x8xf32>
+  func.return %b : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @region_op_region_is_skipped
+// Region op unused ops should not be given a use_set.
+func.func @region_op_region_is_skipped(%arg0: tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1> : tensor<i32>
+  // CHECK: stablehlo.while
+  // CHECK-NOT: mpmd.assign
+  // CHECK: stablehlo.return
+  // CHECK-NOT: mpmd.assign
+  // CHECK: stablehlo.return
+  // CHECK: mpmd.assign
+  %5:2 = stablehlo.while(%iterArg_0 = %0, %iterArg_1 = %arg0) : tensor<i32>, tensor<4x8xf32>
+   cond {
+    %6 = "stablehlo.compare"(%iterArg_0, %0) {comparison_direction = #stablehlo<comparison_direction LT>} : (tensor<i32>, tensor<i32>) -> tensor<i1>
+    "stablehlo.return"(%6) : (tensor<i1>) -> ()
+  } do {
+    %8 = stablehlo.add %iterArg_1, %iterArg_1 : tensor<4x8xf32>
+    %9 = stablehlo.add %iterArg_1, %iterArg_1 : tensor<4x8xf32> // unused op should not be given a use_set
+    "stablehlo.return"(%iterArg_0, %8) : (tensor<i32>, tensor<4x8xf32>) -> ()
+  }
+  %6 = mpmd.assign %5#1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+
+  func.return %6 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_mesh_func_leaves_errors.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_mesh_func_leaves_errors.mlir
new file mode 100644
index 0000000..ee8e163
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_mesh_func_leaves_errors.mlir
@@ -0,0 +1,32 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-assign-mesh-func-leaves -split-input-file -verify-diagnostics
+
+// CHECK-LABEL: func @output_wo_sets
+func.func @output_wo_sets() -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.constant {mpmd.src_set = #mpmd.meshes_with_origins<>, mpmd.use_set = #mpmd.meshes_with_origins<>} dense<1.0> : tensor<4x8xf32>
+  // expected-error @+1 {{Func output 0 has no mesh to be assigned to.}}
+  func.return %0 : tensor<4x8xf32>
+}
+
+// -----
+
+// expected-error @+1 {{Callee @unused_callee_input unused input 1 has no mesh to be assigned to.}}
+func.func private @unused_callee_input(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32> {mpmd.src_set = #mpmd.meshes_with_origins<>}) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// -----
+
+func.func private @unused_op_with_empty_src_set(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+ // expected-error @+1 {{src_set must not be empty for this op.}}
+  %0 = stablehlo.constant {mpmd.src_set = #mpmd.meshes_with_origins<>, mpmd.use_set = #mpmd.meshes_with_origins<>} dense<1.0> : tensor<4x8xf32>
+  func.return %arg0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_using_input_output_constraints.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_using_input_output_constraints.mlir
new file mode 100644
index 0000000..f08983d
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assign_using_input_output_constraints.mlir
@@ -0,0 +1,137 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-assign-using-input-output-constraints='constraints=0:0' 2>&1 | FileCheck %s
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>>
+!m2_tensor = !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>
+
+// CHECK-LABEL: func @output_with_empty_src_set(%arg0: !mpmd.mesh_tensor<"m2"
+func.func @output_with_empty_src_set(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m3", "m2">})
+   -> tensor<4x8xf32> attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign  {origin = "io_constraint_in"}  %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign  {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]] : !mpmd.mesh_tensor<"m2"
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+
+// CHECK-LABEL: func @output_with_use_set(%arg0: !mpmd.mesh_tensor<"m3"
+func.func @output_with_use_set(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2", "m3">})
+  -> tensor<4x8xf32> attributes {topology=#topology} {
+
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign  {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign  {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]] : !mpmd.mesh_tensor<"m3"
+  %0 = stablehlo.add %arg0, %arg0  {mpmd.src_set = #mpmd.meshes_with_origins<"m2">, mpmd.use_set = #mpmd.meshes_with_origins<"m3">}: tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_with_src_set_only(%arg0: !mpmd.mesh_tensor<"m2"
+func.func @output_with_src_set_only(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2", "m3">})
+  -> tensor<4x8xf32> attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign  {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign  {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]] : !mpmd.mesh_tensor<"m2"
+  %0 = stablehlo.add %arg0, %arg0  {mpmd.src_set = #mpmd.meshes_with_origins<"m2">}: tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @input_no_use_set_but_use_set_on_output
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+func.func @input_no_use_set_but_use_set_on_output(
+  %arg0: tensor<4x8xf32>
+)
+  -> tensor<4x8xf32> attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]]
+  // CHECK-SAME:   !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m3", "m2">}: tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @input_no_use_set_but_src_set_on_output
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+func.func @input_no_use_set_but_src_set_on_output(
+  %arg0: tensor<4x8xf32>
+)
+  -> tensor<4x8xf32> attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]]
+  // CHECK-SAME:   !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2">}: tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @input_multi_use_set(%arg0: !mpmd.mesh_tensor<"m2"
+func.func @input_multi_use_set(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2", "m3">})
+  -> tensor<4x8xf32> attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign  {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign  {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]] : !mpmd.mesh_tensor<"m2"
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2">}: tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @input_is_mesh_tensor_with_compatible_mesh(%arg0: !mpmd.mesh_tensor<"m2"
+func.func @input_is_mesh_tensor_with_compatible_mesh(
+  %arg0: !m2_tensor {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2", "m3">})
+  -> tensor<8x16xf32> attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign  {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign  {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]] : !mpmd.mesh_tensor<"m2"
+  %1 = mpmd.unassign  {origin = "io_constraint_in"} %arg0 : (!m2_tensor) -> tensor<8x16xf32>
+  %2 = stablehlo.add %1, %1 : tensor<8x16xf32>
+  func.return %2 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: func @input_is_mesh_tensor_with_incompatible_mesh(%arg0: !mpmd.mesh_tensor<"m2"
+func.func @input_is_mesh_tensor_with_incompatible_mesh(
+  %arg0: !m2_tensor {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2", "m3">})
+  -> tensor<8x16xf32> attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign  {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: return %[[ADD]] : tensor<8x16xf32>
+  // Note it is a no-op. We will introduce the transfer later.
+  %1 = mpmd.unassign  {origin = "io_constraint_in"} %arg0 : (!m2_tensor) -> tensor<8x16xf32>
+  %2 = stablehlo.add %1, %1 {mpmd.src_set = #mpmd.meshes_with_origins<"m1">} : tensor<8x16xf32>
+  func.return %2 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: func @output_already_has_mesh(%arg0: !mpmd.mesh_tensor<"m2"
+func.func @output_already_has_mesh(
+  %arg0: tensor<8x16xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2", "m3">})
+  -> !m2_tensor attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign  {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign  {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]] : !mpmd.mesh_tensor<"m2"
+  %1 = stablehlo.add %arg0, %arg0 : tensor<8x16xf32>
+  %3 = mpmd.assign  {origin = "io_constraint_out"} %1 : (tensor<8x16xf32>) -> !m2_tensor
+  func.return %3 : !m2_tensor
+}
+
+// CHECK-LABEL: func @noop_if_both_already_assigned(%arg0: !mpmd.mesh_tensor<"m2"
+func.func @noop_if_both_already_assigned(
+  %arg0: !m2_tensor {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2", "m3">})
+  -> !m2_tensor attributes {topology=#topology} {
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign  {origin = "io_constraint_in"} %arg0
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[UNASSIGN]], %[[UNASSIGN]]
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign  {origin = "io_constraint_out"} %[[ADD]]
+  // CHECK-NEXT: return %[[ASSIGN]] : !mpmd.mesh_tensor<"m2"
+  %1 = mpmd.unassign  {origin = "io_constraint_in"} %arg0 : (!m2_tensor) -> tensor<8x16xf32>
+  %2 = stablehlo.add %1, %1 : tensor<8x16xf32>
+  %3 = mpmd.assign  {origin = "io_constraint_out"} %2 : (tensor<8x16xf32>) -> !m2_tensor
+  func.return %3 : !m2_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_no_additional_transfers.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_no_additional_transfers.mlir
new file mode 100644
index 0000000..de92565
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_no_additional_transfers.mlir
@@ -0,0 +1,144 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-validate-no-additional-transfers-needed 2>&1 | FileCheck %s
+
+!mesh_1_tensor_ui32 = !mpmd.mesh_tensor<"m1", tensor<ui32>>
+!mesh_1_tensor_1_ui32 = !mpmd.mesh_tensor<"m1", tensor<1xui32>>
+!mesh_1_tensor_2_ui32 = !mpmd.mesh_tensor<"m1", tensor<2xui32>>
+!mesh_1_tensor_5_5_ui32 = !mpmd.mesh_tensor<"m1", tensor<5x5xui32>>
+!mesh_1_tensor_4_4_f32 = !mpmd.mesh_tensor<"m1", tensor<4x4xf32>>
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_1_tensor_8_16_f32 = !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+!mesh_1_tensor_4_16_f32 = !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+!mesh_1_tensor_16_8_f32 = !mpmd.mesh_tensor<"m1", tensor<16x8xf32>>
+
+!mesh_2_tensor_ui32 = !mpmd.mesh_tensor<"m2", tensor<ui32>>
+!mesh_2_tensor_1_ui32 = !mpmd.mesh_tensor<"m2", tensor<1xui32>>
+!mesh_2_tensor_2_ui32 = !mpmd.mesh_tensor<"m2", tensor<2xui32>>
+!mesh_2_tensor_5_5_ui32 = !mpmd.mesh_tensor<"m2", tensor<5x5xui32>>
+!mesh_2_tensor_4_4_f32 = !mpmd.mesh_tensor<"m2", tensor<4x4xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!mesh_2_tensor_4_16_f32 = !mpmd.mesh_tensor<"m2", tensor<4x16xf32>>
+!mesh_2_tensor_8_16_f32 = !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>
+!mesh_2_tensor_16_8_f32 = !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>
+
+// The majority of these tests verify only that validation passes and assume
+// that the DAG unchanged aside from additional attributes. There is one test
+// `dag_unchanged` that verifies that the DAG
+// remains unchanged.
+
+// CHECK-LABEL: func @op_without_src_set_or_use_set(%arg0: tensor<4x8xf32>)
+func.func @op_without_src_set_or_use_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_src_set_only(%arg0: tensor<4x8xf32>)
+func.func @op_with_src_set_only(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_src_set_multiple_only(%arg0: tensor<4x8xf32>)
+func.func @op_with_src_set_multiple_only(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @output_with_use_set_only(%arg0: tensor<4x8xf32>)
+func.func @output_with_use_set_only(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_use_set_multiple_only(%arg0: tensor<4x8xf32>)
+func.func @op_with_use_set_multiple_only(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m2", "m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_src_set_and_use_set(%arg0: tensor<4x8xf32>)
+func.func @op_with_src_set_and_use_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_src_set_and_use_set_multiple(%arg0: tensor<4x8xf32>)
+func.func @op_with_src_set_and_use_set_multiple(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m3", "m2">, mpmd.use_set = #mpmd.meshes_with_origins<"m2", "m3">} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @dag_unchanged
+// CHECK-SAME:     %arg0: tensor<4x8xf32>
+// CHECK-SAME:     %arg1: tensor<8x16xf32>
+// CHECK-SAME:     %arg2: tensor<16x8xf32>
+// CHECK-SAME:     %arg3: tensor<4x16xf32>
+// CHECK-SAME:     %arg4: tensor<16x8xf32>
+func.func @dag_unchanged(
+  %arg0: tensor<4x8xf32>,
+  %arg1: tensor<8x16xf32>,
+  %arg2: tensor<16x8xf32>,
+  %arg3: tensor<4x16xf32>,
+  %arg4: tensor<16x8xf32>)
+  -> (tensor<4x16xf32>, tensor<16x8xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+  // CHECK-NEXT: %0 = stablehlo.add %arg2, %arg4
+  %0 = stablehlo.add %arg2, %arg4 : tensor<16x8xf32>
+
+  // CHECK-NEXT: %1 = mpmd.assign %arg0
+  %1 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  // CHECK-NEXT: %2 = mpmd.assign %arg1
+  %2 = mpmd.assign %arg1 : (tensor<8x16xf32>) -> !mesh_1_tensor_8_16_f32
+
+  // CHECK-NEXT: %3 = mpmd.fragment<mesh="m1", origin=["f1"]> (%1, %2)
+  // CHECK-SAME:  (%arg5: tensor<4x8xf32>, %arg6: tensor<8x16xf32>) {
+  %3 = mpmd.fragment<mesh="m1", origin=["f1"]> (%1, %2)
+    (%arg5: tensor<4x8xf32>, %arg6: tensor<8x16xf32>) {
+
+    // CHECK-NEXT: %6 = stablehlo.dot %arg5, %arg6 : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    %6 = "stablehlo.dot"(%arg5, %arg6) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    // CHECK-NEXT: mpmd.return %6 : tensor<4x16xf32>
+    mpmd.return %6 : tensor<4x16xf32>
+    // CHECK-NEXT: }
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_8_16_f32) -> (!mesh_1_tensor_4_16_f32)
+
+  // CHECK-NEXT: %4 = mpmd.transfer %3
+  %4 = mpmd.transfer %3 : (!mesh_1_tensor_4_16_f32) -> !mesh_2_tensor_4_16_f32
+
+  // CHECK-NEXT: %5 = mpmd.unassign %4
+  %5 = mpmd.unassign %4 : (!mesh_2_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  // CHECK-NEXT: return %5, %0 : tensor<4x16xf32>, tensor<16x8xf32>
+  func.return %5, %0: tensor<4x16xf32>, tensor<16x8xf32>
+}
+
+// CHECK-LABEL: func @func_with_no_topology_does_not_error_because_skipped
+func.func @func_with_no_topology_does_not_error_because_skipped(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {
+  // CHECK-NEXT: %0 = stablehlo.add
+  // CHECK-NEXT: return %0
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<>} : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_no_additional_transfers_failures.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_no_additional_transfers_failures.mlir
new file mode 100644
index 0000000..42e4d7d
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_no_additional_transfers_failures.mlir
@@ -0,0 +1,148 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-validate-no-additional-transfers-needed='error-limit=-1' -verify-diagnostics -split-input-file
+
+func.func @use_set_not_contained_in_src_set_at_all(%arg0: tensor<4x8xf32>)
+  -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // expected-error @+1 {{Mesh assignment is not possible for op as it is used in {m1} but it can only be placed on {m2,m3}}}
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+
+  func.return %0 : tensor<4x8xf32>
+}
+
+// -----
+
+func.func @use_set_not_contained_in_src_set_partially(%arg0: tensor<4x8xf32>)
+  -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // expected-error @+1 {{Mesh assignment is not possible for op as it is used in {m1,m2} but it can only be placed on {m2,m3}}}
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+
+  func.return %0 : tensor<4x8xf32>
+}
+
+
+// -----
+
+func.func @use_set_not_contained_in_src_set_on_assign_op(%arg0: tensor<4x8xf32>)
+  -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // expected-error @+1 {{Mesh assignment is not possible for op as it is used in {m1} but it can only be placed on {m2}}}
+  %0 = mpmd.assign {mpmd.src_set = #mpmd.meshes_with_origins<"m2">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+  func.return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// -----
+
+func.func public @call_op_failure(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+  %arg1: !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  %0 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<3x5xf32>>) -> tensor<3x5xf32> loc("x")
+  %1 = mpmd.call @f(%0, %0) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<3x5xf32>
+  %2 = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<3x5xf32>>) -> tensor<3x5xf32> loc("y")
+  %a2 = stablehlo.add %2, %2 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<3x5xf32>
+  %3 = mpmd.call @f(%a2, %a2) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<3x5xf32>
+  return %1, %3 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+// Only one error is reported on the signature, since the error for arg1 is emitted at %0.
+// expected-error-re @+1 {{Mesh assignment is not possible for arg0 of mpmd.call "f" as it is used in {m1,m2} but it can only be placed on {m2,m3}{{.*}}Input "y": m2{{.*}}Input "x": m1}}
+func.func private @f(
+  %arg0: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">},
+  %arg1: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+) -> tensor<3x5xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+
+  // expected-error-re @+1 {{Mesh assignment is not possible for op as it is used in {m1,m2} but it can only be placed on {m2,m3}{{.*}}Input "y": m2{{.*}}Input "x": m1}}
+  %0 = stablehlo.add %arg1, %arg1 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<3x5xf32>
+
+  return %arg0 : tensor<3x5xf32>
+}
+
+// -----
+
+func.func public @chained_call_op_failure(
+  %arg0: tensor<3x5xf32>,
+  %arg1: tensor<3x5xf32>
+) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  %c0:2 = mpmd.call @f(%arg0, %arg1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  %c1:2 = mpmd.call @f(%c0#0, %c0#1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  return %c1#0, %c1#1 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+// Only one error is reported on the signature, since the error for arg1 is emitted at %0.
+// expected-error @+2 {{Mesh assignment is not possible for mpmd.call "f" as it passes result 0 to arg 0 but they have mismatching mesh assignments: res: {m1[c]}, arg: {m2[a]}}}
+// expected-error @+1 {{Mesh assignment is not possible for mpmd.call "f" as it passes result 1 to arg 1 but they have mismatching mesh assignments: res: {m2[d]}, arg: {m1[b]}}}
+func.func private @f(
+  %arg0: tensor<3x5xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2"["a"]>},
+  %arg1: tensor<3x5xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1"["b"]>}
+) -> (tensor<3x5xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1"["c"]>}, tensor<3x5xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2"["d"]>})
+ attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+
+  return %arg0, %arg1 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+// -----
+
+module @test_printing_with_locs_input_and_named_computation {
+  sdy.mesh @mesh = <["x"=4]>
+  func.func public @main(%arg0: !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>> loc("param[0]")) -> (!mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>)
+    attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>, <"mesh2" : <["x"=4]>>>} {
+    %2 = mpmd.unassign {origin = "user_in", mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["user_in"]>} %arg0 : (!mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>) -> tensor<3x5xf32> loc("param[0]")
+    // expected-error-re @+1 {{Mesh assignment is not possible for op{{.*}}Input "param[0]": mesh1[user_in]{{.*}}named_computation "stage2": mesh2[stage2]}}
+    %5 = stablehlo.sqrt %2 {mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["user_in"]>, mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["stage2"]>} : tensor<3x5xf32>
+    %6 = mpmd.assign {origin = "stage2", mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["stage2"]>} %5 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>  loc("named_computation")
+    %7 = mpmd.fragment<mesh="mesh2", origin=["stage2"]> (%6) (%arg1: tensor<3x5xf32>) {
+      %11 = stablehlo.add %arg1, %arg1 : tensor<3x5xf32>
+      mpmd.return %11 : tensor<3x5xf32>
+    } : (!mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>) -> !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>
+    return %7 : !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>
+  }
+}
+
+// -----
+
+module @test_printing_with_locs_named_tensor_and_output {
+  sdy.mesh @mesh = <["x"=4]>
+  func.func public @main(%arg0: !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>) -> (!mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>)
+    attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>, <"mesh2" : <["x"=4]>>>} {
+    %2 = mpmd.unassign {origin = "stage1", mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["stage1"]>} %arg0 : (!mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>) -> tensor<3x5xf32> loc("named_tensor")
+    // expected-error-re @+1 {{Mesh assignment is not possible for op{{.*}}named_tensor "stage1": mesh1[stage1]{{.*}}Output "result[0]": mesh2[user_out]}}
+    %5 = stablehlo.sqrt %2 {mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["stage1"]>, mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["user_out"]>} : tensor<3x5xf32>
+    %6 = mpmd.assign {origin = "user_out", mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["user_out"]>} %5 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>  loc("result[0]")
+    return %6 : !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>
+  }
+}
+
+
+// -----
+
+module @test_printing_with_locs {
+  sdy.mesh @mesh = <["x"=4]>
+  func.func public @main(%arg0: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["inferred_in"]>, mpmd.use_set = #mpmd.meshes_with_origins<"mesh1"["stage1"]>}) -> (!mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>> {jax.result_info = "result"}) attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>, <"mesh2" : <["x"=4]>>>} {
+    %0 = mpmd.assign {origin = "stage1", mpmd.use_set = #mpmd.meshes_with_origins<"mesh1"["stage1"]>} %arg0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>
+    %1 = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%0) (%arg1: tensor<3x5xf32>) {
+      %11 = stablehlo.add %arg1, %arg1 : tensor<3x5xf32>
+      mpmd.return %11 : tensor<3x5xf32>
+    } : (!mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>
+    %2 = mpmd.unassign {origin = "stage1", mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["stage1"]>} %1 : (!mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>) -> tensor<3x5xf32> loc("named_computation")
+    // expected-error-re @+1 {{Mesh assignment is not possible for op{{.*}}named_computation "stage1": mesh1[stage1]{{.*}}named_computation "stage2": mesh2[stage2]}}
+    %5 = stablehlo.sqrt %2 {mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["stage1"]>, mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["stage2"]>} : tensor<3x5xf32>
+    %6 = mpmd.assign {origin = "stage2", mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["stage2"]>} %5 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>  loc("named_computation")
+    %7 = mpmd.fragment<mesh="mesh2", origin=["stage2"]> (%6) (%arg1: tensor<3x5xf32>) {
+      %11 = stablehlo.add %arg1, %arg1 : tensor<3x5xf32>
+      mpmd.return %11 : tensor<3x5xf32>
+    } : (!mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>) -> !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>
+    return %7 : !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>
+  }
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_src_set_not_empty_failures.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_src_set_not_empty_failures.mlir
new file mode 100644
index 0000000..978bb22
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_assignment_validate_src_set_not_empty_failures.mlir
@@ -0,0 +1,62 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-validate-src-set-not-empty='error-limit=-1' -verify-diagnostics -split-input-file
+
+func.func @multiple_ops_have_empty_src_set(%arg0: tensor<4x8xf32> loc("x"))
+  -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  // expected-error @+1 {{Mesh assignment is not possible for op as its operands are on conflicting meshes}}
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<>} : tensor<4x8xf32>
+  %1 = stablehlo.add %0, %0 {mpmd.src_set = #mpmd.meshes_with_origins<>} : tensor<4x8xf32> // No error since we already errored on the operands.
+
+  // expected-error @+1 {{Mesh assignment is not possible for op as its operands are on conflicting meshes}}
+  %2 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<>} : tensor<4x8xf32>
+  func.return %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// -----
+
+func.func public @callee_args_have_empty_src_set(%arg0: tensor<3x5xf32> loc("x")
+  ) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // expected-error-re @+1 {{Mesh assignment is not possible for op as its operands are on conflicting meshes{{.*}}Input "x"}}
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<>} : tensor<3x5xf32>
+
+  %m1 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m1">} : tensor<3x5xf32>
+  %m2 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2">} : tensor<3x5xf32>
+
+  %2:2 = mpmd.call @shardy_mpmdf(%m1, %0) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  %3:2 = mpmd.call @shardy_mpmdf(%m2, %0) : (tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  return %2#0, %3#0 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+// No error on arg since the caller has empty src_set
+// expected-error @+1 {{Mesh assignment is not possible for arg0 of mpmd.call "f" }}
+func.func private @shardy_mpmdf(%arg0: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<>}, %arg1: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<>}) -> tensor<3x5xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+  return %arg0 : tensor<3x5xf32>
+}
+
+// -----
+
+#loc1 = loc("_named_computation.<locals>.wrapped_fn")
+module @test_printing_with_locs_through_call_ops {
+  func.func public @main(%arg0: !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>> loc("x"), %arg1: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"mesh2"["inferred_in"]>, mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["stage2"]>}) -> (tensor<3x5xf32> {jax.result_info = "result"}) attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>, <"mesh2" : <["x"=4]>>>} {
+    %0 = mpmd.unassign {origin = "user_in", mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["user_in"]>} %arg0 : (!mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>) -> tensor<3x5xf32> loc("x")
+    %1 = mpmd.call @shardy_mpmdf(%0, %arg1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<3x5xf32>
+    return %1 : tensor<3x5xf32>
+  }
+  func.func private @shardy_mpmdf(%arg0: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["user_in"]>}, %arg1: tensor<3x5xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"mesh2"["inferred_in"]>, mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["stage2"]>}) -> tensor<3x5xf32> attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>, <"mesh2" : <["x"=4]>>>} {
+    %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"mesh1"["user_in"]>} : tensor<3x5xf32>
+    %1 = mpmd.assign {origin = "stage2", mpmd.use_set = #mpmd.meshes_with_origins<"mesh2"["stage2"]>} %arg1 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>
+    %2 = mpmd.fragment<mesh="mesh2", origin=["stage2"]> (%1) (%arg2: tensor<3x5xf32>) {
+      %cst = stablehlo.constant dense<1.000000e+00> : tensor<3x5xf32>
+      %5 = stablehlo.add %arg2, %cst : tensor<3x5xf32>
+      mpmd.return %5 : tensor<3x5xf32>
+    } : (!mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>) -> !mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>
+    %3 = mpmd.unassign {origin = "stage2", mpmd.src_set = #mpmd.meshes_with_origins<"mesh2"["stage2"]>} %2 : (!mpmd.mesh_tensor<"mesh2", tensor<3x5xf32>>) -> tensor<3x5xf32> loc(#loc1)
+    // expected-error-re @+1 {{Mesh assignment is not possible for op as its operands{{.*}}Input "x": mesh1[user_in]{{.*}}named_computation "stage2": mesh2[stage2]}}
+    %4 = stablehlo.divide %0, %3 {mpmd.src_set = #mpmd.meshes_with_origins<>} : tensor<3x5xf32>
+    return %4 : tensor<3x5xf32>
+  }
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_convert_reduce_ops.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_convert_reduce_ops.mlir
new file mode 100644
index 0000000..65ab70e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_convert_reduce_ops.mlir
@@ -0,0 +1,72 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-convert-reduce-ops='infer-cross-mesh-reductions=true' 2>&1 | FileCheck --implicit-check-not mpmd.reduce %s
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>>
+
+
+// CHECK-LABEL: func @simple_reduce_chain
+func.func @simple_reduce_chain(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+// CHECK-NEXT: %[[R:.*]] = mpmd.reduce<add> {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} %arg0, %arg1, %arg2 : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: return %[[R]]
+  %4 = stablehlo.add %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %5 = stablehlo.add %4, %arg2 {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  func.return %5 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @reduce_of_reduces
+func.func @reduce_of_reduces(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+// CHECK-NEXT: %[[R:.*]] = mpmd.reduce<max> {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} %arg0, %arg1, %arg0, %arg1 : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: return %[[R]]
+  %41 = stablehlo.maximum %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %42 = stablehlo.maximum %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %5 = stablehlo.maximum %41, %42 {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  func.return %5 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @different_reduces_not_flattened
+func.func @different_reduces_not_flattened(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+// CHECK-NEXT: %[[ADD:.*]] = mpmd.reduce<add> {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} %arg0, %arg1 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[MAX:.*]] = mpmd.reduce<max> {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} %arg0, %arg1, %[[ADD]] : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: return %[[MAX]]
+  %41 = stablehlo.maximum %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %42 = stablehlo.add %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %5 = stablehlo.maximum %41, %42 {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  func.return %5 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @intermediate_reduce_multiple_users
+// We flatten users, but don't replace them if they still have users.
+func.func @intermediate_reduce_multiple_users(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {topology=#topology} {
+// CHECK-NEXT: %[[ADD0:.*]] = mpmd.reduce<add> {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} %arg0, %arg1 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[ADD1:.*]] = mpmd.reduce<add> {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} %arg0, %arg1, %arg0, %arg1 : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: return %[[ADD0]], %[[ADD1]]
+  %41 = stablehlo.add %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %42 = stablehlo.add %arg0, %arg1 {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %5 = stablehlo.add %41, %42 {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  func.return %42, %5 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @concat_reduce
+func.func @concat_reduce(%arg0: tensor<4x1x8xf32>, %arg1: tensor<4x1x8xf32>)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+// CHECK-NEXT: stablehlo.constant
+// CHECK-NEXT: %[[R0:.*]] = stablehlo.reshape %arg0 : (tensor<4x1x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[R1:.*]] = stablehlo.reshape %arg1 : (tensor<4x1x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[R2:.*]] = stablehlo.reshape %arg0 : (tensor<4x1x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[R3:.*]] = stablehlo.reshape %arg1 : (tensor<4x1x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: %[[MAX:.*]] = mpmd.reduce<max> {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} %[[R0]], %[[R1]], %[[R2]], %[[R3]] :
+// CHECK-SAME:     (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT: return %[[MAX]]
+  %init = stablehlo.constant dense<1.0> : tensor<f32>
+  %concat = "stablehlo.concatenate"(%arg0, %arg1, %arg0, %arg1) <{dimension = 1 : i64}>
+    {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} :
+    (tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>) -> tensor<4x4x8xf32>
+  %reduce = stablehlo.reduce(%concat init: %init) applies stablehlo.maximum across dimensions = [1]
+    {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} :
+    (tensor<4x4x8xf32>, tensor<f32>) -> tensor<4x8xf32>
+  func.return %reduce : tensor<4x8xf32>
+}
+
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline.mlir
new file mode 100644
index 0000000..505fb86
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline.mlir
@@ -0,0 +1,749 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-pipeline='infer-transfers=true infer-cross-mesh-reductions=True' 2>&1 | FileCheck %s
+
+!mesh_1_tensor_ui32 = !mpmd.mesh_tensor<"m1", tensor<ui32>>
+!mesh_1_tensor_1_ui32 = !mpmd.mesh_tensor<"m1", tensor<1xui32>>
+!mesh_1_tensor_2_ui32 = !mpmd.mesh_tensor<"m1", tensor<2xui32>>
+!mesh_1_tensor_5_5_ui32 = !mpmd.mesh_tensor<"m1", tensor<5x5xui32>>
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_1_tensor_4_1_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x1x8xf32>>
+!mesh_1_tensor_8_16_f32 = !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+!mesh_1_tensor_4_16_f32 = !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+!mesh_1_tensor_16_8_f32 = !mpmd.mesh_tensor<"m1", tensor<16x8xf32>>
+
+!mesh_2_tensor_ui32 = !mpmd.mesh_tensor<"m2", tensor<ui32>>
+!mesh_2_tensor_1_ui32 = !mpmd.mesh_tensor<"m2", tensor<1xui32>>
+!mesh_2_tensor_2_ui32 = !mpmd.mesh_tensor<"m2", tensor<2xui32>>
+!mesh_2_tensor_5_5_ui32 = !mpmd.mesh_tensor<"m2", tensor<5x5xui32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!mesh_2_tensor_4_1_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x1x8xf32>>
+!mesh_2_tensor_4_16_f32 = !mpmd.mesh_tensor<"m2", tensor<4x16xf32>>
+!mesh_2_tensor_8_16_f32 = !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>
+!mesh_2_tensor_16_8_f32 = !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>
+
+#topology =#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["y"=2]>>>
+
+
+// CHECK-LABEL: func @plain_spmd_module(
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1"
+func.func @plain_spmd_module(%arg0: tensor<4x8xf32>, %arg1: tensor<8x16xf32>, %arg2: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg2)
+// CHECK-SAME:    (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg3, %arg4
+// CHECK-NEXT:    %[[MUL:.*]] = stablehlo.multiply %[[ADD]], %[[ADD]]
+// CHECK-NEXT:    mpmd.return %[[MUL]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_2:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[INFERRED_1]], %arg1)
+// CHECK:       return %[[INFERRED_1]], %[[INFERRED_2]] :
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+  %0 = stablehlo.add %arg0, %arg2 : tensor<4x8xf32>
+  %1 = stablehlo.multiply %0, %0 : tensor<4x8xf32>
+
+  %3 = "stablehlo.dot"(%1, %arg1) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+  func.return %1, %3 : tensor<4x8xf32>, tensor<4x16xf32>
+}
+
+// CHECK-LABEL: func @push_unassign_forward_simple(
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>,
+// CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>)
+func.func @push_unassign_forward_simple(%arg0: tensor<4x8xf32>, %arg1: tensor<8x16xf32>, %arg2: tensor<4x16xf32>)
+  -> (tensor<4x16xf32>, tensor<4x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[FRAGMENT_1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1)
+// CHECK:       %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg2, %[[FRAGMENT_1]])
+// CHECK-SAME:    (%arg3: tensor<4x16xf32>, %arg4: tensor<4x16xf32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg3, %arg4
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_2:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[INFERRED_1]])
+// CHECK-SAME:    (%arg3: tensor<4x16xf32>) {
+// CHECK-NEXT:    %[[MUL:.*]] = stablehlo.multiply %arg3, %arg3
+// CHECK-NEXT:    mpmd.return %[[MUL]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[INFERRED_1]], %[[INFERRED_2]] :
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+  %0 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.assign %arg1 : (tensor<8x16xf32>) -> !mesh_1_tensor_8_16_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%0, %1)
+    (%arg3: tensor<4x8xf32>, %arg4: tensor<8x16xf32>) {
+    %6 = "stablehlo.dot"(%arg3, %arg4) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    mpmd.return %6 : tensor<4x16xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_8_16_f32) -> !mesh_1_tensor_4_16_f32
+
+  %3 = mpmd.unassign %2 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  %4 = stablehlo.add %arg2, %3 : tensor<4x16xf32>
+  %5 = stablehlo.multiply %4, %4 : tensor<4x16xf32>
+
+  func.return %4, %5 : tensor<4x16xf32>, tensor<4x16xf32>
+}
+
+// CHECK-LABEL: func @push_assign_backwards_simple(
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>,
+// CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+func.func @push_assign_backwards_simple(%arg0: tensor<4x8xf32>, %arg1: tensor<8x16xf32>, %arg2: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg2)
+// CHECK-SAME:    (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg3, %arg4
+// CHECK-NEXT:    %[[MUL:.*]] = stablehlo.multiply %[[ADD]], %[[ADD]]
+// CHECK-NEXT:    mpmd.return %[[MUL]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%[[INFERRED_1]], %arg1)
+// CHECK-SAME:    (%arg3: tensor<4x8xf32>, %arg4: tensor<8x16xf32>) {
+// CHECK:       return %[[INFERRED_1]], %[[FRAGMENT]] :
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+  %0 = stablehlo.add %arg0, %arg2 : tensor<4x8xf32>
+  %1 = stablehlo.multiply %0, %0 : tensor<4x8xf32>
+
+  %2 = mpmd.assign %1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %3 = mpmd.assign %arg1 : (tensor<8x16xf32>) -> !mesh_1_tensor_8_16_f32
+
+  %4 = mpmd.fragment<mesh="m1", origin=["f1"]> (%2, %3)
+    (%arg3: tensor<4x8xf32>, %arg4: tensor<8x16xf32>) {
+    %6 = "stablehlo.dot"(%arg3, %arg4) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    mpmd.return %6 : tensor<4x16xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_8_16_f32) -> !mesh_1_tensor_4_16_f32
+
+  %5 = mpmd.unassign %4 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  func.return %1, %5 : tensor<4x8xf32>, tensor<4x16xf32>
+}
+
+// CHECK-LABEL: func @unused_and_identity_fragments(
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>,
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+// CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>)
+func.func @unused_and_identity_fragments(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %arg2: tensor<8x16xf32>)
+  -> (tensor<4x8xf32>, tensor<8x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m2", origin=["f"]> (%arg0)
+// CHECK:       return %[[FRAGMENT]], %arg2
+// CHECK-SAME:    !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+  %0 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m2", origin=["f"]> (%0)
+    (%arg3: tensor<4x8xf32>) {
+    mpmd.return %arg3 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  %2 = mpmd.unassign %1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  func.return %2, %arg2 : tensor<4x8xf32>, tensor<8x16xf32>
+}
+
+// CHECK-LABEL: func @assign_ops_unused_by_user_fragment(
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>,
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>,
+// CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>)
+func.func @assign_ops_unused_by_user_fragment(%arg0: tensor<4x8xf32>, %arg1: tensor<8x16xf32>, %arg2: tensor<4x16xf32>)
+  -> (tensor<4x16xf32>, tensor<4x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[FRAGMENT_1:.*]] = mpmd.fragment<mesh="m2", origin=["f1"]> (%arg0, %arg1)
+// CHECK:       %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg2)
+// CHECK-SAME:    (%arg3: tensor<4x16xf32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg3, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[INFERRED_1]], %[[FRAGMENT_1]] :
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m2", tensor<4x16xf32>>
+  %0 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %1 = mpmd.assign %arg1 : (tensor<8x16xf32>) -> !mesh_2_tensor_8_16_f32
+
+  %2 = mpmd.fragment<mesh="m2", origin=["f1"]> (%0, %1)
+    (%arg3: tensor<4x8xf32>, %arg4: tensor<8x16xf32>) {
+    %6 = "stablehlo.dot"(%arg3, %arg4) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    mpmd.return %6 : tensor<4x16xf32>
+  } : (!mesh_2_tensor_4_8_f32, !mesh_2_tensor_8_16_f32) -> !mesh_2_tensor_4_16_f32
+
+  %3 = mpmd.unassign %2 : (!mesh_2_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  %4 = stablehlo.add %arg2, %arg2 : tensor<4x16xf32>
+
+  func.return %4, %3 : tensor<4x16xf32>, tensor<4x16xf32>
+}
+
+// CHECK-LABEL: func @assign_of_ops_with_only_scalar_operands(
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1", tensor<ui32>>)
+func.func @assign_of_ops_with_only_scalar_operands(%arg0: tensor<ui32>)
+  -> (tensor<ui32>, tensor<5x5xui32>, !mesh_1_tensor_5_5_ui32, !mesh_2_tensor_5_5_ui32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+// CHECK-SAME:    (%arg1: tensor<ui32>) {
+// CHECK-NEXT:    %[[CONST_1:.*]] = stablehlo.constant dense<1> : tensor<ui32>
+// CHECK-NEXT:    %[[ADD_1:.*]] = stablehlo.add %arg1, %[[CONST_1]]
+// CHECK-NEXT:    mpmd.return %[[ADD_1]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[TRANSFER:.*]] = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> !mpmd.mesh_tensor<"m2", tensor<ui32>>
+// CHECK-NEXT:  %[[INFERRED_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[TRANSFER]])
+// CHECK-SAME:    (%arg1: tensor<ui32>) {
+// CHECK-NEXT:    %[[CONST_2:.*]] = stablehlo.constant dense<1> : tensor<ui32>
+// CHECK-NEXT:    %[[ADD_2:.*]] = stablehlo.add %arg1, %[[CONST_2]]
+// CHECK-NEXT:    mpmd.return %[[ADD_2]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_3:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[INFERRED_1]])
+// CHECK-SAME:    (%arg1: tensor<ui32>) {
+// CHECK-NEXT:    %[[BROADCAST_1:.*]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<ui32>) -> tensor<5x5xui32>
+// CHECK-NEXT:    mpmd.return %[[BROADCAST_1]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_4:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[INFERRED_2]])
+// CHECK-SAME:    (%arg1: tensor<ui32>) {
+// CHECK-NEXT:    %[[BROADCAST_2:.*]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<ui32>) -> tensor<5x5xui32>
+// CHECK-NEXT:    mpmd.return %[[BROADCAST_2]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_5:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[INFERRED_4]])
+// CHECK-SAME:    (%arg1: tensor<5x5xui32>) {
+// CHECK-NEXT:    %[[ADD_3:.*]] = stablehlo.add %arg1, %arg1
+// CHECK-NEXT:    mpmd.return %[[ADD_3]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[INFERRED_1]], %[[INFERRED_3]], %[[INFERRED_3]], %[[INFERRED_5]] :
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<ui32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<5x5xui32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<5x5xui32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m2", tensor<5x5xui32>>
+  %0 = stablehlo.constant dense<1> : tensor<ui32>
+  %1 = stablehlo.add %arg0, %0 : tensor<ui32>
+  %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<ui32>) -> tensor<5x5xui32>
+
+  %3 = mpmd.assign %2 : (tensor<5x5xui32>) -> !mesh_1_tensor_5_5_ui32
+
+  %4 = stablehlo.add %2, %2 : tensor<5x5xui32>
+
+  %5 = mpmd.assign %4 : (tensor<5x5xui32>) -> !mesh_2_tensor_5_5_ui32
+
+  func.return %1, %2, %3, %5 : tensor<ui32>, tensor<5x5xui32>, !mesh_1_tensor_5_5_ui32, !mesh_2_tensor_5_5_ui32
+}
+
+// CHECK-LABEL: func @assign_of_non_scalar_const
+func.func @assign_of_non_scalar_const()
+  -> (!mesh_1_tensor_5_5_ui32, !mesh_2_tensor_5_5_ui32, !mesh_1_tensor_5_5_ui32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> () () {
+// CHECK-NEXT:    %[[CONST_1:.*]] = stablehlo.constant dense<1> : tensor<5x5xui32>
+// CHECK-NEXT:    mpmd.return %[[CONST_1]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> () () {
+// CHECK-NEXT:    %[[CONST_2:.*]] = stablehlo.constant dense<1> : tensor<5x5xui32>
+// CHECK-NEXT:    mpmd.return %[[CONST_2]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_3:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[INFERRED_1]])
+// CHECK-SAME:    (%arg0: tensor<5x5xui32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg0, %arg0
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK:       return %[[INFERRED_1]], %[[INFERRED_2]], %[[INFERRED_3]] :
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<5x5xui32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m2", tensor<5x5xui32>>,
+// CHECK-SAME:    !mpmd.mesh_tensor<"m1", tensor<5x5xui32>>
+  %0 = stablehlo.constant dense<1> : tensor<5x5xui32>
+
+  %1 = mpmd.assign %0 : (tensor<5x5xui32>) -> !mesh_1_tensor_5_5_ui32
+  %2 = mpmd.assign %0 : (tensor<5x5xui32>) -> !mesh_2_tensor_5_5_ui32
+
+  %3 = stablehlo.add %0, %0 : tensor<5x5xui32>
+  %4 = mpmd.assign %3 : (tensor<5x5xui32>) -> !mesh_1_tensor_5_5_ui32
+
+  func.return %1, %2, %4 : !mesh_1_tensor_5_5_ui32, !mesh_2_tensor_5_5_ui32, !mesh_1_tensor_5_5_ui32
+}
+
+// CHECK-LABEL: func @push_unassign_forward_then_assign_backwards
+func.func @push_unassign_forward_then_assign_backwards(%arg0: tensor<4x16xf32>, %arg1: tensor<4x16xf32>)
+  -> (tensor<4x16xf32>, tensor<4x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[FRAGMENT_1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+// CHECK:       %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg1)
+// CHECK-SAME:    (%arg2: tensor<4x16xf32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_2:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[FRAGMENT_1]], %[[INFERRED_1]])
+// CHECK-SAME:    (%arg2: tensor<4x16xf32>, %arg3: tensor<4x16xf32>) {
+// CHECK-NEXT:    %[[MUL:.*]] = stablehlo.multiply %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[MUL]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[INFERRED_1]], %[[INFERRED_2]]
+  %0 = mpmd.assign %arg0 : (tensor<4x16xf32>) -> !mesh_1_tensor_4_16_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%0)
+    (%arg2: tensor<4x16xf32>) {
+    mpmd.return %arg2 : tensor<4x16xf32>
+  } : (!mesh_1_tensor_4_16_f32) -> !mesh_1_tensor_4_16_f32
+
+  %2 = mpmd.unassign %1 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  %3 = stablehlo.add %arg1, %arg1 : tensor<4x16xf32>
+  %4 = stablehlo.multiply %2, %3 : tensor<4x16xf32>
+
+  func.return %3, %4 : tensor<4x16xf32>, tensor<4x16xf32>
+}
+
+// CHECK-LABEL: func @push_assign_backwards_then_unassign_forward
+func.func @push_assign_backwards_then_unassign_forward(%arg0: tensor<4x8xf32>, %arg1: tensor<8x16xf32>, %arg2: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg2)
+// CHECK-SAME:    (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg3, %arg4
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_2:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[INFERRED_1]])
+// CHECK-SAME:    (%arg3: tensor<4x8xf32>) {
+// CHECK-NEXT:    %[[MUL:.*]] = stablehlo.multiply %arg3, %arg3
+// CHECK-NEXT:    mpmd.return %[[MUL]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[FRAGMENT_1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%[[INFERRED_1]], %arg1)
+// CHECK:       return %[[INFERRED_2]], %[[FRAGMENT_1]]
+  %0 = stablehlo.add %arg0, %arg2 : tensor<4x8xf32>
+  %1 = stablehlo.multiply %0, %0 : tensor<4x8xf32>
+
+  %2 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %3 = mpmd.assign %arg1 : (tensor<8x16xf32>) -> !mesh_1_tensor_8_16_f32
+
+  %4 = mpmd.fragment<mesh="m1", origin=["f1"]> (%2, %3)
+    (%arg3: tensor<4x8xf32>, %arg4: tensor<8x16xf32>) {
+    %6 = "stablehlo.dot"(%arg3, %arg4) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    mpmd.return %6 : tensor<4x16xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_8_16_f32) -> !mesh_1_tensor_4_16_f32
+
+  %5 = mpmd.unassign %4 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  func.return %1, %5 : tensor<4x8xf32>, tensor<4x16xf32>
+}
+
+// CHECK-LABEL: func @op_between_unassign_and_assign
+func.func @op_between_unassign_and_assign(%arg0: !mesh_1_tensor_4_8_f32, %arg1: tensor<4x8xf32>)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg1)
+// CHECK-SAME:    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    %[[MUL:.*]] = stablehlo.multiply %[[ADD]], %[[ADD]]
+// CHECK-NEXT:    mpmd.return %[[MUL]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[INFERRED_1]]
+  %0 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  %1 = stablehlo.add %0, %arg1 : tensor<4x8xf32>
+  %2 = stablehlo.multiply %1, %1 : tensor<4x8xf32>
+
+  %3 = mpmd.assign %2 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+
+  func.return %3 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @op_with_multiple_results
+func.func @op_with_multiple_results(%arg0: tensor<4x16xf32>, %arg1: tensor<16x8xf32>)
+  -> (tensor<4x16xf32>, tensor<16x8xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[FRAGMENT_1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+// CHECK:       %[[INFERRED:.*]]:2 = mpmd.fragment<mesh="m1", origin=[]> (%[[FRAGMENT_1]], %arg1)
+// CHECK-SAME:    (%arg2: tensor<4x16xf32>, %arg3: tensor<16x8xf32>) {
+// CHECK-NEXT:    %[[OPT_BARRIER:.*]]:2 = stablehlo.optimization_barrier %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[OPT_BARRIER]]#0, %[[OPT_BARRIER]]#1
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[INFERRED]]#0, %[[INFERRED]]#1
+  %0 = mpmd.assign %arg0 : (tensor<4x16xf32>) -> !mesh_1_tensor_4_16_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%0)
+    (%arg2: tensor<4x16xf32>) {
+    mpmd.return %arg2 : tensor<4x16xf32>
+  } : (!mesh_1_tensor_4_16_f32) -> !mesh_1_tensor_4_16_f32
+
+  %2 = mpmd.unassign %1 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  %3:2 = stablehlo.optimization_barrier %2, %arg1 : tensor<4x16xf32>, tensor<16x8xf32>
+
+  func.return %3#0, %3#1 : tensor<4x16xf32>, tensor<16x8xf32>
+}
+
+// CHECK-LABEL: func @op_with_no_results
+func.func @op_with_no_results(%arg0: tensor<4x16xf32>)
+  -> (!mesh_2_tensor_4_16_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg0)
+// CHECK-NEXT:    stablehlo.add %arg1, %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK:       mpmd.fragment<mesh="m2", origin=[]> (%[[INFERRED_1]]) (%arg1
+// CHECK-NEXT:    sdy.sharding_group %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[INFERRED_1]]
+  %1 = stablehlo.add %arg0, %arg0 : tensor<4x16xf32>
+  sdy.sharding_group %1 group_id=0 : tensor<4x16xf32>
+
+  %2 = mpmd.assign %1 : (tensor<4x16xf32>) -> !mesh_2_tensor_4_16_f32
+  func.return %2 : !mesh_2_tensor_4_16_f32
+}
+
+// CHECK-LABEL: func @op_with_no_results_multiple_meshes
+func.func @op_with_no_results_multiple_meshes(%arg0: !mesh_2_tensor_4_16_f32)
+  -> (!mesh_1_tensor_4_16_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[TRANSFER:.*]] = mpmd.transfer %arg0
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[TRANSFER]])
+// CHECK-NEXT:    stablehlo.add %arg1, %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[INFERRED_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg0)
+// CHECK-NEXT:    stablehlo.add %arg1, %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  mpmd.fragment<mesh="m2", origin=[]> (%[[INFERRED_2]]) (%arg1
+// CHECK-NEXT:    sdy.sharding_group %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  mpmd.fragment<mesh="m1", origin=[]> (%[[INFERRED_1]]) (%arg1
+// CHECK-NEXT:    sdy.sharding_group %arg1
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[TRANSFER]]
+  %0 = mpmd.unassign %arg0 : (!mesh_2_tensor_4_16_f32) -> tensor<4x16xf32>
+  %t = mpmd.transfer %arg0 : (!mesh_2_tensor_4_16_f32) -> !mesh_1_tensor_4_16_f32
+
+  %1 = stablehlo.add %0, %0 : tensor<4x16xf32>
+  sdy.sharding_group %1 group_id=0 : tensor<4x16xf32>
+
+  func.return %t : !mesh_1_tensor_4_16_f32
+}
+
+// CHECK-LABEL: func @push_unassign_forward_op_with_regions
+func.func @push_unassign_forward_op_with_regions(%arg0: tensor<4x16xf32>, %arg1: tensor<4x16xf32>, %arg2: tensor<i32>)
+  -> tensor<4x16xf32> attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[FRAGMENT_1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+// CHECK:       %[[INFERRED:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg2, %[[FRAGMENT_1]], %arg1)
+// CHECK-SAME:    (%arg3: tensor<i32>, %arg4: tensor<4x16xf32>, %arg5: tensor<4x16xf32>) {
+// CHECK-NEXT:    %[[CASE:.*]] = "stablehlo.case"(%arg3) ({
+// CHECK-NEXT:      %[[ADD:.*]] = stablehlo.add %arg4, %arg5
+// CHECK-NEXT:      stablehlo.return %[[ADD]]
+// CHECK-NEXT:    }, {
+// CHECK-NEXT:      stablehlo.return %arg5
+// CHECK-NEXT:    })
+// CHECK-NEXT:    mpmd.return %[[CASE]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[INFERRED]]
+  %0 = mpmd.assign %arg0 : (tensor<4x16xf32>) -> !mesh_1_tensor_4_16_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f1"]> (%0)
+    (%arg3: tensor<4x16xf32>) {
+    mpmd.return %arg3 : tensor<4x16xf32>
+  } : (!mesh_1_tensor_4_16_f32) -> !mesh_1_tensor_4_16_f32
+
+  %2 = mpmd.unassign %1 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  %3 = "stablehlo.case"(%arg2) ({
+    %4 = stablehlo.add %2, %arg1 : tensor<4x16xf32>
+    stablehlo.return %4 : tensor<4x16xf32>
+  }, {
+    stablehlo.return %arg1 : tensor<4x16xf32>
+  }) : (tensor<i32>) -> tensor<4x16xf32>
+
+  func.return %3 : tensor<4x16xf32>
+}
+
+// CHECK-LABEL: func @push_assign_backwards_op_with_regions
+func.func @push_assign_backwards_op_with_regions(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %arg2: tensor<i32>)
+  -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1, %arg2)
+// CHECK-SAME:    (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>, %arg5: tensor<i32>) {
+// CHECK-NEXT:    %[[CASE:.*]] = "stablehlo.case"(%arg5) ({
+// CHECK-NEXT:      %[[ADD:.*]] = stablehlo.add %arg3, %arg4
+// CHECK-NEXT:      stablehlo.return %[[ADD]]
+// CHECK-NEXT:    }, {
+// CHECK-NEXT:      stablehlo.return %arg4
+// CHECK-NEXT:    })
+// CHECK-NEXT:    mpmd.return %[[CASE]]
+// CHECK-NEXT:  }
+// CHECK:       return %[[INFERRED]]
+  %0 = "stablehlo.case"(%arg2) ({
+    %4 = stablehlo.add %arg0, %arg1 : tensor<4x8xf32>
+    stablehlo.return %4 : tensor<4x8xf32>
+  }, {
+    stablehlo.return %arg1 : tensor<4x8xf32>
+  }) : (tensor<i32>) -> tensor<4x8xf32>
+
+  %1 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%1)
+    (%arg3: tensor<4x8xf32>) {
+    mpmd.return %arg3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %3 = mpmd.unassign %2 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  func.return %3 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @pushed_assign_of_unassign_deduped_with_transfer
+func.func @pushed_assign_of_unassign_deduped_with_transfer(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_2_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, tensor<4x8xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[TRANSFER:.*]] = mpmd.transfer %arg0
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[TRANSFER]])
+// CHECK-SAME:    (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:    %[[ADD_1:.*]] = stablehlo.add %arg1, %arg1
+// CHECK-NEXT:    %[[ADD_2:.*]] = stablehlo.add %[[ADD_1]], %[[ADD_1]]
+// CHECK-NEXT:    mpmd.return %[[ADD_2]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[TRANSFER]], %[[INFERRED_1]], %[[TRANSFER]]
+  %0 = mpmd.transfer %arg0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %2 = stablehlo.add %1, %1 : tensor<4x8xf32>
+  %3 = stablehlo.add %2, %2 : tensor<4x8xf32>
+  %4 = mpmd.assign %3 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  func.return %0, %4, %1 : !mesh_2_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unassign_pushed_despite_transfer
+func.func @unassign_pushed_despite_transfer(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_2_tensor_4_8_f32, tensor<4x8xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[TRANSFER:.*]] = mpmd.transfer %arg0
+// CHECK-NEXT:  %[[INFERRED:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+// CHECK-SAME:    (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg1, %arg1
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[TRANSFER]], %[[INFERRED]]
+  %0 = mpmd.transfer %arg0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %2 = stablehlo.add %1, %1 : tensor<4x8xf32>
+  func.return %0, %2 : !mesh_2_tensor_4_8_f32, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @multiple_meshes_complex
+func.func @multiple_meshes_complex(%arg0: tensor<4x8xf32>,
+                                   %arg1: tensor<8x16xf32>,
+                                   %arg2: tensor<16x8xf32>,
+                                   %arg3: tensor<4x16xf32>,
+                                   %arg4: tensor<16x8xf32>)
+  -> (tensor<4x16xf32>, tensor<4x8xf32>, tensor<16x8xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// CHECK-NEXT:  %[[INFERRED_1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg2, %arg4) (%arg5: tensor<16x8xf32>, %arg6: tensor<16x8xf32>) {
+// CHECK-NEXT:    %[[ADD_1:.*]] = stablehlo.add %arg5, %arg6 : tensor<16x8xf32>
+// CHECK-NEXT:    mpmd.return %[[ADD_1]] : tensor<16x8xf32>
+// CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m2", tensor<16x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>
+// CHECK-NEXT:  %[[FRAGMENT_1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0, %arg1) (%arg5: tensor<4x8xf32>, %arg6: tensor<8x16xf32>) {
+// CHECK-NEXT:    %[[DOT_1:.*]] = stablehlo.dot %arg5, %arg6 : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+// CHECK-NEXT:    mpmd.return %[[DOT_1]] : tensor<4x16xf32>
+// CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+// CHECK-NEXT:  %[[TRANSFER:.*]] = mpmd.transfer %[[FRAGMENT_1]] : (!mpmd.mesh_tensor<"m1", tensor<4x16xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x16xf32>>
+// CHECK-NEXT:  %[[FRAGMENT_2:.*]] = mpmd.fragment<mesh="m2", origin=["f2"]> (%[[TRANSFER]], %arg2) (%arg5: tensor<4x16xf32>, %arg6: tensor<16x8xf32>) {
+// CHECK-NEXT:    %[[ADD_2:.*]] = stablehlo.add %arg6, %arg6 : tensor<16x8xf32>
+// CHECK-NEXT:    %[[DOT_2:.*]] = stablehlo.dot %arg5, %[[ADD_2]] : (tensor<4x16xf32>, tensor<16x8xf32>) -> tensor<4x8xf32>
+// CHECK-NEXT:    mpmd.return %[[DOT_2]] : tensor<4x8xf32>
+// CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m2", tensor<4x16xf32>>, !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+// CHECK-NEXT:  %[[INFERRED_2:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[FRAGMENT_1]], %arg3) (%arg5: tensor<4x16xf32>, %arg6: tensor<4x16xf32>) {
+// CHECK-NEXT:    %[[ADD_3:.*]] = stablehlo.add %arg5, %arg6 : tensor<4x16xf32>
+// CHECK-NEXT:    %[[ADD_4:.*]] = stablehlo.add %[[ADD_3]], %[[ADD_3]] : tensor<4x16xf32>
+// CHECK-NEXT:    mpmd.return %[[ADD_4]] : tensor<4x16xf32>
+// CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m1", tensor<4x16xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+// CHECK-NEXT:  return %[[INFERRED_2]], %[[FRAGMENT_2]], %[[INFERRED_1]] : !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>
+  %0 = stablehlo.add %arg2, %arg4 : tensor<16x8xf32>
+
+  %1 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.assign %arg1 : (tensor<8x16xf32>) -> !mesh_1_tensor_8_16_f32
+
+  %3 = mpmd.fragment<mesh="m1", origin=["f1"]> (%1, %2)
+    (%arg5: tensor<4x8xf32>, %arg6: tensor<8x16xf32>) {
+    %12 = "stablehlo.dot"(%arg5, %arg6) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    mpmd.return %12 : tensor<4x16xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_8_16_f32) -> (!mesh_1_tensor_4_16_f32)
+
+  %4 = mpmd.transfer %3 : (!mesh_1_tensor_4_16_f32) -> !mesh_2_tensor_4_16_f32
+
+  %5 = stablehlo.add %arg2, %arg2 : tensor<16x8xf32>
+
+  %6 = mpmd.assign %5 : (tensor<16x8xf32>) -> !mesh_2_tensor_16_8_f32
+
+  %7 = mpmd.fragment<mesh="m2", origin=["f2"]> (%4, %6)
+    (%arg5: tensor<4x16xf32>, %arg6: tensor<16x8xf32>) {
+    %12 = "stablehlo.dot"(%arg5, %arg6) : (tensor<4x16xf32>, tensor<16x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %12 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_16_f32, !mesh_2_tensor_16_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  %8 = mpmd.unassign %3 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  %9 = stablehlo.add %8, %arg3 : tensor<4x16xf32>
+  %10 = stablehlo.add %9, %9 : tensor<4x16xf32>
+
+  %11 = mpmd.unassign %7 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  func.return %10, %11, %0 : tensor<4x16xf32>, tensor<4x8xf32>, tensor<16x8xf32>
+}
+
+// CHECK-LABEL: func @reduce_of_reduces(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:                     %arg1: !mpmd.mesh_tensor<"m2"
+func.func @reduce_of_reduces(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32) attributes {topology=#topology} {
+// CHECK-NEXT:  %[[ADD_LOCAL_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg0
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[ADD_LOCAL_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg1, %arg1
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[TRANSFER_2:.*]] = mpmd.transfer %[[ADD_LOCAL_2]]
+// CHECK-NEXT:  %[[ADD_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ADD_LOCAL_1]], %[[TRANSFER_2]])
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[TRANSFER_1:.*]] = mpmd.transfer %[[ADD_LOCAL_1]]
+// CHECK-NEXT:  %[[ADD_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[TRANSFER_1]], %[[ADD_LOCAL_2]])
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[ADD_1]], %[[ADD_2]]
+  %arg0_a = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %arg1_a = mpmd.assign %arg1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %1 = mpmd.unassign %arg0_a : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %2 = mpmd.unassign %arg1_a : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+  %41 = stablehlo.add %1, %2 : tensor<4x8xf32>
+  %42 = stablehlo.add %1, %2 : tensor<4x8xf32>
+  %5 = stablehlo.add %41, %42 : tensor<4x8xf32>
+  %6 = mpmd.assign %5 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %7 = mpmd.assign %5 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  func.return %6, %7 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @concat_reduce(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2"
+func.func @concat_reduce(%arg0: tensor<4x1x8xf32>, %arg1: tensor<4x1x8xf32>)
+  -> !mesh_1_tensor_4_8_f32 attributes {topology=#topology} {
+// CHECK-NEXT:  %[[R1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0
+// CHECK-NEXT:    stablehlo.reshape
+// CHECK:       %[[R2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg1
+// CHECK-NEXT:    stablehlo.reshape
+// CHECK:      %[[R3:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0
+// CHECK-NEXT:    stablehlo.reshape
+// CHECK:       %[[R4:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg1
+// CHECK-NEXT:    stablehlo.reshape
+// CHECK:       %[[MAX_LOCAL_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[R1]], %[[R3]]
+// CHECK-NEXT:    %[[MAX:.*]] = stablehlo.maximum %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[MAX]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[MAX_LOCAL_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[R2]], %[[R4]]
+// CHECK-NEXT:    %[[MAX:.*]] = stablehlo.maximum %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[MAX]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[TRANSFER_2:.*]] = mpmd.transfer %[[MAX_LOCAL_2]]
+// CHECK-NEXT:  %[[MAX_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[MAX_LOCAL_1]], %[[TRANSFER_2]])
+// CHECK-NEXT:    %[[MAX:.*]] = stablehlo.maximum %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[MAX]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[MAX_1]]
+  %init = stablehlo.constant dense<1.0> : tensor<f32>
+  %arg0_a = mpmd.assign %arg0 : (tensor<4x1x8xf32>) -> !mesh_1_tensor_4_1_8_f32
+  %arg1_a = mpmd.assign %arg1 : (tensor<4x1x8xf32>) -> !mesh_2_tensor_4_1_8_f32
+  %1 = mpmd.unassign %arg0_a : (!mesh_1_tensor_4_1_8_f32) -> tensor<4x1x8xf32>
+  %2 = mpmd.unassign %arg1_a : (!mesh_2_tensor_4_1_8_f32) -> tensor<4x1x8xf32>
+  %concat = "stablehlo.concatenate"(%1, %2, %1, %2) <{dimension = 1 : i64}> :
+    (tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>) -> tensor<4x4x8xf32>
+  %reduce = stablehlo.reduce(%concat init: %init) applies stablehlo.maximum across dimensions = [1] :
+    (tensor<4x4x8xf32>, tensor<f32>) -> tensor<4x8xf32>
+  %6 = mpmd.assign %reduce : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  func.return %6 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @src_set_empty_flowing_into_broadcast(%arg0: !mpmd.mesh_tensor<"m1"
+func.func @src_set_empty_flowing_into_broadcast(%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32>)
+  attributes {topology = #topology}
+{
+// CHECK-NEXT:  %[[F1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0
+// CHECK-NEXT:    stablehlo.add
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[T1:.*]] = mpmd.transfer %arg0
+// CHECK-NEXT:  %[[F2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[T1]])
+// CHECK-NEXT:    stablehlo.add
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[T2:.*]] = mpmd.transfer %[[F2]]
+// CHECK-NEXT:  %[[F3:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[F1]], %[[T2]])
+// CHECK-NEXT:    stablehlo.divide
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[F3]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  %1 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  %11 = mpmd.unassign %1 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %22 = mpmd.unassign %2 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  // %3 will have src_set empty
+  %3 = stablehlo.divide %11, %22 : tensor<4x8xf32>
+  %b = mpmd.broadcast %3 : tensor<4x8xf32>
+  %bb = stablehlo.add %b, %b : tensor<4x8xf32>
+  return %b : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_broadcast_e2e.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_broadcast_e2e.mlir
new file mode 100644
index 0000000..dfb43cd
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_broadcast_e2e.mlir
@@ -0,0 +1,98 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-pipeline 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!mesh_3_tensor_4_8_f32 = !mpmd.mesh_tensor<"m3", tensor<4x8xf32>>
+
+
+// In the next test, the result of the broadcast are assigned to m1, through the
+// matmul. There are two options to assign the add to:
+// 1. m2, in which case we would need to transfer its result to m1 so it's used
+// by the matmul.
+// 2. m1, in which case no transfer is needed. We pick this option.
+
+// CHECK-LABEL: func @broadcast_only_user_assigned
+func.func @broadcast_only_user_assigned(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>)
+  ->!mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+
+// CHECK-NEXT: %[[ADD_FRAG:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg1)
+// CHECK-NEXT:   stablehlo.add
+// CHECK-NEXT:   return
+// CHECK-NEXT: }
+// CHECK-NEXT: %[[MUL_F:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ADD_FRAG]], %arg2)
+// CHECK-NEXT:   stablehlo.multiply
+// CHECK-NEXT:   return
+// CHECK-NEXT: }
+  %0 = stablehlo.add %arg0, %arg1 : tensor<4x8xf32>
+  %1 = mpmd.broadcast %0 : tensor<4x8xf32>
+  %2 = stablehlo.multiply %1, %arg2 : tensor<4x8xf32>
+  %3 = mpmd.assign %2 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  func.return %3 : !mesh_1_tensor_4_8_f32
+}
+
+// In the next test, we have a single computational op, which is clearly
+// assigned to m1 (as its operands are assigned to m1 too).
+// The result of this computation is then used in all meshes, so we introduce
+// transfers for every mesh that is not m1.
+
+// CHECK-LABEL: func @chained_broadcast
+func.func @chained_broadcast(%arg0: !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_3_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>, <"m3": <["x"=2]>>>}
+{
+// CHECK-NEXT: %[[ADD_FRAG:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+// CHECK-NEXT:   stablehlo.add
+// CHECK-NEXT:   return
+// CHECK-NEXT: }
+// CHECK-DAG:  mpmd.transfer %0 : {{.*}}m1{{.*}} -> {{.*}}m2{{.*}}
+// CHECK-DAG:  mpmd.transfer %0 : {{.*}}m1{{.*}} -> {{.*}}m3{{.*}}
+  %u = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %add = stablehlo.add %u, %u : tensor<4x8xf32>
+  %b0 = mpmd.broadcast %add : tensor<4x8xf32>
+  %b1 = mpmd.broadcast %b0 : tensor<4x8xf32>
+  %a1 = mpmd.assign %b1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %a2 = mpmd.assign %b1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %a3 = mpmd.assign %b1 : (tensor<4x8xf32>) -> !mesh_3_tensor_4_8_f32
+  func.return %a1, %a2, %a3 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_3_tensor_4_8_f32
+}
+
+// The next test illustrates when a value is indirectly used by a broadcast and by a
+// user of a broadcast.
+
+// CHECK-LABEL: func @escape_broadcast
+func.func @escape_broadcast(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  ->!mesh_3_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>,
+      <"m3": <["x"=2]>>
+    >} {
+
+// CHECK-NEXT: %[[T0:.*]] = mpmd.transfer %arg0 : {{.*}}m1{{.*}} -> {{.*}}m2{{.*}}
+// CHECK-NEXT: %[[MUL_FRAG:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[T0]], %arg1)
+// CHECK-NEXT:   multiply
+// CHECK-NEXT:   return
+// CHECK-NEXT: }
+// CHECK-DAG:  %[[T1:.*]] = mpmd.transfer %[[MUL_FRAG]] : {{.*}}m2{{.*}} -> {{.*}}m3{{.*}}
+// CHECK-DAG:  %[[T2:.*]] = mpmd.transfer %arg0 : {{.*}}m1{{.*}} -> {{.*}}m3{{.*}}
+// CHECK-DAG:  mpmd.fragment<mesh="m3", origin=[]> (%[[T2]], %[[T1]])
+
+  %u0 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  %a0 = mpmd.assign %u0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %u1 = mpmd.unassign %a0 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  %u2 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+  %mul = stablehlo.multiply %u1, %u2 : tensor<4x8xf32>
+  %bcast = mpmd.broadcast %mul : tensor<4x8xf32>
+  %a = mpmd.assign %bcast : (tensor<4x8xf32>) -> !mesh_3_tensor_4_8_f32
+  %t = mpmd.assign %u0 : (tensor<4x8xf32>) -> !mesh_3_tensor_4_8_f32
+  %m3_frag = mpmd.fragment<mesh="m3", origin=[]> (%t, %a) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_3_tensor_4_8_f32, !mesh_3_tensor_4_8_f32) -> !mesh_3_tensor_4_8_f32
+  func.return %m3_frag : !mesh_3_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_call_op.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_call_op.mlir
new file mode 100644
index 0000000..aa75d35
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_call_op.mlir
@@ -0,0 +1,427 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-pipeline -split-input-file 2>&1 | FileCheck %s
+
+!mesh1_3_5 = !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>
+!mesh1_10_3 = !mpmd.mesh_tensor<"mesh1", tensor<10x3xf32>>
+!mesh1_10_5 = !mpmd.mesh_tensor<"mesh1", tensor<10x5xf32>>
+!mesh2_10_5 = !mpmd.mesh_tensor<"mesh2", tensor<10x5xf32>>
+!mesh2_10_7 = !mpmd.mesh_tensor<"mesh2", tensor<10x7xf32>>
+!mesh2_5_7 = !mpmd.mesh_tensor<"mesh2", tensor<5x7xf32>>
+
+
+// Tests that assigns and unassigns in a non-main function will be propagated
+// to its call sites.
+
+// CHECK-LABEL: func.func public @main(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>,
+// CHECK-SAME:   %arg1: !mpmd.mesh_tensor<"mesh2", tensor<5x7xf32>>,
+// CHECK-SAME:   %arg2: !mpmd.mesh_tensor<"mesh1", tensor<10x3xf32>>) ->
+// CHECK-SAME: (!mpmd.mesh_tensor<"mesh2", tensor<10x7xf32>>, !mpmd.mesh_tensor<"mesh2", tensor<10x7xf32>>)
+func.func public @main(%arg0: tensor<3x5xf32>, %arg1: tensor<5x7xf32>, %arg2: tensor<10x3xf32>) -> (tensor<10x7xf32>, tensor<10x7xf32>) attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>, <"mesh2" : <["y"=1]>>>} {
+  // CHECK-NEXT: mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (!mpmd.mesh_tensor<"mesh1", {{.*}}>, !mpmd.mesh_tensor<"mesh2", {{.*}}>, !mpmd.mesh_tensor<"mesh1", {{.*}}>) -> !mpmd.mesh_tensor<"mesh2", {{.*}}>
+  %0 = mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (tensor<3x5xf32>, tensor<5x7xf32>, tensor<10x3xf32>) -> tensor<10x7xf32>
+  // CHECK-NEXT: mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (!mpmd.mesh_tensor<"mesh1", {{.*}}>, !mpmd.mesh_tensor<"mesh2", {{.*}}>, !mpmd.mesh_tensor<"mesh1", {{.*}}>) -> !mpmd.mesh_tensor<"mesh2", {{.*}}>
+  %1 = mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (tensor<3x5xf32>, tensor<5x7xf32>, tensor<10x3xf32>) -> tensor<10x7xf32>
+  // CHECK-NEXT: return
+  return %0, %1 : tensor<10x7xf32>, tensor<10x7xf32>
+}
+
+// CHECK-LABEL: func.func private @shardy_mpmd_mlp(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"mesh1", tensor<3x5xf32>>,
+// CHECK-SAME:   %arg1: !mpmd.mesh_tensor<"mesh2", tensor<5x7xf32>>,
+// CHECK-SAME:   %arg2: !mpmd.mesh_tensor<"mesh1", tensor<10x3xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"mesh2", tensor<10x7xf32>>
+func.func private @shardy_mpmd_mlp(%arg0: tensor<3x5xf32>,
+                                    %arg1: tensor<5x7xf32>,
+                                    %arg2: tensor<10x3xf32>
+) -> tensor<10x7xf32>
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=1]>>, <"mesh2" : <["y"=1]>>>}
+{
+  // CHECK-NEXT: %[[F1:.*]] = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%arg0, %arg2)
+  // CHECK-NEXT:    dot_general
+  // CHECK-NEXT:    mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"mesh1", {{.*}}>, !mpmd.mesh_tensor<"mesh1", {{.*}}>) -> !mpmd.mesh_tensor<"mesh1", {{.*}}>
+  // CHECK-NEXT: %[[T:.*]] = mpmd.transfer %[[F1]] : (!mpmd.mesh_tensor<"mesh1", {{.*}}>) -> !mpmd.mesh_tensor<"mesh2", {{.*}}>
+  // CHECK-NEXT: %[[F2:.*]] = mpmd.fragment<mesh="mesh2", origin=["stage2"]> (%arg1, %[[T]]) (%arg3: tensor<5x7xf32>, %arg4: tensor<10x5xf32>) {
+  // CHECK-NEXT:    dot_general
+  // CHECK-NEXT:    mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"mesh2", {{.*}}>, !mpmd.mesh_tensor<"mesh2", {{.*}}>) -> !mpmd.mesh_tensor<"mesh2", {{.*}}>
+  // CHECK-NEXT: return %[[F2]] : !mpmd.mesh_tensor<"mesh2", {{.*}}>
+
+  %0 = mpmd.assign %arg0 : (tensor<3x5xf32>) -> !mesh1_3_5
+  %1 = mpmd.assign %arg2 : (tensor<10x3xf32>) -> !mesh1_10_3
+  %2 = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%0, %1) (%arg3: tensor<3x5xf32>, %arg4: tensor<10x3xf32>) {
+    %7 = stablehlo.dot_general %arg4, %arg3, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x3xf32>, tensor<3x5xf32>) -> tensor<10x5xf32>
+    mpmd.return %7 : tensor<10x5xf32>
+  } : (!mesh1_3_5, !mesh1_10_3) -> !mesh1_10_5
+  %3 = mpmd.assign %arg1 : (tensor<5x7xf32>) -> !mesh2_5_7
+  %4 = mpmd.transfer %2 : (!mesh1_10_5) -> !mesh2_10_5
+  %5 = mpmd.fragment<mesh="mesh2", origin=["stage2"]> (%3, %4) (%arg3: tensor<5x7xf32>, %arg4: tensor<10x5xf32>) {
+    %7 = stablehlo.dot_general %arg4, %arg3, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x5xf32>, tensor<5x7xf32>) -> tensor<10x7xf32>
+    mpmd.return %7 : tensor<10x7xf32>
+  } : (!mesh2_5_7, !mesh2_10_5) -> !mesh2_10_7
+  %6 = mpmd.unassign %5 : (!mesh2_10_7) -> tensor<10x7xf32>
+  return %6 : tensor<10x7xf32>
+}
+
+// -----
+
+// Tests that unassigns at a call-site are propagated to the body of the
+// called function.
+
+// CHECK-LABEL: func.func public @main(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+// CHECK-SAME:   %arg1: !mpmd.mesh_tensor<"m1", tensor<5x7xf32>>,
+// CHECK-SAME:   %arg2: !mpmd.mesh_tensor<"m1", tensor<10x3xf32>>) ->
+// CHECK-SAME:  (!mpmd.mesh_tensor<"m1", tensor<10x7xf32>>, !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>)
+func.func public @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+                        %arg1: !mpmd.mesh_tensor<"m1", tensor<5x7xf32>>,
+                        %arg2: !mpmd.mesh_tensor<"m1", tensor<10x3xf32>>
+) -> (tensor<10x7xf32>, tensor<10x7xf32>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[C1:.*]] = mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (!mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>) -> !mpmd.mesh_tensor<"m1", {{.*}}>
+  // CHECK-NEXT: %[[C2:.*]] = mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (!mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>) -> !mpmd.mesh_tensor<"m1", {{.*}}>
+  // CHECK-NEXT: return %[[C1]], %[[C2]] : !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>
+  %0 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<3x5xf32>>) -> tensor<3x5xf32>
+  %1 = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m1", tensor<5x7xf32>>) -> tensor<5x7xf32>
+  %2 = mpmd.unassign %arg2 : (!mpmd.mesh_tensor<"m1", tensor<10x3xf32>>) -> tensor<10x3xf32>
+  %3 = mpmd.call @shardy_mpmd_mlp(%0, %1, %2) : (tensor<3x5xf32>, tensor<5x7xf32>, tensor<10x3xf32>) -> tensor<10x7xf32>
+  %4 = mpmd.call @shardy_mpmd_mlp(%0, %1, %2) : (tensor<3x5xf32>, tensor<5x7xf32>, tensor<10x3xf32>) -> tensor<10x7xf32>
+  return %3, %4 : tensor<10x7xf32>, tensor<10x7xf32>
+}
+
+// CHECK-LABEL: func.func private @shardy_mpmd_mlp(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+// CHECK-SAME:   %arg1: !mpmd.mesh_tensor<"m1", tensor<5x7xf32>>,
+// CHECK-SAME:   %arg2: !mpmd.mesh_tensor<"m1", tensor<10x3xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>
+
+func.func private @shardy_mpmd_mlp(%arg0: tensor<3x5xf32>, %arg1: tensor<5x7xf32>, %arg2: tensor<10x3xf32>) -> tensor<10x7xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>} {
+  // CHECK-NEXT: %[[F1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg1, %arg2, %arg0)
+  // CHECK-NEXT:    dot_general
+  // CHECK-NEXT:    dot_general
+  // CHECK-NEXT:    mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>) -> !mpmd.mesh_tensor<"m1", {{.*}}>
+  // CHECK-NEXT: return %[[F1]] : !mpmd.mesh_tensor<"m1", {{.*}}>
+  %0 = stablehlo.dot_general %arg2, %arg0, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x3xf32>, tensor<3x5xf32>) -> tensor<10x5xf32>
+  %1 = stablehlo.dot_general %0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x5xf32>, tensor<5x7xf32>) -> tensor<10x7xf32>
+  return %1 : tensor<10x7xf32>
+}
+
+// -----
+
+// Tests that assigns at a call-site are propagated to the body of the
+// called function.
+
+// CHECK-LABEL: func.func public @main(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+// CHECK-SAME:   %arg1: !mpmd.mesh_tensor<"m1", tensor<5x7xf32>>,
+// CHECK-SAME:   %arg2: !mpmd.mesh_tensor<"m1", tensor<10x3xf32>>) ->
+// CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<10x7xf32>>, !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>)
+func.func public @main(%arg0: tensor<3x5xf32>, %arg1: tensor<5x7xf32>, %arg2: tensor<10x3xf32>) -> (!mpmd.mesh_tensor<"m1", tensor<10x7xf32>>, !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>) attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>} {
+  // CHECK-NEXT: %[[C1:.*]] = mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (!mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>) -> !mpmd.mesh_tensor<"m1", {{.*}}>
+  // CHECK-NEXT: %[[C2:.*]] = mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (!mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>) -> !mpmd.mesh_tensor<"m1", {{.*}}>
+  // CHECK-NEXT: return %[[C1]], %[[C2]] : !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>
+  %0 = mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (tensor<3x5xf32>, tensor<5x7xf32>, tensor<10x3xf32>) -> tensor<10x7xf32>
+  %1 = mpmd.call @shardy_mpmd_mlp(%arg0, %arg1, %arg2) : (tensor<3x5xf32>, tensor<5x7xf32>, tensor<10x3xf32>) -> tensor<10x7xf32>
+  %2 = mpmd.assign %0 : (tensor<10x7xf32>) -> !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>
+  %3 = mpmd.assign %1 : (tensor<10x7xf32>) -> !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>
+  return %2, %3 : !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>, !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>
+}
+
+// CHECK-LABEL: func.func private @shardy_mpmd_mlp(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+// CHECK-SAME:   %arg1: !mpmd.mesh_tensor<"m1", tensor<5x7xf32>>,
+// CHECK-SAME:   %arg2: !mpmd.mesh_tensor<"m1", tensor<10x3xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<10x7xf32>>
+func.func private @shardy_mpmd_mlp(%arg0: tensor<3x5xf32>, %arg1: tensor<5x7xf32>, %arg2: tensor<10x3xf32>) -> tensor<10x7xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>} {
+  // CHECK-NEXT: %[[F1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg1, %arg2, %arg0)
+  // CHECK-NEXT:    dot_general
+  // CHECK-NEXT:    dot_general
+  // CHECK-NEXT:    mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>) -> !mpmd.mesh_tensor<"m1", {{.*}}>
+  // CHECK-NEXT: return %[[F1]] : !mpmd.mesh_tensor<"m1", {{.*}}>
+  %0 = stablehlo.dot_general %arg2, %arg0, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x3xf32>, tensor<3x5xf32>) -> tensor<10x5xf32>
+  %1 = stablehlo.dot_general %0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x5xf32>, tensor<5x7xf32>) -> tensor<10x7xf32>
+  return %1 : tensor<10x7xf32>
+}
+
+// -----
+
+// All targets are used by an assign_op, but one of them is assigned multiple
+// times to the same mesh. This does not prevent inference though.
+
+// CHECK-LABEL: func.func public @main(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>) ->
+// CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>)
+func.func public @main(%arg0: tensor<3x5xf32>)
+  ->
+  (!mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @f(%arg0) : (!mpmd.mesh_tensor<"m1", {{.*}}>) -> !mpmd.mesh_tensor<"m1", {{.*}}>
+  // CHECK-NEXT: return %[[CALL]], %[[CALL]] : !mpmd.mesh_tensor<"m1", {{.*}}>, !mpmd.mesh_tensor<"m1", {{.*}}>
+  %0 = mpmd.call @f(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  %1 = mpmd.assign %0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+  %2 = mpmd.assign %0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+  return %1, %2 : !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+}
+
+// CHECK-LABEL: func.func private @f(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", {{.*}}>
+func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>} {
+  // CHECK-NEXT: return %arg0 : !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+  return %arg0 : tensor<3x5xf32>
+}
+
+// -----
+
+// Tests the case in which a transfer prevents an unassign to be pushed
+// forward.
+
+!mesh1_t = !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+!mesh2_t = !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+
+// CHECK-LABEL: func.func public @main(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>) ->
+// CHECK-SAME: (!mpmd.mesh_tensor<"m2", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>)
+func.func public @main(%arg0: !mesh1_t) -> (!mesh2_t, !mesh2_t)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", {{.*}}>) -> !mpmd.mesh_tensor<"m2", {{.*}}>
+  // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @f(%[[TRANSFER]]) : (!mpmd.mesh_tensor<"m2", {{.*}}>) -> !mpmd.mesh_tensor<"m2", {{.*}}>
+  // CHECK-NEXT: return %[[CALL]], %[[TRANSFER]]
+  %0 = mpmd.unassign %arg0 : (!mesh1_t) -> tensor<3x5xf32>
+  %1 = mpmd.transfer %arg0 : (!mesh1_t) -> !mesh2_t
+  %2 = mpmd.call @f(%0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  %3 = mpmd.assign %2 : (tensor<3x5xf32>) -> !mesh2_t
+  return %3, %1 : !mesh2_t, !mesh2_t
+}
+
+// CHECK-LABEL: func.func private @f(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m2", {{.*}}>
+func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32> attributes
+  {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+  return %arg0 : tensor<3x5xf32>
+}
+
+// -----
+
+!mesh2_t = !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+
+// Not all targets are used by an assign_op. Even so, mesh inference will
+// succeed and we propagate "sideways", from one target (`%2`) to the other
+// (`%0`).
+// CHECK-LABEL: func.func public @main(
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>,
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>) ->
+// CHECK-SAME: (!mpmd.mesh_tensor<"m2", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>)
+func.func public @main(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  ->
+  (tensor<3x5xf32>, !mesh2_t)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[C0:.*]] = mpmd.call @f(%arg0) : (!mpmd.mesh_tensor<"m2", {{.*}}) -> !mpmd.mesh_tensor<"m2", {{.*}}>
+  %0 = mpmd.call @f(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  // CHECK-NEXT: %[[C1:.*]] = mpmd.call @f(%arg1) : (!mpmd.mesh_tensor<"m2", {{.*}}>) -> !mpmd.mesh_tensor<"m2", {{.*}}>
+  %2 = mpmd.call @f(%arg1) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  %3 = mpmd.assign %2 : (tensor<3x5xf32>) -> !mesh2_t
+  // CHECK-NEXT: return %[[C0]], %[[C1]]
+  return %0, %3 : tensor<3x5xf32>, !mesh2_t
+}
+
+func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32> attributes
+  {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+  return %arg0 : tensor<3x5xf32>
+}
+
+// -----
+
+!mesh2_t = !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+
+// @f has an unused result and an unused op.
+// CHECK-LABEL: func.func public @unused_in_callee(%arg0: !mpmd.mesh_tensor<"m2"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME: -> ({{.*}}"m2"{{.*}}, {{.*}}"m2"
+func.func public @unused_in_callee(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>, !mesh2_t)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+{
+  // CHECK-NEXT: %[[C0:.*]]:2 = mpmd.call @f(%arg0, %arg1, %arg1)
+  %0:2 = mpmd.call @f(%arg0, %arg1, %arg1) : (tensor<3x5xf32>,
+    tensor<3x5xf32>, tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  %3 = mpmd.assign %0#0 : (tensor<3x5xf32>) -> !mesh2_t
+  // CHECK-NEXT: return %[[C0]]#0, %[[C0]]#0
+  return %0#0, %3 : tensor<3x5xf32>, !mesh2_t
+}
+
+// CHECK-LABEL: func.func private @f(
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m2"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME: -> ({{.*}}"m2"{{.*}}, {{.*}}"m2"
+func.func private @f(%used: tensor<3x5xf32>, %unused_arg: tensor<3x5xf32>,
+    %unused_res_arg: tensor<3x5xf32>
+  ) -> (tensor<3x5xf32>, tensor<3x5xf32>) attributes
+  {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+  // CHECK-NEXT: %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m2"
+  // CHECK-NEXT:    stablehlo.add
+  // CHECK-NEXT: mpmd.return
+
+  // CHECK:      %[[ADD_FRAG2:.*]] = mpmd.fragment<mesh="m1"
+  // CHECK-NEXT:    stablehlo.add
+  // CHECK-NEXT: mpmd.return
+  // CHECK: return %[[ADD_FRAG1]], %[[ADD_FRAG2]]
+  %unused = stablehlo.add %unused_arg, %unused_arg : tensor<3x5xf32>
+  %used_res = stablehlo.add %used, %used : tensor<3x5xf32>
+  %unused_res = stablehlo.add %unused_res_arg, %unused_res_arg : tensor<3x5xf32>
+  return %used_res, %unused_res : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+// -----
+
+module {
+  // Not all sources of an op are produced by an unassign but this is handled
+  // by inference.
+  // CHECK-LABEL: func.func public @multiple_calls_on_same_func(
+  // CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME: -> ({{.*}}"m1"{{.*}}, {{.*}}"m1"
+  func.func public @multiple_calls_on_same_func(%arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+                         %arg1: tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+    attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+  {
+    // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @f(%arg0)
+    // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @f(%arg1)
+    %0 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<3x5xf32>>) -> tensor<3x5xf32>
+    %1 = mpmd.call @f(%0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    %3 = mpmd.call @f(%arg1) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    return %1, %3 : tensor<3x5xf32>, tensor<3x5xf32>
+  }
+
+  // CHECK-LABEL: func.func private @f(
+  // CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME: -> {{.*}}"m1"{{.*}}
+  func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["y"=1]>>>} {
+    return %arg0 : tensor<3x5xf32>
+  }
+}
+
+// -----
+
+module {
+  // CHECK-LABEL: func.func public @calls_assigned_to_different_meshes(
+  // CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME: -> ({{.*}}"m1"{{.*}}, {{.*}}"m2"
+  func.func public @calls_assigned_to_different_meshes(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>)
+    ->
+    (!mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>)
+    attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+  {
+    // CHECK-NEXT: %[[TRANSFER0:.*]] = mpmd.transfer %arg0
+    // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @f(%arg0, %[[TRANSFER0]])
+    // CHECK-NEXT: %[[TRANSFER1:.*]] = mpmd.transfer %arg1
+    // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @f(%arg1, %[[TRANSFER1]])
+    %0 = mpmd.call @f(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    %1 = mpmd.assign %0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+    %2 = mpmd.call @f(%arg1) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    %3 = mpmd.assign %2 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+    return %1, %3 : !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+  }
+
+  // `f` has args cloned to be assigned to multiple meshes.
+  // CHECK-LABEL: func.func private @f(
+  // CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2"
+  // CHECK-SAME: -> ({{.*}}"m1"{{.*}}, {{.*}}"m2"
+  func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+    return %arg0 : tensor<3x5xf32>
+  }
+}
+
+// -----
+
+module {
+
+  // CHECK-LABEL: func.func public @call_assigned_to_different_meshes(
+  // CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME: -> ({{.*}}"m1"{{.*}}, {{.*}}"m2"
+  func.func public @call_assigned_to_different_meshes(%arg0: tensor<3x5xf32>)
+    ->
+    (!mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>)
+    attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+  {
+    // CHECK-NEXT: %[[TRANSFER0:.*]] = mpmd.transfer %arg0
+    // CHECK-NEXT: %[[CALL:.*]] = mpmd.call @f(%arg0, %[[TRANSFER0]])
+    %0 = mpmd.call @f(%arg0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    %1 = mpmd.assign %0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+    %2 = mpmd.assign %0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+    return %1, %2 : !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+  }
+
+  // `f` has args cloned to be assigned to multiple meshes.
+  // CHECK-LABEL: func.func private @f(
+  // CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2"
+  // CHECK-SAME: -> ({{.*}}"m1"{{.*}}, {{.*}}"m2"
+  func.func private @f(%arg0: tensor<3x5xf32>) -> tensor<3x5xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+    return %arg0 : tensor<3x5xf32>
+  }
+}
+
+// -----
+
+#topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>, <"m3" : <["x"=1]>>>
+module {
+
+  // CHECK-LABEL: func.func public @chained_calls_works_with_multiple_iterations(
+  // CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1"
+  func.func public @chained_calls_works_with_multiple_iterations(
+      %arg0: tensor<3x5xf32>,
+      %arg1: tensor<3x5xf32>,
+      %arg2: tensor<3x5xf32>
+    ) -> (
+    !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+    !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>,
+    !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+  )
+    attributes {topology = #topology}
+  {
+    %1:3 = mpmd.call @f(%arg0, %arg1, %arg2) : (
+      tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+    ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+
+    %3:3 = mpmd.call @f(%1#0, %1#1, %1#2) : (
+      tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+    ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+    %4 = mpmd.assign %3#0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+    %5 = mpmd.assign %3#1 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+    %6 = mpmd.assign %3#2 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+    return %4, %5, %6 : !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+  }
+
+
+  // args (and results) get cloned to each mesh.
+  // CHECK-LABEL: func.func private @f(
+  // CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:    %arg3: !mpmd.mesh_tensor<"m2"
+  // CHECK-SAME:    %arg4: !mpmd.mesh_tensor<"m3"
+  // CHECK-SAME:    %arg5: !mpmd.mesh_tensor<"m2"
+  // CHECK-SAME:    %arg6: !mpmd.mesh_tensor<"m3"
+  // CHECK-SAME:    %arg7: !mpmd.mesh_tensor<"m2"
+  // CHECK-SAME:    %arg8: !mpmd.mesh_tensor<"m3"
+  func.func private @f(
+    %arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>, %arg2: tensor<3x5xf32>
+  ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>) attributes
+  {topology = #topology} {
+    return %arg1, %arg2, %arg0 : tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+  }
+}
+
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_call_op_failures.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_call_op_failures.mlir
new file mode 100644
index 0000000..f24eb80
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_call_op_failures.mlir
@@ -0,0 +1,122 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-pipeline -split-input-file -verify-diagnostics
+
+module {
+
+  // The sources of an edge are produced by different unassigns from different
+  // meshes.
+  func.func public @main(%arg0: !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+                         %arg1: !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+    attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>}
+  {
+    %0 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<3x5xf32>>) -> tensor<3x5xf32>
+    %1 = mpmd.call @f(%0) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    %2 = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<3x5xf32>>) -> tensor<3x5xf32>
+    %3 = mpmd.call @f(%2) : (tensor<3x5xf32>) -> tensor<3x5xf32>
+    return %1, %3 : tensor<3x5xf32>, tensor<3x5xf32>
+  }
+
+  // expected-error @+1 {{Mesh assignment is not possible for arg0 of mpmd.call "f" }}
+  func.func private @f(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>) -> tensor<3x5xf32> attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>>} {
+    return %arg0 : tensor<3x5xf32>
+  }
+}
+
+// -----
+
+#topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>, <"m3" : <["x"=1]>>>
+module {
+
+  // The sources of an edge are produced by different unassigns from different
+  // meshes.
+  func.func public @chained_calls_failure(
+      %arg0: tensor<3x5xf32>,
+      %arg1: tensor<3x5xf32>,
+      %arg2: tensor<3x5xf32>
+    ) -> (
+    !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+    !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>,
+    !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+  )
+    attributes {topology = #topology}
+  {
+    // arg use: m3, m1, m2 - because of broadcast
+    // res use: m1+m3, m1+m2, m2+m3
+    %1:3 = mpmd.call @f(%arg0, %arg1, %arg2) : (
+      tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+    ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+
+    // arg use: m3, m1, m2 - because of broadcast
+    // res use: m1, m2, m3
+    %3:3 = mpmd.call @f(%1#0, %1#1, %1#2) : (
+      tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+    ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+    %4 = mpmd.assign %3#0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+    %5 = mpmd.assign %3#1 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+    %6 = mpmd.assign %3#2 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+    return %4, %5, %6 : !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+  }
+
+  // expected-error @+1 {{Mesh assignment is not possible for mpmd.call "f"}}
+  func.func private @f(
+    %arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>, %arg2: tensor<3x5xf32>
+  ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>) attributes
+  {topology = #topology} {
+    %10 = mpmd.broadcast %arg0 : tensor<3x5xf32>
+    %11 = mpmd.broadcast %arg1 : tensor<3x5xf32>
+    %12 = mpmd.broadcast %arg2 : tensor<3x5xf32>
+    return %11, %12, %10 : tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+  }
+}
+
+// -----
+
+#topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>, <"m3" : <["x"=1]>>>
+module {
+
+  // The sources of an edge are produced by different unassigns from different
+  // meshes.
+  func.func public @chained_calls_no_failure_because_different_callees(
+      %arg0: tensor<3x5xf32>,
+      %arg1: tensor<3x5xf32>,
+      %arg2: tensor<3x5xf32>
+    ) -> (
+    !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>,
+    !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>,
+    !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+  )
+    attributes {topology = #topology}
+  {
+    %1:3 = mpmd.call @f(%arg0, %arg1, %arg2) : (
+      tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+    ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+
+    %3:3 = mpmd.call @f_copy(%1#0, %1#1, %1#2) : (
+      tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+    ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>)
+    %4 = mpmd.assign %3#0 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+    %5 = mpmd.assign %3#1 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+    %6 = mpmd.assign %3#2 : (tensor<3x5xf32>) -> !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+    return %4, %5, %6 : !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>, !mpmd.mesh_tensor<"m3", tensor<3x5xf32>>
+  }
+
+  func.func private @f(
+    %arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>, %arg2: tensor<3x5xf32>
+  ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>) attributes
+  {topology = #topology} {
+    %10 = mpmd.broadcast %arg0 : tensor<3x5xf32>
+    %11 = mpmd.broadcast %arg1 : tensor<3x5xf32>
+    %12 = mpmd.broadcast %arg2 : tensor<3x5xf32>
+    return %11, %12, %10 : tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+  }
+
+  func.func private @f_copy(
+    %arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>, %arg2: tensor<3x5xf32>
+  ) -> (tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>) attributes
+  {topology = #topology} {
+    %10 = mpmd.broadcast %arg0 : tensor<3x5xf32>
+    %11 = mpmd.broadcast %arg1 : tensor<3x5xf32>
+    %12 = mpmd.broadcast %arg2 : tensor<3x5xf32>
+    return %11, %12, %10 : tensor<3x5xf32>, tensor<3x5xf32>, tensor<3x5xf32>
+  }
+}
+
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_create_reductions.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_create_reductions.mlir
new file mode 100644
index 0000000..3412522
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_create_reductions.mlir
@@ -0,0 +1,53 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-pipeline='infer-cross-mesh-reductions=true' 2>&1 | FileCheck %s
+
+
+
+!m1_4x8 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!m2_4x8 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!m3_4x8 = !mpmd.mesh_tensor<"m3", tensor<4x8xf32>>
+
+!m2_4x1x8 = !mpmd.mesh_tensor<"m2", tensor<4x1x8xf32>>
+!m3_4x1x8 = !mpmd.mesh_tensor<"m3", tensor<4x1x8xf32>>
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>>
+
+// CHECK-LABEL: func @assign_outputs_with_empty_src_set(
+func.func @assign_outputs_with_empty_src_set(%arg0: !m3_4x8, %arg1: !m2_4x8)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+// CHECK:       %[[T0:.*]] = mpmd.transfer %arg0 {{.*}}m3{{.*}} -> {{.*}}m2
+// CHECK-NEXT:  %[[ADD:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[T0]], %arg1)
+// CHECK:       return %[[ADD]]
+  %4 = mpmd.unassign %arg0 : (!m3_4x8) -> tensor<4x8xf32>
+  %5 = mpmd.unassign %arg1 : (!m2_4x8) -> tensor<4x8xf32>
+
+  %6 = stablehlo.add %4, %5 : tensor<4x8xf32>
+
+  // The return value will have empty src_set. If
+  // `infer-cross-mesh-reductions = false`, we would have an error
+  // here. But instead, we assign it to the first mesh and introduce transfers.
+  // Note that this isn't optimal, but inferring the right mesh is hard. Until
+  // we improve the inference algorithm, users will need to adjust their
+  // assignments manually if they want optimal results.
+  func.return %6 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @concat_reduce(%arg0: !mpmd.mesh_tensor<"m2"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m3"
+func.func @concat_reduce(%arg0: !m2_4x1x8, %arg1: !m3_4x1x8)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %init = stablehlo.constant dense<1.0> : tensor<f32>
+  %1 = mpmd.unassign %arg0 : (!m2_4x1x8) -> tensor<4x1x8xf32>
+  %2 = mpmd.unassign %arg1 : (!m3_4x1x8) -> tensor<4x1x8xf32>
+  // CHECK-NEXT:  %[[RESHAPE_M2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg0)
+  // CHECK-NEXT:     stablehlo.reshape
+  // CHECK:       %[[RESHAPE_M3:.*]] = mpmd.fragment<mesh="m3", origin=[]> (%arg1)
+  // CHECK-NEXT:     stablehlo.reshape
+  // CHECK:       %[[T0:.*]] = mpmd.transfer %[[RESHAPE_M3]] {{.*}}m3{{.*}} -> {{.*}}m2
+  // CHECK-NEXT:  %[[ADD:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[RESHAPE_M2]], %[[T0]])
+  // CHECK:       return %[[ADD]]
+  %concat = "stablehlo.concatenate"(%1, %2) <{dimension = 1 : i64}> :
+    (tensor<4x1x8xf32>, tensor<4x1x8xf32>) -> tensor<4x2x8xf32>
+  %reduce = stablehlo.reduce(%concat init: %init) applies stablehlo.maximum across dimensions = [1] :
+    (tensor<4x2x8xf32>, tensor<f32>) -> tensor<4x8xf32>
+  func.return %reduce : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_create_transfers.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_create_transfers.mlir
new file mode 100644
index 0000000..33953a3
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_create_transfers.mlir
@@ -0,0 +1,62 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-pipeline='infer-transfers=true' 2>&1 | FileCheck %s
+
+!m1_8x16 = !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+!m2_8x16 = !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>
+!m3_8x16 = !mpmd.mesh_tensor<"m3", tensor<8x16xf32>>
+
+// CHECK-LABEL: func @assign_outputs_with_empty_src_set(
+func.func @assign_outputs_with_empty_src_set(%arg0: !m3_8x16, %arg1: !m2_8x16)
+  -> (tensor<8x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>,
+      <"m3": <["z"=2]>>
+    >} {
+// CHECK:       %[[T0:.*]] = mpmd.transfer %arg0 {{.*}}m3{{.*}} -> {{.*}}m2
+// CHECK-NEXT:  %[[ADD:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[T0]], %arg1)
+// CHECK:       return %[[ADD]]
+  %4 = mpmd.unassign %arg0 : (!m3_8x16) -> tensor<8x16xf32>
+  %5 = mpmd.unassign %arg1 : (!m2_8x16) -> tensor<8x16xf32>
+
+  %6 = stablehlo.add %4, %5 : tensor<8x16xf32>
+
+  // The return value will have empty src_set. If
+  // `infer-transfers = false`, we would have an error here. But instead, we
+  // assign it to the first mesh and introduce transfers. Note that this isn't
+  // optimal, but inferring the right mesh is hard. Until we improve the
+  // inference algorithm, users will need to adjust their assignments manually
+  // if they want optimal results.
+  func.return %6 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: func @create_transfer_on_intermediates(
+// CHECK-SAME:    %arg0: !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>,
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>)
+func.func @create_transfer_on_intermediates(%arg0: tensor<8x16xf32>, %arg1: tensor<8x16xf32>)
+  -> (tensor<8x16xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+// Note in this test we don't add the inputs directly, as transfers are always
+// generated on inputs. What we want to check is that that transfers are created
+// on intermediate values.
+// CHECK-NEXT:  %[[ADD_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+// CHECK:       %[[ADD_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg1)
+// CHECK:       %[[TRANSFER:.*]] = mpmd.transfer %[[ADD_2]] {{.*}}m2{{.*}} -> {{.*}}m1
+// CHECK-NEXT:  %[[ADD_3:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ADD_1]], %[[TRANSFER]])
+// CHECK:       return %[[ADD_3]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<8x16xf32>
+  %1 = stablehlo.add %arg1, %arg1 : tensor<8x16xf32>
+
+
+  %2 = mpmd.assign %0 : (tensor<8x16xf32>) -> !m1_8x16
+  %3 = mpmd.assign %1 : (tensor<8x16xf32>) -> !m2_8x16
+
+  %4 = mpmd.unassign %2 : (!m1_8x16) -> tensor<8x16xf32>
+  %5 = mpmd.unassign %3 : (!m2_8x16) -> tensor<8x16xf32>
+
+  %6 = stablehlo.add %4, %5 : tensor<8x16xf32>
+
+  func.return %6 : tensor<8x16xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_failures.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_failures.mlir
new file mode 100644
index 0000000..8c890b9
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_pipeline_failures.mlir
@@ -0,0 +1,73 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-pipeline -verify-diagnostics -split-input-file
+
+!m1_8x16 = !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+!m2_8x16 = !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>
+#topology =#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["y"=2]>>>
+
+func.func @operands_with_conflicting_meshes_non_reduce(%arg0: !m1_8x16, %arg1: !m2_8x16)
+  -> (tensor<8x16xf32>) attributes {topology=#topology} {
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg3: tensor<8x16xf32>) {
+    mpmd.return %arg3 : tensor<8x16xf32>
+  } : (!m1_8x16) -> !m1_8x16
+
+  %3 = mpmd.fragment<mesh="m2", origin=[]> (%arg1) (%arg3: tensor<8x16xf32>) {
+    mpmd.return %arg3 : tensor<8x16xf32>
+  } : (!m2_8x16) -> !m2_8x16
+
+  %4 = mpmd.unassign %2 : (!m1_8x16) -> tensor<8x16xf32>
+  %5 = mpmd.unassign %3 : (!m2_8x16) -> tensor<8x16xf32>
+
+  // expected-error @+1 {{Mesh assignment is not possible for op as its operands are on conflicting meshes}}
+  %6 = stablehlo.divide %4, %5 : tensor<8x16xf32>
+
+  func.return %6 : tensor<8x16xf32>
+}
+
+// -----
+
+!m1_8x16 = !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+!m2_8x16 = !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>
+#topology =#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["y"=2]>>>
+
+func.func @operands_with_conflicting_meshes_reduce(%arg0: !m1_8x16, %arg1: !m2_8x16)
+  -> (tensor<8x16xf32>) attributes {topology=#topology} {
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg3: tensor<8x16xf32>) {
+    mpmd.return %arg3 : tensor<8x16xf32>
+  } : (!m1_8x16) -> !m1_8x16
+
+  %3 = mpmd.fragment<mesh="m2", origin=[]> (%arg1) (%arg3: tensor<8x16xf32>) {
+    mpmd.return %arg3 : tensor<8x16xf32>
+  } : (!m2_8x16) -> !m2_8x16
+
+  %4 = mpmd.unassign %2 : (!m1_8x16) -> tensor<8x16xf32>
+  %5 = mpmd.unassign %3 : (!m2_8x16) -> tensor<8x16xf32>
+
+  // expected-error @+1 {{Mesh assignment is not possible for op as its operands are on conflicting meshes}}
+  %6 = stablehlo.add %4, %5 : tensor<8x16xf32>
+
+  func.return %6 : tensor<8x16xf32>
+}
+
+// -----
+
+!m1_3x5 = !mpmd.mesh_tensor<"m1", tensor<3x5xf32>>
+!m2_3x5 = !mpmd.mesh_tensor<"m2", tensor<3x5xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+func.func public @src_set_empty_flowing_into_broadcast(%arg0: tensor<3x5xf32>) -> (tensor<3x5xf32>)
+  attributes {topology = #topology}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  %1 = mpmd.assign %0 : (tensor<3x5xf32>) -> !m1_3x5
+  %2 = mpmd.assign %0 : (tensor<3x5xf32>) -> !m2_3x5
+
+  %11 = mpmd.unassign %1 : (!m1_3x5) -> tensor<3x5xf32>
+  %22 = mpmd.unassign %2 : (!m2_3x5) -> tensor<3x5xf32>
+
+  // %3 will have src_set empty
+  // expected-error @+1 {{Mesh assignment is not possible for op as its operands are on conflicting meshes}}
+  %3 = stablehlo.divide %11, %22 : tensor<3x5xf32>
+  %b = mpmd.broadcast %3 : tensor<3x5xf32>
+  %bb = stablehlo.add %b, %b : tensor<3x5xf32>
+  return %b : tensor<3x5xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_src_set.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_src_set.mlir
new file mode 100644
index 0000000..666891c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_src_set.mlir
@@ -0,0 +1,424 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-populate-src-set 2>&1 | FileCheck --implicit-check-not src_set %s
+
+!mesh_1_tensor_ui32 = !mpmd.mesh_tensor<"m1", tensor<ui32>>
+!mesh_1_tensor_1_ui32 = !mpmd.mesh_tensor<"m1", tensor<1xui32>>
+!mesh_1_tensor_2_ui32 = !mpmd.mesh_tensor<"m1", tensor<2xui32>>
+!mesh_1_tensor_5_5_ui32 = !mpmd.mesh_tensor<"m1", tensor<5x5xui32>>
+!mesh_1_tensor_4_4_f32 = !mpmd.mesh_tensor<"m1", tensor<4x4xf32>>
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_1_tensor_8_16_f32 = !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+!mesh_1_tensor_4_16_f32 = !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+!mesh_1_tensor_16_8_f32 = !mpmd.mesh_tensor<"m1", tensor<16x8xf32>>
+
+!mesh_2_tensor_ui32 = !mpmd.mesh_tensor<"m2", tensor<ui32>>
+!mesh_2_tensor_1_ui32 = !mpmd.mesh_tensor<"m2", tensor<1xui32>>
+!mesh_2_tensor_2_ui32 = !mpmd.mesh_tensor<"m2", tensor<2xui32>>
+!mesh_2_tensor_5_5_ui32 = !mpmd.mesh_tensor<"m2", tensor<5x5xui32>>
+!mesh_2_tensor_4_4_f32 = !mpmd.mesh_tensor<"m2", tensor<4x4xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!mesh_2_tensor_4_16_f32 = !mpmd.mesh_tensor<"m2", tensor<4x16xf32>>
+!mesh_2_tensor_8_16_f32 = !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>
+!mesh_2_tensor_16_8_f32 = !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>
+
+// The majority of these tests verify only the src_set and assume that the DAG
+// unchanged aside from additional attributes. There is one test
+// `dag_unchanged_aside_from_src_set_attribute` that verifies that the DAG
+// remains unchanged.
+//
+// The `--implicit-check-not src_set` on FileCheck means that the text "src_set"
+// is only allowed when explicitly specified in a CHECK.
+
+// CHECK-LABEL: func @single_source(%arg0: !mpmd.mesh_tensor<"m1"
+func.func @single_source(%arg0: !mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = stablehlo.add %1, %0 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  func.return %2 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @multi_source_same_mesh_same_origin(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+func.func @multi_source_same_mesh_same_origin(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.unassign {origin = "1"} %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1"["1"]>
+  %2 = mpmd.unassign {origin = "1"} %arg1 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1"["1"]>
+  %3 = stablehlo.add %1, %2 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1"["1"]>
+  %4 = stablehlo.add %0, %3 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1"["1"]>
+  func.return %4 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @multi_source_same_mesh_different_origin(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1"
+func.func @multi_source_same_mesh_different_origin(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> (tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.unassign {origin = "1"} %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1"["1"]>
+  %2 = mpmd.unassign {origin = "2"} %arg1 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1"["2"]>
+  %3 = stablehlo.add %1, %2 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1"["1", "2"]>
+  %4 = stablehlo.add %0, %3 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1"["1", "2"]>
+  func.return %4 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @multi_source_multi_mesh_no_common(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2"
+func.func @multi_source_multi_mesh_no_common(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> (tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  // The src_sets for these ops are empty since `intersect({m1}, {m2}) = {}`,
+  // but they are added so they are detected as a reduce and get
+  // `src_set = union(operand_src_sets)`
+  %3 = stablehlo.add %1, %2 : tensor<4x8xf32> // CHECK-NEXT: src_set = {{.*}}"m1", "m2">
+  %4 = stablehlo.add %0, %3 : tensor<4x8xf32> // CHECK-NEXT: src_set = {{.*}}"m1", "m2">
+  func.return %4 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @multi_source_multi_mesh_common(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2"
+func.func @multi_source_multi_mesh_common(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> (tensor<4x8xf32>, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // Note the constant has no src_set, so it defaults to all meshes and we
+  // can verify that the intersection works when the src_set is absent.
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1", "m2"["transfer"]>
+  %2 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  %3 = stablehlo.add %1, %2 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2"["transfer"]>
+  %4 = stablehlo.add %0, %3 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2"["transfer"]>
+  %5 = mpmd.transfer %arg0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  func.return %4, %5 : tensor<4x8xf32>, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @region_op_region_is_skipped(
+func.func @region_op_region_is_skipped(%arg0: !mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant {mpmd.src_set = #mpmd.meshes_with_origins<"m1">} dense<1> : tensor<i32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %5:2 = stablehlo.while(%iterArg_0 = %0, %iterArg_1 = %1) : tensor<i32>, tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+   cond {
+    %6 = "stablehlo.compare"(%iterArg_0, %0) {comparison_direction = #stablehlo<comparison_direction LT>} : (tensor<i32>, tensor<i32>) -> tensor<i1>
+    "stablehlo.return"(%6) : (tensor<i1>) -> ()
+  } do {
+    %8 = stablehlo.add %iterArg_1, %iterArg_1 : tensor<4x8xf32>
+    "stablehlo.return"(%iterArg_0, %8) : (tensor<i32>, tensor<4x8xf32>) -> ()
+  }
+
+  func.return %5#1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @func_arg_initialization_and_propagation(
+// CHECK-SAME:    %arg0: tensor<4x8xf32> {mpmd.src_set = {{.*}}"m2"["inferred_in"], "m1"["inferred_in"]>, mpmd.use_set = {{.*}}"m2"["stage0", "stage1"], "m1">}
+// CHECK-SAME:    %arg1: tensor<4x8xf32> {mpmd.src_set = {{.*}}"m1"["inferred_in"]>, mpmd.use_set = {{.*}}"m1">})
+func.func @func_arg_initialization_and_propagation(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2"["stage0", "stage1"], "m1">},
+  %arg1: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">}) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %1 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2"["inferred_in"], "m1"["inferred_in"]>
+  %2 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2"["inferred_in"], "m1"["inferred_in"]>
+  %3 = stablehlo.add %1, %2  : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2"["inferred_in"], "m1"["inferred_in"]>
+  func.return %3 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unassign_of_func_arg(
+func.func @unassign_of_func_arg(%arg0: !mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1", "m2"["inferred_in"], "m3"["inferred_in"]>
+  %2 = stablehlo.add %1, %1 {mpmd.use_set = #mpmd.meshes_with_origins<"m3">} : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1", "m2"["inferred_in"], "m3"["inferred_in"]>
+  %3 = stablehlo.add %1, %1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1", "m2"["inferred_in"], "m3"["inferred_in"]>
+
+  func.return %2 : tensor<4x8xf32>
+}
+
+
+// CHECK-LABEL: func @no_naked_ops
+func.func @no_naked_ops(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>,  tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>   // CHECK-NEXT: src_set = {{.*}}"m2">
+  func.return %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unassign_assign
+func.func @unassign_assign(%arg0: !mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %1 = mpmd.unassign %arg0 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  %2 = mpmd.assign %1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32 // CHECK-NEXT: src_set = {{.*}}"m2">
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @assign_transfer_unassign
+func.func @assign_transfer_unassign(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %1 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %2 = mpmd.transfer %1 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %3 = mpmd.unassign %2 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  func.return %3 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @function_without_unassign_op_is_noop
+func.func @function_without_unassign_op_is_noop(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @ops_in_fragments_need_no_analysis
+func.func @ops_in_fragments_need_no_analysis(%arg0: !mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %2 = mpmd.assign %1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %f = mpmd.fragment<mesh="m2", origin=["foo"]> (%2) (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %0 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  func.return %f : !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @unassign_outside_call_op
+func.func @unassign_outside_call_op(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %1 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  %2:2 = mpmd.call @unassign_outside_call_op_f(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3 = stablehlo.add %2#0, %2#0 : tensor<4x8xf32>  // CHECK: stablehlo.add {{.*}}src_set = {{.*}}"m1">
+  %4 = stablehlo.add %2#1, %2#1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  func.return %3, %4 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+// CHECK:      func.func private @unassign_outside_call_op_f
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.src_set = {{.*}}"m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.src_set = {{.*}}"m2">}
+func.func private @unassign_outside_call_op_f(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+
+// CHECK-LABEL: func @unassign_outside_call_op_multiple_calls
+func.func @unassign_outside_call_op_multiple_calls(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %1 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  %2:2 = mpmd.call @unassign_outside_call_op_multiple_calls_f(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3 = mpmd.assign %2#0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32   // CHECK: assign {{.*}}src_set = {{.*}}"m1">
+  %31 = mpmd.unassign %3 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m2">
+
+  %4:2 = mpmd.call @unassign_outside_call_op_multiple_calls_f(%31, %2#1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %5 = stablehlo.add %4#0, %4#0 : tensor<4x8xf32>  // CHECK: stablehlo.add {{.*}}src_set = {{.*}}<>
+  %6 = stablehlo.add %4#1, %4#1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  func.return %5, %6 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+// CHECK:      func.func private @unassign_outside_call_op_multiple_calls_f
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.src_set = {{[^,]*}}<>}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.src_set = {{.*}}"m2">}
+func.func private @unassign_outside_call_op_multiple_calls_f(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}<>
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+
+// CHECK-LABEL: func @unassign_op_is_in_call_body
+func.func @unassign_op_is_in_call_body(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %2:2 = mpmd.call @unassign_op_is_in_call_body_f(%arg0, %arg1) : (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3 = stablehlo.add %2#0, %2#0 : tensor<4x8xf32>  // CHECK: stablehlo.add {{.*}}src_set = {{.*}}"m1">
+  %4 = stablehlo.add %2#1, %2#1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  func.return %3, %4 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+// CHECK:      func.func private @unassign_op_is_in_call_body_f
+func.func private @unassign_op_is_in_call_body_f(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %1 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  %2 = stablehlo.add %0, %0 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %3 = stablehlo.add %1, %1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  func.return %2, %3 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unassign_op_is_in_call_body_multiple_calls
+func.func @unassign_op_is_in_call_body_multiple_calls(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %2:2 = mpmd.call @unassign_op_is_in_call_body_multiple_calls_f(%arg0, %arg1) : (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3 = stablehlo.add %2#0, %2#0 : tensor<4x8xf32>  // CHECK: stablehlo.add {{.*}}src_set = {{.*}}"m1">
+  %4 = stablehlo.add %2#1, %2#1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  %31 = mpmd.assign %3 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %41 = mpmd.assign %4 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK-NEXT: src_set = {{.*}}"m2">
+  %5:2 = mpmd.call @unassign_op_is_in_call_body_multiple_calls_f(%31, %41) : (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %6 = stablehlo.add %5#0, %5#0 : tensor<4x8xf32>  // CHECK: stablehlo.add {{.*}}src_set = {{.*}}"m1">
+  %7 = stablehlo.add %5#1, %5#1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  func.return %6, %7 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+// CHECK:      func.func private @unassign_op_is_in_call_body_multiple_calls_f
+func.func private @unassign_op_is_in_call_body_multiple_calls_f(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %1 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  %2 = stablehlo.add %0, %0 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %3 = stablehlo.add %1, %1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  func.return %2, %3 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// Regression test, to verify that we don't override the src_set of the callee
+// arg. It should correspond the the callee operand, not to the use_set of the
+// callee.
+// CHECK-LABEL: func @callee_arg_src_set_is_from_operand
+func.func @callee_arg_src_set_is_from_operand(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %arg0_a = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %arg1_a = mpmd.assign %arg1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %0 = mpmd.unassign %arg0_a : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: mpmd.unassign {{.*}}src_set = {{.*}}"m1">
+  %1 = mpmd.unassign %arg1_a : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  // CHECK-NEXT: mpmd.call @callee_arg_src_set_is_from_operand_f
+  %2:2 = mpmd.call @callee_arg_src_set_is_from_operand_f(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %2#0, %2#1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+// CHECK:      func.func private @callee_arg_src_set_is_from_operand_f
+// CHECK-SAME:   arg0: tensor<4x8xf32> {{.*}}mpmd.src_set = {{.*}}"m1">
+// CHECK-SAME:   arg1: tensor<4x8xf32> {{.*}}mpmd.src_set = {{.*}}"m2">
+func.func private @callee_arg_src_set_is_from_operand_f(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m3">},
+  %arg1: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m3">}) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @dag_unchanged_aside_from_src_set_attribute
+func.func @dag_unchanged_aside_from_src_set_attribute(
+  %arg0: tensor<4x8xf32>,
+  %arg1: tensor<8x16xf32>,
+  %arg2: tensor<16x8xf32>,
+  %arg3: tensor<4x16xf32>,
+  %arg4: tensor<16x8xf32>)
+  -> (tensor<4x16xf32>, tensor<4x8xf32>, tensor<16x8xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+  // CHECK-NEXT: %0 = stablehlo.add %arg2, %arg4
+  %0 = stablehlo.add %arg2, %arg4 : tensor<16x8xf32>
+
+  // CHECK-NEXT: %1 = mpmd.assign %arg0
+  %1 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  // CHECK-NEXT: %2 = mpmd.assign %arg1
+  %2 = mpmd.assign %arg1 : (tensor<8x16xf32>) -> !mesh_1_tensor_8_16_f32
+
+  // CHECK-NEXT: %3 = mpmd.fragment<mesh="m1", origin=["f1"]> (%1, %2)
+  // CHECK-SAME:  (%arg5: tensor<4x8xf32>, %arg6: tensor<8x16xf32>) {
+  %3 = mpmd.fragment<mesh="m1", origin=["f1"]> (%1, %2)
+    (%arg5: tensor<4x8xf32>, %arg6: tensor<8x16xf32>) {
+
+    // CHECK-NEXT: %12 = stablehlo.dot %arg5, %arg6 : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    %12 = "stablehlo.dot"(%arg5, %arg6) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    // CHECK-NEXT: mpmd.return %12 : tensor<4x16xf32>
+    mpmd.return %12 : tensor<4x16xf32>
+    // CHECK-NEXT: }
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_8_16_f32) -> (!mesh_1_tensor_4_16_f32)
+
+  // CHECK-NEXT: %4 = mpmd.transfer %3
+  %4 = mpmd.transfer %3 : (!mesh_1_tensor_4_16_f32) -> !mesh_2_tensor_4_16_f32
+
+  // CHECK-NEXT: %5 = stablehlo.add %arg2, %arg2
+  %5 = stablehlo.add %arg2, %arg2 : tensor<16x8xf32>
+
+  // CHECK-NEXT: %6 = mpmd.assign %5
+  %6 = mpmd.assign %5 : (tensor<16x8xf32>) -> !mesh_2_tensor_16_8_f32
+
+  // CHECK-NEXT: %7 = mpmd.fragment<mesh="m2", origin=["f2"]> (%4, %6)
+  // CHECK-SAME:   (%arg5: tensor<4x16xf32>, %arg6: tensor<16x8xf32>) {
+  %7 = mpmd.fragment<mesh="m2", origin=["f2"]> (%4, %6)
+    (%arg5: tensor<4x16xf32>, %arg6: tensor<16x8xf32>) {
+    // CHECK-NEXT: %12 = stablehlo.dot %arg5, %arg6 : (tensor<4x16xf32>, tensor<16x8xf32>) -> tensor<4x8xf32>
+    %12 = "stablehlo.dot"(%arg5, %arg6) : (tensor<4x16xf32>, tensor<16x8xf32>) -> tensor<4x8xf32>
+    // CHECK-NEXT: mpmd.return %12 : tensor<4x8xf32>
+    mpmd.return %12 : tensor<4x8xf32>
+  // CHECK-NEXT: }
+  } : (!mesh_2_tensor_4_16_f32, !mesh_2_tensor_16_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  // CHECK-NEXT: %8 = mpmd.unassign {mpmd.src_set = {{.*}}"m1", "m2"["transfer"]>} %3
+  %8 = mpmd.unassign %3 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  // CHECK-NEXT: %9 = stablehlo.add %8, %arg3 {mpmd.src_set = {{.*}}"m1", "m2"["transfer"]>} : tensor<4x16xf32>
+  %9 = stablehlo.add %8, %arg3 : tensor<4x16xf32>
+  // CHECK-NEXT: %10 = stablehlo.add %9, %9 {mpmd.src_set = {{.*}}"m1", "m2"["transfer"]>} : tensor<4x16xf32>
+  %10 = stablehlo.add %9, %9 : tensor<4x16xf32>
+
+  // CHECK-NEXT: %11 = mpmd.unassign {mpmd.src_set = {{.*}}"m2">} %7
+  %11 = mpmd.unassign %7 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  // CHECK-NEXT: return %10, %11, %0 : tensor<4x16xf32>, tensor<4x8xf32>, tensor<16x8xf32>
+  func.return %10, %11, %0 : tensor<4x16xf32>, tensor<4x8xf32>, tensor<16x8xf32>
+}
+
+// CHECK-LABEL: func @func_with_no_topology_no_propagation_because_skipped
+func.func @func_with_no_topology_no_propagation_because_skipped(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {
+  // CHECK-NEXT: %0 = stablehlo.add {{.*}}src_set = {{.*}}"mesh">
+  // CHECK-NEXT: %1 = stablehlo.add
+  // CHECK-NEXT: return %1
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"mesh">} : tensor<4x8xf32>
+  %1 = stablehlo.add %0, %0 : tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @broadcast_src_set
+func.func @broadcast_src_set(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>  attributes {
+  "topology"=#mpmd.topology<
+    <"mesh": <["x"=2]>>
+  >}
+{
+  // The src_set of a broadcast includes all meshes, i.e., it's undefined.
+  // CHECK-NEXT: mpmd.broadcast {mpmd.use_set = {{.*}}"mesh">}
+  %0 = mpmd.broadcast {mpmd.use_set = #mpmd.meshes_with_origins<"mesh">} %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @fori_loop(%arg0: !mpmd.mesh_tensor<"m1"
+func.func @fori_loop(%arg0: !mesh_1_tensor_ui32) -> tensor<ui32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  %0 = mpmd.unassign %arg0 : (!mesh_1_tensor_ui32) -> tensor<ui32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %1 = mpmd.for (%0) {iterations = 3 : ui32, unroll_factor = 3 : ui32} (%arg1: tensor<ui32>, %index: tensor<ui32>) {  // CHECK-NEXT: for {{.*}}arg_attrs = [{mpmd.src_set = {{.*}}"m1">}, {}]
+    %2 = stablehlo.constant dense<1> : tensor<ui32>
+    %3 = stablehlo.add %arg1, %2 : tensor<ui32>  // CHECK: src_set = {{.*}}"m1">
+    %4 = stablehlo.add %3, %index : tensor<ui32>  // CHECK-NEXT: src_set = {{.*}}"m1">
+    mpmd.return %4 : tensor<ui32>
+  } : tensor<ui32>
+  %2 = stablehlo.add %1, %1 : tensor<ui32>  // CHECK: src_set = {{.*}}"m1">
+  return %2 : tensor<ui32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_src_set_and_infer_reductions.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_src_set_and_infer_reductions.mlir
new file mode 100644
index 0000000..ac67b9e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_src_set_and_infer_reductions.mlir
@@ -0,0 +1,113 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-populate-src-set 2>&1 | FileCheck --implicit-check-not src_set --implicit-check-not mpmd.reduce %s
+
+!m1_4x8 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!m2_4x8 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!m3_4x8 = !mpmd.mesh_tensor<"m3", tensor<4x8xf32>>
+
+!m1_4x1x8 = !mpmd.mesh_tensor<"m1", tensor<4x1x8xf32>>
+!m2_4x1x8 = !mpmd.mesh_tensor<"m2", tensor<4x1x8xf32>>
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>>
+
+// The main tests are checked in `mpmd_infer_mesh_populate_src_set.mlir`. This
+// only verifies the infer_reduce logic.
+//
+// The `--implicit-check-not src_set` on FileCheck means that the text "src_set"
+// and "mpmd.reduce" is only allowed when explicitly specified in a CHECK.
+
+
+// CHECK-LABEL: func @simple_reduce
+func.func @simple_reduce(%arg0: !m1_4x8, %arg1: !m2_4x8)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.unassign %arg0 : (!m1_4x8) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!m2_4x8) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  // The src_sets would normally be empty, but we parse the reduce.
+  %3 = stablehlo.maximum %1, %2 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = {{.*}}"m1", "m2">}
+  %4 = stablehlo.maximum %0, %3 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = {{.*}}"m1", "m2">}
+  func.return %4 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @simple_reduce_chain
+func.func @simple_reduce_chain(%arg0: !m1_4x8, %arg1: !m2_4x8, %arg2: !m3_4x8)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %1 = mpmd.unassign %arg0 : (!m1_4x8) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!m2_4x8) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  %3 = mpmd.unassign %arg2 : (!m3_4x8) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m3">
+  %4 = stablehlo.add %1, %2 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = {{.*}}"m1", "m2">}
+  %5 = stablehlo.add %4, %3 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = {{.*}}"m1", "m2", "m3">}
+  func.return %5 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @reduce_of_reduces
+func.func @reduce_of_reduces(%arg0: !m1_4x8, %arg1: !m2_4x8)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %1 = mpmd.unassign %arg0 : (!m1_4x8) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!m2_4x8) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  %41 = stablehlo.add %1, %2 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = {{.*}}"m1", "m2">}
+  %42 = stablehlo.add %1, %2 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = {{.*}}"m1", "m2">}
+  %5 = stablehlo.add %41, %42 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = {{.*}}"m1", "m2">}
+  func.return %5 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @different_reduce_types
+func.func @different_reduce_types(%arg0: !m1_4x8, %arg1: !m2_4x8)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %1 = mpmd.unassign %arg0 : (!m1_4x8) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!m2_4x8) -> tensor<4x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  %41 = stablehlo.add %1, %2 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = {{.*}}"m1", "m2">}
+  %42 = stablehlo.add %1, %2 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<add>, mpmd.src_set = {{.*}}"m1", "m2">}
+  %5 = stablehlo.maximum %41, %42 : tensor<4x8xf32> // CHECK-NEXT: {mpmd.src_set = {{.*}}"m1", "m2">}
+  func.return %5 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @concat_reduce(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m2"
+func.func @concat_reduce(%arg0: !m1_4x1x8, %arg1: !m2_4x1x8)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %init = stablehlo.constant dense<1.0> : tensor<f32>
+  %1 = mpmd.unassign %arg0 : (!m1_4x1x8) -> tensor<4x1x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!m2_4x1x8) -> tensor<4x1x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  // The src_sets would normally be empty, but we parse the concat-reduce pair.
+  // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = {{.*}}"m1", "m2">}
+  %concat = "stablehlo.concatenate"(%1, %2, %1, %2) <{dimension = 1 : i64}> :
+    (tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>) -> tensor<4x4x8xf32>
+  // CHECK-NEXT: {mpmd.reduce = #mpmd.reduction<max>, mpmd.src_set = {{.*}}"m1", "m2">}
+  %reduce = stablehlo.reduce(%concat init: %init) applies stablehlo.maximum across dimensions = [1] :
+    (tensor<4x4x8xf32>, tensor<f32>) -> tensor<4x8xf32>
+  func.return %reduce : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @cannot_parse_concat_reduce_unless_all_collapsed_dim_size_1
+func.func @cannot_parse_concat_reduce_unless_all_collapsed_dim_size_1(%arg0: !m1_4x1x8, %arg1: !m2_4x1x8)
+  -> (tensor<4x8xf32>) attributes {topology=#topology} {
+  %init = stablehlo.constant dense<1.0> : tensor<f32>
+  %1 = mpmd.unassign %arg0 : (!m1_4x1x8) -> tensor<4x1x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!m2_4x1x8) -> tensor<4x1x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  // CHECK-NEXT: src_set = {{.*}}>
+  %concat0 = "stablehlo.concatenate"(%1, %2) <{dimension = 1 : i64}> :
+    (tensor<4x1x8xf32>, tensor<4x1x8xf32>) -> tensor<4x2x8xf32>
+  // CHECK-NEXT: src_set = {{.*}}>
+  %concat = "stablehlo.concatenate"(%concat0, %1, %2) <{dimension = 1 : i64}> :
+    (tensor<4x2x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>) -> tensor<4x4x8xf32>
+  // CHECK-NEXT: src_set = {{.*}}>
+  %reduce = stablehlo.reduce(%concat init: %init) applies stablehlo.maximum across dimensions = [1] :
+    (tensor<4x4x8xf32>, tensor<f32>) -> tensor<4x8xf32>
+  func.return %reduce : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @cannot_parse_concat_reduce_if_concat_multiple_users
+func.func @cannot_parse_concat_reduce_if_concat_multiple_users(%arg0: !m1_4x1x8, %arg1: !m2_4x1x8)
+  -> (tensor<4x8xf32>, tensor<4x4x8xf32>) attributes {topology=#topology} {
+  %init = stablehlo.constant dense<1.0> : tensor<f32>
+  %1 = mpmd.unassign %arg0 : (!m1_4x1x8) -> tensor<4x1x8xf32>  // CHECK: unassign {{.*}}src_set = {{.*}}"m1">
+  %2 = mpmd.unassign %arg1 : (!m2_4x1x8) -> tensor<4x1x8xf32>  // CHECK-NEXT: src_set = {{.*}}"m2">
+  // The src_sets would normally be empty, but we parse the concat-reduce pair.
+  // CHECK-NEXT: src_set = {{.*}}>
+  %concat = "stablehlo.concatenate"(%1, %2, %1, %2) <{dimension = 1 : i64}> :
+    (tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>, tensor<4x1x8xf32>) -> tensor<4x4x8xf32>
+  // CHECK-NEXT: src_set = {{.*}}>
+  %reduce = stablehlo.reduce(%concat init: %init) applies stablehlo.maximum across dimensions = [1] :
+    (tensor<4x4x8xf32>, tensor<f32>) -> tensor<4x8xf32>
+  func.return %reduce, %concat : tensor<4x8xf32>, tensor<4x4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_use_set.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_use_set.mlir
new file mode 100644
index 0000000..03c1308
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_populate_use_set.mlir
@@ -0,0 +1,492 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-populate-use-set 2>&1 | FileCheck --implicit-check-not use_set %s
+
+!mesh_1_tensor_ui32 = !mpmd.mesh_tensor<"m1", tensor<ui32>>
+!mesh_1_tensor_1_ui32 = !mpmd.mesh_tensor<"m1", tensor<1xui32>>
+!mesh_1_tensor_2_ui32 = !mpmd.mesh_tensor<"m1", tensor<2xui32>>
+!mesh_1_tensor_5_5_ui32 = !mpmd.mesh_tensor<"m1", tensor<5x5xui32>>
+!mesh_1_tensor_4_4_f32 = !mpmd.mesh_tensor<"m1", tensor<4x4xf32>>
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_1_tensor_8_16_f32 = !mpmd.mesh_tensor<"m1", tensor<8x16xf32>>
+!mesh_1_tensor_4_16_f32 = !mpmd.mesh_tensor<"m1", tensor<4x16xf32>>
+!mesh_1_tensor_16_8_f32 = !mpmd.mesh_tensor<"m1", tensor<16x8xf32>>
+
+!mesh_2_tensor_ui32 = !mpmd.mesh_tensor<"m2", tensor<ui32>>
+!mesh_2_tensor_1_ui32 = !mpmd.mesh_tensor<"m2", tensor<1xui32>>
+!mesh_2_tensor_2_ui32 = !mpmd.mesh_tensor<"m2", tensor<2xui32>>
+!mesh_2_tensor_5_5_ui32 = !mpmd.mesh_tensor<"m2", tensor<5x5xui32>>
+!mesh_2_tensor_4_4_f32 = !mpmd.mesh_tensor<"m2", tensor<4x4xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!mesh_2_tensor_4_16_f32 = !mpmd.mesh_tensor<"m2", tensor<4x16xf32>>
+!mesh_2_tensor_8_16_f32 = !mpmd.mesh_tensor<"m2", tensor<8x16xf32>>
+!mesh_2_tensor_16_8_f32 = !mpmd.mesh_tensor<"m2", tensor<16x8xf32>>
+
+!mesh_3_tensor_4_8_f32 = !mpmd.mesh_tensor<"m3", tensor<4x8xf32>>
+
+#topology = #mpmd.topology<<"m1" : <["x"=1]>>, <"m2" : <["x"=1]>>, <"m3" : <["x"=1]>>>
+
+// The majority of these tests verify only the use_set and assume that the DAG
+// unchanged aside from additional attributes. There is one test
+// `dag_unchanged_aside_from_use_set_attribute` that verifies that the DAG
+// remains unchanged.
+//
+// The `--implicit-check-not use_set` on FileCheck means that the text "use_set"
+// is only allowed when explicitly specified in a CHECK.
+
+// CHECK-LABEL: func @single_use(%arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1"["abc"]>})
+func.func @single_use(%arg0: tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1"["abc"]>
+  %1 = stablehlo.add %arg0, %0 : tensor<4x8xf32> // CHECK-NEXT: use_set = {{.*}}"m1"["abc"]>
+  // note: ignores origin
+  %2 = mpmd.assign {origin = "abc"} %1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32   // CHECK-NEXT: use_set = {{.*}}"m1"["abc"]>
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @multi_use_same_mesh_same_origin(%arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1"["1"]>})
+func.func @multi_use_same_mesh_same_origin(%arg0: tensor<4x8xf32>)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32> // CHECK-NEXT: use_set = {{.*}}"m1"["1"]>
+  %1 = stablehlo.add %arg0, %0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1"["1"]>
+  %2 = mpmd.assign {origin = "1"} %1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m1"["1"]>
+
+  %3 = stablehlo.add %arg0, %1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1"["1"]>
+  %4 = mpmd.assign {origin = "1"} %3 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m1"["1"]>
+  func.return %2, %4 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @multi_use_same_mesh_different_origin(%arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1"["2", "1"]>})
+func.func @multi_use_same_mesh_different_origin(%arg0: tensor<4x8xf32>)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %1 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1"["2", "1"]>
+  %2 = mpmd.assign {origin = "1"} %1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m1"["1"]>
+
+  %3 = stablehlo.add %arg0, %1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1"["2"]>
+  %4 = mpmd.assign {origin = "2"} %3 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m1"["2"]>
+  func.return %2, %4 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+
+// Regression test to check that we keep the arg's use_set and add to it, rather
+// than replacing it. This is relevant when propagating into MPMD callees:
+// the callee arg may have use_set populated from the caller
+// (e.g. arg -> return -> caller use), and we need to keep that use_set and
+// add to it, rather than erasing it.
+// CHECK-LABEL: func @arg_use_set_not_overridden(%arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2", "m1">})
+func.func @arg_use_set_not_overridden(%arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">})
+  -> (!mesh_1_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %2 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m1">
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @multi_use_multi_mesh(%arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2", "m1">})
+func.func @multi_use_multi_mesh(%arg0: tensor<4x8xf32>)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2", "m1">
+  %1 = stablehlo.add %arg0, %0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2", "m1">
+  %2 = mpmd.assign %1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32 // CHECK-NEXT: use_set = {{.*}}"m1">
+
+  %3 = stablehlo.add %arg0, %1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2">
+  %4 = mpmd.assign %3 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m2">
+  func.return %2, %4 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @region_op_region_is_skipped(%arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">})
+func.func @region_op_region_is_skipped(%arg0: tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1> : tensor<i32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %5:2 = stablehlo.while(%iterArg_0 = %0, %iterArg_1 = %arg0) : tensor<i32>, tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+   cond {
+    %6 = "stablehlo.compare"(%iterArg_0, %0) {comparison_direction = #stablehlo<comparison_direction LT>} : (tensor<i32>, tensor<i32>) -> tensor<i1>
+    "stablehlo.return"(%6) : (tensor<i1>) -> ()
+  } do {
+    %8 = stablehlo.add %iterArg_1, %iterArg_1 : tensor<4x8xf32>
+    "stablehlo.return"(%iterArg_0, %8) : (tensor<i32>, tensor<4x8xf32>) -> ()
+  }
+  %6 = mpmd.assign %5#1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+
+  func.return %6 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @op_and_value_used_as_free_variable_in_region_op(
+// CHECK-SAME:    %arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">},
+// CHECK-SAME:    %arg1: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">})
+func.func @op_and_value_used_as_free_variable_in_region_op(
+  %arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %0 = stablehlo.constant dense<1> : tensor<i32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %1 = stablehlo.constant dense<2.0> : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %5:2 = stablehlo.while(%iterArg_0 = %0, %iterArg_1 = %arg0) : tensor<i32>, tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+   cond {
+    %6 = "stablehlo.compare"(%iterArg_0, %0) {comparison_direction = #stablehlo<comparison_direction LT>} : (tensor<i32>, tensor<i32>) -> tensor<i1>
+    "stablehlo.return"(%6) : (tensor<i1>) -> ()
+  } do {
+    %8 = stablehlo.add %1, %arg1 : tensor<4x8xf32>
+    "stablehlo.return"(%iterArg_0, %8) : (tensor<i32>, tensor<4x8xf32>) -> ()
+  }
+  %6 = mpmd.assign %5#1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+
+  func.return %6 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @no_naked_ops(%arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2", "m1">})
+func.func @no_naked_ops(%arg0: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %1 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %2 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m2">
+  func.return %1, %2 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @unassign_assign
+func.func @unassign_assign(%arg0: !mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // Since unassign is an MPMD op, it will not get a use_set.
+  // The only MPMD op that gets use_sets are assign_ops.
+  %1 = mpmd.unassign %arg0 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}use_set = {{.*}}"m1">
+  %2 = mpmd.assign %1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @assign_transfer_unassign(%arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">})
+func.func @assign_transfer_unassign(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %1 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK-NEXT: use_set = {{.*}}"m2">
+  %2 = mpmd.transfer %1 : (!mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %3 = mpmd.unassign %2 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  func.return %3 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @function_without_assign_op_is_noop
+func.func @function_without_assign_op_is_noop(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @ops_in_fragments_need_no_analysis
+func.func @ops_in_fragments_need_no_analysis(%arg0: !mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %f = mpmd.fragment<mesh="m2", origin=["foo"]> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %0 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  %3 = mpmd.unassign %f : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  %4 = mpmd.assign %3 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  func.return %4 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @assign_outside_call_op
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">}
+func.func @assign_outside_call_op(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+
+  %2:2 = mpmd.call @assign_outside_call_op_f(%0, %arg1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3 = mpmd.assign %2#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  %4 = mpmd.assign %2#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m2">
+  func.return %3, %4 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+// CHECK:      func.func private @assign_outside_call_op_f
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{[^,-]*}}"m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{[^,-]*}}"m2">}
+// CHECK-SAME: -> (tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">}, tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">})
+func.func private @assign_outside_call_op_f(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+
+  // Verify that use_set propagates through call_op to args which
+  // are returned directly.
+  func.return %0, %arg1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+
+// CHECK-LABEL: func @assign_outside_call_op_multiple_calls
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1", "m2">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">}
+func.func @assign_outside_call_op_multiple_calls(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // Note that the first operand of the call_op %0 and %32 have different
+  // use_sets. Thus the use_sets aren't exactly correct since it should both be
+  // {m1,m2}. We will throw a validation error in another pass to handle this,
+  // as the edges out of a call_op can't be assigned to multiple meshes and so
+  // they should have a single-element use_set.
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1", "m2">
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2">
+
+  %2:2 = mpmd.call @assign_outside_call_op_multiple_calls_f(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3 = mpmd.assign %2#0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m2">
+  %31 = mpmd.unassign %3 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}use_set = {{.*}}"m1">
+  %32 = stablehlo.add %31, %31 : tensor<4x8xf32>  // CHECK: add {{.*}}use_set = {{.*}}"m1">
+
+  %4:2 = mpmd.call @assign_outside_call_op_multiple_calls_f(%32, %2#1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %6 = mpmd.assign %4#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  %7 = mpmd.assign %4#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m2">
+  func.return %6, %7 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+// CHECK: func.func private @assign_outside_call_op_multiple_calls_f
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{[^,-]*}}"m1", "m2">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{[^,-]*}}"m2">}
+// CHECK-SAME: -> (tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1", "m2">}, tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">})
+func.func private @assign_outside_call_op_multiple_calls_f(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1", "m2">
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2">
+
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @assign_op_is_in_call_body
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">}
+func.func @assign_op_is_in_call_body(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2">
+
+  %2:2 = mpmd.call @assign_op_is_in_call_body_f(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  func.return %2#0, %2#1 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+// CHECK: func.func private @assign_op_is_in_call_body_f
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">}
+func.func private @assign_op_is_in_call_body_f(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2">
+
+  %3 = mpmd.assign %0: (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  %4 = mpmd.assign %1: (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m2">
+  func.return %3, %4 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @assign_op_is_in_call_body_multiple_calls
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">}
+func.func @assign_op_is_in_call_body_multiple_calls(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2">
+
+  %2:2 = mpmd.call @assign_op_is_in_call_body_multiple_calls_f(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  %3 = mpmd.unassign %2#0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}use_set = {{.*}}"m1">
+  %4 = mpmd.unassign %2#1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>  // CHECK: unassign {{.*}}use_set = {{.*}}"m2">
+  %5 = stablehlo.add %3, %3 : tensor<4x8xf32>  // CHECK: add {{.*}}use_set = {{.*}}"m1">
+  %6 = stablehlo.add %4, %4 : tensor<4x8xf32>  // CHECK: add {{.*}}use_set = {{.*}}"m2">
+
+  %7:2 = mpmd.call @assign_op_is_in_call_body_multiple_calls_f(%5, %6) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+
+  func.return %7#0, %7#1 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+// CHECK: func.func private @assign_op_is_in_call_body_multiple_calls_f
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2">}
+func.func private @assign_op_is_in_call_body_multiple_calls_f(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>  // CHECK-NEXT: use_set = {{.*}}"m2">
+
+  %3 = mpmd.assign %0: (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  %4 = mpmd.assign %1: (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32  // CHECK: assign {{.*}}use_set = {{.*}}"m2">
+  func.return %3, %4 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @call_op_needs_multiple_iterations_to_converge
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m3", "m2", "m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1", "m3", "m2">}
+// CHECK-SAME:   arg2: tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2", "m1", "m3">}
+func.func @call_op_needs_multiple_iterations_to_converge(
+      %arg0: tensor<4x8xf32>,
+      %arg1: tensor<4x8xf32>,
+      %arg2: tensor<4x8xf32>) -> (
+  !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_3_tensor_4_8_f32)
+  attributes {topology=#topology}
+{
+  %1:3 = mpmd.call @call_op_needs_multiple_iterations_to_converge_f(%arg0, %arg1, %arg2) : (
+    tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  ) -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+
+  %3:3 = mpmd.call @call_op_needs_multiple_iterations_to_converge_f(%1#0, %1#1, %1#2) : (
+    tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  ) -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  %4 = mpmd.assign %3#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32 // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  %5 = mpmd.assign %3#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32 // CHECK: assign {{.*}}use_set = {{.*}}"m2">
+  %6 = mpmd.assign %3#2 : (tensor<4x8xf32>) -> !mesh_3_tensor_4_8_f32 // CHECK: assign {{.*}}use_set = {{.*}}"m3">
+
+
+  return %4, %5, %6 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_3_tensor_4_8_f32
+}
+
+// Each of the calls adds one additional use_set mesh. So two calls would
+// give two meshes. The existence of three meshes means that we propagate
+// more times than there are calls
+// CHECK-LABEL: func.func private @call_op_needs_multiple_iterations_to_converge_f(
+// CHECK-SAME:   arg0: tensor<4x8xf32> {mpmd.use_set = {{[^,-]*}}"m3", "m2", "m1">}
+// CHECK-SAME:   arg1: tensor<4x8xf32> {mpmd.use_set = {{[^,-]*}}"m1", "m3", "m2">}
+// CHECK-SAME:   arg2: tensor<4x8xf32> {mpmd.use_set = {{[^,-]*}}"m2", "m1", "m3">}
+// CHECK-SAME: -> (tensor<4x8xf32> {mpmd.use_set = {{.*}}"m1", "m3", "m2">},
+// CHECK-SAME:     tensor<4x8xf32> {mpmd.use_set = {{.*}}"m2", "m1", "m3">},
+// CHECK-SAME:     tensor<4x8xf32> {mpmd.use_set = {{.*}}"m3", "m2", "m1">})
+func.func private @call_op_needs_multiple_iterations_to_converge_f(
+  %arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>
+) -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) attributes
+{topology = #topology} {
+  return %arg1, %arg2, %arg0 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+
+
+// CHECK-LABEL: func @dag_unchanged_aside_from_use_set_attribute
+// CHECK-SAME:     %arg0: tensor<4x8xf32> {mpmd.use_set = {{[^,-]*}}"m1">}
+// CHECK-SAME:     %arg1: tensor<8x16xf32> {mpmd.use_set = {{[^,-]*}}"m1">}
+// CHECK-SAME:     %arg2: tensor<16x8xf32> {mpmd.use_set = {{[^,-]*}}"m2">}
+func.func @dag_unchanged_aside_from_use_set_attribute(
+  %arg0: tensor<4x8xf32>,
+  %arg1: tensor<8x16xf32>,
+  %arg2: tensor<16x8xf32>,
+  %arg3: tensor<4x16xf32>,
+  %arg4: tensor<16x8xf32>)
+  -> (tensor<4x16xf32>, tensor<4x8xf32>, tensor<16x8xf32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["y"=2]>>
+    >} {
+  // CHECK-NEXT: %0 = stablehlo.add %arg2, %arg4
+  %0 = stablehlo.add %arg2, %arg4 : tensor<16x8xf32>
+
+  // CHECK-NEXT: %1 = mpmd.assign {mpmd.use_set = {{.*}}"m1">} %arg0
+  %1 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  // CHECK-NEXT: %2 = mpmd.assign {mpmd.use_set = {{.*}}"m1">} %arg1
+  %2 = mpmd.assign %arg1 : (tensor<8x16xf32>) -> !mesh_1_tensor_8_16_f32
+
+  // CHECK-NEXT: %3 = mpmd.fragment<mesh="m1", origin=["f1"]> (%1, %2)
+  // CHECK-SAME:  (%arg5: tensor<4x8xf32>, %arg6: tensor<8x16xf32>) {
+  %3 = mpmd.fragment<mesh="m1", origin=["f1"]> (%1, %2)
+    (%arg5: tensor<4x8xf32>, %arg6: tensor<8x16xf32>) {
+
+    // CHECK-NEXT: %12 = stablehlo.dot %arg5, %arg6 : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    %12 = "stablehlo.dot"(%arg5, %arg6) : (tensor<4x8xf32>, tensor<8x16xf32>) -> tensor<4x16xf32>
+    // CHECK-NEXT: mpmd.return %12 : tensor<4x16xf32>
+    mpmd.return %12 : tensor<4x16xf32>
+    // CHECK-NEXT: }
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_8_16_f32) -> (!mesh_1_tensor_4_16_f32)
+
+  // CHECK-NEXT: %4 = mpmd.transfer %3
+  %4 = mpmd.transfer %3 : (!mesh_1_tensor_4_16_f32) -> !mesh_2_tensor_4_16_f32
+
+  // CHECK-NEXT: %5 = stablehlo.add %arg2, %arg2 {mpmd.use_set = {{.*}}"m2">}
+  %5 = stablehlo.add %arg2, %arg2 : tensor<16x8xf32>
+
+  // CHECK-NEXT: %6 = mpmd.assign {mpmd.use_set = {{.*}}"m2">} %5
+  %6 = mpmd.assign %5 : (tensor<16x8xf32>) -> !mesh_2_tensor_16_8_f32
+
+  // CHECK-NEXT: %7 = mpmd.fragment<mesh="m2", origin=["f2"]> (%4, %6)
+  // CHECK-SAME:   (%arg5: tensor<4x16xf32>, %arg6: tensor<16x8xf32>) {
+  %7 = mpmd.fragment<mesh="m2", origin=["f2"]> (%4, %6)
+    (%arg5: tensor<4x16xf32>, %arg6: tensor<16x8xf32>) {
+    // CHECK-NEXT: %12 = stablehlo.dot %arg5, %arg6 : (tensor<4x16xf32>, tensor<16x8xf32>) -> tensor<4x8xf32>
+    %12 = "stablehlo.dot"(%arg5, %arg6) : (tensor<4x16xf32>, tensor<16x8xf32>) -> tensor<4x8xf32>
+    // CHECK-NEXT: mpmd.return %12 : tensor<4x8xf32>
+    mpmd.return %12 : tensor<4x8xf32>
+  // CHECK-NEXT: }
+  } : (!mesh_2_tensor_4_16_f32, !mesh_2_tensor_16_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  // CHECK-NEXT: %8 = mpmd.unassign %3
+  %8 = mpmd.unassign %3 : (!mesh_1_tensor_4_16_f32) -> tensor<4x16xf32>
+
+  // CHECK-NEXT: %9 = stablehlo.add %8, %arg3 : tensor<4x16xf32>
+  %9 = stablehlo.add %8, %arg3 : tensor<4x16xf32>
+  // CHECK-NEXT: %10 = stablehlo.add %9, %9 : tensor<4x16xf32>
+  %10 = stablehlo.add %9, %9 : tensor<4x16xf32>
+
+  // CHECK-NEXT: %11 = mpmd.unassign %7
+  %11 = mpmd.unassign %7 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  // CHECK-NEXT: return %10, %11, %0 : tensor<4x16xf32>, tensor<4x8xf32>, tensor<16x8xf32>
+  func.return %10, %11, %0 : tensor<4x16xf32>, tensor<4x8xf32>, tensor<16x8xf32>
+}
+
+// CHECK-LABEL: func @func_with_no_topology_no_propagation_because_skipped
+func.func @func_with_no_topology_no_propagation_because_skipped(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {
+  // CHECK-NEXT: %0 = stablehlo.add
+  // CHECK-NEXT: %1 = stablehlo.add  {{.*}}use_set = {{.*}}"mesh">
+  // CHECK-NEXT: return %1
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  %1 = stablehlo.add %0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"mesh">} : tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @broadcast_is_a_barrier_while_populating_use_set
+func.func @broadcast_is_a_barrier_while_populating_use_set(%arg0: tensor<4x8xf32>) -> (
+  !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>}
+{
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  %add = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  // CHECK-NEXT: %[[BCAST:.*]] = mpmd.broadcast {mpmd.use_set = {{.*}}"m2", "m1">} %[[ADD]] : tensor<4x8xf32>
+  %bcast = mpmd.broadcast %add : tensor<4x8xf32>
+  // CHECK-NEXT: mpmd.assign {mpmd.use_set = {{.*}}"m1">}
+  // CHECK-NEXT: mpmd.assign {mpmd.use_set = {{.*}}"m2">}
+  %a1 = mpmd.assign %bcast : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %a2 = mpmd.assign %bcast : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  func.return %a1, %a2 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @reduce_is_a_barrier_while_populating_use_set
+func.func @reduce_is_a_barrier_while_populating_use_set(%arg0: tensor<4x8xf32>) -> (
+  !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>}
+{
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  %add = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  // CHECK-NEXT: %[[BCAST:.*]] = mpmd.reduce<add> {mpmd.use_set = {{.*}}"m2", "m1">} %[[ADD]], %[[ADD]]
+  %reduce = mpmd.reduce<add> %add, %add : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  // CHECK-NEXT: mpmd.assign {mpmd.use_set = {{.*}}"m1">}
+  // CHECK-NEXT: mpmd.assign {mpmd.use_set = {{.*}}"m2">}
+  %a1 = mpmd.assign %reduce : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %a2 = mpmd.assign %reduce : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  func.return %a1, %a2 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @fori_loop(
+// CHECK-SAME: %arg0: tensor<ui32> {mpmd.use_set = {{.*}}"m1">}
+func.func @fori_loop(%arg0: tensor<ui32>) -> !mesh_1_tensor_ui32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  %0 = stablehlo.add %arg0, %arg0 : tensor<ui32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  %1 = mpmd.for (%0) {iterations = 3 : ui32, unroll_factor = 3 : ui32} (%arg1: tensor<ui32>, %index: tensor<ui32>) {  // CHECK-NEXT: for {{.*}}arg_attrs = [{mpmd.use_set = {{.*}}"m1">}, {mpmd.use_set = {{.*}}"m1">}]
+    %3 = stablehlo.constant dense<1> : tensor<ui32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+    %4 = stablehlo.add %arg1, %3 : tensor<ui32> // CHECK-NEXT: use_set = {{.*}}"m1">
+    %5 = mpmd.call @fori_loop_f(%4, %index) : (tensor<ui32>, tensor<ui32>) -> tensor<ui32>
+    mpmd.return %5 : tensor<ui32>
+  } : tensor<ui32>
+  %2 = mpmd.assign %1 : (tensor<ui32>) -> !mesh_1_tensor_ui32  // CHECK: assign {{.*}}use_set = {{.*}}"m1">
+  return %2 : !mesh_1_tensor_ui32
+}
+
+// CHECK-LABEL: func private @fori_loop_f(
+// CHECK-SAME: %arg0: tensor<ui32> {mpmd.use_set = {{[^,-]*}}"m1">}
+// CHECK-SAME: %arg1: tensor<ui32> {mpmd.use_set = {{[^,-]*}}"m1">}
+// CHECK-SAME: -> (tensor<ui32> {mpmd.use_set = {{.*}}"m1">})
+func.func private @fori_loop_f(%arg0: tensor<ui32>, %arg1: tensor<ui32>) -> tensor<ui32>
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>>} {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<ui32>  // CHECK-NEXT: use_set = {{.*}}"m1">
+  return %0 : tensor<ui32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_using_analysis.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_using_analysis.mlir
new file mode 100644
index 0000000..a7707d2
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_using_analysis.mlir
@@ -0,0 +1,813 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-rewrite-using-analysis 2>&1 | FileCheck %s
+
+!mesh_1_tensor_ui32 = !mpmd.mesh_tensor<"m1", tensor<ui32>>
+!mesh_1_tensor_f32 = !mpmd.mesh_tensor<"m1", tensor<f32>>
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_ui32 = !mpmd.mesh_tensor<"m1", tensor<ui32>>
+!mesh_2_tensor_f32 = !mpmd.mesh_tensor<"m2", tensor<f32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+!mesh_3_tensor_4_8_f32 = !mpmd.mesh_tensor<"m3", tensor<4x8xf32>>
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>>
+
+// CHECK-LABEL: func @simple_rewrite(%arg0: tensor<4x8xf32>
+func.func @simple_rewrite(%arg0: tensor<4x8xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"m1">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">}) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // CHECK-NEXT:  %[[ASSIGN:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[ADD_FRAG:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN]]) (%arg1: tensor<4x8xf32>) {
+  // CHECK-NEXT:    %[[CONST:.*]] = stablehlo.constant dense<1.000000e+00>
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg1, %[[CONST]]
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  // CHECK-NEXT:  %[[UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG]]
+  // CHECK-NEXT:  %[[RET_ASSIGN:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m1">} %[[UNASSIGN]]
+  // CHECK-NEXT:  return %[[RET_ASSIGN]] : !mpmd.mesh_tensor<"m1"
+  %0 = stablehlo.constant {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} dense<1.0> : tensor<4x8xf32>
+  %2 = stablehlo.add %arg0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  %3 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %2 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  func.return %3 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @rewrite_with_duplication(%arg0: tensor<4x8xf32>
+func.func @rewrite_with_duplication(%arg0: tensor<4x8xf32> {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">, mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+
+  // CHECK-DAG:  %[[ASSIGN_ARG0_1:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1",
+
+  // CHECK-DAG:   %[[ADD_FRAG_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN_ARG0_1]]) (%arg1: tensor<4x8xf32>) {
+  // CHECK-NEXT:    %[[CONST_1:.*]] = stablehlo.constant dense<1.000000e+00>
+  // CHECK-NEXT:    %[[ADD_1:.*]] = stablehlo.add %arg1, %[[CONST_1]]
+  // CHECK-NEXT:    mpmd.return %[[ADD_1]]
+  // CHECK-NEXT:  }
+  // CHECK-DAG:   %[[UNASSIGN_1:.*]] = mpmd.unassign %[[ADD_FRAG_1]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-DAG:   %[[ASSIGN_ARG0_2:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2",
+  // CHECK-DAG:   %[[ADD_FRAG_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ASSIGN_ARG0_2]]) (%arg1: tensor<4x8xf32>) {
+  // CHECK-NEXT:    %[[CONST_2:.*]] = stablehlo.constant dense<1.000000e+00>
+  // CHECK-NEXT:    %[[ADD_2:.*]] = stablehlo.add %arg1, %[[CONST_2]]
+  // CHECK-NEXT:    mpmd.return %[[ADD_2]]
+  // CHECK-NEXT:  }
+  // CHECK-DAG:   %[[UNASSIGN_2:.*]] = mpmd.unassign %[[ADD_FRAG_2]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-DAG:   %[[ASSIGN_1:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m1">} %[[UNASSIGN_1]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-DAG:   %[[ASSIGN_2:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m2">} %[[UNASSIGN_2]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-DAG:   return %[[ASSIGN_1]], %[[ASSIGN_2]] :
+  // CHECK-SAME:     !mpmd.mesh_tensor<"m1"
+  // CHECK-SAME:     !mpmd.mesh_tensor<"m2"
+  %0 = stablehlo.constant {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} dense<1.0> : tensor<4x8xf32>
+  %2 = stablehlo.add %arg0, %0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %3 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %2 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %5 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %2 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  func.return %3, %5 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @call_op
+func.func @call_op(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // CHECK-NEXT:  %[[ARG0_ASSIGN:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG0_ASSIGN]]) (%arg2
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  // CHECK-NEXT:  %[[F0_UNASSIGN:.*]] = mpmd.unassign %1 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-NEXT:  %[[ARG1_ASSIGN:.*]] = mpmd.assign %arg1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ARG1_ASSIGN]]) (%arg2
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  // CHECK-NEXT:  %[[F1_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG1]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-NEXT:  %[[F0_ASSIGN:.*]] = mpmd.assign %[[F0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[F1_ASSIGN:.*]] = mpmd.assign %[[F1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // We verify that the call op is replaced but the call_counter is kept.
+  // CHECK-NEXT:  %[[CALL:.*]]:2 = mpmd.call @call_op_f(%[[F0_ASSIGN]], %[[F1_ASSIGN]])
+  // CHECK-SAME:         {call_counter = 0 : ui32}
+
+  // CHECK-NEXT: mpmd.unassign %[[CALL]]#0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-NEXT: mpmd.unassign %[[CALL]]#1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+
+  %2:2 = mpmd.call @call_op_f(%0, %1) {call_counter = 0 : ui32}
+    : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %6 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %2#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %7 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %2#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  func.return %6, %7 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK: func.func private @call_op_f
+// CHECK-SAME:   arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:   arg1: !mpmd.mesh_tensor<"m2"
+func.func private @call_op_f(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">},
+  %arg1: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">}
+) -> (
+  tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">},
+  tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // CHECK-DAG: %[[UNASSIGN_ARG0:.*]] = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-DAG: %[[UNASSIGN_ARG1:.*]] = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-DAG: %[[ASSIGN_ARG0:.*]] = mpmd.assign %[[UNASSIGN_ARG0]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-DAG: %[[ASSIGN_ARG1:.*]] = mpmd.assign %[[UNASSIGN_ARG1]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+  // CHECK-DAG:   %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN_ARG0]]) (%arg2
+  // CHECK-DAG:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-DAG:    mpmd.return %[[ADD]]
+  // CHECK-DAG:  }
+
+  // CHECK-DAG:   %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ASSIGN_ARG1]]) (%arg2
+  // CHECK-DAG:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-DAG:    mpmd.return %[[ADD]]
+  // CHECK-DAG:  }
+
+  // CHECK-DAG: %[[F0_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG0]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-DAG: %[[F1_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG1]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-DAG: mpmd.assign %[[F0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-DAG: mpmd.assign %[[F1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+
+// CHECK-LABEL: func @call_op_multiple_calls
+func.func @call_op_multiple_calls(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // CHECK-NEXT:  %[[ARG0_ASSIGN:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG0_ASSIGN]]) (%arg2
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  // CHECK-NEXT:  %[[F0_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG0]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-NEXT:  %[[ARG1_ASSIGN:.*]] = mpmd.assign %arg1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ARG1_ASSIGN]]) (%arg2
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  // CHECK-NEXT:  %[[F1_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG1]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-NEXT:  %[[F0_ASSIGN:.*]] = mpmd.assign %[[F0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[F1_ASSIGN:.*]] = mpmd.assign %[[F1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[CALL0:.*]]:2 = mpmd.call @call_op_multiple_calls_f(%[[F0_ASSIGN]], %[[F1_ASSIGN]])
+  // CHECK-NEXT:  %[[CALL0_UNASSIGN0:.*]] = mpmd.unassign %[[CALL0]]#0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-NEXT:  %[[CALL0_UNASSIGN1:.*]] = mpmd.unassign %[[CALL0]]#1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-NEXT:  %[[CALL0_ASSIGN0:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m1">} %[[CALL0_UNASSIGN0]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[CALL0_UNASSIGN0:.*]] = mpmd.unassign %[[CALL0_ASSIGN0]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-NEXT:  %[[CALL0_ASSIGN0:.*]] = mpmd.assign %[[CALL0_UNASSIGN0]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[CALL0_ASSIGN1:.*]] = mpmd.assign %[[CALL0_UNASSIGN1]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[CALL1:.*]]:2 = mpmd.call @call_op_multiple_calls_f(%[[CALL0_ASSIGN0]], %[[CALL0_ASSIGN1]])
+  // CHECK-NEXT:  %[[CALL1_UNASSIGN0:.*]] = mpmd.unassign %[[CALL1]]#0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-NEXT:  %[[CALL1_UNASSIGN1:.*]] = mpmd.unassign %[[CALL1]]#1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-NEXT:  %[[CALL1_ASSIGN0:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m1">} %[[CALL1_UNASSIGN0]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[CALL1_ASSIGN1:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m2">} %[[CALL1_UNASSIGN1]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT:  return %[[CALL1_ASSIGN0]], %[[CALL1_ASSIGN1]]
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+
+  %2:2 = mpmd.call @call_op_multiple_calls_f(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %2#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %31 = mpmd.unassign %3 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+
+
+  %4:2 = mpmd.call @call_op_multiple_calls_f(%31, %2#1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %6 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %4#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %7 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %4#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  func.return %6, %7 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+// CHECK: func.func private @call_op_multiple_calls_f
+// CHECK-SAME:   arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:   arg1: !mpmd.mesh_tensor<"m2"
+func.func private @call_op_multiple_calls_f(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">},
+  %arg1: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">}
+) -> (
+  tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">},
+  tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">}
+)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // CHECK:       %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> ({{.*}}) (%arg2
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  // CHECK:       %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m2", origin=[]> ({{.*}}) (%arg2
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @call_op_unused_output
+func.func @call_op_unused_output(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // CHECK-NEXT:  %[[ARG0_ASSIGN:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG0_ASSIGN]]) (%arg2
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  // CHECK-NEXT:  %[[F0_UNASSIGN:.*]] = mpmd.unassign %1 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-NEXT:  %[[ARG1_ASSIGN:.*]] = mpmd.assign %arg1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ARG1_ASSIGN]]) (%arg2
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg2
+  // CHECK-NEXT:    mpmd.return %[[ADD]]
+  // CHECK-NEXT:  }
+  // CHECK-NEXT:  %[[F1_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG1]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-NEXT:  %[[F0_ASSIGN:.*]] = mpmd.assign %[[F0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[F1_ASSIGN:.*]] = mpmd.assign %[[F1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT:  %[[CALL:.*]]:2 = mpmd.call @call_op_unused_output_f(%[[F0_ASSIGN]], %[[F1_ASSIGN]])
+
+  // CHECK-NEXT: mpmd.unassign %[[CALL]]#0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+
+  %2:2 = mpmd.call @call_op_unused_output_f(%0, %1) {call_counter = 0 : ui32}
+    : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %6 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %2#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  func.return %6, %6 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK: func.func private @call_op_unused_output_f
+// CHECK-SAME:   arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:   arg1: !mpmd.mesh_tensor<"m2"
+func.func private @call_op_unused_output_f(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">},
+  %arg1: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">}
+) -> (
+  tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">},
+  tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">}
+)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // CHECK-DAG: %[[UNASSIGN_ARG0:.*]] = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-DAG: %[[UNASSIGN_ARG1:.*]] = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-DAG: %[[ASSIGN_ARG0:.*]] = mpmd.assign %[[UNASSIGN_ARG0]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-DAG: %[[ASSIGN_ARG1:.*]] = mpmd.assign %[[UNASSIGN_ARG1]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+  // CHECK-DAG:   %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN_ARG0]]) (%arg2
+  // CHECK-DAG:   %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ASSIGN_ARG1]]) (%arg2
+
+  // CHECK-DAG: %[[F0_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG0]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  // CHECK-DAG: %[[F1_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG1]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-DAG: mpmd.assign %[[F0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-DAG: mpmd.assign %[[F1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @rewrite_with_region_op_with_wrapping
+func.func @rewrite_with_region_op_with_wrapping(%arg0: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // Given that there are no consumer fragments, the resulting two fragments
+  //  result from wrapping the `while` op with fragments.
+
+  // CHECK:        %[[WHILE_FRAG_1:.*]]:2 = mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-DAG:      %[[CONST:.*]] = stablehlo.constant dense<1>
+  // CHECK-DAG:      %[[WHILE:.*]]:2 = stablehlo.while
+  // CHECK-DAG:      mpmd.return %[[WHILE]]#0, %[[WHILE]]#1
+  // CHECK-NEXT:    }
+  // CHECK:        %[[WHILE_FRAG_2:.*]]:2 = mpmd.fragment<mesh="m2", origin=[]>
+  // CHECK-DAG:      %[[CONST:.*]] = stablehlo.constant dense<1>
+  // CHECK-DAG:      %[[WHILE:.*]]:2 = stablehlo.while
+  // CHECK-DAG:      mpmd.return %[[WHILE]]#0, %[[WHILE]]#1
+  // CHECK-NEXT:    }
+  %0 = stablehlo.constant {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} dense<1> : tensor<i32>
+  %5:2 = stablehlo.while(%iterArg_0 = %0, %iterArg_1 = %arg0)  : tensor<i32>, tensor<4x8xf32>
+   attributes {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+   cond {
+    %6 = "stablehlo.compare"(%iterArg_0, %0) {comparison_direction = #stablehlo<comparison_direction LT>} : (tensor<i32>, tensor<i32>) -> tensor<i1>
+    "stablehlo.return"(%6) : (tensor<i1>) -> ()
+  } do {
+    %8 = stablehlo.add %iterArg_1, %iterArg_1 : tensor<4x8xf32>
+    "stablehlo.return"(%iterArg_0, %8) : (tensor<i32>, tensor<4x8xf32>) -> ()
+  }
+  %6 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %5#1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %7 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %5#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  func.return %6, %7 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @rewrite_with_region_op_wrap_because_no_cloning_allowed
+func.func @rewrite_with_region_op_wrap_because_no_cloning_allowed(%arg0: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  // Although the while loop is consumed by fragments, it has multiple users and
+  // therefore won't be absorbed into the consumers, as cloning is disabled.
+  // Instead, it will also be wrapped in two freshly created fragments.
+
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // CHECK:        %[[WHILE_FRAG_1:.*]]:2 = mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-DAG:      %[[CONST:.*]] = stablehlo.constant dense<1>
+  // CHECK-DAG:      %[[WHILE:.*]]:2 = stablehlo.while
+  // CHECK-DAG:      mpmd.return %[[WHILE]]#0, %[[WHILE]]#1
+  // CHECK-NEXT:    }
+  // CHECK:        %[[WHILE_FRAG_2:.*]]:2 = mpmd.fragment<mesh="m2", origin=[]>
+  // CHECK-DAG:      %[[CONST:.*]] = stablehlo.constant dense<1>
+  // CHECK-DAG:      %[[WHILE:.*]]:2 = stablehlo.while
+  // CHECK-DAG:      mpmd.return %[[WHILE]]#0, %[[WHILE]]#1
+  // CHECK-NEXT:    }
+  // CHECK:         mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK:         mpmd.fragment<mesh="m2", origin=[]>
+  %0 = stablehlo.constant {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} dense<1> : tensor<i32>
+  %5:2 = stablehlo.while(%iterArg_0 = %0, %iterArg_1 = %arg0)  : tensor<i32>, tensor<4x8xf32>
+   attributes {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+   cond {
+    %6 = "stablehlo.compare"(%iterArg_0, %0) {comparison_direction = #stablehlo<comparison_direction LT>} : (tensor<i32>, tensor<i32>) -> tensor<i1>
+    "stablehlo.return"(%6) : (tensor<i1>) -> ()
+  } do {
+    %8 = stablehlo.add %iterArg_1, %iterArg_1 : tensor<4x8xf32>
+    "stablehlo.return"(%iterArg_0, %8) : (tensor<i32>, tensor<4x8xf32>) -> ()
+  }
+  %6 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %5#1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %8 = mpmd.fragment<mesh="m1", origin=[]> (%6) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %7 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %5#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %9 = mpmd.fragment<mesh="m2", origin=[]> (%7) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  func.return %8, %9 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @rewrite_with_region_op_via_inline
+func.func @rewrite_with_region_op_via_inline(%arg0: tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  // The while loop has a single fragment consumer and therefore can be inlined.
+
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // CHECK-NEXT: %[[ASSIGN:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[FRAG:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN]])
+  // CHECK-NEXT:    %[[CONST:.*]] = stablehlo.constant dense<1>
+  // CHECK-NEXT:    %[[WHILE:.*]]:2 = stablehlo.while
+  // Only the second result is used.
+  // CHECK:         mpmd.return %[[WHILE]]#1
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[FRAG]]
+
+  %0 = stablehlo.constant {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} dense<1> : tensor<i32>
+  %5:2 = stablehlo.while(%iterArg_0 = %0, %iterArg_1 = %arg0)  : tensor<i32>, tensor<4x8xf32>
+   attributes {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+   cond {
+    %6 = "stablehlo.compare"(%iterArg_0, %0) {comparison_direction = #stablehlo<comparison_direction LT>} : (tensor<i32>, tensor<i32>) -> tensor<i1>
+    "stablehlo.return"(%6) : (tensor<i1>) -> ()
+  } do {
+    %8 = stablehlo.add %iterArg_1, %iterArg_1 : tensor<4x8xf32>
+    "stablehlo.return"(%iterArg_0, %8) : (tensor<i32>, tensor<4x8xf32>) -> ()
+  }
+  %6 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %5#1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %8 = mpmd.fragment<mesh="m1", origin=[]> (%6) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %8 : !mesh_1_tensor_4_8_f32
+}
+
+// Caller expectations:
+// 1. arg0 is used in two meshes, so it will be transferred.
+// 2. The `multiply` op is assigned to two meshes. Therefore, we should see it
+// cloned and in two different fragments. Each clone is used in a different call
+// operand.
+// 3. The `add` op needs no cloning.
+
+// CHECK-LABEL: func @call_op_with_multi_result_assignment
+func.func @call_op_with_multi_result_assignment(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_1_tensor_f32, !mesh_2_tensor_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+
+  // CHECK-NEXT: %[[ARG0_ASSIGN_0:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[MULT_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG0_ASSIGN_0]])
+  // CHECK-NEXT:   stablehlo.multiply
+  // CHECK:      %[[MULT_FRAG0_UNASSIGN:.*]] = mpmd.unassign %[[MULT_FRAG0]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-NEXT: %[[ARG0_ASSIGN_1:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[MULT_FRAG1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ARG0_ASSIGN_1]]) (%arg2: tensor<4x8xf32>) {
+  // CHECK-NEXT:   stablehlo.multiply
+  // CHECK:      %[[MULT_FRAG1_UNASSIGN:.*]] = mpmd.unassign %[[MULT_FRAG1]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-NEXT: %[[ARG1_ASSIGN:.*]] = mpmd.assign %arg1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[ADD_FRAG:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ARG1_ASSIGN]]) (%arg2: tensor<4x8xf32>) {
+  // CHECK-NEXT:   stablehlo.add
+  // CHECK:      %[[ADD_FRAG_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  // CHECK-NEXT: %[[MULT_FRAG0_ASSIGN:.*]] = mpmd.assign %[[MULT_FRAG0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[MULT_FRAG1_ASSIGN:.*]] = mpmd.assign %[[MULT_FRAG1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[ADD_FRAG_ASSIGN:.*]] = mpmd.assign %[[ADD_FRAG_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT: mpmd.call @call_op_g(%[[MULT_FRAG0_ASSIGN]], %[[ADD_FRAG_ASSIGN]], %[[MULT_FRAG1_ASSIGN]])
+
+  %0 = stablehlo.multiply %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+
+  %2:3 = mpmd.call @call_op_g(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<f32>)
+  %a1 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %2#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %a2 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %2#0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  %7 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %2#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  %a3 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %2#2 : (tensor<f32>) -> !mesh_1_tensor_f32
+  %a4 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %2#2 : (tensor<f32>) -> !mesh_2_tensor_f32
+  func.return %a1, %a2, %7, %a3, %a4 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_1_tensor_f32, !mesh_2_tensor_f32
+}
+
+// Callee expectations:
+// 1. The `multiply` op is assigned to two meshes. Therefore, we should see it
+// cloned and in two different fragments. The result of each fragment is
+// returned by the callee function, i.e., the pass will replicate function
+// results to guarantee assignment to a single mesh of the op.
+// 2. The `add` op is assigned to a single mesh. Therefore, we should see it
+// in a single fragment.
+// 3. The `constant` op is assigned to two meshes. Similar behaviour to (1).
+// 4. %arg0 is replicated to guarantee every argument is assigned to a single
+// mesh.
+
+// CHECK: func.func private @call_op_g
+// CHECK-SAME:   arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:   arg1: !mpmd.mesh_tensor<"m2"
+// CHECK-SAME:   arg2: !mpmd.mesh_tensor<"m2"
+func.func private @call_op_g(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">},
+  %arg1: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">}
+) -> (
+  tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">},
+  tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m2">},
+  tensor<f32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // CHECK:       mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-NEXT:    stablehlo.multiply
+  // CHECK:       mpmd.fragment<mesh="m2", origin=[]>
+  // CHECK-NEXT:    stablehlo.multiply
+
+  // CHECK:       mpmd.fragment<mesh="m2", origin=[]>
+  // CHECK-NEXT:    stablehlo.add
+
+  // CHECK:       mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-NEXT:    stablehlo.constant
+
+  // CHECK:       mpmd.fragment<mesh="m2", origin=[]>
+  // CHECK-NEXT:    stablehlo.constant
+
+  // CHECK:       return {{.*}}m1{{.*}}m2{{.*}}m1{{.*}}m2{{.*}}m2
+  %0 = stablehlo.multiply %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} : tensor<4x8xf32>
+  %2 = stablehlo.constant  {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} dense<1.000000e+00> : tensor<f32>
+  func.return %0, %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<f32>
+}
+
+// CHECK-LABEL: func @call_op_noop_return
+func.func @call_op_noop_return(%arg0: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+// CHECK-NEXT: %[[ARG0_ASSIGN_0:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-NEXT: %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG0_ASSIGN_0]]) (%arg1
+// CHECK-NEXT:   stablehlo.add %arg1, %arg1
+// CHECK-NEXT:   mpmd.return
+// CHECK-NEXT: }
+// CHECK-NEXT: %[[F0_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG0]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+// CHECK-NEXT: %[[ARG0_ASSIGN_1:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+// CHECK-NEXT: %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[ARG0_ASSIGN_1]]) (%arg1
+// CHECK-NEXT:   stablehlo.add %arg1, %arg1
+// CHECK-NEXT:   mpmd.return
+// CHECK-NEXT: }
+// CHECK-NEXT: %[[F1_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG1]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+
+// CHECK-NEXT: %[[F0_ASSIGN:.*]] = mpmd.assign %[[F0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-NEXT: %[[F1_ASSIGN:.*]] = mpmd.assign %[[F1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+// CHECK-NEXT: %[[CALL:.*]]:2 = mpmd.call @call_op_noop_f(%[[F0_ASSIGN]], %[[F1_ASSIGN]])
+// CHECK-DAG:  %[[CALL_0_UNASSIGN:.*]] = mpmd.unassign %[[CALL]]#0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG:  %[[CALL_1_UNASSIGN:.*]] = mpmd.unassign %[[CALL]]#1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  %3 = mpmd.call @call_op_noop_f(%0) : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  %6 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %3 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %7 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %3 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+// CHECK-DAG:  %[[CALL_0_ASSIGN:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m1">} %[[CALL_0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-DAG:  %[[CALL_1_ASSIGN:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m2">} %[[CALL_1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+// CHECK:      return %[[CALL_0_ASSIGN]], %[[CALL_1_ASSIGN]]
+  func.return %6, %7 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+// CHECK-LABEL: func private @call_op_noop_f
+// CHECK-SAME:   arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:   arg1: !mpmd.mesh_tensor<"m2"
+func.func private @call_op_noop_f(%arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+) -> (tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+// CHECK-DAG: %[[ARG0_UNASSIGN:.*]] = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG: %[[ARG1_UNASSIGN:.*]] = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG: %[[ARG0_ASSIGN:.*]] = mpmd.assign %[[ARG0_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-DAG: %[[ARG1_ASSIGN:.*]] = mpmd.assign %[[ARG1_UNASSIGN]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+// CHECK-NEXT: return %[[ARG0_ASSIGN]], %[[ARG1_ASSIGN]]
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @call_op_chained_calls
+func.func @call_op_chained_calls(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+// Each of these `add` operators wil be replicated in both meshes. This means
+// we will have four fragments, two per mesh.
+// CHECK: mpmd.fragment<mesh="m1"
+// CHECK: mpmd.fragment<mesh="m2"
+// CHECK: mpmd.fragment<mesh="m1"
+// CHECK: mpmd.fragment<mesh="m2"
+
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+
+// CHECK:      %[[CALL0:.*]]:4 = mpmd.call @call_op_chained_calls_f
+// CHECK-SAME:   {call_counter = 0 : ui32}
+// CHECK-SAME:   (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+
+// CHECK-DAG:  %[[CALL0_UNASSIGN2:.*]] = mpmd.unassign %[[CALL0]]#2 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG:  %[[CALL0_ASSIGN2:.*]] = mpmd.assign %[[CALL0_UNASSIGN2]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-DAG:  %[[CALL0_UNASSIGN0:.*]] = mpmd.unassign %[[CALL0]]#0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG:  %[[CALL0_ASSIGN0:.*]] = mpmd.assign %[[CALL0_UNASSIGN0]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-DAG:  %[[CALL0_UNASSIGN3:.*]] = mpmd.unassign %[[CALL0]]#3 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG:  %[[CALL0_ASSIGN3:.*]] = mpmd.assign %[[CALL0_UNASSIGN3]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-DAG:  %[[CALL0_UNASSIGN1:.*]] = mpmd.unassign %[[CALL0]]#1 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG:  %[[CALL0_ASSIGN1:.*]] = mpmd.assign %[[CALL0_UNASSIGN1]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-NEXT: %[[CALL1:.*]]:4 = mpmd.call @call_op_chained_calls_f(%[[CALL0_ASSIGN0]], %[[CALL0_ASSIGN1]], %[[CALL0_ASSIGN2]], %[[CALL0_ASSIGN3]])
+// CHECK-SAME:     {call_counter = 1 : ui32}
+// CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+// CHECK-DAG:  %[[CALL1_UNASSIGN2:.*]] = mpmd.unassign %[[CALL1]]#2 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG:  %[[CALL1_UNASSIGN0:.*]] = mpmd.unassign %[[CALL1]]#0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG:  %[[CALL1_UNASSIGN3:.*]] = mpmd.unassign %[[CALL1]]#3 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG:  %[[CALL1_UNASSIGN1:.*]] = mpmd.unassign %[[CALL1]]#1 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+
+  %2:2 = mpmd.call @call_op_chained_calls_f(%0, %1) {call_counter = 0 : ui32}
+    : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3:2 = mpmd.call @call_op_chained_calls_f(%2#0, %2#1) {call_counter = 1 : ui32}
+    : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %7 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %3#0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %72 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %3#0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  %8 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %3#1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %82 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m2">} %3#1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+  func.return %7, %72, %8, %82 : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+// CHECK: func.func private @call_op_chained_calls_f
+// CHECK-SAME:   arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:   arg1: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:   arg2: !mpmd.mesh_tensor<"m2"
+// CHECK-SAME:   arg3: !mpmd.mesh_tensor<"m2"
+func.func private @call_op_chained_calls_f(
+  %arg0: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">},
+  %arg1: tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+) -> (
+    tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">},
+    tensor<4x8xf32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+// CHECK: mpmd.fragment<mesh="m1"
+// CHECK: mpmd.fragment<mesh="m2"
+// CHECK: mpmd.fragment<mesh="m1"
+// CHECK: mpmd.fragment<mesh="m2"
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+  %1 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @multiple_use_by_single_fragment_via_single_assign(%arg0: tensor<4x8xf32>
+func.func @multiple_use_by_single_fragment_via_single_assign(%arg0: tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // Although the fragment has two uses of the same value, the resulting
+  //  fragment has a single operand.
+  // CHECK-NEXT: %[[ARG0_ASSIGN:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG0_ASSIGN]])
+  // CHECK-SAME:   (%arg1: tensor<4x8xf32>)
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[FRAGMENT]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  %1 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%1, %1) (%arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg1, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @multiple_use_by_single_fragment_via_many_assign(%arg0: tensor<4x8xf32>
+func.func @multiple_use_by_single_fragment_via_many_assign(%arg0: tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // Although the fragment has two uses of the same value, via different
+  // assigns, the resulting fragment has a single operand.
+  // CHECK-NEXT: %[[ARG0_ASSIGN:.*]] = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG0_ASSIGN]])
+  // CHECK-SAME:   (%arg1: tensor<4x8xf32>)
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[FRAGMENT]]
+  %0 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  %a1 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %a2 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%a1, %a2) (%arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg1, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @multiple_fragment_users_but_op_is_constant(
+func.func @multiple_fragment_users_but_op_is_constant(%arg0: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // Although the constant has multiple fragment consumers, it is still cloned
+  // into each one as it has no operands.
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-NEXT:   constant
+  // CHECK:      mpmd.fragment<mesh="m1", origin=[]>
+  // CHECK-NEXT:   constant
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%1) (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %3 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %4 = mpmd.fragment<mesh="m1", origin=[]> (%3) (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %2, %4 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @single_fragment_consumer_but_used_by_return(%arg0: tensor<4x8xf32>
+func.func @single_fragment_consumer_but_used_by_return(%arg0: tensor<4x8xf32>) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // The constant cannot be inlined into the consumer fragment because it is
+  // used by the return statement.
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=[]> () ()
+  // CHECK-NEXT:   constant
+  // CHECK:      mpmd.fragment<mesh="m1", origin=[]>
+  %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  %1 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%1) (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %3 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  func.return %2, %3 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @many_user_assigns_but_two_meshes
+func.func @many_user_assigns_but_two_meshes(%arg0: tensor<8x16xi32>)
+  -> (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>,
+      !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>,
+      !mpmd.mesh_tensor<"mesh2", tensor<8x16xi32>>,
+      !mpmd.mesh_tensor<"mesh2", tensor<8x16xi32>>)
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>, <"mesh2" : <["x"=4]>>>}
+{
+// CHECK:      mpmd.fragment<mesh="mesh1", origin=[]>
+// CHECK-NEXT:   stablehlo.multiply %arg1, %arg1
+// CHECK:      mpmd.fragment<mesh="mesh2", origin=[]>
+// CHECK-NEXT:   stablehlo.multiply %arg1, %arg1
+
+  %m = stablehlo.multiply %arg0, %arg0 : tensor<8x16xi32>
+  %a1 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %a2 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %a3 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh2", tensor<8x16xi32>>
+  %a4 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh2", tensor<8x16xi32>>
+  return %a1, %a2, %a3, %a4 : !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh2", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh2", tensor<8x16xi32>>
+}
+
+// Illustrates a scenario where we end up with the same value (%arg0) passed to
+// a fragment twice.
+// CHECK-LABEL: func @op_operands_used_by_consumer
+func.func @op_operands_used_by_consumer(%arg0: !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+// CHECK-NEXT: %[[ARG0_UNASSIGN:.*]] = mpmd.unassign %arg0
+// CHECK-NEXT: %[[ARG0_ASSIGN:.*]] = mpmd.assign %[[ARG0_UNASSIGN]]
+// CHECK-NEXT: mpmd.fragment<mesh="m1", origin=[]> (%arg0, %[[ARG0_ASSIGN]])
+// CHECK-NEXT:   add
+// CHECK-NEXT:   multiply
+  %u = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %1 = stablehlo.add %u, %u : tensor<4x8xf32>
+  %a = mpmd.assign %1 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %2 = mpmd.fragment<mesh="m1", origin=[]> (%a, %arg0) (%arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg1, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @fori_loop
+func.func @fori_loop(%arg0: tensor<ui32> {mpmd.src_set = #mpmd.meshes_with_origins<"m1">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">}) -> (tensor<ui32>, tensor<ui32>)
+attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>>} {
+  // CHECK-NEXT:  %[[ARG0_ASSIGN:.*]] = mpmd.assign %arg0 : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG0_ASSIGN]]) (%arg1: tensor<ui32>) {
+  // CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<ui32>
+  // CHECK-NEXT:    mpmd.return %[[ADD]] : tensor<ui32>
+  // CHECK-NEXT:  }
+  // CHECK-NEXT:  %[[F0_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG0]] : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+
+  // CHECK-DAG:   %[[F0_ASSIGN:.*]] = mpmd.assign %[[F0_UNASSIGN]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:   %[[F1_ASSIGN:.*]] = mpmd.assign %[[F0_UNASSIGN]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  %[[FOR:.*]]:2 = mpmd.for (%[[F0_ASSIGN]], %[[F1_ASSIGN]]) {iterations = 3 : ui32, unroll_factor = 3 : ui32} (
+  // CHECK-SAME:    %arg1: !mpmd.mesh_tensor<"m1", tensor<ui32>>,
+  // CHECK-SAME:    %arg2: !mpmd.mesh_tensor<"m1", tensor<ui32>>,
+  // CHECK-SAME:    %index: !mpmd.mesh_tensor<"m1", tensor<ui32>>) {
+  // CHECK-DAG:     %[[UNASSIGN_ARG1:.*]] = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+  // CHECK-DAG:     %[[UNASSIGN_ARG2:.*]] = mpmd.unassign %arg2 : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+  // CHECK-DAG:     %[[UNASSIGN_INDEX:.*]] = mpmd.unassign %index : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+
+  // CHECK-DAG:     %[[ASSIGN_INDEX:.*]] = mpmd.assign %[[UNASSIGN_INDEX]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:     %[[ASSIGN_ARG1:.*]] = mpmd.assign %[[UNASSIGN_ARG1]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:       %[[ADD_FRAG1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN_INDEX]], %[[ASSIGN_ARG1]]) (%arg3: tensor<ui32>, %arg4: tensor<ui32>) {
+  // CHECK-NEXT:      %[[CONST:.*]] = stablehlo.constant dense<1> : tensor<ui32>
+  // CHECK-NEXT:      %[[ADD0:.*]] = stablehlo.add %arg4, %[[CONST]] : tensor<ui32>
+  // CHECK-NEXT:      %[[ADD1:.*]] = stablehlo.add %[[ADD0]], %arg3 : tensor<ui32>
+  // CHECK-NEXT:      mpmd.return %[[ADD1]] : tensor<ui32>
+  // CHECK-NEXT:    } : (!mpmd.mesh_tensor<"m1", tensor<ui32>>, !mpmd.mesh_tensor<"m1", tensor<ui32>>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:     %[[F0_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG1]] : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+
+  // CHECK-DAG:     %[[ASSIGN_ARG2:.*]] = mpmd.assign %[[UNASSIGN_ARG2]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:       %[[ADD_FRAG2:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN_ARG2]]) (%arg3: tensor<ui32>) {
+  // CHECK-NEXT:      %[[CONST:.*]] = stablehlo.constant dense<1> : tensor<ui32>
+  // CHECK-NEXT:      %[[ADD0:.*]] = stablehlo.add %arg3, %[[CONST]] : tensor<ui32>
+  // CHECK-NEXT:      mpmd.return %[[ADD0]] : tensor<ui32>
+  // CHECK-NEXT:    } : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:     %[[F1_UNASSIGN:.*]] = mpmd.unassign %[[ADD_FRAG2]] : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+
+  // CHECK-DAG:     %[[F0_ASSIGN:.*]] = mpmd.assign %[[F0_UNASSIGN]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:     %[[F1_ASSIGN:.*]] = mpmd.assign %[[F1_UNASSIGN]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:    mpmd.return %[[F0_ASSIGN]], %[[F1_ASSIGN]] : !mpmd.mesh_tensor<"m1", tensor<ui32>>, !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  } : !mpmd.mesh_tensor<"m1", tensor<ui32>>, !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  return %[[FOR]]#0, %[[FOR]]#1 : !mpmd.mesh_tensor<"m1", tensor<ui32>>, !mpmd.mesh_tensor<"m1", tensor<ui32>>
+
+  %1 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m1">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<ui32>
+  %2:2 = mpmd.for (%1, %1) {arg_attrs = [{mpmd.src_set = #mpmd.meshes_with_origins<"m1">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">}, {mpmd.src_set = #mpmd.meshes_with_origins<"m1">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">}, {mpmd.use_set = #mpmd.meshes_with_origins<"m1">}], iterations = 3 : ui32, unroll_factor = 3 : ui32} (%arg1: tensor<ui32>, %arg2: tensor<ui32>, %index: tensor<ui32>) {
+    %3 = stablehlo.constant {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} dense<1> : tensor<ui32>
+    %4 = stablehlo.add %arg1, %3 {mpmd.src_set = #mpmd.meshes_with_origins<"m1">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<ui32>
+    %5 = stablehlo.add %4, %index {mpmd.src_set = #mpmd.meshes_with_origins<"m1">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<ui32>
+    %6 = stablehlo.add %arg2, %3 {mpmd.src_set = #mpmd.meshes_with_origins<"m1">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<ui32>
+    mpmd.return %5, %6 : tensor<ui32>, tensor<ui32>
+  } : tensor<ui32>, tensor<ui32>
+  return %2#0, %2#1 : tensor<ui32>, tensor<ui32>
+}
+
+// CHECK-LABEL: func @fori_loop_with_call
+func.func @fori_loop_with_call(%arg0: tensor<ui32>) -> (!mesh_1_tensor_ui32)
+attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>} {
+  // CHECK-NEXT:  %[[ASSIGN_ARG0:.*]] = mpmd.assign %arg0 : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  %[[FOR:.*]] = mpmd.for (%[[ASSIGN_ARG0]]) {iterations = 3 : ui32, unroll_factor = 3 : ui32} (%arg1: !mpmd.mesh_tensor<"m1", tensor<ui32>>, %index: tensor<ui32>) {
+  // CHECK-NEXT:    %[[UNASSIGN_ARG1:.*]] = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+  // CHECK-DAG:     %[[ASSIGN_ARG1_1:.*]] = mpmd.assign %[[UNASSIGN_ARG1]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:     %[[ASSIGN_ARG1_2:.*]] = mpmd.assign %[[UNASSIGN_ARG1]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:    %[[CALL:.*]] = mpmd.call @fori_loop_with_call_f(%[[ASSIGN_ARG1_1]], %[[ASSIGN_ARG1_2]]) {mpmd.use_set = {{.*}}"m1">} : (!mpmd.mesh_tensor<"m1", tensor<ui32>>, !mpmd.mesh_tensor<"m1", tensor<ui32>>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:    %[[UNASSIGN_CALL:.*]] = mpmd.unassign %[[CALL]] : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+  // CHECK-NEXT:    %[[ASSIGN_CALL:.*]] = mpmd.assign %[[UNASSIGN_CALL]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:    mpmd.return %[[ASSIGN_CALL]] : !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  } : !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  %[[UNASSIGN_FOR:.*]] = mpmd.unassign %[[FOR]] : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+  // CHECK-NEXT:  %[[ASSIGN_FOR:.*]] = mpmd.assign {mpmd.use_set = {{.*}}"m1">} %[[UNASSIGN_FOR]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  return %[[ASSIGN_FOR]] : !mpmd.mesh_tensor<"m1", tensor<ui32>>
+
+  %0 = mpmd.for (%arg0) {arg_attrs = [{}, {mpmd.use_set = #mpmd.meshes_with_origins<"m1">}], iterations = 3 : ui32, unroll_factor = 3 : ui32} (%arg1: tensor<ui32>, %index: tensor<ui32>) {
+    %2 = mpmd.call @fori_loop_with_call_f(%arg1, %arg1) {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : (tensor<ui32>, tensor<ui32>) -> tensor<ui32>
+    mpmd.return %2 : tensor<ui32>
+  } : tensor<ui32>
+  %1 = mpmd.assign {mpmd.use_set = #mpmd.meshes_with_origins<"m1">} %0 : (tensor<ui32>) -> !mesh_1_tensor_ui32
+  return %1 : !mesh_1_tensor_ui32
+}
+
+// CHECK-LABEL: func private @fori_loop_with_call_f
+func.func private @fori_loop_with_call_f(%arg0: tensor<ui32>, %arg1: tensor<ui32> {mpmd.use_set = #mpmd.meshes_with_origins<"m1">}) -> tensor<ui32>
+attributes {topology = #mpmd.topology<<"m1" : <["x"=1]>>>} {
+  // CHECK-DAG:   %[[UNASSIGN_ARG0:.*]] = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+  // CHECK-DAG:   %[[UNASSIGN_ARG1:.*]] = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+  // CHECK-DAG:   %[[ASSIGN_ARG0:.*]] = mpmd.assign %[[UNASSIGN_ARG0]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:   %[[ASSIGN_ARG1:.*]] = mpmd.assign %[[UNASSIGN_ARG1]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-DAG:   %[[ADD_FRAG0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ASSIGN_ARG0]], %[[ASSIGN_ARG1]]) (%arg2: tensor<ui32>, %arg3: tensor<ui32>) {
+  // CHECK-NEXT:    %[[ADD0:.*]] = stablehlo.add %arg2, %arg3 : tensor<ui32>
+  // CHECK-NEXT:    mpmd.return %[[ADD0]] : tensor<ui32>
+  // CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m1", tensor<ui32>>, !mpmd.mesh_tensor<"m1", tensor<ui32>>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  %[[UNASSIGN_F0:.*]] = mpmd.unassign %[[ADD_FRAG0]] : (!mpmd.mesh_tensor<"m1", tensor<ui32>>) -> tensor<ui32>
+  // CHECK-NEXT:  %[[ASSIGN_F0:.*]] = mpmd.assign %[[UNASSIGN_F0]] : (tensor<ui32>) -> !mpmd.mesh_tensor<"m1", tensor<ui32>>
+  // CHECK-NEXT:  return %[[ASSIGN_F0]] : !mpmd.mesh_tensor<"m1", tensor<ui32>>
+
+  %0 = stablehlo.add %arg0, %arg1 : tensor<ui32>
+  return %0 : tensor<ui32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_using_analysis_failures.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_using_analysis_failures.mlir
new file mode 100644
index 0000000..4dcf13b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_using_analysis_failures.mlir
@@ -0,0 +1,22 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-rewrite-using-analysis -mpmd-infer-mesh-finalize -verify-diagnostics
+
+// These failures should never happen when running the entire pass pipeline,
+// since validation should catch them first. But they act as a sanity check that
+// all ops are indeed assigned properly.
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>>
+!m1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!m2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+func.func @assign_unassign_still_exist(%arg0: !m1_tensor)
+  -> !m2_tensor attributes {topology=#topology} {
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!m1_tensor) -> !m1_tensor
+  // expected-error @+1 {{assigns, unassigns or broadcasts are not allowed after mesh inference}}
+  %3 = mpmd.unassign %1 : (!m1_tensor) -> tensor<4x8xf32>
+  // expected-error @+1 {{assigns, unassigns or broadcasts are not allowed after mesh inference}}
+  %5 = mpmd.assign %3 : (tensor<4x8xf32>) -> !m2_tensor
+
+  func.return %5 : !m2_tensor
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_with_cloning.mlir b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_with_cloning.mlir
new file mode 100644
index 0000000..f041c89
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/infer_mesh_rewrite_with_cloning.mlir
@@ -0,0 +1,239 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-rewrite-using-analysis='max-clones=3' 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @wrap_as_used_by_return
+func.func @wrap_as_used_by_return(%arg0: !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, %arg1: tensor<8x16xi32>)
+  -> (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>)
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>>}
+{
+// CHECK-NEXT: %[[ASSIGN_ARG1:.*]] = mpmd.assign %arg1
+// CHECK-NEXT: %[[INFERRED_0:.*]] = mpmd.fragment<mesh="mesh1", origin=[]> (%[[ASSIGN_ARG1]])
+// CHECK-NEXT:   multiply
+// CHECK-NEXT:   mpmd.return
+// CHECK-NEXT: }
+// CHECK-NEXT: %[[FRAG_UNASSIGN:.*]] = mpmd.unassign %[[INFERRED_0]]
+// CHECK-NEXT: %[[FRAG_ASSIGN:.*]] = mpmd.assign %[[FRAG_UNASSIGN]]
+// CHECK-NEXT: %[[FRAG:.*]] = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg0, %[[FRAG_ASSIGN]])
+// CHECK-SAME:   (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>)
+// CHECK-NEXT:     %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:     mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: %[[MULT_ASSIGN2:.*]] = mpmd.assign %[[FRAG_UNASSIGN]]
+// CHECK-NEXT: return %[[FRAG]], %[[MULT_ASSIGN2]]
+
+  // The multply op cannot be cloned into the fragment as it is used by an op
+  // that is not a fragment, i.e. the return op.
+  %m = stablehlo.multiply %arg1, %arg1 : tensor<8x16xi32>
+  %am1 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %1 = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg0, %am1) (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x16xi32>
+    mpmd.return %2 : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %am2 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  return %1, %am2 : !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+}
+
+// CHECK-LABEL: @many_fragment_users_within_limit
+func.func @many_fragment_users_within_limit(%arg0: tensor<8x16xi32>, %arg1: !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>)
+  -> (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>)
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>>}
+{
+// CHECK:      %[[USER1:.*]] = mpmd.fragment<mesh="mesh1", origin=["m1"]>
+// CHECK-NEXT:    stablehlo.subtract
+// CHECK-NEXT:    stablehlo.add
+
+// CHECK:      %[[USER2:.*]] = mpmd.fragment<mesh="mesh1", origin=["m1"]>
+// CHECK-NEXT:    stablehlo.subtract
+// CHECK-NEXT:    stablehlo.add
+
+// CHECK:      %[[USER3:.*]] = mpmd.fragment<mesh="mesh1", origin=["m1"]>
+// CHECK-NEXT:    stablehlo.subtract
+// CHECK-NEXT:    stablehlo.add
+
+// CHECK:      return %[[USER1]], %[[USER2]], %[[USER3]]
+  %s = stablehlo.subtract %arg0, %arg0 : tensor<8x16xi32>
+  %as = mpmd.assign %s : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %user1 = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %as) (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x16xi32>
+    mpmd.return %2 : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %user2 = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %as) (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x16xi32>
+    mpmd.return %2 : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %user3 = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %as) (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x16xi32>
+    mpmd.return %2 : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  return %user1, %user2, %user3 : !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+}
+
+// CHECK-LABEL: @many_fragment_users_above_limit
+func.func @many_fragment_users_above_limit(%arg0: tensor<8x16xi32>, %arg1: !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>)
+  -> (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>)
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>>}
+{
+// CHECK:      %[[INFERRED:.*]] = mpmd.fragment<mesh="mesh1", origin=[]>
+// CHECK-NEXT:    stablehlo.subtract
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT: }
+// CHECK-NEXT: %[[INF_UNASSIGN:.*]] = mpmd.unassign %[[INFERRED]]
+// CHECK-NEXT: %[[INF_ASSIGN:.*]] = mpmd.assign %[[INF_UNASSIGN]]
+// CHECK:      %[[USER1:.*]] = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %[[INF_ASSIGN]])
+// CHECK-NEXT:    stablehlo.add
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT: }
+// CHECK:      %[[USER2:.*]] = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %[[INF_ASSIGN]])
+// CHECK-NEXT:    stablehlo.add
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT: }
+// CHECK:      %[[USER3:.*]] = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %[[INF_ASSIGN]])
+// CHECK-NEXT:    stablehlo.add
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT: }
+// CHECK:      %[[USER4:.*]] = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %[[INF_ASSIGN]])
+// CHECK-NEXT:    stablehlo.add
+// CHECK-NEXT:    mpmd.return
+// CHECK-NEXT: }
+// CHECK:      return %[[USER1]], %[[USER2]], %[[USER3]], %[[USER4]]
+  %s = stablehlo.subtract %arg0, %arg0 : tensor<8x16xi32>
+  %as = mpmd.assign %s : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %user1 = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %as) (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x16xi32>
+    mpmd.return %2 : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %user2 = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %as) (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x16xi32>
+    mpmd.return %2 : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %user3 = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %as) (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x16xi32>
+    mpmd.return %2 : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %user4 = mpmd.fragment<mesh="mesh1", origin=["m1"]> (%arg1, %as) (%arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x16xi32>
+    mpmd.return %2 : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  return %user1, %user2, %user3, %user4 : !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+}
+
+// CHECK-LABEL: func @many_user_assigns_single_fragment_consumer
+func.func @many_user_assigns_single_fragment_consumer(%arg0: tensor<8x16xi32>)
+  -> (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>)
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>>}
+{
+// CHECK-NEXT: %[[ARG_ASSIGN:.*]] = mpmd.assign %arg0
+// CHECK-NEXT: mpmd.fragment<mesh="mesh1", origin=[]> (%[[ARG_ASSIGN]])
+// CHECK-NEXT:   stablehlo.multiply
+// CHECK-NEXT:   add
+// CHECK-NEXT:   add
+// CHECK-NEXT:   add
+// CHECK-NEXT:   mpmd.return
+// CHECK-NEXT: }
+// CHECK-NEXT: return
+  %m = stablehlo.multiply %arg0, %arg0 : tensor<8x16xi32>
+  %a1 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %a2 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %a3 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %a4 = mpmd.assign %m : (tensor<8x16xi32>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  %consumer = mpmd.fragment<mesh="mesh1", origin=[]> (%a1, %a2, %a3, %a4)
+    (%arg1: tensor<8x16xi32>, %arg2: tensor<8x16xi32>, %arg3: tensor<8x16xi32>, %arg4: tensor<8x16xi32>)
+  {
+    %a = stablehlo.add %arg1, %arg2 : tensor<8x16xi32>
+    %b = stablehlo.add %a, %arg3 : tensor<8x16xi32>
+    %c = stablehlo.add %b, %arg4 : tensor<8x16xi32>
+    mpmd.return %c : tensor<8x16xi32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>, !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+  return %consumer : !mpmd.mesh_tensor<"mesh1", tensor<8x16xi32>>
+}
+
+// CHECK-LABEL: func @inline_op_with_free_vars
+func.func @inline_op_with_free_vars(%arg0: tensor<4x16xf32>, %arg1: tensor<4x16xf32>, %arg2: tensor<i32>)
+  -> !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>> attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=4]>>>}
+{
+// CHECK-DAG:  %[[ARG2_ASSIGN:.*]] = mpmd.assign %arg2
+// CHECK-DAG:  %[[ARG1_ASSIGN:.*]] = mpmd.assign %arg1
+// CHECK-DAG:  %[[ARG0_ASSIGN:.*]] = mpmd.assign %arg0
+
+// CHECK-NEXT: mpmd.fragment<mesh="mesh1", origin=[]> (%[[ARG1_ASSIGN]], %[[ARG2_ASSIGN]], %[[ARG0_ASSIGN]])
+// CHECK-SAME: (%arg3: tensor<4x16xf32>, %arg4: tensor<i32>, %arg5: tensor<4x16xf32>)
+// CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg5, %arg5 : tensor<4x16xf32>
+// CHECK-NEXT:   %[[CASE:.*]] = "stablehlo.case"(%arg4) ({
+// CHECK-NEXT:     %[[ADD:.*]] = stablehlo.add %[[MULT]], %arg3
+// CHECK-NEXT:     stablehlo.return %[[ADD]]
+// CHECK-NEXT:   }, {
+// CHECK-NEXT:     stablehlo.return %arg3 : tensor<4x16xf32>
+// CHECK-NEXT:   }) : (tensor<i32>) -> tensor<4x16xf32>
+// CHECK-NEXT:   mpmd.return %[[CASE]] : tensor<4x16xf32>
+// CHECK-NEXT: }
+  %var = stablehlo.multiply %arg0, %arg0 : tensor<4x16xf32>
+  %case = "stablehlo.case" (%arg2) ({
+    %4 = stablehlo.add %var, %arg1 : tensor<4x16xf32>
+    stablehlo.return %4 : tensor<4x16xf32>
+  }, {
+    stablehlo.return %arg1 : tensor<4x16xf32>
+  }) : (tensor<i32>) -> tensor<4x16xf32>
+
+  %a = mpmd.assign %case : (tensor<4x16xf32>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>
+  %single_consumer = mpmd.fragment<mesh="mesh1", origin=[]> (%a) (%arg3: tensor<4x16xf32>) {
+    mpmd.return %arg3 : tensor<4x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>
+  func.return %single_consumer : !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>
+}
+
+// CHECK-LABEL: func @clone_op_with_free_vars
+func.func @clone_op_with_free_vars(%arg0: tensor<4x16xf32>, %arg1: tensor<4x16xf32>, %arg2: tensor<i32>)
+  -> (!mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>, !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>) attributes {
+    "topology"=#mpmd.topology<<"mesh1": <["x"=4]>>>}
+{
+
+// CHECK-DAG:  %[[ARG2_ASSIGN_C1:.*]] = mpmd.assign %arg2
+// CHECK-DAG:  %[[ARG1_ASSIGN_C1:.*]] = mpmd.assign %arg1
+// CHECK-DAG:  %[[ARG0_ASSIGN_C1:.*]] = mpmd.assign %arg0
+
+// CHECK-NEXT: %[[CONSUMER1:.*]] = mpmd.fragment<mesh="mesh1", origin=[]> (%[[ARG1_ASSIGN_C1]], %[[ARG2_ASSIGN_C1]], %[[ARG0_ASSIGN_C1]])
+// CHECK-SAME: (%arg3: tensor<4x16xf32>, %arg4: tensor<i32>, %arg5: tensor<4x16xf32>)
+// CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg5, %arg5 : tensor<4x16xf32>
+// CHECK-NEXT:   %[[CASE:.*]] = "stablehlo.case"(%arg4) ({
+// CHECK-NEXT:     %[[ADD:.*]] = stablehlo.add %[[MULT]], %arg3
+// CHECK-NEXT:     stablehlo.return %[[ADD]]
+// CHECK-NEXT:   }, {
+// CHECK-NEXT:     stablehlo.return %arg3 : tensor<4x16xf32>
+// CHECK-NEXT:   }) : (tensor<i32>) -> tensor<4x16xf32>
+// CHECK-NEXT:   mpmd.return %[[CASE]] : tensor<4x16xf32>
+// CHECK-NEXT: }
+
+// CHECK-DAG:  %[[ARG2_ASSIGN_C2:.*]] = mpmd.assign %arg2
+// CHECK-DAG:  %[[ARG1_ASSIGN_C2:.*]] = mpmd.assign %arg1
+// CHECK-DAG:  %[[ARG0_ASSIGN_C2:.*]] = mpmd.assign %arg0
+
+// CHECK-NEXT: %[[CONSUMER2:.*]] = mpmd.fragment<mesh="mesh1", origin=[]> (%[[ARG1_ASSIGN_C2]], %[[ARG2_ASSIGN_C2]], %[[ARG0_ASSIGN_C2]])
+// CHECK-SAME: (%arg3: tensor<4x16xf32>, %arg4: tensor<i32>, %arg5: tensor<4x16xf32>)
+// CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg5, %arg5 : tensor<4x16xf32>
+// CHECK-NEXT:   %[[CASE:.*]] = "stablehlo.case"(%arg4) ({
+// CHECK-NEXT:     %[[ADD:.*]] = stablehlo.add %[[MULT]], %arg3
+// CHECK-NEXT:     stablehlo.return %[[ADD]]
+// CHECK-NEXT:   }, {
+// CHECK-NEXT:     stablehlo.return %arg3 : tensor<4x16xf32>
+// CHECK-NEXT:   }) : (tensor<i32>) -> tensor<4x16xf32>
+// CHECK-NEXT:   mpmd.return %[[CASE]] : tensor<4x16xf32>
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[CONSUMER1]], %[[CONSUMER2]]
+  %var = stablehlo.multiply %arg0, %arg0 : tensor<4x16xf32>
+  %case = "stablehlo.case" (%arg2) ({
+    %4 = stablehlo.add %var, %arg1 : tensor<4x16xf32>
+    stablehlo.return %4 : tensor<4x16xf32>
+  }, {
+    stablehlo.return %arg1 : tensor<4x16xf32>
+  }) : (tensor<i32>) -> tensor<4x16xf32>
+
+  %a = mpmd.assign %case : (tensor<4x16xf32>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>
+  %consumer1 = mpmd.fragment<mesh="mesh1", origin=[]> (%a) (%arg3: tensor<4x16xf32>) {
+    mpmd.return %arg3 : tensor<4x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>
+  %consumer2 = mpmd.fragment<mesh="mesh1", origin=[]> (%a) (%arg3: tensor<4x16xf32>) {
+    mpmd.return %arg3 : tensor<4x16xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>
+  func.return %consumer1, %consumer2 : !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>, !mpmd.mesh_tensor<"mesh1", tensor<4x16xf32>>
+}
+
diff --git a/shardy/dialect/mpmd/transforms/import/test/inline_nested_annotations.mlir b/shardy/dialect/mpmd/transforms/import/test/inline_nested_annotations.mlir
new file mode 100644
index 0000000..22fd5c7
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/inline_nested_annotations.mlir
@@ -0,0 +1,74 @@
+// RUN: mpmd_opt %s -mpmd-inline-nested-user-exposed-ops='assignment=f1@m1,f2@m1' 2>&1 | FileCheck %s
+
+
+// CHECK-LABEL: func @nested_named_comp_assignment
+func.func @nested_named_comp_assignment(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+// CHECK-NEXT: mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>)
+// CHECK-NEXT:   %[[CONST:.*]] = stablehlo.constant dense<1.000000e+00>
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %[[CONST]]
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+    %11 = mpmd.named_computation<"f2"> (%arg1, %0) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+      %10 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+      mpmd.return %10 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %11 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @nested_named_comp_may_be_unassigned
+func.func @nested_named_comp_may_be_unassigned(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+// CHECK-NEXT: mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>)
+// CHECK-NEXT:   %[[CONST:.*]] = stablehlo.constant dense<1.000000e+00>
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %[[CONST]]
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+    %11 = mpmd.named_computation<"a_name_not_in_the_assignment"> (%arg1, %0) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+      %10 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+      mpmd.return %10 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %11 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @nested_unary_mpmd_ops
+func.func @nested_unary_mpmd_ops(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+// CHECK-NEXT: mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>)
+// CHECK-NEXT:   mpmd.return %arg1
+// CHECK-NEXT: }
+  %0 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %1 = mpmd.named_tensor %arg1 name="f2" : tensor<4x8xf32> // Same assignment as the parent.
+    %2 = mpmd.named_tensor %1 name="f3" : tensor<4x8xf32>  // Assignment not in the user assignment map.
+    %3 = mpmd.broadcast %2 : tensor<4x8xf32>
+    %4 = mpmd.reduce<none> %3 : (tensor<4x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @doubly_nested_named_computation
+func.func @doubly_nested_named_computation(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+// CHECK-NEXT: mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>)
+// CHECK-NEXT:   mpmd.return %arg1
+// CHECK-NEXT: }
+  %0 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %1 = mpmd.named_computation<"f1"> (%arg1) (%arg2: tensor<4x8xf32>) {
+      %2 = mpmd.named_computation<"f1"> (%arg2) (%arg3: tensor<4x8xf32>) {
+        mpmd.return %arg3 : tensor<4x8xf32>
+      }: (tensor<4x8xf32>) -> tensor<4x8xf32>
+      mpmd.return %2 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/inline_nested_annotations_failed.mlir b/shardy/dialect/mpmd/transforms/import/test/inline_nested_annotations_failed.mlir
new file mode 100644
index 0000000..e7cf0c8
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/inline_nested_annotations_failed.mlir
@@ -0,0 +1,75 @@
+// RUN: mpmd_opt %s -split-input-file -mpmd-inline-nested-user-exposed-ops='assignment=f1@m1/1,f2@m2,f3@m1/2' -verify-diagnostics
+
+func.func @nested_named_comp_assignment_different_mesh(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.constant dense<0.0> : tensor<4x8xf32>
+    // expected-error@+1 {{NamedComputation 'f2' is nested in a NamedComputation 'f1' which has a different mesh or stage assignment.}}
+    %11 = mpmd.named_computation<"f2"> (%arg1, %0) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+      %10 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+      mpmd.return %10 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %11 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// -----
+
+func.func @nested_different_named_comp_assignment_different_stage(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.constant dense<0.0> : tensor<4x8xf32>
+    // expected-error@+1 {{NamedComputation 'f3' is nested in a NamedComputation 'f1' which has a different mesh or stage assignment.}}
+    %11 = mpmd.named_computation<"f3"> (%arg1, %0) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+      %10 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+      mpmd.return %10 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %11 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// -----
+
+func.func @nested_named_computation_assignment_different_name(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+  %1 = mpmd.named_computation<"f2"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.constant dense<0.0> : tensor<4x8xf32>
+    // expected-error@+1 {{NamedComputation 'f1' is nested in a NamedComputation 'f2' which has a different mesh or stage assignment.}}
+    %11 = mpmd.named_computation<"f1"> (%arg1, %0) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+      %10 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+      mpmd.return %10 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %11 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// -----
+
+func.func @nested_named_computation_assignment_different_stage(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.constant dense<0.0> : tensor<4x8xf32>
+    // expected-error@+1 {{NamedComputation 'f3' is nested in a NamedComputation 'f1' which has a different mesh or stage assignment.}}
+    %11 = mpmd.named_computation<"f3"> (%arg1, %0) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+      %10 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+      mpmd.return %10 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+    mpmd.return %11 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// -----
+
+func.func @nested_unary_mpmd_ops(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>} {
+  %0 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    // expected-error@+1 {{NamedTensor 'f2' is nested in a NamedComputation 'f1' which has a different mesh assignment.}}
+    %1 = mpmd.named_tensor %arg1 name="f2" : tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/insert_nameless_clones_of_negligible_ops.mlir b/shardy/dialect/mpmd/transforms/import/test/insert_nameless_clones_of_negligible_ops.mlir
new file mode 100644
index 0000000..54396bb
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/insert_nameless_clones_of_negligible_ops.mlir
@@ -0,0 +1,33 @@
+// RUN: mpmd_opt %s -mpmd-insert-nameless-clone-of-negligible-ops 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @main
+func.func @main() -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {mesh_shape = #sdy.mesh<["x"=1]>}
+{
+  // The name computation remains unmodified.
+  // CHECK-NEXT: %[[NC:.*]]:3 = mpmd.named_computation<"nc"> () () {
+  // CHECK-NEXT:   stablehlo.constant dense<0.000000e+00>
+  // CHECK-NEXT:   stablehlo.constant dense<1.000000e+00>
+  // CHECK-NEXT:   stablehlo.add
+  // CHECK-NEXT:   stablehlo.constant dense<2.000000e+00>
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[PUSHED1:.*]] = stablehlo.constant dense<1.000000e+00>
+  // CHECK-NEXT: %[[PUSHED2:.*]] = stablehlo.constant dense<2.000000e+00>
+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[PUSHED1]], %[[NC]]#1
+  // CHECK-NEXT: return %[[ADD]], %[[NC]]#2
+  %1:3 = mpmd.named_computation<"nc"> () () {
+    // Used only within the named_computation. Should not be pushed out.
+    %2 = stablehlo.constant dense<0.0> : tensor<4x8xf32>
+    // Used within the named_computation and by the return op. Should be pushed
+    // out.
+    %3 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+    %4 = stablehlo.add %2, %3 : tensor<4x8xf32>
+    // Used only by the return op. Should be pushed out.
+    %5 = stablehlo.constant dense<2.0> : tensor<4x8xf32>
+    mpmd.return %3, %4, %5 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : () -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  %5 = stablehlo.add %1#0, %1#1 : tensor<4x8xf32>
+  // The %1#2 should not be replaced.
+  func.return %5, %1#2 : tensor<4x8xf32>, tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/introduce_transfers.mlir b/shardy/dialect/mpmd/transforms/import/test/introduce_transfers.mlir
new file mode 100644
index 0000000..433e1d5
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/introduce_transfers.mlir
@@ -0,0 +1,134 @@
+// RUN: mpmd_opt %s -mpmd-introduce-transfers 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @push_assign_through_single_add
+func.func @push_assign_through_single_add(%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+// CHECK-DAG:  %[[ARG1_TRANSFER:.*]] = mpmd.transfer %arg1
+// CHECK-DAG:  %[[ADD_FRAG:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %[[ARG1_TRANSFER]])
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-DAG:  return %[[ADD_FRAG]]
+  %m1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %m2 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+  %add = stablehlo.add %m1, %m2 : tensor<4x8xf32>
+  %add_m1 = mpmd.assign %add : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  func.return %add_m1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @push_assign_through_multiple_add
+func.func @push_assign_through_multiple_add(%arg0: !mesh_1_tensor_4_8_f32,
+  %arg1: !mesh_2_tensor_4_8_f32, %arg2: !mesh_1_tensor_4_8_f32,
+  %arg3: !mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+
+// CHECK-DAG:  %[[ARG1_TRANSFER:.*]] = mpmd.transfer %arg1
+// CHECK-DAG:  %[[ADD_FRAG_0:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %[[ARG1_TRANSFER]])
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg4, %arg5
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-DAG:  %[[ADD_FRAG_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ADD_FRAG_0]], %arg2)
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg4, %arg5
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-DAG:  %[[ARG3_TRANSFER:.*]] = mpmd.transfer %arg3
+// CHECK-DAG:  %[[ADD_FRAG_2:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ARG3_TRANSFER]], %[[ADD_FRAG_1]])
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg4, %arg5
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-DAG: return %[[ADD_FRAG_2]]
+  %arg0_m1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg1_m2 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg2_m1 = mpmd.unassign %arg2 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg3_m2 = mpmd.unassign %arg3 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  %add_0 = stablehlo.add %arg0_m1, %arg1_m2 : tensor<4x8xf32>
+  %add_1 = stablehlo.add %add_0, %arg2_m1 : tensor<4x8xf32>
+  %add_2 = stablehlo.add %arg3_m2, %add_1 : tensor<4x8xf32>
+
+  %add_m1 = mpmd.assign %add_2 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  func.return %add_m1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @push_assign_only_if_adding_unassign
+func.func @push_assign_only_if_adding_unassign(%arg0: !mesh_1_tensor_4_8_f32,
+  %arg1: !mesh_2_tensor_4_8_f32, %arg2: !mesh_1_tensor_4_8_f32,
+  %arg3: !mesh_2_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // No AssignOps are pushed through since one of the operands is neither an
+  // UnassignOp nor an AddOp.
+  // CHECK-NOT: fragment
+  %arg0_m1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg0_modified = stablehlo.negate %arg0_m1 : tensor<4x8xf32>
+  %arg1_m2 = mpmd.unassign %arg1 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg2_m1 = mpmd.unassign %arg2 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg3_m2 = mpmd.unassign %arg3 : (!mesh_2_tensor_4_8_f32) -> tensor<4x8xf32>
+
+  %add_0 = stablehlo.add %arg0_modified, %arg1_m2 : tensor<4x8xf32>
+  %add_1 = stablehlo.add %add_0, %arg2_m1 : tensor<4x8xf32>
+  %add_2 = stablehlo.add %arg3_m2, %add_1 : tensor<4x8xf32>
+
+  %add_m1 = mpmd.assign %add_2 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  func.return %add_m1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @transfers_are_deduped
+func.func @transfers_are_deduped(%arg0: !mesh_1_tensor_4_8_f32) -> (!mesh_2_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0
+  // CHECK-NEXT: return %[[TRANSFER]], %[[TRANSFER]], %[[TRANSFER]]
+  %arg0_m1_0 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg0_m2_0 = mpmd.assign %arg0_m1_0 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  %arg0_m1_1 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg0_m2_1 = mpmd.assign %arg0_m1_1 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  %arg0_m1_2 = mpmd.unassign %arg0 : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg0_m1_2_self = mpmd.assign %arg0_m1_2 : (tensor<4x8xf32>) -> !mesh_1_tensor_4_8_f32
+  %arg0_m1_3 = mpmd.unassign %arg0_m1_2_self : (!mesh_1_tensor_4_8_f32) -> tensor<4x8xf32>
+  %arg0_m2_2 = mpmd.assign %arg0_m1_3 : (tensor<4x8xf32>) -> !mesh_2_tensor_4_8_f32
+
+  func.return %arg0_m2_0, %arg0_m2_1, %arg0_m2_2 : !mesh_2_tensor_4_8_f32, !mesh_2_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @assign_of_unassign_with_memory_kinds_becomes_transfer_on_same_mesh
+func.func @assign_of_unassign_with_memory_kinds_becomes_transfer_on_same_mesh(
+    %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">
+  ) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="device">
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+// CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0
+// CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">)
+// CHECK-SAME: -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="device">
+  %m1 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">) -> tensor<4x8xf32>
+  %m2 = mpmd.assign %m1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="device">
+  func.return %m2 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="device">
+}
+
+// CHECK-LABEL: func @assign_of_unassign_with_memory_kinds_becomes_transfer_on_different_mesh
+func.func @assign_of_unassign_with_memory_kinds_becomes_transfer_on_different_mesh(
+    %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">
+  ) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, memory_kind="device">
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+// CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0
+// CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">)
+// CHECK-SAME: -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, memory_kind="device">
+  %m1 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">) -> tensor<4x8xf32>
+  %m2 = mpmd.assign %m1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, memory_kind="device">
+  func.return %m2 : !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, memory_kind="device">
+}
+
+// CHECK-LABEL: func @no_memory_kind_to_pinned_host_is_transfer
+func.func @no_memory_kind_to_pinned_host_is_transfer(
+    %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  ) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+// CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0
+// CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+// CHECK-SAME: -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">
+  %m1 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %m2 = mpmd.assign %m1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">
+  func.return %m2 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/map_computations_to_stages.mlir b/shardy/dialect/mpmd/transforms/import/test/map_computations_to_stages.mlir
new file mode 100644
index 0000000..ddb2a88
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/map_computations_to_stages.mlir
@@ -0,0 +1,24 @@
+// RUN: mpmd_opt %s -mpmd-map-named-ops-to-mpmd-ops='assignment=c1@m1/0,c2@m2/0,c3@m1/1' -mpmd-introduce-transfers 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @simple_assignment
+func.func @simple_assignment(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["c1"], stage=0>
+  // CHECK: mpmd.transfer
+  // CHECK: mpmd.fragment<mesh="m2", origin=["c2"], stage=0>
+  // CHECK: mpmd.transfer
+  // CHECK: mpmd.fragment<mesh="m1", origin=["c3"], stage=1>
+  %1 = mpmd.named_computation<"c1"> (%arg0) (%arg3: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  %2 = mpmd.named_computation<"c2"> (%1) (%arg3: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  %3 = mpmd.named_computation<"c3"> (%2) (%arg3: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %3 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/map_input_output_to_mesh.mlir b/shardy/dialect/mpmd/transforms/import/test/map_input_output_to_mesh.mlir
new file mode 100644
index 0000000..f0cc49a
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/map_input_output_to_mesh.mlir
@@ -0,0 +1,45 @@
+// RUN: mpmd_opt %s -mpmd-map-input-output-to-mesh='input-assignment=0@m1,1@m2 output-assignment=0@m1,1@m2' 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+
+// CHECK: func.func @multiple_input_output_mesh_assignment(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+// CHECK-DAG: %[[UNASSIGN_1:.*]] = mpmd.unassign {origin = "user_in"} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG: %[[UNASSIGN_2:.*]] = mpmd.unassign {origin = "user_in"} %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK: %[[ADD_1:.*]] = stablehlo.add %[[UNASSIGN_1]], %[[UNASSIGN_2]] : tensor<4x8xf32>
+// CHECK: %[[ADD_2:.*]] = stablehlo.add %[[ADD_1]], %[[ADD_1]] : tensor<4x8xf32>
+// CHECK-DAG: %[[ASSIGN_1:.*]] = mpmd.assign {origin = "user_out"} %[[ADD_1]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-DAG: %[[ASSIGN_2:.*]] = mpmd.assign {origin = "user_out"} %[[ADD_2]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+// CHECK: return %[[ASSIGN_1]], %[[ASSIGN_2]] : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+func.func @multiple_input_output_mesh_assignment(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {
+    "topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+ %1 = stablehlo.add %arg0, %arg1 : tensor<4x8xf32>
+ %2 = stablehlo.add %1, %1 : tensor<4x8xf32>
+ func.return %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func private @private_function_should_not_be_mapped
+// CHECK-NOT: mpmd.mesh_tensor
+// CHECK-NOT: mpmd.unassign
+// CHECK-NOT: mpmd.assign
+func.func private @private_function_should_not_be_mapped(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  %1 = stablehlo.add %arg0, %arg0 : tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @same_output_mapped_to_different_meshes(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+// CHECK-DAG: %[[UNASSIGN_3:.*]]= mpmd.unassign {origin = "user_in"} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK-DAG: %[[UNASSIGN_4:.*]] = mpmd.unassign {origin = "user_in"} %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+// CHECK: %[[ADD_3:.*]] = stablehlo.add
+// CHECK-DAG: %[[ASSIGN_MESH_1:.*]] = mpmd.assign {origin = "user_out"} %[[ADD_3]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-DAG: %[[ASSIGN_MESH_2:.*]] = mpmd.assign {origin = "user_out"} %[[ADD_3]] : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+// CHECK:  return %[[ASSIGN_MESH_1]], %[[ASSIGN_MESH_2]]
+func.func @same_output_mapped_to_different_meshes(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {
+     "topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>>} {
+  %1 = stablehlo.add %arg0, %arg1 : tensor<4x8xf32>
+  func.return %1, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/map_io_to_mesh_with_memory_kinds.mlir b/shardy/dialect/mpmd/transforms/import/test/map_io_to_mesh_with_memory_kinds.mlir
new file mode 100644
index 0000000..e1f13ff
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/map_io_to_mesh_with_memory_kinds.mlir
@@ -0,0 +1,41 @@
+// RUN: mpmd_opt %s -mpmd-map-input-output-to-mesh='input-assignment=0@m#pinned_host,1@m#device output-assignment=1@m#pinned_host,2@m#device' -verify-diagnostics -split-input-file 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @map_inputs_to_mesh_with_memory_kind
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="device">
+// CHECK-SAME: (tensor<4x8xf32>, !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">, !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="device">)
+func.func @map_inputs_to_mesh_with_memory_kind(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=2]>>>}
+{
+  // CHECK-DAG: mpmd.unassign {origin = "user_in"}  %arg0 : (!mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">)
+  // CHECK-DAG: mpmd.unassign {origin = "user_in"}  %arg1 : (!mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="device">)
+  // CHECK: %[[C:.*]] = stablehlo.constant
+  // CHECK-DAG: mpmd.assign {origin = "user_out"}  %[[C]] {{.*}} -> !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="device">
+  // CHECK-DAG: mpmd.assign {origin = "user_out"}  %[[C]] {{.*}} -> !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">
+  %0 = stablehlo.add %arg0, %arg1 : tensor<4x8xf32>
+  %1 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  func.return %0, %1, %1 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @memory_kinds_in_type_and_attr
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="device">
+// CHECK-SAME: (tensor<4x8xf32> {mhlo.memory_kind = "device"}
+// CHECK-SAME: , !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">
+// CHECK-SAME: , !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="device">)
+func.func @memory_kinds_in_type_and_attr(%arg0: tensor<4x8xf32> {mhlo.memory_kind = "pinned_host"}, %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32> {mhlo.memory_kind = "device"}, tensor<4x8xf32>, tensor<4x8xf32> {mhlo.memory_kind = "device"})
+  attributes {"topology"=#mpmd.topology<<"m": <["x"=2]>>>}
+{
+  // CHECK-DAG: mpmd.unassign {origin = "user_in"}  %arg0 : (!mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">)
+  // CHECK-DAG: mpmd.unassign {origin = "user_in"}  %arg1 : (!mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="device">)
+  // CHECK: %[[C:.*]] = stablehlo.constant
+  // CHECK-DAG: mpmd.assign {origin = "user_out"}  %[[C]] {{.*}} -> !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="device">
+  // CHECK-DAG: mpmd.assign {origin = "user_out"}  %[[C]] {{.*}} -> !mpmd.mesh_tensor<"m", tensor<4x8xf32>, memory_kind="pinned_host">
+  %0 = stablehlo.add %arg0, %arg1 : tensor<4x8xf32>
+  %1 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  func.return %0, %1, %1 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/map_named_computations_to_meshes.mlir b/shardy/dialect/mpmd/transforms/import/test/map_named_computations_to_meshes.mlir
new file mode 100644
index 0000000..e80d95b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/map_named_computations_to_meshes.mlir
@@ -0,0 +1,39 @@
+// RUN: mpmd_opt %s -mpmd-map-named-ops-to-mpmd-ops='assignment=f1@m1,f2@m1' 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @simple_assignment
+func.func @simple_assignment(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+// CHECK:      %[[ASSIGN:.*]] = mpmd.assign {origin = "f1"}  %arg0
+// CHECK-NEXT: %[[FRAG:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%[[ASSIGN]]) (%arg1: {{.*}} {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign {origin = "f1"}  %[[FRAG]]
+// CHECK-NEXT: return %[[UNASSIGN]]
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg3: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @simple_assignment_transpose
+func.func @simple_assignment_transpose(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+// CHECK:      %[[ASSIGN:.*]] = mpmd.assign {origin = "f1"}  %arg0
+// CHECK-NEXT: %[[FRAG:.*]] = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%[[ASSIGN]]) (%arg1: {{.*}} {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign {origin = "f1"}  %[[FRAG]]
+// CHECK-NEXT: return %[[UNASSIGN]]
+  %1 = mpmd.named_computation<"f1"(1)> (%arg0) (%arg3: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/map_named_ops_to_meshes_with_memory_kinds.mlir b/shardy/dialect/mpmd/transforms/import/test/map_named_ops_to_meshes_with_memory_kinds.mlir
new file mode 100644
index 0000000..d335739
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/map_named_ops_to_meshes_with_memory_kinds.mlir
@@ -0,0 +1,30 @@
+// RUN: mpmd_opt %s -mpmd-map-named-ops-to-mpmd-ops='assignment=x@m1#pinned_host,y@m1#device,f1@m1#pinned_host' -mpmd-map-named-ops-to-mpmd-ops 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @assign_to_pinned_host_and_device
+func.func @assign_to_pinned_host_and_device(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // CHECK-NEXT: assign {origin = "x"} {{.*}} (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">
+  // CHECK-NEXT: unassign {origin = "x"} {{.*}} (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="pinned_host">) -> tensor<4x8xf32>
+  // CHECK-NEXT: assign {origin = "y"} {{.*}} (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="device">
+  // CHECK-NEXT: unassign {origin = "y"} {{.*}} (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, memory_kind="device">) -> tensor<4x8xf32>
+  %0 = mpmd.named_tensor %arg0 name="x" : tensor<4x8xf32>
+  %1 = mpmd.named_tensor %0 name="y" : tensor<4x8xf32>
+  return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @map_nc_with_memory_kind
+func.func @map_nc_with_memory_kind(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+// CHECK:      %[[ASSIGN:.*]] = mpmd.assign {origin = "f1"} %arg0
+// CHECK-NEXT: %[[FRAG:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%[[ASSIGN]]) (%arg1: {{.*}} {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+// CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign {origin = "f1"} %[[FRAG]]
+// CHECK-NEXT: return %[[UNASSIGN]]
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg3: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/map_named_tensors_to_meshes.mlir b/shardy/dialect/mpmd/transforms/import/test/map_named_tensors_to_meshes.mlir
new file mode 100644
index 0000000..c03e70c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/map_named_tensors_to_meshes.mlir
@@ -0,0 +1,60 @@
+// RUN: mpmd_opt %s -mpmd-map-named-ops-to-mpmd-ops='assignment=actual_reference@m1,n1@m1,n2@m2' -mpmd-introduce-transfers 2>&1 | FileCheck %s
+
+!mesh_1_replicated = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_replicated = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @only_actual
+func.func @only_actual(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // CHECK:      %[[ASSIGN:.*]] = mpmd.assign {origin = "actual_reference"} %arg0
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign {origin = "actual_reference"} %[[ASSIGN]]
+  // CHECK-NEXT: return %[[UNASSIGN]]
+  %0 = mpmd.named_tensor %arg0 name="actual_reference" : tensor<4x8xf32>
+  return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @actual_and_unused
+func.func @actual_and_unused(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // CHECK:      %[[ASSIGN:.*]] = mpmd.assign {origin = "actual_reference"} %arg0
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign {origin = "actual_reference"} %[[ASSIGN]]
+  // CHECK-NEXT: return %[[UNASSIGN]]
+  %0 = mpmd.named_tensor %arg0 name="actual_reference" : tensor<4x8xf32>
+  %1 = mpmd.named_tensor %0 name="unused" : tensor<4x8xf32>
+  return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @only_unused
+func.func @only_unused(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // CHECK-NEXT: return %arg0
+  %0 = mpmd.named_tensor %arg0 name="unused1" : tensor<4x8xf32>
+  %1 = mpmd.named_tensor %0 name="unused2" : tensor<4x8xf32>
+  %2 = mpmd.named_tensor %1 name="unused2" : tensor<4x8xf32>
+  return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @duplicate_actual
+func.func @duplicate_actual(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // CHECK:      %[[ASSIGN_1:.*]] = mpmd.assign {origin = "actual_reference"} %arg0
+  // CHECK-NEXT: %[[UNASSIGN_1:.*]] = mpmd.unassign {origin = "actual_reference"} %[[ASSIGN_1]]
+  // CHECK-NEXT: return %[[UNASSIGN_1]]
+  %0 = mpmd.named_tensor %arg0 name="actual_reference" : tensor<4x8xf32>
+  %1 = mpmd.named_tensor %0 name="actual_reference" : tensor<4x8xf32>
+  return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @named_tensor_transfer
+func.func @named_tensor_transfer(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>} {
+  // CHECK:      %[[ASSIGN:.*]] = mpmd.assign {origin = "n1"} %arg0
+  // CHECK-NEXT: %[[TRANSFER_0:.*]] = mpmd.transfer %[[ASSIGN]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[TRANSFER_1:.*]] = mpmd.transfer %[[TRANSFER_0]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[UNASSIGN:.*]] = mpmd.unassign {origin = "n1"} %[[TRANSFER_1]]
+  // CHECK-NEXT: return %[[UNASSIGN]]
+  %0 = mpmd.named_tensor %arg0 name="n1" : tensor<4x8xf32>
+  %1 = mpmd.named_tensor %0 name="n2" : tensor<4x8xf32>
+  %2 = mpmd.named_tensor %1 name="n1" : tensor<4x8xf32>
+  return %2 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/mesh_inference_finalize.mlir b/shardy/dialect/mpmd/transforms/import/test/mesh_inference_finalize.mlir
new file mode 100644
index 0000000..e367691
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/mesh_inference_finalize.mlir
@@ -0,0 +1,179 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-finalize 2>&1 | FileCheck -implicit-check-not use_set -implicit-check-not src_set %s
+
+// CHECK-LABEL: func @assign_of_unassign_same_mesh_no_other_users
+func.func @assign_of_unassign_same_mesh_no_other_users(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">},
+  %arg1: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>>}
+{
+  %0 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %1 = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %2 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %3 = mpmd.assign %1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT: return %arg0, %arg1
+  return %2, %3 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @assign_of_unassign_same_mesh_with_other_users
+func.func @assign_of_unassign_same_mesh_with_other_users(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">},
+  %arg1: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>>}
+{
+  %0 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %1 = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %2 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %3 = mpmd.assign %1 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0 :  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  %4 = mpmd.assign %0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+  // CHECK-NEXT: return %arg0, %arg1, %[[TRANSFER]]
+  return %2, %3, %4 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @duplicate_assign
+func.func @duplicate_assign(%arg0: tensor<4x8xf32>)
+  -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>>}
+{
+  %2 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %3 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT: return %arg0, %arg0
+  return %2, %3 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @different_mesh_assign
+// CHECK-SAME:   %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+func.func @different_mesh_assign(%arg0: tensor<4x8xf32>)
+  -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>>}
+{
+  %2 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0 :  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  %3 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT: return %arg0, %[[TRANSFER]]
+  return %2, %3 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @dedup_assign_of_unassign_and_transfer
+func.func @dedup_assign_of_unassign_and_transfer(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+) -> (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>>}
+{
+  // CHECK-NEXT: %[[FRAGMENT_M1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+  // CHECK:      %[[TRANSFER:.*]] = mpmd.transfer %[[FRAGMENT_M1]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  // CHECK-NEXT: %[[FRAGMENT_M2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[TRANSFER]])
+  // CHECK:      return %[[TRANSFER]], %[[FRAGMENT_M2]]
+
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %y = mpmd.unassign %f : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %z = mpmd.assign %y : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  %w = mpmd.transfer %f : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  %user_z = mpmd.fragment<mesh="m2", origin=[]> (%z) (%arg1: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  return %w, %user_z : !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+}
+
+
+// CHECK-LABEL: func @rewrite_broadcast
+func.func @rewrite_broadcast(%arg0: tensor<4x8xf32>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>} {
+  // CHECK-NEXT: %[[FRAG:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0)
+  // CHECK-NEXT:   stablehlo.add
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %[[FRAG]]
+  // CHECK-NEXT: return %[[FRAG]], %[[TRANSFER]]
+  %0 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %1 = mpmd.fragment<mesh="m1", origin=[]> (%0) (%arg1: tensor<4x8xf32>) {
+    %8 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %8 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %2 = mpmd.unassign %1 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %3 = mpmd.assign %2 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %4 = mpmd.unassign %3 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %5 = mpmd.broadcast %4 : tensor<4x8xf32>
+  %6 = mpmd.assign %5 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %7 = mpmd.assign %5 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  return %6, %7 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @lower_mpmd_reduce_single_user(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:                     %arg1: !mpmd.mesh_tensor<"m2"
+func.func @lower_mpmd_reduce_single_user(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m3", tensor<4x8xf32>> attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>, <"m3" : <["z"=2]>>>} {
+// CHECK-NEXT:  %[[ADD_LOCAL_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg0
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[ADD_LOCAL_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg1, %arg1
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[TRANSFER_1:.*]] = mpmd.transfer %[[ADD_LOCAL_1]]
+// CHECK-NEXT:  %[[TRANSFER_2:.*]] = mpmd.transfer %[[ADD_LOCAL_2]]
+// CHECK-NEXT:  %[[ADD_FINAL:.*]] = mpmd.fragment<mesh="m3", origin=[]> (%[[TRANSFER_1]], %[[TRANSFER_2]])
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[ADD_FINAL]]
+  %0 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %1 = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %2 = mpmd.reduce<add>  {mpmd.use_set = #mpmd.meshes_with_origins<"m3">} %0, %1, %0, %1 : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  %3 = mpmd.assign %2 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m3", tensor<4x8xf32>>
+  return %3 : !mpmd.mesh_tensor<"m3", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @lower_mpmd_reduce_multiple_users(%arg0: !mpmd.mesh_tensor<"m1"
+// CHECK-SAME:                     %arg1: !mpmd.mesh_tensor<"m2"
+func.func @lower_mpmd_reduce_multiple_users(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>, <"m3" : <["z"=2]>>>} {
+// CHECK-NEXT:  %[[ADD_LOCAL_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg0
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[ADD_LOCAL_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%arg1, %arg1
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[TRANSFER_2:.*]] = mpmd.transfer %[[ADD_LOCAL_2]]
+// CHECK-NEXT:  %[[ADD_1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%[[ADD_LOCAL_1]], %[[TRANSFER_2]])
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  %[[TRANSFER_1:.*]] = mpmd.transfer %[[ADD_LOCAL_1]]
+// CHECK-NEXT:  %[[ADD_2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[TRANSFER_1]], %[[ADD_LOCAL_2]])
+// CHECK-NEXT:    %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:    mpmd.return %[[ADD]]
+// CHECK-NEXT:  }
+// CHECK-NEXT:  return %[[ADD_1]], %[[ADD_2]]
+  %0 = mpmd.unassign %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %1 = mpmd.unassign %arg1 : (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) -> tensor<4x8xf32>
+  %2 = mpmd.reduce<add>  {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} %0, %1, %0, %1 : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  %3 = mpmd.assign %2 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %4 = mpmd.assign %2 : (tensor<4x8xf32>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  return %3, %4 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @erases_leftover_use_set_and_src_set
+// CHECK-NOT: meshes_with_origins
+// CHECK-NOT: use_set
+// CHECK-NOT: src_set
+func.func @erases_leftover_use_set_and_src_set(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">}
+) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+  attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["y"=2]>>>}
+{
+  %f = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %13 = stablehlo.add %arg1, %arg1 {mpmd.use_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+    %14 = stablehlo.add %arg1, %arg1 {mpmd.src_set = #mpmd.meshes_with_origins<"m1", "m2">} : tensor<4x8xf32>
+    mpmd.return %13 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  return %f : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/mesh_inference_finalize_with_memory_kinds.mlir b/shardy/dialect/mpmd/transforms/import/test/mesh_inference_finalize_with_memory_kinds.mlir
new file mode 100644
index 0000000..af0d3d0
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/mesh_inference_finalize_with_memory_kinds.mlir
@@ -0,0 +1,44 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-finalize -split-input-file -verify-diagnostics 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @single_assign_to_pinned_host
+// CHECK-SAME: !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="pinned_host">
+func.func @single_assign_to_pinned_host(%arg0: tensor<4x8xi32>) -> !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="pinned_host">
+  attributes {topology = #mpmd.topology<<"m" : <["devices"=8]>>>}
+{
+  %0 = mpmd.assign %arg0 : (tensor<4x8xi32>) -> !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="pinned_host">
+  return %0 : !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="pinned_host">
+}
+
+// -----
+
+// CHECK-LABEL: func @multiple_assign_to_pinned_host
+// CHECK-SAME: !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="pinned_host">
+func.func @multiple_assign_to_pinned_host(%arg0: tensor<4x8xi32>)
+  attributes {topology = #mpmd.topology<<"m" : <["devices"=8]>>>}
+{
+  %0 = mpmd.assign %arg0 : (tensor<4x8xi32>) -> !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="pinned_host">
+  %1 = mpmd.assign %arg0 : (tensor<4x8xi32>) -> !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="pinned_host">
+  return
+}
+
+// -----
+
+// expected-error @+1 {{Argument 0 has different memory kinds assigned to it.}}
+func.func @multiple_assign_users_with_different_memory_kinds(%arg0: tensor<4x8xi32>)
+  attributes {topology = #mpmd.topology<<"m" : <["devices"=8]>>>}
+{
+  %0 = mpmd.assign %arg0 : (tensor<4x8xi32>) -> !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="pinned_host">
+  %1 = mpmd.assign %arg0 : (tensor<4x8xi32>) -> !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="device">
+  return
+}
+
+// -----
+
+// expected-error @+1 {{Argument 0 has different memory kinds assigned to it. Found at least one user with undefined memory kind and at least one user with a memory kind.}}
+func.func @multiple_assign_with_memory_kinds_defined_and_undefined(%arg0: tensor<4x8xi32>)
+  attributes {topology = #mpmd.topology<<"m" : <["devices"=8]>>>}
+{
+  %0 = mpmd.assign %arg0 : (tensor<4x8xi32>) -> !mpmd.mesh_tensor<"m", tensor<4x8xi32>>
+  %1 = mpmd.assign %arg0 : (tensor<4x8xi32>) -> !mpmd.mesh_tensor<"m", tensor<4x8xi32>, memory_kind="device">
+  return
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/mesh_inference_validation_error_limit.mlir b/shardy/dialect/mpmd/transforms/import/test/mesh_inference_validation_error_limit.mlir
new file mode 100644
index 0000000..6bfdcd0
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/mesh_inference_validation_error_limit.mlir
@@ -0,0 +1,16 @@
+// RUN: mpmd_opt %s -mpmd-infer-mesh-validate-no-additional-transfers-needed='error-limit=1' -verify-diagnostics
+
+// Although this function has two errors, the error limit was set to one.
+// Therefore, we only emit one error per test.
+// NOTE: If we add more tests, we need to split the file, each test resets the error count to zero.
+
+func.func @only_one_conflict_reported(%arg0: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {"topology"=#mpmd.topology<
+    <"m1": <["x"=2]>>, <"m2": <["y"=2]>>, <"m3": <["z"=2]>>
+  >} {
+  %0 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  // expected-error @+1 {{Mesh assignment is not possible for op as it is used in {m1} but it can only be placed on {m2,m3}}}
+  %1 = stablehlo.add %arg0, %arg0 {mpmd.src_set = #mpmd.meshes_with_origins<"m2", "m3">, mpmd.use_set = #mpmd.meshes_with_origins<"m1">} : tensor<4x8xf32>
+  func.return %0, %1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/push_unassign_to_call_op.mlir b/shardy/dialect/mpmd/transforms/import/test/push_unassign_to_call_op.mlir
new file mode 100644
index 0000000..a708752
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/push_unassign_to_call_op.mlir
@@ -0,0 +1,169 @@
+// RUN: mpmd_opt %s -mpmd-introduce-transfers 2>&1 | FileCheck %s
+
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>
+
+// CHECK-LABEL: func @test_single_call_op_unassign_cancelled_out_with_assign
+// CHECK-NEXT: %[[UNASSN:.*]] = mpmd.unassign %arg0 {{.*}}"m1"
+// CHECK-NEXT: %[[CALL_RESULT:.*]] = mpmd.call @f(%[[UNASSN]])
+func.func @test_single_call_op_unassign_cancelled_out_with_assign(%arg0 : !mesh_1_tensor) -> !mesh_1_tensor attributes {topology=#topology}
+{
+  %0 = mpmd.unassign %arg0: (!mesh_1_tensor) -> tensor<4x8xf32>
+  %2 = mpmd.call @f(%0) : (tensor<4x8xf32>) ->  !mesh_1_tensor
+  func.return %2 :  !mesh_1_tensor
+}
+// CHECK-LABEL: func private @f(%arg0: tensor<4x8xf32>)
+// CHECK-NEXT: %[[ASSN:.*]] = mpmd.assign %arg0 {{.*}}"m1"
+// CHECK-NEXT: %[[NEW_TRANSFER:.*]] = mpmd.transfer %[[ASSN]]
+// CHECK-NEXT: %[[EXISTING_TRANSFER:.*]] = mpmd.transfer %[[NEW_TRANSFER]]
+// CHECK-NEXT: return %[[EXISTING_TRANSFER]]
+func.func private @f(%arg0 : tensor<4x8xf32>) -> !mesh_1_tensor
+attributes {topology=#topology} {
+  %assign = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_2_tensor
+  %transfer_back_to_m1 = mpmd.transfer %assign : (!mesh_2_tensor) -> !mesh_1_tensor
+  func.return %transfer_back_to_m1 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @test_should_push_in_if_all_conditions_met_on_multiple_calls
+// CHECK-NEXT: %[[UNASSN0:.*]] = mpmd.unassign %arg0 {{.*}}"m1"
+// CHECK-NEXT: %[[UNASSN1:.*]] = mpmd.unassign %arg1 {{.*}}"m1"
+// CHECK-NEXT: %[[CALL_RESULT_1:.*]] = mpmd.call @f_multiple_calls(%[[UNASSN0]])
+// CHECK-NEXT: %[[CALL_RESULT_2:.*]] = mpmd.call @f_multiple_calls(%[[UNASSN1]])
+func.func @test_should_push_in_if_all_conditions_met_on_multiple_calls(%arg0 : !mesh_1_tensor, %arg1 : !mesh_1_tensor) -> (!mesh_1_tensor, !mesh_1_tensor) attributes {
+  topology=#topology}
+{
+  %0 = mpmd.unassign %arg0: (!mesh_1_tensor) -> tensor<4x8xf32>
+  %1 = mpmd.unassign %arg1: (!mesh_1_tensor) -> tensor<4x8xf32>
+
+  %2 = mpmd.call @f_multiple_calls(%0) : (tensor<4x8xf32>) ->  !mesh_1_tensor
+  %3 = mpmd.call @f_multiple_calls(%1) : (tensor<4x8xf32>) ->  !mesh_1_tensor
+  func.return %2, %3 :  !mesh_1_tensor, !mesh_1_tensor
+}
+// CHECK-LABEL: func private @f_multiple_calls(%arg0: tensor<4x8xf32>)
+// CHECK-NEXT: %[[ASSN:.*]] = mpmd.assign %arg0 {{.*}}"m1"
+// CHECK-NEXT: %[[NEW_TRANSFER:.*]] = mpmd.transfer %[[ASSN]]
+// CHECK-NEXT: %[[EXISTING_TRANSFER:.*]] = mpmd.transfer %[[NEW_TRANSFER]]
+// CHECK-NEXT: return %[[EXISTING_TRANSFER]]
+func.func private @f_multiple_calls(%arg0 : tensor<4x8xf32>) -> !mesh_1_tensor
+    attributes {topology=#topology}{
+  %assign = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_2_tensor
+  %transfer_back_to_m1 = mpmd.transfer %assign : (!mesh_2_tensor) -> !mesh_1_tensor
+  func.return %transfer_back_to_m1 : !mesh_1_tensor
+}
+
+// CHECK-LABEL: func @test_should_not_push_if_mesh_different
+// CHECK-NEXT: %[[UNASSIGN_RESULT:.*]] = mpmd.unassign %arg0
+// CHECK-NEXT: %[[UNASSIGN_RESULT:.*]] = mpmd.unassign %arg1
+func.func @test_should_not_push_if_mesh_different(%arg0 : !mesh_1_tensor, %arg1 : !mesh_2_tensor) -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {
+  topology=#topology}
+{
+  %0 = mpmd.unassign %arg0: (!mesh_1_tensor) -> tensor<4x8xf32>
+  %1 = mpmd.unassign %arg1: (!mesh_2_tensor) -> tensor<4x8xf32>
+
+  %2 = mpmd.call @f2(%0) : (tensor<4x8xf32>) ->  tensor<4x8xf32>
+  %3 = mpmd.call @f2(%1) : (tensor<4x8xf32>) ->  tensor<4x8xf32>
+  func.return %2, %3 :  tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+func.func private @f2(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {topology=#topology}  {
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @test_should_not_push_if_not_all_operand_are_from_unassign
+// CHECK: %[[UNASSIGN_RESULT:.*]] = mpmd.unassign %arg0
+func.func @test_should_not_push_if_not_all_operand_are_from_unassign(%arg0 : !mesh_1_tensor) -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {
+  topology=#topology}
+{
+  %0 = mpmd.unassign %arg0: (!mesh_1_tensor) -> tensor<4x8xf32>
+  %1 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+
+  %2 = mpmd.call @f3(%0) : (tensor<4x8xf32>) ->  tensor<4x8xf32>
+  %3 = mpmd.call @f3(%1) : (tensor<4x8xf32>) ->  tensor<4x8xf32>
+  func.return %2, %3 :  tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+func.func private @f3(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+    attributes {topology=#topology} {
+  func.return %arg0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @test_should_not_push_if_arg_not_assigned_later
+// CHECK-NEXT: %[[UNASSIGN_RESULT:.*]] = mpmd.unassign %arg0
+// CHECK-NEXT: %[[CALL_RESULT:.*]] = mpmd.call @f4(%[[UNASSIGN_RESULT]])
+func.func @test_should_not_push_if_arg_not_assigned_later(%arg0 : !mesh_1_tensor) -> tensor<4x8xf32> attributes {
+  topology=#topology}
+{
+  %0 = mpmd.unassign %arg0: (!mesh_1_tensor) -> tensor<4x8xf32>
+  %2 = mpmd.call @f4(%0) : (tensor<4x8xf32>) ->  tensor<4x8xf32>
+  func.return %2 :  tensor<4x8xf32>
+}
+func.func private @f4(%arg0 : tensor<4x8xf32>) -> tensor<4x8xf32>
+attributes {
+  topology=#topology} {
+  func.return %arg0 :tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @test_should_not_push_if_callee_has_no_input
+// CHECK-NEXT: %[[CALL_RESULT:.*]] = mpmd.call @f5()
+func.func @test_should_not_push_if_callee_has_no_input() -> tensor<4x8xf32> attributes {
+  topology=#topology}
+{
+  %2 = mpmd.call @f5() : () ->  tensor<4x8xf32>
+  func.return %2 :  tensor<4x8xf32>
+}
+func.func private @f5() -> tensor<4x8xf32>
+attributes {topology=#topology} {
+  %1 = stablehlo.constant dense<1.0> : tensor<4x8xf32>
+  func.return %1 :tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @test_should_not_push_if_no_defining_op_of_arg
+// CHECK-NEXT: %[[CALL_RESULT:.*]] = mpmd.call @f6(%arg0)
+func.func @test_should_not_push_if_no_defining_op_of_arg(%arg0 : !mesh_1_tensor) -> !mesh_1_tensor attributes {
+  topology=#topology}
+{
+  %2 = mpmd.call @f6(%arg0) : (!mesh_1_tensor) ->  !mesh_1_tensor
+  func.return %2 :  !mesh_1_tensor
+}
+func.func private @f6(%arg1 : !mesh_1_tensor) -> !mesh_1_tensor
+attributes {
+  topology=#topology} {
+  func.return %arg1 :!mesh_1_tensor
+}
+
+
+// CHECK-LABEL: func @test_should_push_in_if_all_conditions_met_on_chained_calls
+// CHECK-NEXT: %[[UNASSN0:.*]] = mpmd.unassign %arg0 {{.*}}"m1"
+// CHECK-NEXT: %[[UNASSN1:.*]] = mpmd.unassign %arg1 {{.*}}"m1"
+// CHECK-NEXT: %[[CALL_RESULT_1:.*]]:2 = mpmd.call @f_chained_calls(%[[UNASSN0]], %[[UNASSN1]])
+// CHECK-NEXT: %[[CALL_RESULT_2:.*]]:2 = mpmd.call @f_chained_calls(%[[CALL_RESULT_1]]#1, %[[CALL_RESULT_1]]#0)
+func.func @test_should_push_in_if_all_conditions_met_on_chained_calls(
+  %arg0 : !mesh_1_tensor, %arg1 : !mesh_1_tensor
+) -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {topology=#topology}
+{
+  %0 = mpmd.unassign %arg0: (!mesh_1_tensor) -> tensor<4x8xf32>
+  %1 = mpmd.unassign %arg1: (!mesh_1_tensor) -> tensor<4x8xf32>
+
+  %2:2 = mpmd.call @f_chained_calls(%0, %1) : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  %3:2 = mpmd.call @f_chained_calls(%2#1, %2#0) : (tensor<4x8xf32>, tensor<4x8xf32>) ->  (tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %3#0, %3#1 :  tensor<4x8xf32>, tensor<4x8xf32>
+}
+// CHECK-LABEL: func private @f_chained_calls
+// CHECK-NEXT: %[[ASSN2:.*]] = mpmd.assign %arg1 {{.*}}"m1"
+// CHECK-NEXT: %[[ASSN1:.*]] = mpmd.assign %arg0 {{.*}}"m1"
+// CHECK-NEXT: %[[UNASSN1:.*]] = mpmd.unassign %[[ASSN1]] {{.*}}"m1"
+// CHECK-NEXT: %[[NEW_TRANSFER:.*]] = mpmd.transfer %[[ASSN2]]
+// CHECK-NEXT: %[[UNASSN2:.*]] = mpmd.unassign %[[NEW_TRANSFER]] {{.*}}"m2"
+// CHECK-NEXT: return %[[UNASSN1]], %[[UNASSN2]]
+func.func private @f_chained_calls(%arg0 : tensor<4x8xf32>, %arg1 : tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+    attributes {topology=#topology}{
+  %assign1 = mpmd.assign %arg0 : (tensor<4x8xf32>) -> !mesh_1_tensor
+  %unassign1 = mpmd.unassign %assign1 : (!mesh_1_tensor) -> tensor<4x8xf32>
+
+  %assign2 = mpmd.assign %arg1 : (tensor<4x8xf32>) -> !mesh_2_tensor
+  %unassign2 = mpmd.unassign %assign2 : (!mesh_2_tensor) -> tensor<4x8xf32>
+
+  func.return %unassign1, %unassign2 : tensor<4x8xf32>, tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/simplify_named_computations.mlir b/shardy/dialect/mpmd/transforms/import/test/simplify_named_computations.mlir
new file mode 100644
index 0000000..3198742
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/simplify_named_computations.mlir
@@ -0,0 +1,254 @@
+// RUN: mpmd_opt %s -mpmd-simplify-named-computations 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @no_duplicate_operands_or_results
+func.func @no_duplicate_operands_or_results(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> tensor<4x8xf32>
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]] = mpmd.named_computation<"f"(2)> (%arg0, %arg1)
+// CHECK-SAME:   (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[NC]]
+  %0 = mpmd.named_computation<"f"(2)> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %2 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @duplicate_operands
+func.func @duplicate_operands(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>)
+  -> tensor<4x8xf32>
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]] = mpmd.named_computation<"f"(1)> (%arg0, %arg1, %arg2)
+// CHECK-SAME:   (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg3, %arg4
+// CHECK-NEXT:   %[[MUL:.*]] = stablehlo.multiply %arg5, %arg3
+// CHECK-NEXT:   %[[SUB:.*]] = stablehlo.subtract %[[ADD]], %arg5
+// CHECK-NEXT:   %[[DIV:.*]] = stablehlo.divide %[[MUL]], %arg5
+// CHECK-NEXT:   %[[POW:.*]] = stablehlo.power %[[SUB]], %[[DIV]]
+// CHECK-NEXT:   mpmd.return %[[POW]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[NC]]
+  %0 = mpmd.named_computation<"f"(1)> (%arg0, %arg1, %arg2, %arg0, %arg2, %arg2)
+    (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>,
+     %arg6: tensor<4x8xf32>, %arg7: tensor<4x8xf32>, %arg8: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+    %2 = stablehlo.multiply %arg5, %arg6 : tensor<4x8xf32>
+    %3 = stablehlo.subtract %1, %arg7 : tensor<4x8xf32>
+    %4 = stablehlo.divide %2, %arg8 : tensor<4x8xf32>
+    %5 = stablehlo.power %3, %4 : tensor<4x8xf32>
+    mpmd.return %5 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>,
+       tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+      -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @duplicate_and_noop_results
+func.func @duplicate_and_noop_results(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>,
+      tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]]:2 = mpmd.named_computation<"f"> (%arg0, %arg1)
+// CHECK-SAME:   (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:   %[[MUL:.*]] = stablehlo.multiply %arg2, %arg3
+// CHECK-NEXT:   mpmd.return %[[ADD]], %[[MUL]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %arg0, %[[NC]]#0, %[[NC]]#1,
+// CHECK-SAME:        %[[NC]]#0, %arg0, %[[NC]]#0
+  %0:6 = mpmd.named_computation<"f"> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    %2 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %arg2, %1, %2, %1, %arg2, %1 :
+      tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>,
+      tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>)
+      -> (tensor<4x8xf32>, tensor<4x8xf32>,
+          tensor<4x8xf32>, tensor<4x8xf32>,
+          tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5 :
+    tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>,
+    tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @duplicate_operands_and_results
+func.func @duplicate_operands_and_results(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]]:2 = mpmd.named_computation<"f"> (%arg0, %arg1)
+// CHECK-SAME:   (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg3
+// CHECK-NEXT:   %[[MUL:.*]] = stablehlo.multiply %[[ADD]], %arg2
+// CHECK-NEXT:   mpmd.return %[[ADD]], %[[MUL]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[NC]]#0, %[[NC]]#1, %[[NC]]#1
+  %0:3 = mpmd.named_computation<"f"> (%arg0, %arg1, %arg0)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    %2 = stablehlo.multiply %1, %arg4 : tensor<4x8xf32>
+    mpmd.return %1, %2, %2 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+      -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %0#0, %0#1, %0#2 :
+    tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unused_operand
+func.func @unused_operand(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)
+  -> tensor<4x8xf32>
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]] = mpmd.named_computation<"f"> (%arg0)
+// CHECK-SAME:   (%arg2: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[NC]]
+  %0 = mpmd.named_computation<"f"> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %1 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unused_result
+func.func @unused_result(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]] = mpmd.named_computation<"f"> (%arg0)
+// CHECK-SAME:   (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[NC]]
+  %0:2 = mpmd.named_computation<"f"> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+      %1 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      // This value is not used outside the named computation. It'll be removed.
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %0#0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @unused_result_causes_operand_to_be_removed
+func.func @unused_result_causes_operand_to_be_removed(%arg0: tensor<4x8xf32>)
+  -> tensor<4x8xf32>
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]] = mpmd.named_computation<"f"> (%arg0)
+// CHECK-SAME:   (%arg1: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[NC]]
+  %0 = mpmd.named_computation<"f"> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+      mpmd.return %arg2 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  %1:2 = mpmd.named_computation<"f"> (%arg0, %0)
+    (%arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>) {
+      %1 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+      // This value is not used outside the named computation. It'll be removed
+      // and so it the named computation that produces the operand re to %arg2.
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %1, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %1#0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @noop_named_computation
+func.func @noop_named_computation(%arg0: tensor<4x8xf32>)
+  -> tensor<4x8xf32>
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: return %arg0
+  %0 = mpmd.named_computation<"f"> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @noop_result_with_opt_barrier
+func.func @noop_result_with_opt_barrier(%arg0: tensor<4x8xf32>)
+  -> tensor<4x8xf32>
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]] = mpmd.named_computation<"f"> (%arg0)
+// CHECK-NEXT:   %[[OPTB:.*]] = stablehlo.optimization_barrier %arg1
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %[[OPTB]], %[[OPTB]] : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[NC]]
+  %0:2 = mpmd.named_computation<"f"> (%arg0, %arg0)
+    (%arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>) {
+    %optb:2  = stablehlo.optimization_barrier %arg1, %arg2 : tensor<4x8xf32>, tensor<4x8xf32>
+    %1 = stablehlo.add %optb#0, %optb#0 : tensor<4x8xf32>
+    mpmd.return %1, %optb#1 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %0#0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @noop_result_with_opt_barrier_multiple_uses_not_simplified
+func.func @noop_result_with_opt_barrier_multiple_uses_not_simplified(
+  %arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]]:2 = mpmd.named_computation<"f"> (%arg0, %arg1)
+// CHECK-NEXT:   %[[OPTB:.*]]:2 = stablehlo.optimization_barrier %arg2, %arg3
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %[[OPTB]]#0, %[[OPTB]]#0 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]], %[[OPTB]]#1
+// CHECK-NEXT: }
+// CHECK-NEXT: return %[[NC]]#0, %[[NC]]#1
+  %0:3 = mpmd.named_computation<"f"> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %optb:2  = stablehlo.optimization_barrier %arg2, %arg3 : tensor<4x8xf32>, tensor<4x8xf32>
+    %1 = stablehlo.add %optb#0, %optb#0 : tensor<4x8xf32>
+    mpmd.return %1, %optb#1, %optb#1 : tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %0#0, %0#1 : tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @noop_results_and_duplicate_operand
+func.func @noop_results_and_duplicate_operand(
+  %arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %arg2: tensor<4x8xf32>)
+  -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+// CHECK-NEXT: %[[NC:.*]] = mpmd.named_computation<"f"> (%arg1)
+// CHECK-SAME:   (%arg3: tensor<4x8xf32>) {
+// CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+// CHECK-NEXT:   mpmd.return %[[ADD]]
+// CHECK-NEXT: }
+// CHECK-NEXT: return %arg0, %arg0, %[[NC]], %arg2
+  %0:4 = mpmd.named_computation<"f"> (%arg0, %arg0, %arg1, %arg2)
+    (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>, %arg5: tensor<4x8xf32>,
+     %arg6: tensor<4x8xf32>) {
+    %1 = stablehlo.add %arg5, %arg5 : tensor<4x8xf32>
+    mpmd.return %arg3, %arg4, %1, %arg6 :
+      tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+      -> (tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>)
+  func.return %0#0, %0#1, %0#2, %0#3 :
+    tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>, tensor<4x8xf32>
+}
+
+// The all_gather cannot be removed because it isn't pure.
+// CHECK-LABEL: func @all_results_unused_but_not_pure
+func.func @all_results_unused_but_not_pure(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32>
+  attributes {mesh_shape = #sdy.mesh<["x"=4]>} {
+  // CHECK-NEXT: %[[NC:.*]] = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+  // CHECK-NEXT:   %[[AG:.*]] = "stablehlo.all_gather"(%arg1)
+  // CHECK-NEXT:   mpmd.return %1 : tensor<4x16xf32>
+  // CHECK-NEXT: } : (tensor<4x8xf32>) -> tensor<4x16xf32>
+  // CHECK-NEXT: return %arg0
+  %0:3 = mpmd.named_computation<"f1"> (%arg0) (%arg1: tensor<4x8xf32>) {
+    %1 = "stablehlo.all_gather"(%arg1) {
+      all_gather_dim = 1 : i64,
+      replica_groups = dense<[[0, 1], [2, 3]]> : tensor<2x2xi64>
+    } : (tensor<4x8xf32>) -> tensor<4x16xf32>
+    %2 = stablehlo.add %1, %1 : tensor<4x16xf32>
+    mpmd.return %arg1, %1, %2 : tensor<4x8xf32>, tensor<4x16xf32>, tensor<4x16xf32>
+  } : (tensor<4x8xf32>) -> (tensor<4x8xf32>, tensor<4x16xf32>, tensor<4x16xf32>)
+  return %arg0 : tensor<4x8xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/import/test/validate_no_named_ops.mlir b/shardy/dialect/mpmd/transforms/import/test/validate_no_named_ops.mlir
new file mode 100644
index 0000000..355d0d6
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/test/validate_no_named_ops.mlir
@@ -0,0 +1,76 @@
+// RUN: mpmd_opt %s -mpmd-validate-named-ops-in-mpmd-func 2>&1 | FileCheck %s
+
+// CHECK-LABEL: func @named_computation_inside_non_mpmd_function
+func.func @named_computation_inside_non_mpmd_function(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {
+// CHECK-ERROR: {{.*}}{{Named computations can only be nested in mpmd functions or mpmd ops.}}
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg3: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @named_computation_inside_non_mpmd_ops
+func.func @named_computation_inside_non_mpmd_ops(%arg0: tensor<i32>) -> tensor<i32> {
+// CHECK-ERROR: {{.*}}{{Named computations can only be nested in mpmd functions or mpmd ops.}}
+  %0 = "stablehlo.case"(%arg0) ({
+    %named_tensor = mpmd.named_tensor %arg0 name="tensor" : tensor<i32>
+    stablehlo.return %named_tensor : tensor<i32>
+  }, {
+    stablehlo.return %arg0 : tensor<i32>
+  }) : (tensor<i32>) -> tensor<i32>
+  return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: func @named_tensor_inside_non_mpmd_function
+func.func @named_tensor_inside_non_mpmd_function(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {
+// CHECK-ERROR: {{.*}}{{Named tensors can only be nested in mpmd functions or mpmd ops.}}
+  %0 = mpmd.named_tensor %arg0 name="tensor" : tensor<4x8xf32>
+  return %0 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @named_tensor_inside_non_mpmd_ops
+func.func @named_tensor_inside_non_mpmd_ops(%arg0: tensor<i32>) -> tensor<i32> attributes {} {
+// CHECK-ERROR: {{.*}}{{Named tensors can only be nested in mpmd functions or mpmd ops.}}
+  %0 = "stablehlo.case"(%arg0) ({
+    %named_tensor = mpmd.named_tensor %arg0 name="tensor" : tensor<i32>
+    stablehlo.return %named_tensor : tensor<i32>
+  }, {
+    stablehlo.return %arg0 : tensor<i32>
+  }) : (tensor<i32>) -> tensor<i32>
+  return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: func @named_tensor_inside_named_computation_is_ok
+func.func @named_tensor_inside_named_computation_is_ok(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg3: tensor<4x8xf32>) {
+    %named_tensor = mpmd.named_tensor %arg3 name="tensor" : tensor<4x8xf32>
+    %10 = stablehlo.add %named_tensor, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+// CHECK-LABEL: func @named_computation_inside_named_computation_is_ok
+func.func @named_computation_inside_named_computation_is_ok(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {
+  %1 = mpmd.named_computation<"outer"> (%arg0) (%arg3: tensor<4x8xf32>) {
+    %inner_named_computation = mpmd.named_computation<"inner"> (%arg3) (%arg4: tensor<4x8xf32>) {
+      mpmd.return %arg4 : tensor<4x8xf32>
+    } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+    %10 = stablehlo.add %inner_named_computation, %arg3 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+// CHECK-LABEL: func @named_computation_inside_mpmd_function_is_ok
+func.func @named_computation_inside_mpmd_function_is_ok(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> attributes {
+    "topology"=#topology} {
+  %1 = mpmd.named_computation<"f1"> (%arg0, %arg0) (%arg3: tensor<4x8xf32>, %arg4: tensor<4x8xf32>) {
+    %10 = stablehlo.add %arg3, %arg4 : tensor<4x8xf32>
+    mpmd.return %10 : tensor<4x8xf32>
+  } : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<4x8xf32>
+  func.return %1 : tensor<4x8xf32>
+}
+
diff --git a/shardy/dialect/mpmd/transforms/import/validate_named_ops_in_mpmd_func.cc b/shardy/dialect/mpmd/transforms/import/validate_named_ops_in_mpmd_func.cc
new file mode 100644
index 0000000..5e6f2a3
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/import/validate_named_ops_in_mpmd_func.cc
@@ -0,0 +1,71 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/sdy/ir/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_VALIDATENAMEDOPSINMPMDFUNCPASS
+#include "shardy/dialect/mpmd/transforms/import/passes.h.inc"
+
+namespace {
+
+class ValidateNamedOpsInMpmdFuncPass
+    : public impl::ValidateNamedOpsInMpmdFuncPassBase<
+          ValidateNamedOpsInMpmdFuncPass> {
+  using ValidateNamedOpsInMpmdFuncPassBase::ValidateNamedOpsInMpmdFuncPassBase;
+
+  bool IsImmediateParentMpmdFuncOrMpmdOp(Operation* op) {
+    Operation* parent_op = op->getParentOp();
+    if (func::FuncOp func = dyn_cast<func::FuncOp>(parent_op)) {
+      return IsMpmdFunction(func);
+    }
+    return sdy::inDialect<mpmd::MpmdDialect>(parent_op);
+  }
+
+  void runOnOperation() final {
+    // Check named computations and named tensors are only nested in mpmd
+    // function (function with topology).
+    getOperation().walk([&](Operation* op) {
+      if (auto named_computation = dyn_cast<NamedComputationOp>(op)) {
+        if (!IsImmediateParentMpmdFuncOrMpmdOp(named_computation)) {
+          emitError(named_computation->getLoc())
+              << "Named computations can only be nested in mpmd functions or "
+                 "mpmd ops.";
+        }
+      }
+
+      if (auto named_tensor = dyn_cast<NamedTensorOp>(op)) {
+        if (!IsImmediateParentMpmdFuncOrMpmdOp(named_tensor)) {
+          emitError(named_tensor->getLoc())
+              << "Named tensors can only be nested in mpmd functions or mpmd "
+                 "ops.";
+        }
+      }
+    });
+  }
+};
+
+}  // namespace
+
+}  // namespace mlir::mpmd
+
diff --git a/shardy/dialect/mpmd/transforms/optimize/BUILD b/shardy/dialect/mpmd/transforms/optimize/BUILD
new file mode 100644
index 0000000..5ab2087
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/BUILD
@@ -0,0 +1,108 @@
+# The MPMD optimize passes and pipeline.
+
+# load("@rules_cc//cc:cc_library.bzl", "cc_library")
+# load("@rules_cc//cc:cc_test.bzl", "cc_test")
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library", "td_library")
+
+package(default_visibility = ["//visibility:public"])
+
+td_library(
+    name = "passes_td_files",
+    srcs = [
+        "passes.td",
+    ],
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+gentbl_cc_library(
+    name = "passes_inc",
+    tbl_outs = {
+        "passes.h.inc": [
+            "-gen-pass-decls",
+            "-name=MpmdOptimize",
+        ],
+        "g3doc/mpmd_optimize_passes.md": ["-gen-pass-doc"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "passes.td",
+    deps = [":passes_td_files"],
+)
+
+cc_library(
+    name = "passes",
+    srcs = [
+        "optimize_pipeline.cc",
+        "remat_fragment.cc",
+        "scheduler.cc",
+    ],
+    hdrs = [
+        "passes.h",
+        "scheduler.h",
+    ],
+    deps = [
+        ":passes_inc",
+        ":pipeline_schedule",
+        ":utils",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/transforms/common:distributed_function_pass",
+        "//shardy/dialect/mpmd/transforms/common:passes",
+        "//shardy/dialect/mpmd/transforms/common:utils",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:Analysis",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+    ],
+)
+
+cc_library(
+    name = "pipeline_schedule",
+    srcs = ["pipeline_schedule.cc"],
+    hdrs = ["pipeline_schedule.h"],
+    deps = [
+        ":utils",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/transforms/common:utils",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_library(
+    name = "utils",
+    srcs = ["utils.cc"],
+    hdrs = ["utils.h"],
+    deps = [
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/transforms/common:utils",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_test(
+    name = "utils_test",
+    srcs = ["utils_test.cc"],
+    deps = [
+        ":utils",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/ir:register",
+        "//shardy/dialect/mpmd/transforms/common:testing_utils",
+        "@com_google_googletest//:gtest_main",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Parser",
+        "@llvm-project//mlir:Support",
+    ],
+)
diff --git a/shardy/dialect/mpmd/transforms/optimize/optimize_pipeline.cc b/shardy/dialect/mpmd/transforms/optimize/optimize_pipeline.cc
new file mode 100644
index 0000000..767b23b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/optimize_pipeline.cc
@@ -0,0 +1,122 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <utility>
+
+#include "llvm/Support/CommandLine.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Pass/PassRegistry.h"
+#include "shardy/dialect/mpmd/transforms/common/merge_fragments.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"
+#include "shardy/dialect/mpmd/transforms/optimize/passes.h"
+#include "shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.h"
+#include "shardy/dialect/mpmd/transforms/optimize/scheduler.h"
+
+namespace mlir::mpmd {
+
+using ::mlir::func::FuncOp;
+
+void addOptimizePipeline(OpPassManager& pm, OptimizeOptions options) {
+  // Apply as many optimizations as possible before inlining.
+  pm.addNestedPass<FuncOp>(createRemoveTransferCyclesPass());
+
+  // TODO(jupvfranco): consider moving inlining to import.
+  AddCallInliningRelatedPasses(pm);
+  // Merge any inferred fragments with user-defined fragments that could not be
+  // merged before because of CallOps.
+  if (!options.mergeAfterScheduling) {
+    pm.addNestedPass<FuncOp>(createMergeInferredFragmentsPass());
+  }
+
+  // Merge fragments according to the user-specified rules. Do this before other
+  // merge passes since those modify the origins of fragments, invalidating the
+  // rules.
+  if (!options.fragmentMergeRules.empty()) {
+    pm.addNestedPass<FuncOp>(createRuleBasedMergePass(
+        RuleBasedMergePassOptions{std::move(options.fragmentMergeRules)}));
+  }
+
+  // Adds all pipeline scheduling related passes.
+  // Merge fragments into scheduling units.
+  AddSchedulingPreprocessingPasses(pm, options.splitBwdFragments,
+                                   options.verifyScheduleUnits);
+  AddSchedulingPass(pm, options.pipelineSchedule);
+
+  // The remat passes will run after inlining the call ops and scheduling.
+  // The reason why we choose to remat after scheduling is so that we don't need
+  // to schedule the remat fragments. For example, given the following fragments
+  // on the same mesh in order: F0, F1, B1, B0, if we remat before schedule, we
+  // would have F0, F1, F1(remat), B1, F0(remat), B0 and need to worry
+  // about scheduling the remat fragments.
+  if (options.applyFragmentRemat) {
+    pm.addNestedPass<FuncOp>(createRematFragmentPass(
+        RematFragmentPassOptions{options.mergeRematFragments}));
+  }
+
+  // Verify fragments assigned to the same stage were merged, i.e., it's not
+  // possible to have two distinct fragments representing the same stage. Users
+  // must use different stages to achieve that kind of program.
+  pm.addNestedPass<FuncOp>(createVerifyStageMergingPass());
+
+  if (options.mergeAfterScheduling) {
+    AddMergeInferredFragmentsPasses(
+        pm, options.absorbInferredFragmentsOnEntryPointFunction,
+        options.cloneInferredFragments);
+  }
+
+  // Merge fragments as specified by the user.
+  if (options.mergeForwardWithBackward) {
+    pm.addNestedPass<FuncOp>(createMergeForwardWithBackwardPass());
+  }
+}
+
+namespace {
+
+struct OptimizePipelineOptions
+    : public PassPipelineOptions<OptimizePipelineOptions> {
+  Option<bool> mergeAfterScheduling{
+      *this, "merge-after-scheduling",
+      llvm::cl::desc(
+          "Whether to merge inferred fragments only after scheduling."),
+      llvm::cl::init(false)};
+
+  Option<PipelineSchedule> pipelineSchedule{
+      *this, "pipeline-schedule",
+      llvm::cl::desc("The pipeline schedule to use."),
+      llvm::cl::init(PipelineSchedule::k1F1B),
+      llvm::cl::values(
+          clEnumValN(PipelineSchedule::kNone, "none", "No schedule"),
+          clEnumValN(PipelineSchedule::k1F1B, "1F1B", "1F1B schedule"),
+          clEnumValN(PipelineSchedule::kCircular, "Circular",
+                     "Circular schedule"))};
+};
+
+}  // namespace
+
+void registerOptimizePipeline() {
+  PassPipelineRegistration<OptimizePipelineOptions>(
+      "mpmd-optimize-pipeline",
+      "Run the standard set of passes to optimize an MPMD program.",
+      [](OpPassManager& pm, const OptimizePipelineOptions& pipelineOptions) {
+        OptimizeOptions options;
+        options.mergeAfterScheduling = pipelineOptions.mergeAfterScheduling;
+        options.pipelineSchedule = pipelineOptions.pipelineSchedule;
+        addOptimizePipeline(pm, options);
+      });
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/optimize/passes.h b/shardy/dialect/mpmd/transforms/optimize/passes.h
new file mode 100644
index 0000000..d0ffed5
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/passes.h
@@ -0,0 +1,74 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_PASSES_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_PASSES_H_
+
+// IWYU pragma: begin_keep
+
+#include <memory>
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/distributed_function_pass.h"
+#include "shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.h"
+
+// IWYU pragma: end_keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DECL
+#define GEN_PASS_REGISTRATION
+#include "shardy/dialect/mpmd/transforms/optimize/passes.h.inc"
+
+// Options for the optimize pipeline.
+struct OptimizeOptions {
+  // A list of fragment merge rules.
+  SmallVector<FragmentMergeRule> fragmentMergeRules;
+  // Whether to merge inferred fragments only after scheduling.
+  bool mergeAfterScheduling = false;
+  // Whether to split backward fragments.
+  bool splitBwdFragments = false;
+  // Whether to verify if merging created the right number of scheduling units.
+  bool verifyScheduleUnits = false;
+  // Whether to identify matching forward and backward fragments and clone the
+  // forward fragment immediately.
+  bool applyFragmentRemat = false;
+  // Whether remat fragments can be merged with their consumer fragments.
+  bool mergeRematFragments = false;
+  // Whether to merge forward fragments with backward fragments.
+  bool mergeForwardWithBackward = false;
+  // Whether to absorb inferred fragments into user-defined fragments on
+  // entry-point functions.
+  bool absorbInferredFragmentsOnEntryPointFunction = false;
+  // Whether to clone inferred fragments when merging.
+  bool cloneInferredFragments = false;
+  // The pipeline schedule to use.
+  PipelineSchedule pipelineSchedule = PipelineSchedule::k1F1B;
+};
+
+// Adds the standard set of passes to optimize an MPMD program.
+void addOptimizePipeline(OpPassManager& pm, OptimizeOptions options);
+
+// Register the `-mpmd-optimize-pipeline`.
+void registerOptimizePipeline();
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_PASSES_H_
diff --git a/shardy/dialect/mpmd/transforms/optimize/passes.td b/shardy/dialect/mpmd/transforms/optimize/passes.td
new file mode 100644
index 0000000..38ee3ed
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/passes.td
@@ -0,0 +1,87 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+def RematFragmentPass : PassBase<"mpmd-remat-fragment", "DistributedFunctionPass"> {
+  let summary = "Rematerializes fragments.";
+  let description = [{
+    Finds pairs of fragments (forward+backward) that need to be rematerialized
+    and clones every forward fragment before its backward user, replace all
+    backward uses of values produced by the forward fragment with the cloned
+    counterparts. This can be used for activation rematerialization in pipeline
+    parallelism.
+
+    When `merge_remat_fragments` is true, then we merge the remat fragments into
+    their consumer fragments.
+  }];
+
+  let options = [
+    Option<"mergeRematFragments", "merge-remat-fragments", "bool",
+           /*default=*/"false",
+           "Whether to merge the remat fragments into their consumer "
+           "fragments.">
+  ];
+}
+
+def PipelineSchedulerPass :
+    PassBase<"mpmd-pipeline-scheduler", "DistributedFunctionPass"> {
+  let summary = "Reorders the fragments to obtain a given pipeline schedule.";
+  let description = [{
+    Reorders fragments according to a pipeline schedule. The scheduling
+    algorithm relies on a _happens before_ function that takes two fragments
+    `f1` and `f2` as arguments and checks if `f1` *must be* scheduled before
+    `f2`. This function requires that:
+      - `f1` and `f2` are assigned to the same mesh,
+      - `f1` and `f2` are both scheduling units (i.e., user defined fragments
+      with call_counters defined), and
+      - `f1` does not depend on `f2` and `f2` does not depend on `f1`.
+    And it should be enough to express many state-of-the-art pipeline schedules.
+
+    For each fragment `f1` that must be scheduled before `f2`, the scheduler
+    pass creates a control-dependency from `f1` to `f2`. Then, it applies a
+    topological sort on the module to guarantee that all dependencies are
+    respected (and the program is in a valid SSA form). Finally, the pass
+    removes from the graph any control-dependency introduced.
+  }];
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+
+  let options = [
+    Option<"mustHappenBefore", "must-happen-before",
+           "FragmentComparatorOption", /*default=*/
+           "FragmentComparatorOption::GetBuiltIn(PipelineSchedule::k1F1B)",
+           "A comparator that determines whether a fragment must be scheduled "
+           "before another. Can be parsed from a built-in `PipelineSchedule` "
+           "as follows: `builtin:<schedule-as-string>`.">
+  ];
+}
+
+def SchedulingUnitVerifierPass :
+    PassBase<"mpmd-scheduling-units-verifier", "DistributedFunctionPass"> {
+  let summary = "Verifies if the program contains the required scheduling units.";
+}
+
+// TODO: b/378099938 - Remove this pass once we have a better way to handle
+// transfers while merging fragments. We need this now because having a transfer
+// in between two fragments prevents the merge pass from merging them.
+def MoveTransfersToProducerPass :
+    PassBase<"mpmd-move-transfers-to-producer", "DistributedFunctionPass"> {
+  let summary = "Moves transfers next to their producers.";
+  let description = [{
+    Moves transfers next to their producers: if the operand is a block argument,
+    move the transfer to the beginning of the block, otherwise move it after the
+    defining op.
+  }];
+}
diff --git a/shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.cc b/shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.cc
new file mode 100644
index 0000000..eef6b3b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.cc
@@ -0,0 +1,624 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.h"
+
+#include <algorithm>
+#include <cmath>
+#include <cstddef>
+#include <cstdint>
+#include <functional>
+#include <optional>
+#include <string>
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/optimize/utils.h"
+
+namespace mlir::mpmd {
+
+std::optional<PipelineSchedule> ParsePipelineSchedule(StringRef schedule_str) {
+  if (schedule_str.equals_insensitive("none")) {
+    return PipelineSchedule::kNone;
+  }
+  if (schedule_str.equals_insensitive("1F1B")) {
+    return PipelineSchedule::k1F1B;
+  }
+  if (schedule_str.equals_insensitive("GPipe")) {
+    return PipelineSchedule::kGPipe;
+  }
+  if (schedule_str.equals_insensitive("Circular")) {
+    return PipelineSchedule::kCircular;
+  }
+  if (schedule_str.equals_insensitive("CircularWithReversedBackward")) {
+    return PipelineSchedule::kCircularWithReversedBackward;
+  }
+  if (schedule_str.equals_insensitive("GPipeBut1F1BForLastMesh")) {
+    return PipelineSchedule::kGPipeBut1F1BForLastMesh;
+  }
+  if (schedule_str.equals_insensitive("ZeroBubbleH1")) {
+    return PipelineSchedule::kZeroBubbleH1;
+  }
+  if (schedule_str.equals_insensitive("ZeroBubbleH2ZeroTxLatency")) {
+    return PipelineSchedule::kZeroBubbleH2ZeroTxLatency;
+  }
+  if (schedule_str.equals_insensitive("ZeroBubbleH2HalfTxLatency")) {
+    return PipelineSchedule::kZeroBubbleH2HalfTxLatency;
+  }
+  if (schedule_str.equals_insensitive("ZeroBubbleH2FullTxLatency")) {
+    return PipelineSchedule::kZeroBubbleH2FullTxLatency;
+  }
+  if (schedule_str.equals_insensitive("ParallelPipelinesWithWrapAround")) {
+    return PipelineSchedule::kParallelPipelinesWithWrapAround;
+  }
+
+  return std::nullopt;
+}
+
+std::string ToString(PipelineSchedule schedule) {
+  switch (schedule) {
+    case PipelineSchedule::kNone:
+      return "none";
+    case PipelineSchedule::k1F1B:
+      return "1F1B";
+    case PipelineSchedule::kGPipe:
+      return "GPipe";
+    case PipelineSchedule::kCircular:
+      return "Circular";
+    case PipelineSchedule::kCircularWithReversedBackward:
+      return "CircularWithReversedBackward";
+    case PipelineSchedule::kGPipeBut1F1BForLastMesh:
+      return "GPipeBut1F1BForLastMesh";
+    case PipelineSchedule::kZeroBubbleH1:
+      return "ZeroBubbleH1";
+    case PipelineSchedule::kZeroBubbleH2ZeroTxLatency:
+      return "ZeroBubbleH2ZeroTxLatency";
+    case PipelineSchedule::kZeroBubbleH2HalfTxLatency:
+      return "ZeroBubbleH2HalfTxLatency";
+    case PipelineSchedule::kZeroBubbleH2FullTxLatency:
+      return "ZeroBubbleH2FullTxLatency";
+    case PipelineSchedule::kParallelPipelinesWithWrapAround:
+      return "ParallelPipelinesWithWrapAround";
+  }
+}
+
+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                              FragmentComparatorOption comparator) {
+  if (comparator.schedule) {
+    return os << "builtin:" << ToString(*comparator.schedule);
+  }
+  return os << "custom";
+}
+
+namespace {
+
+// Returns true if `fragment1` must happen before `fragment2` in a 1F1B
+// schedule, or false otherwise.
+// Requires: IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2).
+//
+// Example of schedule obtained:
+//  m0: F0 F1 F2 F3          B0 F4 B1 F5 B2    B3    B4    B5
+//  m1:    F0 F1 F2       B0 F3 B1 F4 B2 F5 B3    B4    B5
+//  m3:       F0 F1    B0 F2 B1 F3 B2 F4 B3 F5 B4    B5
+//  m4:          F0 B0 F1 B1 F2 B2 F3 B3 F4 B4 F5 B5
+// Where Fi stands for Forward stage of call_counter (i.e., microbatch) i and Bi
+// stands for Backward of call_counter i.
+
+// Based on 1F1B scheduling presented in PipeDream-2BW:
+//   https://arxiv.org/abs/2006.09503
+// Though note that this does not give any guarantees w.r.t. staleness.
+bool OneFOneBMustHappenBefore(FragmentOp fragment1, FragmentOp fragment2) {
+  // This guarantees that the transpose counts and call counts of each fragment
+  // are defined.
+  SDY_CHECK(IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2));
+  int64_t call_counter_f1 = *TryToFindCallCounter(fragment1);
+  int64_t call_counter_f2 = *TryToFindCallCounter(fragment2);
+  int64_t transpose_count_f1 = *TryToFindSingleTransposeCount(fragment1);
+  int64_t transpose_count_f2 = *TryToFindSingleTransposeCount(fragment2);
+
+  const int num_meshes = GetNumMeshes(fragment1);
+  const int mesh_id = GetMeshIndex(fragment1);
+
+  // The following two conditions guarantee the forward and backward fragments
+  // are interleaved in the steady state of the pipeline.
+
+  // Example: in mesh/stage 0 of pipeline of depth 4, the backward computation
+  // of microbatch 0 must be scheduled before the forward computation of
+  // microbatch 4: 0 == 4 - 4 + 0.
+  if (transpose_count_f1 == 1 && transpose_count_f2 == 0) {
+    return call_counter_f1 == call_counter_f2 - num_meshes + mesh_id;
+  }
+
+  // Example: in mesh/stage 0 of pipeline of depth 4, the forward computation of
+  // microbatch 5 must be scheduled before the backward computation of
+  // microbatch 2: 5 == 2 + 4 - (0 + 1).
+  if (transpose_count_f1 == 0 && transpose_count_f2 == 1) {
+    return call_counter_f1 == call_counter_f2 + num_meshes - (mesh_id + 1);
+  }
+
+  // If the fragments have the same transpose count, guarantee that the
+  // call_counter ordering is preserved.
+  if (transpose_count_f1 == transpose_count_f2) {
+    return call_counter_f1 < call_counter_f2;
+  }
+  return false;
+}
+
+// Returns true if `fragment1` must happen before `fragment2` in a GPipe
+// schedule, or false otherwise.
+// Requires: IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2).
+// Based on https://arxiv.org/pdf/1811.06965.pdf.
+bool GPipeMustHappenBefore(FragmentOp fragment1, FragmentOp fragment2) {
+  // This guarantees that the transpose counts and call counts of each fragment
+  // are defined.
+  SDY_CHECK(IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2));
+  int call_counter_f1 = *TryToFindCallCounter(fragment1);
+  int call_counter_f2 = *TryToFindCallCounter(fragment2);
+  int transpose_count_f1 = *TryToFindSingleTransposeCount(fragment1);
+  int transpose_count_f2 = *TryToFindSingleTransposeCount(fragment2);
+
+  if (transpose_count_f1 == transpose_count_f2) {
+    return call_counter_f1 < call_counter_f2;
+  }
+  return transpose_count_f1 < transpose_count_f2;
+}
+
+// Returns true if `fragment1` must happen before `fragment2` in a schedule that
+// looks like GPipe in all meshes, except the last one, where we interleave Fwd
+// and Bwd like in 1F1B. The advantage of this schedule is that we can avoid
+// fragment remat in the last mesh.
+// Example:
+//   F0 F1 F2 F3       B0 B1    B2    B3
+//      F0 F1 F2 F3 B0 B1    B2    B3
+//         F0 B0 F1 B1 F2 B2 F3 B3
+//
+// Requires: IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2).
+bool GPipeBut1F1BLastMeshHappenBefore(FragmentOp fragment1,
+                                      FragmentOp fragment2) {
+  const int num_meshes = GetNumMeshes(fragment1);
+  const int mesh_id = GetMeshIndex(fragment1);
+  if (mesh_id == num_meshes - 1) {
+    return OneFOneBMustHappenBefore(fragment1, fragment2);
+  }
+  return GPipeMustHappenBefore(fragment1, fragment2);
+}
+
+// This is the ZeroBubble H1 schedule from: https://arxiv.org/pdf/2401.10241.pdf
+// F0  F1  F2  F3              Bᵃ0 Bʷ0 F4  Bᵃ1 Bʷ1 Bᵃ2 Bʷ2 Bᵃ3 Bʷ3 Bᵃ4 Bʷ4
+//     F0  F1  F2          Bᵃ0 F3  Bᵃ1 Bʷ0 F4  Bᵃ2 Bʷ1 Bᵃ3 Bʷ2 Bᵃ4 Bʷ3 Bʷ4
+//         F0  F1      Bᵃ0 F2  Bᵃ1 F3  Bᵃ2 Bʷ0 F4  Bᵃ3 Bʷ1 Bᵃ4 Bʷ2 Bʷ3 Bʷ4
+//             F0  Bᵃ0 F1  Bᵃ1 F2  Bᵃ2 F3  Bᵃ3 Bʷ0 F4  Bᵃ4 Bʷ1 Bʷ2 Bʷ3 Bʷ4
+// Namely the schedule relies on splitting the backwards pass into two
+// computations: (i) backpropagation (Bᵃ above), and (ii) parameter gradient
+// computation (Bʷ) above. The only small difference is that our splitting of
+// backward fragments will not split the fragment on the last stage, since there
+// are no transfers to other stages there.
+bool ZeroBubbleH1MustHappenBefore(FragmentOp fragment1, FragmentOp fragment2) {
+  // This guarantees that the transpose counts and call counts of each fragment
+  // are defined.
+  SDY_CHECK(IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2));
+  int64_t call_counter_f1 = *TryToFindCallCounter(fragment1);
+  int64_t call_counter_f2 = *TryToFindCallCounter(fragment2);
+  int64_t transpose_count_f1 = *TryToFindSingleTransposeCount(fragment1);
+  int64_t transpose_count_f2 = *TryToFindSingleTransposeCount(fragment2);
+
+  const int num_meshes = GetNumMeshes(fragment1);
+  const int mesh_id = GetMeshIndex(fragment1);
+
+  bool is_wgrad_f1 = IsSplitDropTransferred(fragment1);
+  bool is_wgrad_f2 = IsSplitDropTransferred(fragment2);
+
+  // The following two conditions guarantee the forward and backward fragments
+  // are interleaved in the steady state of the pipeline. They are just like
+  // 1F1B but specialized to actual back-propagation fragments.
+
+  // Clause 1: Ba(i) < F(i + num_meshes - mesh_id)
+  if (transpose_count_f1 == 1 && !is_wgrad_f1 && transpose_count_f2 == 0) {
+    return call_counter_f1 == call_counter_f2 - num_meshes + mesh_id;
+  }
+  // Clause 2: F(i + num_meshes - mesh_id - 1) < Ba(i)
+  if (transpose_count_f1 == 0 && transpose_count_f2 == 1 && !is_wgrad_f2) {
+    return call_counter_f1 == call_counter_f2 + num_meshes - (mesh_id + 1);
+  }
+
+  // The rest of the conditions position the parameter gradient fragments.
+  // Clause 3: Bw(i) < F(i + num_meshes)
+  // e.g. Bw(0) < F(4) above.
+  if (transpose_count_f1 == 1 && (is_wgrad_f1 || mesh_id == 0) &&
+      transpose_count_f2 == 0) {
+    return call_counter_f2 - call_counter_f1 == num_meshes;
+  }
+  // Clause 4: Ba(i + mesh_id) < Bw(i)
+  // e.g.
+  // mesh0:  Ba(0) < Bw(0)
+  // mesh1:  Ba(1) < Bw(0)
+  // mesh2:  Ba(2) < Bw(0)
+  // mesh3:  Ba(3) < Bw(0)
+  if (transpose_count_f1 == 1 && !is_wgrad_f1 && transpose_count_f2 == 1 &&
+      is_wgrad_f2) {
+    return call_counter_f1 - call_counter_f2 == mesh_id;
+  }
+
+  // This is just needed for transitively completing Clauses 3 and 2, needed for
+  // the final phase where there may be no remaining forward to anchor to.
+  // Bw(i) < Ba(i + mesh_id + 1)
+  if (transpose_count_f1 == 1 && is_wgrad_f1 && transpose_count_f2 == 1 &&
+      !is_wgrad_f2) {
+    return call_counter_f2 - call_counter_f1 == mesh_id + 1;
+  }
+
+  return false;
+}
+
+// A function to calculate, for a given mesh, how many forward microbatches
+// need to be streamed in, before we can schedule the first backward.
+using InitFwdPerMeshFn = std::function<int(int)>;
+
+bool ZeroBubbleH2MustHappenBefore(FragmentOp fragment1, FragmentOp fragment2,
+                                  InitFwdPerMeshFn init_fwd_per_mesh) {
+  SDY_CHECK(IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2));
+  int64_t call_counter_f1 = *TryToFindCallCounter(fragment1);
+  int64_t call_counter_f2 = *TryToFindCallCounter(fragment2);
+  int64_t transpose_count_f1 = *TryToFindSingleTransposeCount(fragment1);
+  int64_t transpose_count_f2 = *TryToFindSingleTransposeCount(fragment2);
+
+  const int num_meshes = GetNumMeshes(fragment1);
+  const int mesh_id = GetMeshIndex(fragment1);
+
+  bool is_wgrad_f1 = IsSplitDropTransferred(fragment1);
+  bool is_wgrad_f2 = IsSplitDropTransferred(fragment2);
+
+  // How many fwd we are allowed to stream before entering steady state.
+  int init_fwd = init_fwd_per_mesh(mesh_id);
+  // The ZeroBubbleH2 pipeline is diagonally symmetric (replacing forward with
+  // backwards parameter gradient) so the following quantity is also part of the
+  // schedule invariants below.
+  int complement_init_fwd = init_fwd_per_mesh(num_meshes - mesh_id - 1);
+
+  // Initial phase.
+  // Clause 1: F(i) <= B(_) for i < init_fwd.
+  if (transpose_count_f1 == 0 && transpose_count_f2 == 1 &&
+      call_counter_f1 < init_fwd) {
+    return true;
+  }
+
+  // Clause 2: Ba(i) < F(i + init_fwd)
+  if (transpose_count_f1 == 1 && !is_wgrad_f1 && transpose_count_f2 == 0 &&
+      call_counter_f2 >= init_fwd) {
+    return call_counter_f2 - call_counter_f1 == init_fwd;
+  }
+  // Clause 3: F(i + init_fwd - 1) < Ba(i)
+  if (transpose_count_f1 == 0 && call_counter_f1 >= init_fwd &&
+      transpose_count_f2 == 1 && !is_wgrad_f2) {
+    return call_counter_f1 - call_counter_f2 == init_fwd - 1;
+  }
+
+  // Clause 4: Ba(i + complement_init_fwd - 1) < Bw(i)
+  if (transpose_count_f1 == 1 && !is_wgrad_f1 && transpose_count_f2 == 1 &&
+      is_wgrad_f2) {
+    return call_counter_f1 - call_counter_f2 == complement_init_fwd - 1;
+  }
+  // Clause 5: Bw(i) < Ba(i + complement_init_fwd)
+  if (transpose_count_f1 == 1 && is_wgrad_f1 && transpose_count_f2 == 1 &&
+      !is_wgrad_f2) {
+    return call_counter_f2 - call_counter_f1 == complement_init_fwd;
+  }
+  return false;
+}
+
+// This is a variant of ZeroBubble H2 (https://arxiv.org/pdf/2401.10241.pdf.)
+// The key difference is that the function below is equipped with a parameter
+// `latency_stage_fraction` which specifies, as a float between 0.0f and 1.0f
+// how much time activation forwarding transfers take compared to a stage
+// compute time. For instance, a value 1.0f means that transfers take as much as
+// a whole forward stage; a value 0.0f means that transfers are negligible.
+//
+// The way we use this number is to determine the 'eagerness' of the schedule
+// per mesh, that is the number of forward micro-batches we need to stream in
+// prior to the steady state in order to fully hide the pipeline bubble.
+// We illustrate this with an example below:
+//
+// mesh0: fwd|tx                                  bwd
+// mesh1:       fwd|tx                      bwd|tx
+// mesh2:             fwd|tx          bwd|tx
+// mesh3:                   fwd|bwd|tx
+//        |<-----------------e0------------------>|
+//              |<-----------e1----------->|
+//                    |<-----e2------>|
+//                          |e3|
+//
+// In particular e_i determines this number for each mesh index i. The function
+// below analytically computes these numbers.
+bool LatencyHidingZeroBubbleH2MustHappenBefore(float latency_stage_fraction,
+                                               FragmentOp fragment1,
+                                               FragmentOp fragment2) {
+  SDY_CHECK_GE(latency_stage_fraction, 0.0f);
+  SDY_CHECK_LE(latency_stage_fraction, 1.0f);
+
+  int num_meshes = GetNumMeshes(fragment1);
+
+  // The `init_fwds_per_mesh` returns the e_i in the diagram above, for
+  // every mesh_i. This it the number of forward microbatches that can execute
+  // before the first backwards microbatch can be executed on this mesh.
+  auto init_fwds_per_mesh = [num_meshes, latency_stage_fraction](int mesh_id) {
+    // The number of transfers from the beginning until the first backward
+    // fragment can execute on mesh_id, see the diagram above. We call this the
+    // "initial" path of the first microbatch in the pipeline.
+    float num_init_transfers = 2.0f * (num_meshes - mesh_id - 1);
+    // How much compute has happened in that initial first microbatch path, i.e.
+    // until the point where the first backward fragment can execute on mesh_id.
+    // The assumption that time(fwd) == time(bwd) (NB: this is just the backprop
+    // bwd) may need to be revisited for real use cases.
+    float num_init_compute = 2.0f * (num_meshes - mesh_id) - 1.0f;
+    return std::floor(num_init_compute +
+                      num_init_transfers * latency_stage_fraction);
+  };
+  return ZeroBubbleH2MustHappenBefore(fragment1, fragment2, init_fwds_per_mesh);
+}
+
+// Returns true if `fragment1` must happen before `fragment2` in a parallel
+// pipeline with wrap-around situation.
+//
+// E.g. if we have 3 meshes and 3 microbatches. Note that the microbatch starts
+// on different meshes.
+//
+// F1             F2    F3      <-- Mesh 1
+//    F1    F2             F3   <-- Mesh 2
+//       F1    F2    F3         <-- Mesh 3
+// ~~>
+// F1 F3 F2
+// F2 F1 F3
+// F3 F2 F1
+//
+// Requires:
+// - IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2)
+// - Only forward fragments (in theory we could support backward fragments)
+// - Mesh names are mesh1, mesh2, ..., mesh{n} and call_counter goes from 1 to
+// n. We use this information for scheduling. Note that it's also ok to start
+// from 0.
+// - The entrypoint for mesh{i} is call_counter {i}
+//
+// Note that for each mesh, the order of fragments is the array
+// [F{n}, F{n-1}, ..., F{1}] rotated such that
+// the leading fragment is F{mesh_index}.
+bool ParallelPipelinesWithWrapAroundMustHappenBefore(FragmentOp fragment1,
+                                                     FragmentOp fragment2) {
+  // This guarantees that the transpose counts and call counts of each fragment
+  // are defined.
+  SDY_CHECK(IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2));
+  // Only allowed for forwards for now.
+  SDY_CHECK(IsForwardFragment(fragment1));
+  SDY_CHECK(IsForwardFragment(fragment2));
+
+  int64_t call_counter_f1 = *TryToFindCallCounter(fragment1);
+  int64_t call_counter_f2 = *TryToFindCallCounter(fragment2);
+  SDY_CHECK_NE(call_counter_f1, call_counter_f2)
+      << "Should not have duplicate call counter.";
+
+  int64_t mesh_num = 0;
+  SDY_CHECK(llvm::to_integer(fragment1.getMeshName().drop_until(
+                                 [](char c) { return llvm::isDigit(c); }),
+                             mesh_num));
+  // The entrypoint to mesh{i} is call_counter {i}, so this always happens
+  // before.
+  if (call_counter_f1 == mesh_num || call_counter_f2 == mesh_num) {
+    return call_counter_f1 == mesh_num;
+  }
+
+  // `mesh_num` is the pivot. If both call_counters are on the same side of
+  // the pivot, we flip the order. But if they are on different
+  // sides, then we take the order as per normal.
+  if ((call_counter_f1 > mesh_num && call_counter_f2 > mesh_num) ||
+      (call_counter_f1 < mesh_num && call_counter_f2 < mesh_num)) {
+    return call_counter_f1 > call_counter_f2;
+  }
+  return call_counter_f1 < call_counter_f2;
+}
+
+// Returns true if (f1, f2) are in (lexicographic) order, and false otherwise.
+// If ascending is true then we compare the elements using < else >.
+// Requires: f1 != f2.
+bool LexicographicCompare(ArrayRef<int> f1, ArrayRef<int> f2, bool ascending) {
+  auto in_order = [ascending](int a, int b) {
+    return ascending ? a < b : a > b;
+  };
+  for (auto [f1_val, f2_val] : llvm::zip(f1, f2)) {
+    if (in_order(f1_val, f2_val)) {
+      return true;
+    }
+    if (f1_val == f2_val) continue;
+
+    return false;
+  }
+  SDY_CHECK(false) << "Unreachable. We expect f1 != f2.";
+}
+
+bool CircularMustHappenBeforeBase(FragmentOp fragment1, FragmentOp fragment2,
+                                  bool reversed_backward) {
+  // This guarantees that the transpose counts and call counts of each fragment
+  // are defined.
+  SDY_CHECK(IsSchedulingUnit(fragment1) && IsSchedulingUnit(fragment2));
+  const int call_counter_f1 = *TryToFindCallCounter(fragment1);
+  const int call_counter_f2 = *TryToFindCallCounter(fragment2);
+  const int transpose_count_f1 = *TryToFindSingleTransposeCount(fragment1);
+  const int transpose_count_f2 = *TryToFindSingleTransposeCount(fragment2);
+
+  if (!fragment1.getStageIdAttr() || !fragment2.getStageIdAttr()) {
+    // Giving up. We cannot schedule for circular pipelining without stages.
+    SDY_LOG(ERROR) << "Cannot schedule for circular pipelining without stages.";
+    return false;
+  }
+
+  const int stage_f1 = fragment1.getStageIdAttr().getInt();
+  const int stage_f2 = fragment2.getStageIdAttr().getInt();
+
+  const int num_meshes = GetNumMeshes(fragment1);
+
+  if (transpose_count_f1 != transpose_count_f2) {
+    // Forward fragments always happen before backward fragments.
+    return transpose_count_f1 < transpose_count_f2;
+  }
+
+  // transpose_count_f1 == transpose_count_f2, i.e., they're both forward *or*
+  // both backward.
+
+  // Given N meshes, with Circular pipelining, we want to execute N forward
+  // fragments of the first logical stage assigned to a mesh, followed by N
+  // forward fragments of the second logical stage assigned to the same mesh, as
+  // so on, until all microbatches of all logical stages have been scheduled.
+  // And then repeat this process for the backward fragments. We call a
+  // subsequence of N fragments a *phase*.
+  //
+  // E.g., say you have a a model partitioned across 2 meshes (physical stages)
+  // and 4 (logical) stages as follows (bwd pass omitted for simplicity):
+  //
+  // mesh0: A   C
+  // mesh1:   B   D
+  //
+  // With 4 microbatches, we would get the following schedule:
+  //
+  // A1 A2 C1 C2 A3 A4 C3 C4
+  //    B1 B2 D1 D2 B3 B4 D4 D4
+  //
+  // In a schedule of 2 meshes, we have phases of length 2.
+  // The pipe `|` operator shows us the different phases of the schedule:
+  // A1 A2 C1 C2 | A3 A4 C3 C4
+  //    B1 B2 D1 D2 | B3 B4 D4 D4
+  //
+  // The phase of a fragment is defined as follows:
+  const int phase_f1 = call_counter_f1 / num_meshes;
+  const int phase_f2 = call_counter_f2 / num_meshes;
+
+  SmallVector<int> f1 = {phase_f1, stage_f1, call_counter_f1};
+  SmallVector<int> f2 = {phase_f2, stage_f2, call_counter_f2};
+
+  // Fragments are first sorted by phase. If any two fragments are in the same
+  // phase, then we sort them by stage. If they're in the same stage, then we
+  // sort them by call_counter (or conceptually by call_counter % num_meshes).
+
+  // Forward fragments.
+  if (transpose_count_f1 == 0) {
+    return LexicographicCompare(f1, f2, /*ascending=*/true);
+  }
+
+  // Backward fragments.
+  if (reversed_backward) {
+    return LexicographicCompare(f1, f2, /*ascending=*/false);
+  }
+
+  // Backward fragments with call_counter (and phase) in ascending order.
+  // Naturally, the stage is in descending order (so we swap them before
+  // comparison).
+  std::swap(f1[1], f2[1]);
+  return LexicographicCompare(f1, f2, /*ascending=*/true);
+}
+
+bool CircularWithReversedBackwardMustHappenBefore(FragmentOp fragment1,
+                                                  FragmentOp fragment2) {
+  return CircularMustHappenBeforeBase(fragment1, fragment2,
+                                      /*reversed_backward=*/true);
+}
+
+bool CircularMustHappenBefore(FragmentOp fragment1, FragmentOp fragment2) {
+  return CircularMustHappenBeforeBase(fragment1, fragment2,
+                                      /*reversed_backward=*/false);
+}
+
+}  // namespace
+
+FragmentComparator BuiltinFragmentComparator(PipelineSchedule schedule) {
+  switch (schedule) {
+    case PipelineSchedule::kNone: {
+      return [](FragmentOp, FragmentOp) { return false; };
+    }
+    case PipelineSchedule::k1F1B: {
+      return OneFOneBMustHappenBefore;
+    }
+    case PipelineSchedule::kGPipe: {
+      return GPipeMustHappenBefore;
+    }
+    case PipelineSchedule::kGPipeBut1F1BForLastMesh: {
+      return GPipeBut1F1BLastMeshHappenBefore;
+    }
+    case PipelineSchedule::kZeroBubbleH1: {
+      return ZeroBubbleH1MustHappenBefore;
+    }
+    case PipelineSchedule::kZeroBubbleH2ZeroTxLatency: {
+      return [](FragmentOp f1, FragmentOp f2) {
+        return LatencyHidingZeroBubbleH2MustHappenBefore(0.0f, f1, f2);
+      };
+    }
+    case PipelineSchedule::kZeroBubbleH2HalfTxLatency: {
+      return [](FragmentOp f1, FragmentOp f2) {
+        return LatencyHidingZeroBubbleH2MustHappenBefore(0.5f, f1, f2);
+      };
+    }
+    case PipelineSchedule::kZeroBubbleH2FullTxLatency: {
+      return [](FragmentOp f1, FragmentOp f2) {
+        return LatencyHidingZeroBubbleH2MustHappenBefore(1.0f, f1, f2);
+      };
+    }
+    case PipelineSchedule::kParallelPipelinesWithWrapAround: {
+      return ParallelPipelinesWithWrapAroundMustHappenBefore;
+    }
+    case PipelineSchedule::kCircularWithReversedBackward: {
+      return CircularWithReversedBackwardMustHappenBefore;
+    }
+    case PipelineSchedule::kCircular: {
+      return CircularMustHappenBefore;
+    }
+  }
+}
+
+}  // namespace mlir::mpmd
+
+namespace llvm::cl {
+
+using ::mlir::mpmd::FragmentComparator;
+using ::mlir::mpmd::FragmentComparatorOption;
+
+template class basic_parser<FragmentComparatorOption>;
+
+bool parser<FragmentComparatorOption>::parse(Option& opt, StringRef,
+                                             StringRef arg,
+                                             FragmentComparatorOption& value) {
+  if (auto schedule = mlir::mpmd::ParsePipelineSchedule(arg)) {
+    value.value = mlir::mpmd::BuiltinFragmentComparator(*schedule);
+    value.schedule = schedule;
+    return true;
+  }
+  return opt.error("unknown PropagationSchedule: " + arg);
+}
+
+void parser<FragmentComparatorOption>::printOptionDiff(
+    const Option& opt, const FragmentComparatorOption& value,
+    const OptVal& defaultValue, size_t globalWidth) const {
+  printOptionName(opt, globalWidth);
+  outs() << "= " << value << "\n";
+}
+
+void parser<FragmentComparatorOption>::anchor() {}
+
+}  // namespace llvm::cl
diff --git a/shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.h b/shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.h
new file mode 100644
index 0000000..c387fd1
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.h
@@ -0,0 +1,98 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_PIPELINE_SCHEDULE_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_PIPELINE_SCHEDULE_H_
+
+#include <cstddef>
+#include <functional>
+#include <optional>
+#include <string>
+
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+enum class PipelineSchedule {
+  kNone,
+  k1F1B,
+  kGPipe,
+  kCircular,
+  kCircularWithReversedBackward,
+  kGPipeBut1F1BForLastMesh,
+  kZeroBubbleH1,
+  kZeroBubbleH2ZeroTxLatency,
+  kZeroBubbleH2HalfTxLatency,
+  kZeroBubbleH2FullTxLatency,
+  kParallelPipelinesWithWrapAround,
+};
+
+// Parses the given string as a `PipelineSchedule` or return std::nullopt if
+// parsing failed.
+std::optional<PipelineSchedule> ParsePipelineSchedule(StringRef schedule_str);
+
+// Returns a string representation of the given `schedule`.
+std::string ToString(PipelineSchedule schedule);
+
+using FragmentComparator =
+    std::function<bool(FragmentOp, FragmentOp)>;
+
+// Returns a fragment comparator for the given pipeline schedule.
+FragmentComparator BuiltinFragmentComparator(PipelineSchedule schedule);
+
+// A `FragmentComparator` option with a custom parser/printer.
+struct FragmentComparatorOption {
+  FragmentComparator value;
+  // If the comparator was derived from a built-in `PipelineSchedule`, this
+  // will contain the schedule, otherwise it will be `std::nullopt`.
+  std::optional<PipelineSchedule> schedule;
+
+  static FragmentComparatorOption GetBuiltIn(PipelineSchedule schedule) {
+    return FragmentComparatorOption{BuiltinFragmentComparator(schedule)};
+  }
+};
+
+// This is needed by MLIR, however we can't really print anything meaningful
+// about a specific instance.
+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,
+                              FragmentComparatorOption comparator);
+
+}  // namespace mlir::mpmd
+
+namespace llvm::cl {
+
+extern template class basic_parser<mlir::mpmd::FragmentComparatorOption>;
+
+template <>
+class parser<mlir::mpmd::FragmentComparatorOption>
+    : public basic_parser<mlir::mpmd::FragmentComparatorOption> {
+ public:
+  parser(Option& opt) : basic_parser(opt) {}
+  bool parse(Option& opt, StringRef argName, StringRef arg,
+             mlir::mpmd::FragmentComparatorOption& value);
+  StringRef getValueName() const override { return "fragment-comparator"; }
+  void printOptionDiff(const Option& opt,
+                       const mlir::mpmd::FragmentComparatorOption& value,
+                       const OptVal& defaultValue, size_t globalWidth) const;
+  void anchor() override;
+};
+
+}  // namespace llvm::cl
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_PIPELINE_SCHEDULE_H_
diff --git a/shardy/dialect/mpmd/transforms/optimize/remat_fragment.cc b/shardy/dialect/mpmd/transforms/optimize/remat_fragment.cc
new file mode 100644
index 0000000..3b4f2ef
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/remat_fragment.cc
@@ -0,0 +1,146 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <string>
+
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/optimize/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/optimize/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_REMATFRAGMENTPASS
+#include "shardy/dialect/mpmd/transforms/optimize/passes.h.inc"
+
+namespace {
+
+// Rematerializes one forward_fragment matched with a backward_fragment.
+// When `merge_remat_fragments=true`, we merge the rematerialized fragment with
+// its backward consumer. We mark the resulting remat fragment (either merged or
+// unmerged), as this is often useful for debugging.
+//
+// Requires: CanRemat(forward_fragment, backward_fragment).
+void RematFragment(IRRewriter& rewriter, FragmentOp forward_fragment,
+                   FragmentOp backward_fragment, bool merge_remat_fragments) {
+  SDY_CHECK(CanRemat(forward_fragment, backward_fragment));
+  rewriter.setInsertionPoint(backward_fragment);
+  // Clone the forward fragment.
+  FragmentOp remat_fragment =
+      cast<FragmentOp>(rewriter.clone(*forward_fragment));
+  rewriter.replaceUsesWithIf(
+      forward_fragment.getResults(), remat_fragment.getResults(),
+      [&](OpOperand& use) { return use.getOwner() == backward_fragment; });
+  if (merge_remat_fragments) {
+    FragmentOp merged_fragment = MergeRegionOps(
+        remat_fragment, backward_fragment, rewriter,
+        /*num_static_args=*/0, /*replace_producer_use_in_consumer_block=*/
+        [](OpOperand&, Value) {
+          SDY_CHECK(false) << "Fragment ops shouldn't have free variables";
+        },
+        GetFragmentOriginUnion(remat_fragment, backward_fragment, rewriter),
+        backward_fragment.getMeshNameAttr(),
+        backward_fragment.getStageIdAttr());
+    MarkAsRemat(merged_fragment, rewriter);
+  } else {
+    MarkAsRemat(remat_fragment, rewriter);
+  }
+}
+
+// Iterates over a function, identify forward and backward fragment pairs that
+// need rematerialization and rematerializes one by one. If there are multiple
+// backward fragments matching a forward fragment, remat all of them.
+void RematFragments(IRRewriter& rewriter, func::FuncOp func_op,
+                    bool merge_remat_fragments) {
+  SmallVector<FragmentOp> all_forward_fragments;
+  for (Operation& op : func_op.getOps()) {
+    if (auto fragment = dyn_cast<FragmentOp>(&op);
+        fragment && IsForwardFragment(fragment)) {
+      all_forward_fragments.push_back(fragment);
+    }
+  }
+
+  for (FragmentOp forward_fragment : all_forward_fragments) {
+    // Get the users of forward_fragment that can be rematerialized and sort
+    // them by their program order. The sorting is needed because `getUsers()`
+    // does not guarantee returning the users in program order and when a user
+    // that is later in the program appear first and gets rematted first (by
+    // `RematFragment`), it prevents the earlier users to match and remat
+    // correctly.
+    DenseSet<Operation*> users;
+    for (Operation* user : forward_fragment->getUsers()) {
+      if (auto backward_fragment = dyn_cast<FragmentOp>(user);
+          backward_fragment && CanRemat(forward_fragment, backward_fragment)) {
+        users.insert(user);
+      }
+    }
+    if (users.size() > 1 && SDY_VLOG_IS_ON(1)) {
+      std::string fragment_metadata;
+      llvm::raw_string_ostream fragment_metadata_stream(fragment_metadata);
+      forward_fragment.printFragmentMetadata(fragment_metadata_stream);
+      SDY_LOG(INFO)
+          << "A forward fragment matched multiple backward fragments. "
+             "metadata= "
+          << fragment_metadata;
+    }
+
+    // In the case where multiple backward fragments match the same forward
+    // fragment, we have a few options:
+    // 1. Only add a remat fragment in front of the first backward fragment
+    // and replace uses for the forward fragment in the other backward
+    // fragments or
+    // 2. Add a remat fragment in front of all backward fragments.
+    // Theoretically we could also do anything in between, e.g., add a remat
+    // fragment in front of m out of the n matched backward fragments (where 1
+    // <= m <= n). All of these options are valid strategies. The tradeoff is
+    // that the more remat fragments we add, the more we trade runtime for
+    // memory. We are choosing option 2 because it's easier to implement. We
+    // may support both options later to have more control over remat.
+    for (Operation* user : users) {
+      auto backward_fragment = dyn_cast<FragmentOp>(user);
+      RematFragment(rewriter, forward_fragment, backward_fragment,
+                    merge_remat_fragments);
+    }
+  }
+}
+
+class RematFragmentPass
+    : public impl::RematFragmentPassBase<RematFragmentPass> {
+  using RematFragmentPassBase::RematFragmentPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    MLIRContext* context = func_op->getContext();
+    IRRewriter rewriter(context);
+    RematFragments(rewriter, func_op, mergeRematFragments);
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/optimize/scheduler.cc b/shardy/dialect/mpmd/transforms/optimize/scheduler.cc
new file mode 100644
index 0000000..722a65f
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/scheduler.cc
@@ -0,0 +1,273 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/optimize/scheduler.h"
+
+#include <algorithm>
+#include <cstdint>
+#include <optional>
+
+#include "mlir/Analysis/TopologicalSortUtils.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassRegistry.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/Passes.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"
+#include "shardy/dialect/mpmd/transforms/optimize/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.h"
+#include "shardy/dialect/mpmd/transforms/optimize/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_PIPELINESCHEDULERPASS
+#define GEN_PASS_DEF_SCHEDULINGUNITVERIFIERPASS
+#define GEN_PASS_DEF_MOVETRANSFERSTOPRODUCERPASS
+#include "shardy/dialect/mpmd/transforms/optimize/passes.h.inc"
+
+namespace {
+
+using ::mlir::func::FuncOp;
+
+// Adds a control dependency in the graph so that `fragment2` depends on
+// `fragment1`.
+// NOTE: this creates an ill-formed fragment.
+void AddControlDependency(FragmentOp fragment1, FragmentOp fragment2,
+                          DenseMap<FragmentOp, int>& ctrl_dependency_counter) {
+  // We add a new operand at the end.
+  int operand_index = fragment2.getNumOperands();
+  fragment2->insertOperands(operand_index, {fragment1->getResult(0)});
+  ctrl_dependency_counter[fragment2] += 1;
+}
+
+// Removes all control dependencies added, so that all fragments are well-formed
+// again.
+void RemoveAllControlDependencies(
+    DenseMap<FragmentOp, int>& ctrl_dependency_counter) {
+  for (auto& [fragment, counter] : ctrl_dependency_counter) {
+    const int start_index = fragment->getNumOperands() - counter;
+    fragment->eraseOperands(start_index, counter);
+  }
+}
+
+class PipelineSchedulerPass
+    : public impl::PipelineSchedulerPassBase<PipelineSchedulerPass> {
+  using PipelineSchedulerPassBase::PipelineSchedulerPassBase;
+
+ private:
+  void runOnFunc(FuncOp func_op) override {
+    if (!IsMpmdFunction(func_op)) return;
+
+    // 1. Collect all fragments.
+    std::vector<FragmentOp> all_fragments;
+    for (Operation& op : func_op.getOps()) {
+      if (auto fragment = dyn_cast<FragmentOp>(&op);
+          fragment && IsSchedulingUnit(fragment)) {
+        all_fragments.push_back(fragment);
+      }
+    }
+
+    // A map to keep track of all control dependencies added to fragments.
+    // For each fragment, we keep track of number of control-dependencies added.
+    // Once we have reordered all the fragments, we can then use this
+    // information to remove any control-dependencies from the program.
+    DenseMap<FragmentOp, int> ctrl_dependencies;
+
+    // 2. Add control dependencies between some pairs of fragments.
+    int count_control_dependencies = 0;
+    for (FragmentOp fragment1 : all_fragments) {
+      for (FragmentOp fragment2 : all_fragments) {
+        if (fragment1 == fragment2) continue;
+
+        if (fragment1.getMeshName() != fragment2.getMeshName()) continue;
+
+        // TODO(jupvfranco): we typically expect to see at least 2 * num_stages
+        // * num_microbatches. For high numbers of stages or microbatches this
+        // may become slow. Consider pre-computing a reachability matrix.
+        // Alternatively, consider removing this check completely and define a
+        // post-reorder check to verify that for any control-dependency added,
+        // the source appears before the destination of the dependency.
+        if (TargetDependsOnSourceOp(fragment1, fragment2) ||
+            TargetDependsOnSourceOp(fragment2, fragment1)) {
+          continue;
+        }
+
+        if (mustHappenBefore.value(fragment1, fragment2)) {
+          AddControlDependency(fragment1, fragment2, ctrl_dependencies);
+          count_control_dependencies++;
+        }
+      }
+    }
+    SDY_LOG(INFO) << "Introduced " << count_control_dependencies
+                  << " control dependencies for scheduling\n";
+
+    // 3. Sort the graph topologically to guarantee that all dependencies are
+    // respected.
+    sortTopologically(&func_op.getBody().front());
+
+    // 4. Remove the inserted control-dependencies.
+    RemoveAllControlDependencies(ctrl_dependencies);
+  }
+};
+
+// Returns the number of microbatches in the program.
+// TODO(jupvfranco): This code assumes that microbatching is zero- or one-
+// based. Can we generalize this?
+uint32_t GetNumMicrobatches(FuncOp func_op) {
+  uint32_t max_call_counter = 0;
+  bool is_zero_based = false;
+  func_op.walk([&max_call_counter, &is_zero_based](FragmentOp fragment) {
+    if (auto call_counter = TryToFindCallCounter(fragment)) {
+      if (*call_counter == 0) {
+        is_zero_based = true;
+      }
+      max_call_counter = std::max(max_call_counter, *call_counter);
+    }
+  });
+  return max_call_counter + (is_zero_based ? 1 : 0);
+}
+
+class SchedulingUnitVerifierPass
+    : public impl::SchedulingUnitVerifierPassBase<SchedulingUnitVerifierPass> {
+  using SchedulingUnitVerifierPassBase::SchedulingUnitVerifierPassBase;
+
+ private:
+  void runOnFunc(FuncOp func_op) override {
+    if (!IsMpmdFunction(func_op)) {
+      return;
+    }
+
+    const uint32_t num_microbatches = GetNumMicrobatches(func_op);
+    if (num_microbatches == 0) {
+      SDY_LOG(WARNING)
+          << "Function is not microbatched and therefore cannot be "
+             "rescheduled.";
+      // We exit instead of emitting an error so that this won't affect init
+      // functions that are typically not microbatched.
+      return;
+    }
+
+    // Check if every mesh has `num_microbatches` scheduling units, half of them
+    // forward and the other half backward.
+    // TODO(jupvfranco): This works for the simple schedules we support now, but
+    // we need to revisit this logic.
+    for (NamedMeshAttr mesh : GetSchedulableMeshes(func_op)) {
+      int count_fwd = 0, count_bwd = 0;
+      for (Operation& op : func_op.getOps()) {
+        auto fragment = dyn_cast<FragmentOp>(&op);
+        if (!fragment || !IsSchedulingUnit(fragment) ||
+            fragment.getMeshName() != mesh.getName()) {
+          continue;
+        }
+        if (*TryToFindSingleTransposeCount(fragment) == 0) {
+          count_fwd++;
+        } else {
+          count_bwd++;
+        }
+      }
+      if (count_fwd != num_microbatches) {
+        func_op.emitWarning("Number of forward scheduling units in mesh ")
+            << mesh.getName() << " does not match expected number for "
+            << num_microbatches << " microbatches. Got " << count_fwd << ".";
+      }
+
+      if (count_bwd != num_microbatches) {
+        func_op.emitWarning("Number of backward scheduling units in mesh ")
+            << mesh.getName() << " does not match expected number for "
+            << num_microbatches << " microbatches. Got " << count_bwd << ".";
+      }
+    }
+  }
+};
+
+class MoveTransfersToProducerPass
+    : public impl::MoveTransfersToProducerPassBase<
+          MoveTransfersToProducerPass> {
+  using MoveTransfersToProducerPassBase::MoveTransfersToProducerPassBase;
+
+ private:
+  void runOnFunc(FuncOp func) override {
+    IRRewriter rewriter(func.getContext());
+    func.walk([&](TransferOp transfer) {
+      if (auto arg = dyn_cast<BlockArgument>(transfer.getOperand())) {
+        rewriter.moveOpBefore(transfer, arg.getOwner(),
+                              arg.getOwner()->begin());
+      } else {
+        rewriter.moveOpAfter(transfer, transfer.getOperand().getDefiningOp());
+      }
+    });
+  }
+};
+
+}  // namespace
+
+void AddSchedulingPass(
+    OpPassManager& pm, PipelineSchedule pipeline_schedule,
+    std::optional<FragmentComparator> override_must_happen_before) {
+  PipelineSchedulerPassOptions options;
+  options.mustHappenBefore.value = override_must_happen_before.value_or(
+      BuiltinFragmentComparator(pipeline_schedule));
+  if (!override_must_happen_before) {
+    options.mustHappenBefore.schedule = pipeline_schedule;
+    SDY_LOG(INFO) << "Reordering computation for "
+                  << ToString(pipeline_schedule) << " schedule.";
+  }
+  pm.addNestedPass<FuncOp>(createPipelineSchedulerPass(options));
+}
+
+void AddSchedulingPreprocessingPasses(OpPassManager& pm,
+                                      bool split_bwd_fragments,
+                                      bool verify_schedule_units) {
+  // The following seems like a good thing to always do, to keep the module
+  // more tidy and merged, even if we are not going to actually do any
+  // scheduling.
+  // Move transfers to right after their producers. Without this pass, if we
+  // have a producer fragment followed by transfers, then a consumer fragment,
+  // even if the operands of the transfers are from a different producer
+  // fragment, we are not able to merge the producer and consumer fragments.
+  // This pass moves the transfers to right after the producer, which allows
+  // the merge pass to do its job.
+  pm.addNestedPass<FuncOp>(createMoveTransfersToProducerPass());
+  pm.addNestedPass<FuncOp>(
+      createMergeUserDefinedFragmentsIntoSchedulingUnitsPass());
+  if (verify_schedule_units) {
+    pm.addNestedPass<FuncOp>(createSchedulingUnitVerifierPass());
+  }
+
+  // TODO(dvytin): Run split_bwd_fragments independently of the schedule.
+  //
+  // Furthermore, we now do the split after verification, which ensures that
+  // the generic verification code we have still works. But we should consider
+  // defining schedule-specific verification conditions (and even passes to
+  // prepare the module for a given schedule.)
+  // TODO(dvytin): Investigate how to define schedule-specific verification.
+  if (split_bwd_fragments) {
+    pm.addNestedPass<FuncOp>(createSplitBwdFragmentsPass());
+    // TODO(jupvfranco): Do we really need canonicalizations here? Tests seem to
+    // fail without it.
+    pm.addPass(createCanonicalizerPass(
+        GreedyRewriteConfig().setRegionSimplificationLevel(
+            GreedySimplifyRegionLevel::Disabled)));
+    pm.addNestedPass<FuncOp>(createFragmentDcePass());
+  }
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/optimize/scheduler.h b/shardy/dialect/mpmd/transforms/optimize/scheduler.h
new file mode 100644
index 0000000..0352515
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/scheduler.h
@@ -0,0 +1,46 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_SCHEDULER_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_SCHEDULER_H_
+
+#include <optional>
+
+#include "mlir/Pass/PassManager.h"
+#include "shardy/dialect/mpmd/transforms/optimize/pipeline_schedule.h"
+
+namespace mlir::mpmd {
+
+// Adds scheduling pass. If an `override_must_happen_before` fragment comparator
+// is passed in then we ignore any flag-based schedule and use this comparator
+// instead.
+void AddSchedulingPass(OpPassManager& pm, PipelineSchedule pipeline_schedule,
+                       std::optional<FragmentComparator>
+                           override_must_happen_before = std::nullopt);
+
+// Adds all passes needed for pipeline scheduling. This includes merge of
+// fragments into scheduling units and verification of scheduling units.
+//
+// When `split_bwd_fragments` is true, then we split backward fragments into
+// a fragment whose results are transferred, and one that isn't. This is so that
+// we can execute the transfers earlier (e.g. as per Near-Zero Bubble
+// Pipeline).
+void AddSchedulingPreprocessingPasses(mlir::OpPassManager& pm,
+                                      bool split_bwd_fragments,
+                                      bool verify_schedule_units);
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_SCHEDULER_H_
diff --git a/shardy/dialect/mpmd/transforms/optimize/test/BUILD b/shardy/dialect/mpmd/transforms/optimize/test/BUILD
new file mode 100644
index 0000000..ed5f1fd
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/test/BUILD
@@ -0,0 +1,21 @@
+# Lit tests for the MPMD optimize passes.
+
+load("//shardy:lit.bzl", "glob_lit_tests")
+
+package(default_visibility = ["//visibility:public"])
+
+filegroup(
+    name = "test_data",
+    testonly = True,
+    data = [
+        "//shardy/tools:mpmd_opt",
+        "@llvm-project//llvm:FileCheck",
+    ],
+)
+
+glob_lit_tests(
+    name = "all_tests",
+    data = [":test_data"],
+    driver = "@llvm-project//mlir:run_lit.sh",
+    test_file_exts = ["mlir"],
+)
diff --git a/shardy/dialect/mpmd/transforms/optimize/test/fragment_remat.mlir b/shardy/dialect/mpmd/transforms/optimize/test/fragment_remat.mlir
new file mode 100644
index 0000000..8fa996a
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/test/fragment_remat.mlir
@@ -0,0 +1,194 @@
+// RUN: mpmd_opt %s -mpmd-remat-fragment 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @should_remat_if_all_conditions_met
+func.func @should_remat_if_all_conditions_met(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // Forward fragment's result is used by the backward fragment below and
+  // they have matching call_counters. So we should insert a remat before the backward fragment.
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  // CHECK-NEXT: mpmd.return
+  // CHECK-NEXT: }
+  %forward_result = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // Another fragment is between the forward fragment above and the backward fragment below. The remat fragment should be inserted right before the backward fragment.
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f2"]> (%arg0)
+  // CHECK-NEXT: mpmd.return
+  // CHECK-NEXT: }
+  %another_forward_result = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 2 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // Backward fragment.
+  // CHECK-NEXT: %[[REMAT_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {call_counter = 1 : ui32, remat}
+  // CHECK-NEXT: mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%[[REMAT_RESULT]])
+  %backward_result = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%forward_result) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  return %backward_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_not_remat_if_result_of_forward_not_used_by_backward_fragment
+func.func @should_not_remat_if_result_of_forward_not_used_by_backward_fragment(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // Forward fragment's result is not used by the backward fragment below.
+  // So no remat should happen.
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  %forward_result = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // A fragment that goes between the forward and backward fragments.
+  %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // Backward fragment.
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f2"(1)]> (%arg0)
+  // CHECK-NOT: {remat}
+  %backward_result = mpmd.fragment<mesh="m1", origin=["f2"(1)]> (%arg0) {call_counter = 1 : ui32}  (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  return %backward_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_not_remat_if_call_counters_mismatch
+func.func @should_not_remat_if_call_counters_mismatch(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // The call_counter of the forward and backward operations are different.
+  // So no remat should happen.
+  // CHECK: %[[FORWARD_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  %forward_result = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32}  (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // A fragment that goes between the forward and backward fragments.
+  %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // Backward fragment.
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%[[FORWARD_RESULT]])
+  // CHECK-NOT: {remat}
+  %backward_result = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%forward_result) {call_counter = 100 : ui32}  (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  return %backward_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_not_remat_if_not_backward_fragment
+func.func @should_not_remat_if_not_backward_fragment(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // The second fragment is not a backward fragment (transpose count is not 1).
+  // So no remat should happen.
+  // CHECK: %[[FORWARD_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  %forward_result = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32}  (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"(100)]> (%[[FORWARD_RESULT]])
+  // CHECK-NOT: {remat}
+  %non_backward_result = mpmd.fragment<mesh="m1", origin=["f1"(100)]> (%forward_result) {call_counter = 100 : ui32}  (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  return %non_backward_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_not_remat_if_backward_immediately_after_forward
+func.func @should_not_remat_if_backward_immediately_after_forward(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // Forward fragment.
+  // CHECK: %[[FORWARD_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  %forward_result = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // Backward fragment.
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%[[FORWARD_RESULT]])
+  // CHECK-NOT: {remat}
+  %backward_result = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%forward_result) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  return %backward_result : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @should_remat_both_if_two_matching_backward_fragments
+func.func @should_remat_both_if_two_matching_backward_fragments(%arg0: !mesh_1_tensor_4_8_f32)
+-> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // Forward fragment.
+  // CHECK: %[[FORWARD_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  %forward_result = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+     mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // A fragment that goes between the forward and backward fragments.
+  %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // There are two backward fragments matching the forward fragment.
+  // We should do remat for both.
+  // CHECK: %[[REMAT_RESULT1:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {call_counter = 1 : ui32, remat}
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%[[REMAT_RESULT1]])
+  %backward_result_0 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%forward_result) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK: %[[REMAT_RESULT2:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {call_counter = 1 : ui32, remat}
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%[[REMAT_RESULT2]])
+  %backward_result_1 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%forward_result) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.abs %arg2: tensor<4x8xf32>
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+ return %backward_result_0, %backward_result_1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32}
+
+// CHECK-LABEL: func @should_only_remat_once_if_multiple_results_of_forward_used_by_backward
+func.func @should_only_remat_once_if_multiple_results_of_forward_used_by_backward(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+  // Both of forward fragment's results is used by the backward fragment below.
+  // We should only insert one remat fragment before the backward fragment.
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  // CHECK-NEXT: stablehlo.add
+  // CHECK-NEXT: mpmd.return
+  // CHECK-NEXT: }
+  %forward_result_0, %forward_result_1 = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %arg2, %2 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+  // A fragment that goes between the forward and backward fragments. It should be kept as it is.
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f2"]> (%arg0) {call_counter = 1 : ui32}
+  // CHECK-NEXT: mpmd.return
+  // CHECK-NEXT: }
+  %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // Backward fragment. Remat should be inserted before this.
+  // CHECK-NEXT: %[[REMAT_RESULT:.*]]:2 = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {call_counter = 1 : ui32, remat}
+  // CHECK-NEXT: stablehlo.add
+  // CHECK-NEXT: mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%[[REMAT_RESULT]]#0, %[[REMAT_RESULT]]#1) {call_counter = 1 : ui32}
+  %backward_result = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%forward_result_0, %forward_result_1) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %4 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %4 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  return %forward_result_1 : !mesh_1_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func @different_stages_mean_no_remat
+func.func @different_stages_mean_no_remat(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0) {call_counter = 1 : ui32}
+  // CHECK: mpmd.fragment<mesh="m1", origin=["f"(1)], stage=1> (%0) {call_counter = 1 : ui32}
+  %0 = mpmd.fragment<mesh="m1", origin=["f"], stage=0> (%arg0) {call_counter = 1 : ui32} (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"(1)], stage=1> (%0) {call_counter = 1 : ui32} (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/optimize/test/fragment_remat_merged_mode.mlir b/shardy/dialect/mpmd/transforms/optimize/test/fragment_remat_merged_mode.mlir
new file mode 100644
index 0000000..36b4c1a
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/test/fragment_remat_merged_mode.mlir
@@ -0,0 +1,52 @@
+// RUN: mpmd_opt %s -mpmd-remat-fragment='merge-remat-fragments=true'  2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @merge_remat_into_bwd_consumers
+func.func @merge_remat_into_bwd_consumers(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+
+  // CHECK-NEXT: %[[FORWARD:.*]] = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0)
+  // CHECK-NEXT:   stablehlo.add
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  %forward = mpmd.fragment<mesh="m1", origin=["f1"]> (%arg0) {call_counter = 1 : ui32} (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.add %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %0 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // This fragment is unused, but necessary to guarantee that (%forward,
+  // %backward) form a candidate pair for remat, i.e., they cannot be adjacent
+  // in the program.
+  // CHECK-NEXT: %[[_:.*]] = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0)
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  %in_between = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0) {call_counter = 1 : ui32} (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // The forward fragment is rematted and merged into its first backward user.
+  // CHECK-NEXT: %[[BACKWARD:.*]] = mpmd.fragment<mesh="m1", origin=["f1", "f1"(1)]> (%arg0) {remat} (%arg1: tensor<4x8xf32>)
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   subtract
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  %backward = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%forward) {call_counter = 1 : ui32} (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.subtract %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %0 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // The forward fragment is rematted and merged into its second backward user.
+  // CHECK-NEXT: %[[ANOTHER_BACKWARD:.*]] = mpmd.fragment<mesh="m1", origin=["f1", "f1"(1)]> (%arg0) {remat} (%arg1: tensor<4x8xf32>)
+  // CHECK-NEXT:   add
+  // CHECK-NEXT:   multiply
+  // CHECK-NEXT:   return
+  // CHECK-NEXT: }
+  %another_backward = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%forward) {call_counter = 1 : ui32} (%arg1: tensor<4x8xf32>) {
+    %0 = stablehlo.multiply %arg1, %arg1 : tensor<4x8xf32>
+    mpmd.return %0 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  // CHECK-NEXT: %[[FORWARD]], %[[BACKWARD]], %[[ANOTHER_BACKWARD]]
+  return %forward, %backward, %another_backward : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/optimize/test/import_and_optimize_pipeline.mlir b/shardy/dialect/mpmd/transforms/optimize/test/import_and_optimize_pipeline.mlir
new file mode 100644
index 0000000..55d25ee
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/test/import_and_optimize_pipeline.mlir
@@ -0,0 +1,35 @@
+// RUN: mpmd_opt %s -mpmd-import-pipeline='name-to-mesh-assignment=f1@m1/1,f2@m1/2 merge-after-scheduling=false' -mpmd-optimize-pipeline='pipeline-schedule=Circular merge-after-scheduling=false' -split-input-file 2>&1 | FileCheck %s
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+
+// Can't get Circular pipeline because of merging.
+// CHECK-LABEL: func.func public @main
+func.func public @main(%arg0: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>, tensor<3x5xf32>) attributes {topology=#topology}
+{
+  // Note: not circular. Circular would be stage 1, 1, 2, 2.
+  // CHECK: stage=1{{.*}}call_counter = 0
+  // CHECK: stage=2{{.*}}call_counter = 0
+  // CHECK: stage=1{{.*}}call_counter = 1
+  // CHECK: stage=2{{.*}}call_counter = 1
+  %2:2 = mpmd.call @f(%arg0) {call_counter = 0 : ui32} : (tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  %3:2 = mpmd.call @f(%2#0) {call_counter = 1 : ui32} : (tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+
+  return %2#1, %3#1 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+func.func private @f(%arg0: tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>) {
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg3: tensor<3x5xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<3x5xf32>
+    mpmd.return %10 : tensor<3x5xf32>
+  } : (tensor<3x5xf32>) -> tensor<3x5xf32>
+
+  %2 = mpmd.named_computation<"f2"> (%1) (%arg3: tensor<3x5xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<3x5xf32>
+    mpmd.return %10 : tensor<3x5xf32>
+  } : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  %3 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  %4 = stablehlo.add %2, %3 : tensor<3x5xf32>
+
+  return %3, %4 : tensor<3x5xf32>, tensor<3x5xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/optimize/test/import_and_optimize_pipeline_merge_after_scheduling.mlir b/shardy/dialect/mpmd/transforms/optimize/test/import_and_optimize_pipeline_merge_after_scheduling.mlir
new file mode 100644
index 0000000..5364dd4
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/test/import_and_optimize_pipeline_merge_after_scheduling.mlir
@@ -0,0 +1,35 @@
+// RUN: mpmd_opt %s -mpmd-import-pipeline='name-to-mesh-assignment=f1@m1/1,f2@m1/2 merge-after-scheduling=true' -mpmd-optimize-pipeline='pipeline-schedule=Circular merge-after-scheduling=true' -split-input-file 2>&1 | FileCheck %s
+
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>
+
+// Because we merge after scheduling, we can get a circular pipeline.
+// CHECK-LABEL: func.func public @main
+func.func public @main(%arg0: tensor<3x5xf32>)
+  -> (tensor<3x5xf32>, tensor<3x5xf32>) attributes {topology=#topology}
+{
+  // Note: Circular is stage 1, 1, 2, 2.
+  // CHECK: stage=1{{.*}}call_counter = 0
+  // CHECK: stage=1{{.*}}call_counter = 1
+  // CHECK: stage=2{{.*}}call_counter = 0
+  // CHECK: stage=2{{.*}}call_counter = 1
+  %2:2 = mpmd.call @f(%arg0) {call_counter = 0 : ui32} : (tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+  %3:2 = mpmd.call @f(%2#0) {call_counter = 1 : ui32} : (tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+
+  return %2#1, %3#1 : tensor<3x5xf32>, tensor<3x5xf32>
+}
+
+func.func private @f(%arg0: tensor<3x5xf32>) -> (tensor<3x5xf32>, tensor<3x5xf32>) {
+  %1 = mpmd.named_computation<"f1"> (%arg0) (%arg3: tensor<3x5xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<3x5xf32>
+    mpmd.return %10 : tensor<3x5xf32>
+  } : (tensor<3x5xf32>) -> tensor<3x5xf32>
+
+  %2 = mpmd.named_computation<"f2"> (%1) (%arg3: tensor<3x5xf32>) {
+    %10 = stablehlo.add %arg3, %arg3 : tensor<3x5xf32>
+    mpmd.return %10 : tensor<3x5xf32>
+  } : (tensor<3x5xf32>) -> tensor<3x5xf32>
+  %3 = stablehlo.add %arg0, %arg0 : tensor<3x5xf32>
+  %4 = stablehlo.add %2, %3 : tensor<3x5xf32>
+
+  return %3, %4 : tensor<3x5xf32>, tensor<3x5xf32>
+}
diff --git a/shardy/dialect/mpmd/transforms/optimize/test/move_transfers_to_producer.mlir b/shardy/dialect/mpmd/transforms/optimize/test/move_transfers_to_producer.mlir
new file mode 100644
index 0000000..ce7f3c5
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/test/move_transfers_to_producer.mlir
@@ -0,0 +1,44 @@
+// RUN: mpmd_opt %s -mpmd-move-transfers-to-producer 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>> >
+
+// CHECK-LABEL: func private @move_transfer_right_after_producer
+func.func private @move_transfer_right_after_producer
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#topology}
+{
+  // CHECK-NEXT: %[[FRAGMENT1:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]>
+  // CHECK-NEXT: mpmd.return
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %0
+  // CHECK-NEXT: %[[FRAGMENT2:.*]] = mpmd.fragment<mesh="m1", origin=["another_producer"]>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["another_producer"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %transfer = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  func.return %1, %transfer : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
+
+// CHECK-LABEL: func private @move_transfer_of_block_arg_at_beginning_of_block
+func.func private @move_transfer_of_block_arg_at_beginning_of_block
+  (%arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_2_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32)
+  attributes {"topology"=#topology}
+{
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0
+  // CHECK-NEXT: %[[FRAGMENT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %transfer = mpmd.transfer %arg0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  func.return %0, %transfer : !mesh_1_tensor_4_8_f32, !mesh_2_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/optimize/test/verify_scheduling_units.mlir b/shardy/dialect/mpmd/transforms/optimize/test/verify_scheduling_units.mlir
new file mode 100644
index 0000000..8c2e2ad
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/test/verify_scheduling_units.mlir
@@ -0,0 +1,58 @@
+// RUN: mpmd_opt %s -mpmd-scheduling-units-verifier -verify-diagnostics -split-input-file 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// The topology has two meshes, but m2 is unused. Moreover, m1 doesn't
+// have any fragment which is user-defined.
+// CHECK-LABEL: func @main
+// expected-warning@+3 {{Number of backward scheduling units in mesh m2 does not match expected number for 1 microbatches. Got 0.}}
+// expected-warning@+2 {{Number of forward scheduling units in mesh m2 does not match expected number for 1 microbatches. Got 0.}}
+// expected-warning@+1 {{Number of forward scheduling units in mesh m1 does not match expected number for 1 microbatches. Got 0.}}
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"(1)]> (%0) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %1 : !mesh_1_tensor_4_8_f32
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @main
+// This test has too many fwd scheduling units
+// expected-warning@+1 {{Number of forward scheduling units in mesh m1 does not match expected number for 1 microbatches. Got 2.}}
+func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>
+    >} {
+  %0 = mpmd.fragment<mesh="m1", origin=["f"]> (%arg0) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %1 = mpmd.fragment<mesh="m1", origin=["f"]> (%0) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  %2 = mpmd.fragment<mesh="m1", origin=["f"(1)]> (%1) {call_counter = 0 : ui32}
+    (%arg1: tensor<4x8xf32>) {
+    mpmd.return %arg1 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
diff --git a/shardy/dialect/mpmd/transforms/optimize/utils.cc b/shardy/dialect/mpmd/transforms/optimize/utils.cc
new file mode 100644
index 0000000..ebd04d4
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/utils.cc
@@ -0,0 +1,164 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/optimize/utils.h"
+
+#include <cstdint>
+#include <functional>
+#include <iterator>
+#include <optional>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+
+namespace mlir::mpmd {
+
+namespace {
+
+bool CallCounterMatchesForRemat(FragmentOp forward_fragment,
+                                FragmentOp backward_fragment) {
+  std::optional<int64_t> forward_call_counter =
+      TryToFindCallCounter(forward_fragment);
+  std::optional<int64_t> backward_call_counter =
+      TryToFindCallCounter(backward_fragment);
+  return (forward_call_counter.has_value() &&
+          backward_call_counter.has_value() &&
+          *forward_call_counter == *backward_call_counter);
+}
+
+}  // namespace
+
+bool IsForwardFragment(FragmentOp fragment) {
+  std::optional<int64_t> transpose_count =
+      TryToFindSingleTransposeCount(fragment);
+  return transpose_count.has_value() && *transpose_count == 0;
+}
+
+bool IsBackwardFragment(FragmentOp fragment) {
+  std::optional<int64_t> transpose_count =
+      TryToFindSingleTransposeCount(fragment);
+  // We only consider backward fragments that have transpose_count 1, which
+  // means there is exactly one Jax AD transpose. If the value is
+  // greater than 1, it means there are multiple jax.grad and it is unclear what
+  // remat means in this case.
+  return transpose_count.has_value() && *transpose_count == 1;
+}
+
+bool CanRemat(FragmentOp forward_fragment, FragmentOp backward_fragment) {
+  return IsForwardFragment(forward_fragment) &&
+         IsBackwardFragment(backward_fragment) &&
+         CallCounterMatchesForRemat(forward_fragment, backward_fragment) &&
+         !IsExecutedImmediatelyAfter(forward_fragment, backward_fragment) &&
+         forward_fragment.getStageIdAttr() ==
+             backward_fragment.getStageIdAttr();
+}
+
+SmallVector<mpmd::NamedMeshAttr> GetSchedulableMeshes(func::FuncOp func) {
+  auto all_meshes = mpmd::GetTopologyMeshes(func);
+  SmallVector<mpmd::NamedMeshAttr> hbm_meshes;
+  llvm::copy_if(all_meshes, std::back_inserter(hbm_meshes),
+                  [](const mpmd::NamedMeshAttr& mesh) {
+                    return !mesh.getName().ends_with("#cpu");
+                  });
+  return hbm_meshes;
+}
+
+// Returns the number of meshes in the pipeline.
+// TODO(jupvfranco): This code is assuming that every mesh in the topology is a
+// pipeline stage. Generalize this.
+int GetNumMeshes(Operation* op) {
+  return GetSchedulableMeshes(op->getParentOfType<func::FuncOp>()).size();
+}
+
+// Returns the index of the mesh of `fragment` in the pipeline.
+// TODO(jupvfranco): This code is assuming that the meshes appear in the order
+// of stages. Generalize this once we have stages.
+int GetMeshIndex(FragmentOp fragment) {
+  for (auto [index, mesh] : llvm::enumerate(
+           GetSchedulableMeshes(fragment->getParentOfType<func::FuncOp>()))) {
+    if (mesh.getName() == fragment.getMeshName()) {
+      return index;
+    }
+  }
+  // At this point we are guaranteed that every fragment's mesh is in the
+  // topology.
+  SDY_CHECK(false) << "Mesh not found: " << fragment.getMeshName().str();
+}
+
+// Checks if a fragment is a scheduling unit, i.e., it is a user fragment,
+// it has a call_counter and a single transpose_count which is 0 or 1.
+bool IsSchedulingUnit(FragmentOp fragment) {
+  if (!fragment.isUserFragment()) {
+    return false;
+  }
+
+  if (!TryToFindCallCounter(fragment).has_value()) return false;
+
+  if (auto transpose_count = TryToFindSingleTransposeCount(fragment);
+      transpose_count.has_value()) {
+    return *transpose_count == 0 || *transpose_count == 1;
+  }
+
+  return false;
+}
+
+namespace {
+
+// Callback type for `VisitOpUseTree`.
+using PostActionCallBack = std::function<bool(Operation*)>;
+
+// Visits all users in a depth-first way, starting from current, with the
+// constraint that the traversal remains within the same block as the
+// `barrier_op` and never visits a node _after_ the `barrier_op`. After all the
+// users have been recursively visited it invokes the `action` callback. The
+// traversal terminates early if one of these callbacks returns `false`.
+bool VisitOpUseTree(Operation* current, Operation* barrier_op,
+                    DenseSet<Operation*>& visited, PostActionCallBack action) {
+  // Invariant: we will always have started with a `current` op at the same
+  // block as `barrier` op; hence it is always possible to trace all recursive
+  // users of `current` to ancestors in the same block as `barrier_op`.
+  current = GetAncestorInBlock(barrier_op->getBlock(), current);
+  // Done traversing if we are already visited or are beyond the barrier.
+  if (barrier_op->isBeforeInBlock(current) || visited.contains(current)) {
+    return true;
+  }
+
+  for (Value result : current->getResults()) {
+    bool any_failure = llvm::any_of(result.getUsers(), [&](Operation* user) {
+      return !VisitOpUseTree(user, barrier_op, visited, action);
+    });
+    if (any_failure) return false;
+  }
+  visited.insert(current);
+  return action(current);
+}
+
+}  // namespace
+
+bool TargetDependsOnSourceOp(Operation* src_op, Operation* tgt_op) {
+  SDY_CHECK(src_op->getBlock() == tgt_op->getBlock());
+  DenseSet<Operation*> visited;
+  return !VisitOpUseTree(src_op, tgt_op, visited,
+                         [&](Operation* op) { return op != tgt_op; });
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/optimize/utils.h b/shardy/dialect/mpmd/transforms/optimize/utils.h
new file mode 100644
index 0000000..abbc690
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/utils.h
@@ -0,0 +1,66 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_UTILS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_UTILS_H_
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+// Returns true if fragment is a forward fragment, i.e., if the transpose count
+// is 0.
+bool IsForwardFragment(FragmentOp fragment);
+
+// Returns true if fragment is a backward fragment, i.e., if the transpose count
+// is 1.
+bool IsBackwardFragment(FragmentOp fragment);
+
+// Given two fragments, returns true if the former can be rematerialized for
+// consumption of the latter, i.e., we check that:
+// - forward_fragment is a forward fragment
+// - backward_fragment is a backward fragment.
+// - Their call counters match.
+// - Their stage ids match, when defined.
+bool CanRemat(FragmentOp forward_fragment, FragmentOp backward_fragment);
+
+// Returns the meshes in the topology that are schedulable for pipeline
+// computations. I.e. they are not CPU meshes.
+SmallVector<NamedMeshAttr> GetSchedulableMeshes(func::FuncOp func);
+
+// Returns the number of meshes in the pipeline.
+// TODO(jupvfranco): This code is assuming that every mesh in the topology is a
+// pipeline stage. Generalize this.
+int GetNumMeshes(Operation* op);
+
+// Returns the index of the mesh of `fragment` in the pipeline.
+// TODO(jupvfranco): This code is assuming that the meshes appear in the order
+// of stages. Generalize this once we have stages.
+int GetMeshIndex(FragmentOp fragment);
+
+// Checks if a fragment is a scheduling unit, i.e., it is a user fragment,
+// it has a call_counter and a single transpose_count which is 0 or 1.
+bool IsSchedulingUnit(FragmentOp fragment);
+
+// Does `tgt_op` have (conservatively) any dataflow dependency on `src_op`?
+// Precondition: `tgt_op` and `src_op` must be in the same block.
+bool TargetDependsOnSourceOp(Operation* src_op, Operation* tgt_op);
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_OPTIMIZE_UTILS_H_
diff --git a/shardy/dialect/mpmd/transforms/optimize/utils_test.cc b/shardy/dialect/mpmd/transforms/optimize/utils_test.cc
new file mode 100644
index 0000000..65e762c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/optimize/utils_test.cc
@@ -0,0 +1,380 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/optimize/utils.h"
+
+#include <string>
+#include <string_view>
+
+#include "llvm/ADT/StringRef.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OwningOpRef.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Parser/Parser.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/register.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/testing_utils.h"
+#include <gtest/gtest.h>
+
+using ::mlir::func::FuncOp;
+
+namespace mlir::mpmd {
+namespace {
+
+TEST(CanRemat, ShouldReturnTrueIfAllConditionsAreMet) {
+  const char kProgram[] = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      // Forward fragment, result %0 is used by the backward fragment below.
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // A fragment that goes between the forward and backward fragments.
+      %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // Backward fragment.
+      %1 = mpmd.fragment<mesh="m1", origin=["f2"(1)]> (%0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+      return %1 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  Region::OpIterator it = func_op.getOps().begin();
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp fwd_fragment = cast<FragmentOp>(*it);
+  it++;  // Now it should point to the fragment between the forward and backward
+         // fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  it++;  // Now it should point to the backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp bwd_fragment = cast<FragmentOp>(*it);
+
+  EXPECT_TRUE(CanRemat(fwd_fragment, bwd_fragment));
+}
+
+TEST(CanRemat, ShouldReturnFalseIfNotForward) {
+  const char kProgram[] = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      // This is not a forward fragment because of the transpose count.
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(1)]> (%arg0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // A fragment that goes between the not-forward and backward fragments.
+      %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // Backward fragment.
+      %1 = mpmd.fragment<mesh="m1", origin=["f2"(1)]> (%0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+      return %1 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  Region::OpIterator it = func_op.getOps().begin();
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp not_fwd_fragment = cast<FragmentOp>(*it);
+  it++;  // Now it should point to the fragment between the not-forward and
+         // backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  it++;  // Now it should point to the backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp bwd_fragment = cast<FragmentOp>(*it);
+
+  EXPECT_FALSE(CanRemat(not_fwd_fragment, bwd_fragment));
+}
+
+TEST(CanRemat, ShouldReturnFalseIfNotBackward) {
+  const char kProgram[] = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      // Forward fragment.
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // A fragment that goes between the forward and non-backward fragments.
+      %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // This is not a backward fragment because the transpose count is not 1.
+      %1 = mpmd.fragment<mesh="m1", origin=["f2"(100)]> (%0)(%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+      return %1 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  Region::OpIterator it = func_op.getOps().begin();
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp fwd_fragment = cast<FragmentOp>(*it);
+  it++;  // Now it should point to the fragment between the forward and
+         // non-backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  it++;  // Now it should point to the backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp not_bwd_fragment = cast<FragmentOp>(*it);
+
+  EXPECT_FALSE(CanRemat(fwd_fragment, not_bwd_fragment));
+}
+
+TEST(CanRemat,
+     ShouldReturnFalseIfCallCounterOfForwardAndBackwardFragmentsDonotMatch) {
+  const char kProgram[] = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      // Forward fragment with call_counter 1.
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // A fragment that goes between the forward and backward fragments.
+      %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // Backward fragment with call_counter 100, which does not match the forward fragment.
+      %1 = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%0) {call_counter = 100 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+      return %1 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  Region::OpIterator it = func_op.getOps().begin();
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp fwd_fragment = cast<FragmentOp>(*it);
+  it++;  // Now it should point to the fragment between the forward and backward
+         // fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  it++;  // Now it should point to the backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp bwd_fragment = cast<FragmentOp>(*it);
+
+  EXPECT_FALSE(CanRemat(fwd_fragment, bwd_fragment));
+}
+
+TEST(CanRemat, ShouldReturnFalseIfBackwardFragmentsIsImmediatelyAfterForward) {
+  const char kProgram[] = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      // Forward fragment.
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // Backward fragment is immediately after the forward fragment.
+      %1 = mpmd.fragment<mesh="m1", origin=["f2"(1)]> (%0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+      return %1 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  Region::OpIterator it = func_op.getOps().begin();
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp fwd_fragment = cast<FragmentOp>(*it++);
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp bwd_fragment = cast<FragmentOp>(*it);
+
+  EXPECT_FALSE(CanRemat(fwd_fragment, bwd_fragment));
+}
+
+TEST(CanRemat, ShouldReturnFalseIfStagesDontMatch) {
+  const char kProgram[] = R"mlir(
+    !mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    func.func @main(%arg0: !mesh_1_tensor_4_8_f32)
+      -> (!mesh_1_tensor_4_8_f32) attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+      // Forward fragment, result %0 is used by the backward fragment below.
+      %0 = mpmd.fragment<mesh="m1", origin=["f1"(0)], stage=123> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // A fragment that goes between the forward and backward fragments.
+      %tmp = mpmd.fragment<mesh="m1", origin=["f2"(0)]> (%arg0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+      // Backward fragment.
+      %1 = mpmd.fragment<mesh="m1", origin=["f2"(1)], stage=321> (%0) {call_counter = 1 : ui32} (%arg2: tensor<4x8xf32>) {
+        mpmd.return %arg2 : tensor<4x8xf32>
+      } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+
+      return %1 : !mesh_1_tensor_4_8_f32
+    }
+  )mlir";
+
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module =
+      parseSourceString<ModuleOp>(kProgram, &context);
+  FuncOp func_op = GetMainFunction(*module);
+
+  Region::OpIterator it = func_op.getOps().begin();
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp fwd_fragment = cast<FragmentOp>(*it);
+  it++;  // Now it should point to the fragment between the forward and backward
+         // fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  it++;  // Now it should point to the backward fragment.
+  SDY_CHECK(it != func_op.getOps().end());
+  FragmentOp bwd_fragment = cast<FragmentOp>(*it);
+
+  EXPECT_FALSE(CanRemat(fwd_fragment, bwd_fragment));
+}
+
+bool TargetDependsOnSourceOpHelper(std::string_view program) {
+  MLIRContext context;
+  loadAllRequiredDialects(&context);
+  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
+  SDY_CHECK(module);
+  auto main_fn = GetMainFunction(*module);
+  Operation* src_op = GetOpWithAttribute<Operation*>(main_fn, "source");
+  Operation* tgt_op = GetOpWithAttribute<Operation*>(main_fn, "target");
+  return TargetDependsOnSourceOp(src_op, tgt_op);
+}
+
+TEST(TargetDependsOnSourceOpTesting, ImmediateDependency) {
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<16x32xf32>) -> tensor<16x32xf32> {
+      %0 = mpmd.named_computation<"f"> (%arg0) {source} (%arg1: tensor<16x32xf32>) {
+        %1 = stablehlo.multiply %arg1, %arg1 : tensor<16x32xf32>
+        mpmd.return %1 : tensor<16x32xf32>
+      } : (tensor<16x32xf32>) -> tensor<16x32xf32>
+      %2 = stablehlo.add %0, %0 {target} : tensor<16x32xf32>
+      func.return %2 : tensor<16x32xf32>
+    })mlir";
+  EXPECT_TRUE(TargetDependsOnSourceOpHelper(program));
+}
+
+TEST(TargetDependsOnSourceOpTesting, InternalDependency) {
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<16x32xf32>) -> tensor<16x32xf32> {
+      %0 = mpmd.named_computation<"f"> (%arg0) (%arg1: tensor<16x32xf32>) {
+        %1 = stablehlo.multiply %arg1, %arg1 {source} : tensor<16x32xf32>
+        %2 = stablehlo.add %1, %1 {target} : tensor<16x32xf32>
+        mpmd.return %2 : tensor<16x32xf32>
+      } : (tensor<16x32xf32>) -> tensor<16x32xf32>
+      func.return %0 : tensor<16x32xf32>
+    })mlir";
+  EXPECT_TRUE(TargetDependsOnSourceOpHelper(program));
+}
+
+TEST(TargetDependsOnSourceOpTesting, NoDependencyTargetBeforeSource) {
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<16x32xf32>) -> tensor<16x32xf32> {
+      %0 = mpmd.named_computation<"f"> (%arg0) {target} (%arg1: tensor<16x32xf32>) {
+        %1 = stablehlo.multiply %arg1, %arg1 : tensor<16x32xf32>
+        mpmd.return %1 : tensor<16x32xf32>
+      } : (tensor<16x32xf32>) -> tensor<16x32xf32>
+      %2 = stablehlo.add %0, %0 {source} : tensor<16x32xf32>
+      func.return %2 : tensor<16x32xf32>
+    })mlir";
+  EXPECT_FALSE(TargetDependsOnSourceOpHelper(program));
+}
+
+TEST(TargetDependsOnSourceOpTesting, NoDependency) {
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<256xf32>) -> tensor<f32> {
+      %0 = stablehlo.add %arg0, %arg0 {source} : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
+      %1 = mpmd.named_computation<"f"> (%arg0) {target} (%arg1: tensor<256xf32>) {
+        %2 = stablehlo.multiply %arg1, %arg1 : tensor<256xf32>
+        mpmd.return %2 : tensor<256xf32>
+      } : (tensor<256xf32>) -> tensor<256xf32>
+      %3 = stablehlo.dot %1, %0 : (tensor<256xf32>, tensor<256xf32>) -> tensor<f32>
+      func.return %3 : tensor<f32>
+    })mlir";
+
+  EXPECT_FALSE(TargetDependsOnSourceOpHelper(program));
+}
+
+TEST(TargetDependsOnSourceOpTesting, DependencyWithInternalOp) {
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<256xf32>) -> tensor<f32> {
+      %0 = stablehlo.add %arg0, %arg0 {source} : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
+      %1 = mpmd.named_computation<"f"> (%0) {target} (%arg1: tensor<256xf32>) {
+        %2 = stablehlo.multiply %arg1, %arg1 : tensor<256xf32>
+        mpmd.return %2 : tensor<256xf32>
+      } : (tensor<256xf32>) -> tensor<256xf32>
+      %3 = stablehlo.dot %1, %0 : (tensor<256xf32>, tensor<256xf32>) -> tensor<f32>
+      func.return %3 : tensor<f32>
+    })mlir";
+
+  EXPECT_TRUE(TargetDependsOnSourceOpHelper(program));
+}
+
+TEST(TargetDependsOnSourceOpTesting, RecursiveDependencyWithInternalOps) {
+  const std::string program = R"mlir(
+    func.func @main(%arg0: tensor<256xf32>) -> tensor<f32> {
+      %0 = stablehlo.add %arg0, %arg0 {source} : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
+      %1 = mpmd.named_computation<"f"> (%0) {target} (%arg1: tensor<256xf32>) {
+        %2 = stablehlo.multiply %arg1, %arg1 : tensor<256xf32>
+        mpmd.return %2 : tensor<256xf32>
+      } : (tensor<256xf32>) -> tensor<256xf32>
+      %3 = mpmd.named_computation<"f"> (%1) {target} (%arg1: tensor<256xf32>) {
+        %2 = stablehlo.multiply %arg1, %arg1 : tensor<256xf32>
+        mpmd.return %2 : tensor<256xf32>
+      } : (tensor<256xf32>) -> tensor<256xf32>
+      %4 = stablehlo.dot %3, %3 {target} : (tensor<256xf32>, tensor<256xf32>) -> tensor<f32>
+      func.return %4 : tensor<f32>
+    })mlir";
+
+  EXPECT_TRUE(TargetDependsOnSourceOpHelper(program));
+}
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/passes.cc b/shardy/dialect/mpmd/transforms/passes.cc
new file mode 100644
index 0000000..a18eb0e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/passes.cc
@@ -0,0 +1,44 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "shardy/dialect/mpmd/transforms/passes.h"
+
+#include "shardy/dialect/mpmd/transforms/common/passes.h"
+#include "shardy/dialect/mpmd/transforms/export/passes.h"
+#include "shardy/dialect/mpmd/transforms/import/infer_mesh_assignment.h"
+#include "shardy/dialect/mpmd/transforms/import/passes.h"
+#include "shardy/dialect/mpmd/transforms/optimize/passes.h"
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h"
+
+namespace mlir::mpmd {
+
+void registerAllMpmdPassesAndPipelines() {
+  registerMpmdCommonPasses();
+
+  registerMpmdExportPasses();
+  registerExportPipeline();
+
+  registerMpmdImportPasses();
+  registerImportPipeline();
+  registerInferMeshPipeline();
+
+  registerMpmdOptimizePasses();
+  registerOptimizePipeline();
+
+  registerMpmdShardingPropagationPasses();
+  registerShardingPropagationPipeline();
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/passes.h b/shardy/dialect/mpmd/transforms/passes.h
new file mode 100644
index 0000000..27ca0a4
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/passes.h
@@ -0,0 +1,26 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_PASSES_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_PASSES_H_
+
+namespace mlir::mpmd {
+
+// Registers all MPMD passes and pipelines.
+void registerAllMpmdPassesAndPipelines();
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_PASSES_H_
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/BUILD b/shardy/dialect/mpmd/transforms/sharding_propagation/BUILD
new file mode 100644
index 0000000..3616fbb
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/BUILD
@@ -0,0 +1,73 @@
+# The MPMD sharding propagation passes and pipeline.
+
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library", "td_library")
+
+package(default_visibility = ["//visibility:public"])
+
+td_library(
+    name = "passes_td_files",
+    srcs = [
+        "passes.td",
+    ],
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+gentbl_cc_library(
+    name = "passes_inc",
+    tbl_outs = {
+        "passes.h.inc": [
+            "-gen-pass-decls",
+            "-name=MpmdShardingPropagation",
+        ],
+        "g3doc/mpmd_sharding_propagation_passes.md": ["-gen-pass-doc"],
+    },
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "passes.td",
+    deps = [":passes_td_files"],
+)
+
+cc_library(
+    name = "passes",
+    srcs = [
+        "convert_sdy_constants.cc",
+        "convert_sdy_shardings_to_mpmd_types.cc",
+        "enforce_user_shardings.cc",
+        "extract_reshards_from_inter_mesh_transfers.cc",
+        "sharding_propagation_pipeline.cc",
+    ],
+    hdrs = [
+        "passes.h",
+    ],
+    deps = [
+        ":passes_inc",
+        ":utils",
+        "//shardy/common:logging",
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/mpmd/transforms/common:distributed_function_pass",
+        "//shardy/dialect/mpmd/transforms/common:passes",
+        "//shardy/dialect/mpmd/transforms/common:utils",
+        "//shardy/dialect/sdy/ir:dialect",
+        "//shardy/dialect/sdy/transforms/export:explicit_reshards_util",
+        "//shardy/dialect/sdy/transforms/propagation:passes",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Rewrite",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@stablehlo//:stablehlo_ops",
+    ],
+)
+
+cc_library(
+    name = "utils",
+    hdrs = ["utils.h"],
+    deps = [
+        "//shardy/dialect/mpmd/ir:dialect",
+        "//shardy/dialect/sdy/ir:dialect",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/convert_sdy_constants.cc b/shardy/dialect/mpmd/transforms/sharding_propagation/convert_sdy_constants.cc
new file mode 100644
index 0000000..5401acb
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/convert_sdy_constants.cc
@@ -0,0 +1,78 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <memory>
+#include <utility>
+
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/Rewrite/FrozenRewritePatternSet.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_CONVERTSDYCONSTANTSPASS
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h.inc"
+
+namespace {
+
+// Converts sdy::ConstantOp to stablehlo::ConstantOp.
+class ConstantPattern : public OpConversionPattern<sdy::ConstantOp> {
+  using OpConversionPattern::OpConversionPattern;
+
+  LogicalResult matchAndRewrite(
+      sdy::ConstantOp op, OpAdaptor adaptor,
+      ConversionPatternRewriter& rewriter) const override {
+    // We use the generic op builder so that unregistered attributes will be
+    // added to the new op.
+    rewriter.replaceOpWithNewOp<stablehlo::ConstantOp>(
+        op, op->getResultTypes(), adaptor.getOperands(), op->getAttrs());
+    return success();
+  }
+};
+
+class ConvertSdyConstantsPass
+    : public impl::ConvertSdyConstantsPassBase<ConvertSdyConstantsPass> {
+  using ConvertSdyConstantsPassBase::ConvertSdyConstantsPassBase;
+
+  LogicalResult initialize(MLIRContext* context) final {
+    target = std::make_shared<ConversionTarget>(*context);
+    target->addIllegalOp<sdy::ConstantOp>();
+    target->addLegalOp<stablehlo::ConstantOp>();
+
+    RewritePatternSet patternsInternal(context);
+    patternsInternal.add<ConstantPattern>(context);
+    patterns = std::move(patternsInternal);
+
+    return success();
+  }
+
+  void runOnOperation() final {
+    if (failed(applyPartialConversion(getOperation(), *target, patterns))) {
+      signalPassFailure();
+    }
+  }
+
+ private:
+  std::shared_ptr<ConversionTarget> target;
+  FrozenRewritePatternSet patterns;
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/convert_sdy_shardings_to_mpmd_types.cc b/shardy/dialect/mpmd/transforms/sharding_propagation/convert_sdy_shardings_to_mpmd_types.cc
new file mode 100644
index 0000000..95cc59b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/convert_sdy_shardings_to_mpmd_types.cc
@@ -0,0 +1,144 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <optional>
+
+#include "llvm/ADT/StringRef.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/Visitors.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/WalkResult.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/common/utils.h"
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/sdy/ir/constants.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+#include "shardy/dialect/sdy/transforms/export/explicit_reshards_util.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_CONVERTSDYSHARDINGSTOMPMDTYPESPASS
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h.inc"
+
+namespace {
+
+sdy::TensorShardingAttr GetShardingFromMeshTensorValue(Value value) {
+  auto mesh_tensor_type = dyn_cast<MeshTensorType>(value.getType());
+  SDY_CHECK(mesh_tensor_type);
+  std::optional<sdy::TensorShardingAttr> sharding =
+      mesh_tensor_type.getSharding();
+  return sharding.value_or(sdy::TensorShardingAttr());
+}
+
+class ConvertSdyShardingsToMpmdTypesPass
+    : public impl::ConvertSdyShardingsToMpmdTypesPassBase<
+          ConvertSdyShardingsToMpmdTypesPass> {
+  using ConvertSdyShardingsToMpmdTypesPassBase::
+      ConvertSdyShardingsToMpmdTypesPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) override {
+    func_op->walk([&](Operation* op) {
+      TypeSwitch<Operation*, void>(op)
+          .Case<FragmentOp>([](FragmentOp fragment) {
+            if (std::optional<sdy::TensorShardingPerValueAttr> in_shardings =
+                    fragment.getInShardings()) {
+              for (OpOperand& operand : fragment->getOpOperands()) {
+                UpdateValueTypeWithSharding(
+                    operand.get(),
+                    in_shardings->getSharding(operand.getOperandNumber()));
+              }
+              // Remove the inshardings attr since it's been moved to the type.
+              fragment.removeInShardingsAttr();
+            }
+            if (std::optional<sdy::TensorShardingPerValueAttr> out_shardings =
+                    fragment.getOutShardings()) {
+              for (OpResult result : fragment->getResults()) {
+                UpdateValueTypeWithSharding(
+                    result,
+                    out_shardings->getSharding(result.getResultNumber()));
+              }
+              // Remove the outshardings attr since it's been moved to the type.
+              fragment.removeOutShardingsAttr();
+            }
+          })
+          .Case<TransferOp>([](TransferOp transfer) {
+            sdy::TensorShardingAttr result_sharding =
+                sdy::getSharding(transfer.getResult());
+            UpdateValueTypeWithSharding(transfer.getTensor(), result_sharding);
+            UpdateValueTypeWithSharding(transfer.getResult(), result_sharding);
+          })
+          .Case<ForOp>([](ForOp for_op) {
+            for (OpOperand& operand : for_op->getOpOperands()) {
+              sdy::TensorShardingAttr sharding =
+                  sdy::getSharding(operand.get());
+              UpdateValueTypeWithSharding(operand.get(), sharding);
+              UpdateValueTypeWithSharding(
+                  for_op.getRegion().getArgument(operand.getOperandNumber()),
+                  sharding);
+            }
+            for (OpResult op_result : for_op.getResults()) {
+              UpdateValueTypeWithSharding(op_result,
+                                          sdy::getSharding(op_result));
+            }
+          });
+      op->removeAttr(sdy::kShardingAttr);
+      return WalkResult::advance();
+    });
+    // Remove the sharding attribute in args and results and update the function
+    // signature.
+    for (unsigned arg_num = 0; arg_num < func_op.getNumArguments(); ++arg_num) {
+      UpdateValueTypeWithSharding(
+          func_op.getArgument(arg_num),
+          sdy::getSharding(func_op.getArgument(arg_num)));
+      func_op.removeArgAttr(arg_num, sdy::kShardingAttr);
+    }
+
+    for (unsigned result_num = 0; result_num < func_op.getNumResults();
+         ++result_num) {
+      func_op.removeResultAttr(result_num, sdy::kShardingAttr);
+    }
+    func_op->removeAttr(sdy::kShardingAttr);
+    UpdateFunctionType(func_op);
+
+    // Verify that all transfers have the same operand and result sharding.
+    func_op->walk([](TransferOp transfer) {
+      sdy::TensorShardingAttr result_sharding =
+          GetShardingFromMeshTensorValue(transfer.getResult());
+      sdy::TensorShardingAttr operand_sharding =
+          GetShardingFromMeshTensorValue(transfer.getTensor());
+
+      if (sdy::shouldReshard(operand_sharding, result_sharding)) {
+        transfer->emitError()
+            << "Transfer op has different shardings for the "
+               "tensor and result, tensor sharding: "
+            << operand_sharding
+            << ", result sharding: "
+            << result_sharding;
+        return WalkResult::interrupt();
+      }
+      return WalkResult::advance();
+    });
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/enforce_user_shardings.cc b/shardy/dialect/mpmd/transforms/sharding_propagation/enforce_user_shardings.cc
new file mode 100644
index 0000000..1f8faf4
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/enforce_user_shardings.cc
@@ -0,0 +1,189 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstddef>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/Support/Threading.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/utils.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_ENFORCEUSERSHARDINGSPASS
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h.inc"
+
+namespace {
+
+// If the sharding has sub axis in a dimension, replace it with the prefix of
+// the sharding axes until the first sub axis. This is necessary because we use
+// jax.sharding.NamedSharding for input/output shardings, which doesn't support
+// sub axes.
+// TODO: b/414535722 - Remove this temporary solution once we support sub axes
+// shardings.
+sdy::TensorShardingAttr MaybeDropSubAxesShardingSuffix(
+    sdy::TensorShardingAttr sharding) {
+  SmallVector<sdy::DimensionShardingAttr> dim_shardings;
+  dim_shardings.reserve(sharding.getRank());
+  for (sdy::DimensionShardingAttr dim_sharding : sharding.getDimShardings()) {
+    size_t prefix_to_keep = 0;
+    for (const auto& [i, axis] : llvm::enumerate(dim_sharding.getAxes())) {
+      if (axis.getSubAxisInfo()) {
+        break;
+      }
+      ++prefix_to_keep;
+    }
+    if (prefix_to_keep != dim_sharding.getAxes().size()) {
+      dim_shardings.push_back(sdy::DimensionShardingAttr::get(
+          sharding.getContext(),
+          dim_sharding.getAxes().take_front(prefix_to_keep),
+          /*is_closed=*/true));
+    } else {
+      dim_shardings.push_back(dim_sharding);
+    }
+  }
+  return sdy::TensorShardingAttr::get(
+      sharding.getContext(), sharding.getMeshOrRef(), dim_shardings,
+      sharding.getReplicatedAxes(), sharding.getUnreducedAxes());
+}
+
+void emitSubAxisWarning(llvm::once_flag& once_flag, StringRef value_name,
+                        unsigned value_index,
+                        sdy::TensorShardingAttr old_sharding,
+                        sdy::TensorShardingAttr user_sharding, Location loc) {
+  llvm::call_once(once_flag, [=]() {
+    emitWarning(loc, "Sub-axes sharding found for ")
+        << value_name << " " << value_index
+        << ". This is not supported, and sharding will be tuncated. "
+           "Original sharding: "
+        << old_sharding << "; truncated sharding: " << user_sharding
+        << " Please contact the MPMD team.";
+  });
+}
+
+// Enforces the user specified sharding for the given block argument.
+// - If the block argument is used by a fragment: e.g.,
+// fragment<mesh="m", origin=[], in_shardings=[<@mesh, [{"x"},
+// {?}]>]> (%arg0)
+// If the user specified sharding is different from the in_shardings for the
+// argument, we update the sharding of the fragment input to be the user
+// specified one.
+// - If the block argument is used by a transfer op, we set the sharding of
+// the result of the transfer op to be the user specified one.
+void EnforceUserInputSharding(BlockArgument arg) {
+  sdy::TensorShardingAttr old_sharding = sdy::getSharding(arg);
+  if (!old_sharding) {
+    return;
+  }
+
+  sdy::TensorShardingAttr user_sharding =
+      MaybeDropSubAxesShardingSuffix(old_sharding);
+  if (user_sharding != old_sharding) {
+    sdy::setSharding(arg, user_sharding);
+    static llvm::once_flag log_arg_with_sub_axes_once;
+    emitSubAxisWarning(log_arg_with_sub_axes_once, "arg", arg.getArgNumber(),
+                       old_sharding, user_sharding, arg.getLoc());
+  }
+
+  for (OpOperand& use : arg.getUses()) {
+    if (auto fragment_user = dyn_cast<FragmentOp>(use.getOwner())) {
+      unsigned operand_num = use.getOperandNumber();
+      SmallVector<sdy::TensorShardingAttr> all_in_shardings =
+          fragment_user.getBlockArgumentEdgeOwnerShardings();
+      if (all_in_shardings.empty()) {
+        all_in_shardings = SmallVector<sdy::TensorShardingAttr>(
+            fragment_user->getNumOperands(), sdy::TensorShardingAttr());
+      }
+      sdy::TensorShardingAttr fragment_in_sharding =
+          all_in_shardings[operand_num];
+      if (fragment_in_sharding != user_sharding) {
+        all_in_shardings[operand_num] = user_sharding;
+        fragment_user.setBlockArgumentEdgeOwnerShardings(all_in_shardings);
+      }
+    }
+    // We don't need to handle the case where the argument is used by a transfer
+    // op because the later `ExtractReshardFromInterMeshTransfersPass` will
+    // introduce a reshard fragment to handle the case where the user-specified
+    // argument sharding is different from the transfer op result sharding.
+  }
+}
+
+// Enforces the user specified sharding for the given return operand.
+void EnforceUserResultSharding(OpOperand& return_operand, func::FuncOp func) {
+  sdy::TensorShardingAttr old_sharding =
+      sdy::getFuncResultSharding(func, return_operand.getOperandNumber());
+  if (!old_sharding) {
+    return;
+  }
+  sdy::TensorShardingAttr user_sharding =
+      MaybeDropSubAxesShardingSuffix(old_sharding);
+  if (user_sharding != old_sharding) {
+    sdy::setSharding(return_operand.get(), user_sharding);
+
+    static llvm::once_flag log_result_with_sub_axes_once;
+    emitSubAxisWarning(log_result_with_sub_axes_once, "result",
+                       return_operand.getOperandNumber(), old_sharding,
+                       user_sharding, sdy::getBodyTerminator(func)->getLoc());
+  }
+
+  Value result = return_operand.get();
+  if (FragmentOp defining_fragment = result.getDefiningOp<FragmentOp>()) {
+    // They could be the same type but note but with different open/closed dims.
+    // For now, we call the pass to close shardings, but this needs cleaning up.
+    // There is no need to update fragment users shardings of this
+    // `defining_fragment` like we do for transfers below because propagation
+    // have taken care of it.
+    defining_fragment.setUserSpecifiedResultSharding(
+        cast<OpResult>(return_operand.get()).getResultNumber(), user_sharding);
+  } else if (auto transfer_op = result.getDefiningOp<TransferOp>()) {
+    sdy::setSharding(transfer_op.getResult(), user_sharding);
+    UpdateFragmentUserInShardings(transfer_op, user_sharding);
+  }
+}
+
+class EnforceUserShardingsPass
+    : public impl::EnforceUserShardingsPassBase<EnforceUserShardingsPass> {
+  using EnforceUserShardingsPassBase::EnforceUserShardingsPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func) final {
+    if (!IsEntryPointFunction(func) || !IsMpmdFunction(func)) {
+      return;
+    }
+
+    for (BlockArgument arg : func.getArguments()) {
+      EnforceUserInputSharding(arg);
+    }
+
+    for (OpOperand& return_operand :
+         func.front().getTerminator()->getOpOperands()) {
+      EnforceUserResultSharding(return_operand, func);
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/extract_reshards_from_inter_mesh_transfers.cc b/shardy/dialect/mpmd/transforms/sharding_propagation/extract_reshards_from_inter_mesh_transfers.cc
new file mode 100644
index 0000000..1c20415
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/extract_reshards_from_inter_mesh_transfers.cc
@@ -0,0 +1,277 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/Support/Threading.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/common/logging.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/mpmd/ir/utils.h"
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h"  // IWYU pragma: keep
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/utils.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/utils.h"
+#include "shardy/dialect/sdy/transforms/export/explicit_reshards_util.h"
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DEF_EXTRACTRESHARDSFROMINTERMESHTRANSFERSPASS
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h.inc"
+
+namespace {
+
+using ::mlir::sdy::TensorShardingAttr;
+
+template <typename... OpTys>
+bool hasAnyUserOfTypeExcept(Value value, Operation* except) {
+  return llvm::any_of(value.getUsers(), [&except](Operation* user) {
+    return isa<OpTys...>(user) && user != except;
+  });
+}
+
+RankedTensorType GetLocalTensorType(RankedTensorType global_type,
+                                    TensorShardingAttr sharding,
+                                    sdy::MeshAttr mesh) {
+  if (!sharding) {
+    return global_type;
+  }
+  return sharding.getLocalTensorType(global_type, mesh);
+}
+
+// Returns true if the resharding should be done at the producer site, i.e., if
+// the destination tensor is on host, or if the source tensor is not on host and
+// is smaller (after sharding) than the destination tensor.
+bool ReshardAtProducerSite(MeshTensorType src_mesh_type,
+                           MeshTensorType dst_mesh_type,
+                           TensorShardingAttr src_sharding_or_null,
+                           TensorShardingAttr dst_sharding_or_null,
+                           sdy::MeshAttr mesh) {
+  SDY_CHECK(!src_mesh_type.isOnHost() || !dst_mesh_type.isOnHost());
+
+  if (dst_mesh_type.isOnHost()) {
+    return true;
+  }
+
+  if (src_mesh_type.isOnHost()) {
+    return false;
+  }
+
+  int64_t src_num_elements =
+      GetLocalTensorType(src_mesh_type.getGlobalTensorType(),
+                         src_sharding_or_null, mesh)
+          .getNumElements();
+  int64_t dst_num_elements =
+      GetLocalTensorType(dst_mesh_type.getGlobalTensorType(),
+                         dst_sharding_or_null, mesh)
+          .getNumElements();
+  return src_num_elements > dst_num_elements;
+}
+
+void HandleTransfer(TransferOp transfer, RewriterBase& rewriter,
+                    sdy::MeshAttr mesh) {
+  auto src_mesh_type =
+      cast<mpmd::MeshTensorType>(transfer.getTensor().getType());
+  auto dst_mesh_type = cast<mpmd::MeshTensorType>(transfer.getType());
+
+  if (transfer.isIntraMesh()) {
+    return;
+  }
+
+  // TODO: jupvfranco - We need a better way to handle resharding on host.
+  if (src_mesh_type.isOnHost() && dst_mesh_type.isOnHost()) {
+    transfer->emitError()
+        << "Resharding on host not supported with an mpmd.transfer.";
+    return;
+  }
+
+  TensorShardingAttr src_sharding_or_null =
+      sdy::getSharding(transfer.getTensor());
+  TensorShardingAttr dst_sharding_or_null =
+      sdy::getSharding(transfer.getResult());
+
+  // No resharding.
+  if (!sdy::shouldReshard(src_sharding_or_null, dst_sharding_or_null)) {
+    return;
+  }
+
+  // TODO: jupvfranco - the following two cases should have been supported
+  // according to StaticInputProgramPolicy::Config::SetupResharding. However,
+  // I still get an error for an N:1 case. Disabling this for now.
+  // 1:N resharding, i.e., replicated to sharded case.
+  // if (src_mesh_type.isFullyReplicated() &&
+  //     !dst_mesh_type.isFullyReplicated()) {
+  //   return;
+  // }
+  // // N:1 resharding, i.e., sharded to replicated case.
+  // if (!src_mesh_type.isFullyReplicated() &&
+  //     dst_mesh_type.isFullyReplicated()) {
+  //   return;
+  // }
+
+  static llvm::once_flag onceFlag;
+  sdy::emitOpWarningOnce(onceFlag, transfer,
+                         "Found a resharding transfer in the module. This may "
+                         "cause performance issues.");
+
+  auto reshard_body = [](ArrayRef<Value> args,
+                         OpBuilder&) -> llvm::SmallVector<Value> {
+    return {args.front()};
+  };
+  if (ReshardAtProducerSite(src_mesh_type, dst_mesh_type, src_sharding_or_null,
+                            dst_sharding_or_null, mesh)) {
+    // Reshard at producer-site because destination type is smaller than source
+    // type. I.e.,
+    //   %x = op : <M, D>
+    //   %t = transfer (%x) : (<M, D>) -> <M', D'>
+    //    ~>
+    //   %x = op : <M, D'>
+    //   %t = transfer (%x) : (<M, D'>) -> <M', D'>
+    //
+    // Or if the transferred value is an argument:
+    //   %t = transfer (%arg) : (<M, D>) -> <M', D'>
+    //    ~>
+    //   %r = fragment (%arg) : (<M, D>) -> <M, D'>
+    //   %t = transfer (%r) : (<M, D'>) -> <M', D'>
+    OpOperand& operand = transfer->getOpOperand(0);
+    Value value = operand.get();
+    MeshTensorType new_operand_type = MeshTensorType::get(
+        rewriter.getContext(), src_mesh_type.getMeshName(),
+        dst_mesh_type.getRankedTensorType());
+    if (isa<BlockArgument>(value) || isa<TransferOp>(value.getDefiningOp())) {
+      // We do not want to update the type of the block argument, not to
+      // interfere with the function signature (and its shardings).
+      rewriter.setInsertionPoint(transfer);
+      FragmentOp reshard = FragmentOp::createMeshFragmentWithGlobalBody(
+          value.getLoc(), /*user_origin=*/{}, src_mesh_type.getMeshName(),
+          value, new_operand_type, rewriter, reshard_body);
+      reshard.setUserSpecifiedResultSharding(0, dst_sharding_or_null);
+      operand.set(reshard.getResult(0));
+      return;
+    }
+
+    // If the value is used by a terminator, we need to create a fragment to
+    // make sure we don't interfere with the function signature (and its
+    // shardings). Similarly, if the value is used by *another* transfer, we
+    // also create a fragment to reshard it, so that we don't create more
+    // resharding transfers.
+    if (hasAnyUserOfTypeExcept<TransferOp, func::ReturnOp>(value, transfer)) {
+      rewriter.setInsertionPoint(transfer);
+      FragmentOp reshard = FragmentOp::createMeshFragmentWithGlobalBody(
+          value.getLoc(), /*user_origin=*/{}, new_operand_type.getMeshName(),
+          value, value.getType(), rewriter, reshard_body);
+      reshard.setUserSpecifiedResultSharding(0, src_sharding_or_null);
+      rewriter.replaceUsesWithIf(
+          value, reshard.getResult(0), [transfer](OpOperand& use) {
+            Operation* owner = use.getOwner();
+            // Automatically excludes `reshard`.
+            return isa<TransferOp, func::ReturnOp>(owner) && owner != transfer;
+          });
+    }
+
+    // At this point, `value` is produced by a fragment and used by fragments
+    // only. Thus, it is safe to simply update its type.
+    SDY_CHECK(isa<FragmentOp>(value.getDefiningOp()));
+    value.setType(new_operand_type);
+    sdy::setSharding(value, dst_sharding_or_null);
+    return;
+  }
+
+  // Reshard at consumer-site because source type is smaller than destination
+  // type. I.e.,
+  //   %t = transfer (_) : (<M, D>) -> <M', D'>
+  //        consumer (%t) : (<M', D'>) -> ...
+  //    ~>
+  //   %t = transfer (_) : (<M, D>) -> <M', D>
+  //        consumer (%t) : (<M', D>) -> ...
+  // Or:
+  //   %t = transfer (_) : (<M, D>) -> <M', D'>
+  //        return (%t)
+  //    ~>
+  //   %t = transfer : (<M, D>) -> <M', D>
+  //        fragment(%t) : (<M', D>) -> <M', D'>
+  rewriter.setInsertionPointAfter(transfer);
+  auto new_transfer = rewriter.replaceOpWithNewOp<TransferOp>(
+      transfer,
+      MeshTensorType::get(rewriter.getContext(), dst_mesh_type.getMeshName(),
+                          src_mesh_type.getRankedTensorType()),
+      transfer.getOperand());
+  if (src_sharding_or_null) {
+    sdy::setSharding(new_transfer.getResult(), src_sharding_or_null);
+  }
+
+  // Update the in_shardings attr of fragment users.
+  // We need to do this because the later
+  // `ConvertSdyShardingsToMpmdTypesPass` that moves the sharding to
+  // `MeshTensorType` will read the in-shardings from the fragment.
+  UpdateFragmentUserInShardings(new_transfer,
+    src_sharding_or_null);
+
+  if (sdy::hasAnyUserOfType<TransferOp, func::ReturnOp>(
+          new_transfer.getResult())) {
+    FragmentOp reshard = FragmentOp::createMeshFragmentWithGlobalBody(
+        new_transfer.getLoc(), /*user_origin=*/{}, dst_mesh_type.getMeshName(),
+        new_transfer.getResult(),
+        MeshTensorType::get(rewriter.getContext(), dst_mesh_type.getMeshName(),
+                            dst_mesh_type.getRankedTensorType()),
+        rewriter, reshard_body);
+    reshard.setUserSpecifiedResultSharding(0, dst_sharding_or_null);
+    rewriter.replaceUsesWithIf(
+        new_transfer.getResult(), reshard.getResult(0), [](OpOperand& use) {
+          return isa<TransferOp, func::ReturnOp>(use.getOwner());
+        });
+  }
+  SDY_CHECK(
+      llvm::all_of(new_transfer.getResult().getUsers(),
+                   [](Operation* user) { return isa<FragmentOp>(user); }));
+}
+
+class ExtractReshardsFromInterMeshTransfersPass
+    : public impl::ExtractReshardsFromInterMeshTransfersPassBase<
+          ExtractReshardsFromInterMeshTransfersPass> {
+  using ExtractReshardsFromInterMeshTransfersPassBase::
+      ExtractReshardsFromInterMeshTransfersPassBase;
+
+ private:
+  void runOnFunc(func::FuncOp func_op) final {
+    SDY_CHECK(mpmd::IsMpmdFunction(func_op))
+        << "Expected pass to be applied on MPMD partitioning code path.";
+    if (!mpmd::HasHomogeneousTopology(func_op)) {
+      // This is needed to guarantee that when we push a distributed type to
+      // the mesh_tensor type of a block argument, we create well-formed types.
+      // TODO: jupvfranco - relax this condition. Even if the meshes in the
+      // topology are different, pushing distributed types to arguments can
+      // still form valid types.
+      return;
+    }
+    IRRewriter rewriter(&getContext());
+    // Assumes that the topology is homogeneous so we can just get the first
+    // mesh.
+    sdy::MeshAttr mesh = mpmd::GetTopologyMeshes(func_op).front().getMesh();
+    func_op.walk(
+        [&](TransferOp transfer) { HandleTransfer(transfer, rewriter, mesh); });
+  }
+};
+
+}  // namespace
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/passes.h b/shardy/dialect/mpmd/transforms/sharding_propagation/passes.h
new file mode 100644
index 0000000..e13a58e
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/passes.h
@@ -0,0 +1,51 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_SHARDING_PROPAGATION_PASSES_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_SHARDING_PROPAGATION_PASSES_H_
+
+// IWYU pragma: begin_keep
+
+#include <memory>
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassOptions.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/transforms/common/distributed_function_pass.h"
+
+// IWYU pragma: end_keep
+
+namespace mlir::mpmd {
+
+#define GEN_PASS_DECL
+#define GEN_PASS_REGISTRATION
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h.inc"
+
+// Adds the standard set of passes to propagate shardings in an MPMD program,
+// which set the shardings in mesh tensors across the MPMD program based on user
+// specified shardings.
+//
+// `sdyDumpDir` specified the dump directory to pass to the SDY propagation
+// pipeline.
+void addShardingPropagationPipeline(OpPassManager& pm, StringRef sdyDumpDir);
+
+// Register the `-mpmd-sharding-propagation-pipeline`.
+void registerShardingPropagationPipeline();
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_SHARDING_PROPAGATION_PASSES_H_
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/passes.td b/shardy/dialect/mpmd/transforms/sharding_propagation/passes.td
new file mode 100644
index 0000000..e7aa509
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/passes.td
@@ -0,0 +1,95 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+def ConvertSdyShardingsToMpmdTypesPass :
+    PassBase<"mpmd-convert-sdy-shardings-to-mpmd-types",
+             "DistributedFunctionPass"> {
+  let summary = "Moves shardings from op attrs to `!mpmd.mesh_tensor` types.";
+  let description = [{
+    Moves shardings from the attributes of MPMD ops (e.g. fragments, transfer)
+    to the `MeshTensorType` of their results. Assuming we apply SDY propagation
+    before this pass, the SPMD shardings are attached to the op's attributes.
+    This pass moves the sharding to `MeshTensorType`s since later passes require
+    the type to contain a sharding.
+
+    This pass also removes any sharding from ops that don't have a
+    `MeshTensorType`, i.e., ops inside `mpmd.fragment` ops.
+  }];
+
+  let dependentDialects = ["mlir::mpmd::MpmdDialect"];
+}
+
+def ConvertSdyConstantsPass :
+    Pass<"mpmd-convert-sdy-constants", "func::FuncOp"> {
+  let summary = "Converts `sdy.constant` ops into `stablehlo.constant`.";
+  let description = [{
+    Converts any `sdy.constant` op, that isn't foldable, into a
+    `stablehlo.constant` op. There is no reason to prevent constant folding
+    since we are stripping shardings away from constants in
+    `mpmd-convert-sdy-shardings-to-mpmd-types`.
+  }];
+
+  let dependentDialects = ["mlir::stablehlo::StablehloDialect"];
+}
+
+def EnforceUserShardingsPass :
+    PassBase<"mpmd-enforce-user-shardings", "DistributedFunctionPass"> {
+  let summary = "Enforces the user specified shardings for inputs and outputs.";
+  let description = [{
+    Enforces the input and output shardings of fragments that take function
+    arguments or produce function results respectively, to be the ones specified
+    by the user, i.e., the input and outputs shardings of the function.
+
+    After this pass, fragment and transfer users of function arguments and
+    producers of function results should have the same shardings as the ones
+    specified by the user. If the user did not specify a sharding for an input
+    or output, this pass keeps the sharding that propagation assigned.
+
+    Precondition:
+    - The user shardings are set on the function's arguments and results as
+      attributes.
+    - The fragment shardings are set on `in_shardings` and `out_shardings`
+      attributes.
+  }];
+
+  let dependentDialects = ["mlir::sdy::SdyDialect"];
+}
+
+// TODO: b/329842439 - Longer term, we need a better strategy or better support
+// for this.
+def ExtractReshardsFromInterMeshTransfersPass :
+    PassBase<"mpmd-extract-reshards-from-inter-mesh-transfers",
+             "DistributedFunctionPass"> {
+  let summary = "Moves SPMD resharding around an inter-mesh transfer to inside "
+                "a fragment.";
+  let description = [{
+    Ensures that all inter-mesh transfers do not (SPMD) reshard the array (their
+    in and out shardings are the same), by updating the types of
+    producer/consumer fragments or by creating inferred fragments for
+    non-fragment producers/consumers.
+
+    This is needed as MPMD runtimes have limitations w.r.t. supported
+    reshardings.
+
+    This pass is only applied to MPMD functions in global view and with a
+    homogeneous topology.
+
+    Precondition: all shardings are specified as op attributes and not in types.
+  }];
+
+  let dependentDialects = ["mlir::mpmd::MpmdDialect", "mlir::sdy::SdyDialect"];
+}
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/sharding_propagation_pipeline.cc b/shardy/dialect/mpmd/transforms/sharding_propagation/sharding_propagation_pipeline.cc
new file mode 100644
index 0000000..62b0aaa
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/sharding_propagation_pipeline.cc
@@ -0,0 +1,66 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Pass/PassRegistry.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/transforms/common/passes.h"
+#include "shardy/dialect/mpmd/transforms/sharding_propagation/passes.h"
+#include "shardy/dialect/sdy/transforms/propagation/passes.h"
+
+namespace mlir::mpmd {
+
+void addShardingPropagationPipeline(OpPassManager& pm,
+                                    llvm::StringRef sdyDumpDir) {
+  // Uniquify function inputs and outputs, in case the same fragment result or
+  // function input is returned multiple times with different shardings.
+  pm.addNestedPass<func::FuncOp>(createUniquifyFunctionInputsOutputsPass());
+
+  // Run sdy propagation.
+  sdy::PropagationOptions options;
+  options.dumpDirectory = sdyDumpDir;
+  options.avoidExportForPartitioning = true;
+  options.skipInline = true;
+  sdy::addPropagationPipeline(pm, options);
+
+  // Enforce user specified shardings.
+  pm.addNestedPass<func::FuncOp>(createEnforceUserShardingsPass());
+
+  // Extract reshard from inter-mesh transfers.
+  pm.addNestedPass<func::FuncOp>(
+      createExtractReshardsFromInterMeshTransfersPass());
+
+  // Add the shardings back to `MeshTensorType`. Before this pass, the shardings
+  // are on the attributes of fragments and transfer ops.
+  pm.addNestedPass<func::FuncOp>(createConvertSdyShardingsToMpmdTypesPass());
+
+  // No need to keep SDY constants, which prevent constant folding, since we are
+  // stripping shardings away from constants in
+  // `mpmd-convert-sdy-shardings-to-mpmd-types`.
+  pm.addNestedPass<func::FuncOp>(createConvertSdyConstantsPass());
+}
+
+void registerShardingPropagationPipeline() {
+  PassPipelineRegistration<>(
+      "mpmd-sharding-propagation-pipeline",
+      "Run the standard set of passes to propagate shardings in an "
+      "MPMD program.",
+      [](OpPassManager& pm) {
+        return addShardingPropagationPipeline(pm, /*sdyDumpDir=*/"");
+      });
+}
+
+}  // namespace mlir::mpmd
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/test/BUILD b/shardy/dialect/mpmd/transforms/sharding_propagation/test/BUILD
new file mode 100644
index 0000000..74e909b
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/test/BUILD
@@ -0,0 +1,21 @@
+# Lit tests for the MPMD sharding propagation passes.
+
+load("//shardy:lit.bzl", "glob_lit_tests")
+
+package(default_visibility = ["//visibility:public"])
+
+filegroup(
+    name = "test_data",
+    testonly = True,
+    data = [
+        "//shardy/tools:mpmd_opt",
+        "@llvm-project//llvm:FileCheck",
+    ],
+)
+
+glob_lit_tests(
+    name = "all_tests",
+    data = [":test_data"],
+    driver = "@llvm-project//mlir:run_lit.sh",
+    test_file_exts = ["mlir"],
+)
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/test/add_sdy_data_flow_edges.mlir b/shardy/dialect/mpmd/transforms/sharding_propagation/test/add_sdy_data_flow_edges.mlir
new file mode 100644
index 0000000..2c217d1
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/test/add_sdy_data_flow_edges.mlir
@@ -0,0 +1,76 @@
+// RUN: mpmd_opt %s -sdy-add-data-flow-edges 2>&1 | FileCheck %s
+
+sdy.mesh @mesh = <["x"=4, "y"=2]>
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+// CHECK-LABEL: func @fragment_data_flow_edges_are_added_correctly
+func.func @fragment_data_flow_edges_are_added_correctly(
+   %arg0: !mesh_1_tensor_4_8_f32
+     {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+   %arg1: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {"topology"=#mpmd.topology<<"m1": <["x"=4]>>>} {
+    %0 = mpmd.fragment<mesh="m1", origin=[], in_shardings=[<@mesh, [{"x"}, {}]>], out_shardings=[<@mesh, [{"x", ?}, {?}]>]>
+      (%arg0) (%arg2: tensor<4x8xf32>) {
+      // CHECK: sdy.data_flow_edge %arg2 sharding=<@mesh, [{"x"}, {}]> : tensor<4x8xf32>
+      %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %3 : tensor<4x8xf32>
+    } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+    // CHECK: %[[RESULT:.*]] = sdy.data_flow_edge %0 sharding=<@mesh, [{"x", ?}, {?}]>
+    // CHECK: return %[[RESULT]]
+    func.return %0 : !mesh_1_tensor_4_8_f32
+  }
+
+// CHECK-LABEL: func @fragment_with_multiple_args_and_results
+func.func @fragment_with_multiple_args_and_results(
+   %arg0: !mesh_1_tensor_4_8_f32, %arg1: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>} {
+    %0:2 = mpmd.fragment<mesh="m1", origin=[], in_shardings=[<@mesh, [{"x"}, {}]>, <@mesh, [{}, {"x"}]>]>
+      (%arg0, %arg1)
+      (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+      // CHECK: sdy.data_flow_edge %arg2 sharding=<@mesh, [{"x"}, {}]>
+      // CHECK: sdy.data_flow_edge %arg3 sharding=<@mesh, [{}, {"x"}]>
+      %3 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+      mpmd.return %3, %3 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+    // CHECK-NOT: sharding=
+    // CHECK: %[[RESULT:.*]] = sdy.data_flow_edge %0#0
+    // CHECK: %[[RESULT1:.*]] = sdy.data_flow_edge %0#1
+    func.return %0#0 : !mesh_1_tensor_4_8_f32
+  }
+
+// CHECK-LABEL: @for_loop_with_sharding
+func.func @for_loop_with_sharding(%arg0: tensor<10xui32>, %arg1: tensor<10xui32>)
+  -> (tensor<10xui32>, tensor<10xui32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2, "y"=2]>>
+    >} {
+    %0:2 = mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x", ?}]>, <@mesh, [{"y", ?}]>]>, unroll_factor = 3 : ui32}
+      (%arg2: tensor<10xui32>, %arg3: tensor<10xui32>, %index: tensor<ui32>) {
+      %1 = stablehlo.broadcast_in_dim %index, dims = [] : (tensor<ui32>) -> tensor<10xui32>
+      %2 = stablehlo.add %arg2, %1 : tensor<10xui32>
+      %3 = stablehlo.add %arg2, %arg3 : tensor<10xui32>
+      mpmd.return %2, %3 : tensor<10xui32>, tensor<10xui32>
+  } : tensor<10xui32>, tensor<10xui32>
+  // CHECK %[[RESULT:.*]] = sdy.data_flow_edge %0#0 sharding=<@mesh, [{"x", ?}]> : tensor<10xui32>
+  // CHECK %[[RESULT1:.*]] = sdy.data_flow_edge %0#1 sharding=<@mesh, [{"y", ?}]> : tensor<10xui32>
+  func.return %0#0, %0#1 : tensor<10xui32>, tensor<10xui32>
+}
+
+// CHECK-LABEL: @for_loop_without_sharding
+func.func @for_loop_without_sharding(%arg0: tensor<10xui32>, %arg1: tensor<10xui32>)
+  -> (tensor<10xui32>, tensor<10xui32>) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>
+    >} {
+    %0:2 = mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, unroll_factor = 3 : ui32}
+      (%arg2: tensor<10xui32>, %arg3: tensor<10xui32>, %index: tensor<ui32>) {
+      %1 = stablehlo.broadcast_in_dim %index, dims = [] : (tensor<ui32>) -> tensor<10xui32>
+      %2 = stablehlo.add %arg2, %1 : tensor<10xui32>
+      %3 = stablehlo.add %arg2, %arg3 : tensor<10xui32>
+      mpmd.return %2, %3 : tensor<10xui32>, tensor<10xui32>
+  } : tensor<10xui32>, tensor<10xui32>
+  // CHECK %[[RESULT:.*]] = sdy.data_flow_edge %0#0 : tensor<10xui32>
+  // CHECK %[[RESULT1:.*]] = sdy.data_flow_edge %0#1 : tensor<10xui32>
+  func.return %0#0, %0#1 : tensor<10xui32>, tensor<10xui32>
+}
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/test/convert_sdy_shardings_to_mpmd_types.mlir b/shardy/dialect/mpmd/transforms/sharding_propagation/test/convert_sdy_shardings_to_mpmd_types.mlir
new file mode 100644
index 0000000..0366fe6
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/test/convert_sdy_shardings_to_mpmd_types.mlir
@@ -0,0 +1,136 @@
+// RUN: mpmd_opt %s -mpmd-convert-sdy-shardings-to-mpmd-types 2>&1 | FileCheck -implicit-check-not sdy.sharding -implicit-check-not in_shardings %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+
+module {
+sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: func @fragment_with_input_and_result_shardings
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+func.func @fragment_with_input_and_result_shardings(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x", ?}, {?}]>}) attributes {topology = #topology} {
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"x"}, {}]>], out_shardings=[<@mesh, [{"x"}, {}]>]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : tensor<4x8xf32>
+      mpmd.return %2 : tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+
+// CHECK-LABEL: func @fragment_with_only_input_sharding
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+func.func @fragment_with_only_input_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) attributes {topology = #topology} {
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"x"}, {}]>]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : tensor<4x8xf32>
+      mpmd.return %2 : tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+
+
+// CHECK-LABEL: func @fragment_with_only_result_shardings
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+func.func @fragment_with_only_result_shardings(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x", ?}, {?}]>}) attributes {topology = #topology} {
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"], out_shardings=[<@mesh, [{"x"}, {}]>]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : tensor<4x8xf32>
+      mpmd.return %2 : tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+
+// CHECK-LABEL: func @fragment_without_input_or_result_shardings
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+func.func @fragment_without_input_or_result_shardings(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) attributes {topology = #topology} {
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %2 : tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+
+// CHECK-LABEL: func @transfer_has_no_sharding
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+func.func @transfer_has_no_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+    %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+    return %0 : !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  }
+
+  // CHECK-LABEL: func @single_transfer_has_sharding
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+func.func @single_transfer_has_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.transfer %arg0
+    // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+    %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+    return %0 : !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  }
+
+// CHECK-LABEL: func @fragment_with_transfer_fragment_result_has_multiple_uses
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+// CHECK-SAME: -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+// CHECK-SAME: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+func.func @fragment_with_transfer_fragment_result_has_multiple_uses(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+   !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x", ?}, {?}]>}) attributes {topology = #topology} {
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"x"}, {}]>]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : tensor<4x8xf32>
+      mpmd.return %2 : tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    // CHECK: mpmd.transfer %0
+    // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+    %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} %0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+    return %0, %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  }
+
+// CHECK-LABEL: func @for_loop_with_fragment_nested
+func.func @for_loop_with_fragment_nested(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}, %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) -> (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>, <"m2" : <["x"=2]>>>} {
+  // CHECK: mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, unroll_factor = 3 : ui32}
+  // CHECK-SAME: (%arg2: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+  // CHECK-SAME: %arg3: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>, %index: tensor<ui32>)
+  %0:2 = mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>, <@mesh, [{"x"}, {}]>]>, unroll_factor = 3 : ui32} (%arg2: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg3: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %index: tensor<ui32>) {
+    %2:2 = mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"x"}, {}]>], out_shardings = [<@mesh, [{"x"}, {}]>, <@mesh, [{"x"}, {}]>]> (%arg2) (%arg4: tensor<4x8xf32>) {
+      %3 = stablehlo.add %arg4, %arg4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : tensor<4x8xf32>
+      mpmd.return %3, %3 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    mpmd.return %2#0, %2#1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  } : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} %0#0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  return %1, %0#1 : !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+}
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/test/convert_sdy_shardings_to_mpmd_types_failures.mlir b/shardy/dialect/mpmd/transforms/sharding_propagation/test/convert_sdy_shardings_to_mpmd_types_failures.mlir
new file mode 100644
index 0000000..f4a8770
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/test/convert_sdy_shardings_to_mpmd_types_failures.mlir
@@ -0,0 +1,38 @@
+// RUN: mpmd_opt %s -mpmd-convert-sdy-shardings-to-mpmd-types -split-input-file 2>&1 | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+
+module {
+sdy.mesh @mesh = <["x"=2]>
+// CHECK-LABEL: func @transfer_with_different_tensor_and_result_shardings
+func.func @transfer_with_different_tensor_and_result_shardings(
+  %arg0: !mesh_1_tensor_4_8_f32 {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>}) ->
+  (!mesh_2_tensor_4_8_f32) attributes {topology = #topology} {
+    // expected-error @+1: Transfer op has different shardings for the tensor and result, tensor sharding: #sdy.sharding<@mesh, [{}, {"x"}]>, result sharding: #sdy.sharding<@mesh, [{"x"}, {}]>
+    %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} %arg0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+    return %0 : !mesh_2_tensor_4_8_f32
+  }
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2, "y"=2]>>,<"m2": <["x"=2, "y"=2]>>>
+
+module {
+sdy.mesh @mesh = <["x"=2, "y"=2]>
+// CHECK-LABEL: func @transfer_result_has_different_sharding_at_fragment
+func.func @transfer_result_has_different_sharding_at_fragment(
+  %arg0: !mesh_1_tensor_4_8_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {}]>}) ->
+  (!mesh_2_tensor_4_8_f32) attributes {topology = #topology} {
+    // expected-error @+1: Transfer op has different shardings for the tensor and result, tensor sharding: #sdy.sharding<@mesh, [{"y"}, {}]>, result sharding: #sdy.sharding<@mesh, [{}, {"x"}]>
+    %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {}]>]>} %arg0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+    %1 = mpmd.fragment<mesh="m2", origin=["f"],  in_shardings=[<@mesh, [{}, {"x"}]>]> (%0) (%arg2: tensor<4x8xf32>) {
+      mpmd.return %arg2 : tensor<4x8xf32>
+  } : (!mesh_2_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+    return %0 : !mesh_2_tensor_4_8_f32
+  }
+}
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/test/enforce_user_shardings.mlir b/shardy/dialect/mpmd/transforms/sharding_propagation/test/enforce_user_shardings.mlir
new file mode 100644
index 0000000..acc13a2
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/test/enforce_user_shardings.mlir
@@ -0,0 +1,259 @@
+// RUN: mpmd_opt %s -mpmd-enforce-user-shardings -verify-diagnostics -split-input-file | FileCheck %s
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+
+module {
+sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: func @enforce_user_arg_sharding
+// The user specified sharding for %arg0 but not for %arg1.
+// The fragment should get the user specified sharding for %arg0 but keep the sharding for %arg1.
+// The transfer op should get the user specified sharding.
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+func.func @enforce_user_arg_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"x"}, {}]>, <@mesh, [{?}, {?}]>]>
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{}, {"x"}]>, <@mesh, [{?}, {?}]>]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+      mpmd.return %2 : tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    // CHECK: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} %arg0 :
+    // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+    %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} %arg0  : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+    return %1 : !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  }
+}
+
+
+// -----
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+module {
+  sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: func @no_user_arg_sharding_should_keep_fragment_in_sharding
+func.func @no_user_arg_sharding_should_keep_fragment_in_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"x"}, {}]>]> (%arg0)
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"x"}, {}]>]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %2: tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
+
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+
+module {
+sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: func @enforce_user_sharding_for_fragment_result
+func.func @enforce_user_sharding_for_fragment_result(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.fragment<mesh="m1", origin=["producer"], out_shardings=[<@mesh, [{"x"}, {}]>, <@mesh, [{?}, {?}]>]> (%arg0)
+    %0:2 = mpmd.fragment<mesh="m1", origin=["producer"], out_shardings=[<@mesh, [{}, {"x"}]>, <@mesh, [{?}, {?}]>]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %2, %arg2 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    return %0#0, %0#1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,  !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+module {
+  sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: func @enforce_user_sharding_for_transfer_result_if_no_original_sharding
+func.func @enforce_user_sharding_for_transfer_result_if_no_original_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) attributes {topology = #topology} {
+    // CHECK: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} %arg0 :
+    // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    %1 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    return %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+module {
+  sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: func @enforce_user_sharding_for_transfer_result
+func.func @enforce_user_sharding_for_transfer_result(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) attributes {topology = #topology} {
+    // CHECK: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} %arg0 :
+    // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    return %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
+
+// -----
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+module {
+  sdy.mesh @mesh = <["x"=2]>
+
+  // CHECK-LABEL: func @no_user_result_sharding_should_keep_fragment_result_sharding
+func.func @no_user_result_sharding_should_keep_fragment_result_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.fragment<mesh="m1", origin=["producer"], out_shardings=[<@mesh, [{"x"}, {}]>]> (%arg0)
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"], out_shardings=[<@mesh, [{"x"}, {}]>]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %2: tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2]>>,<"m2": <["x"=2]>>>
+module {
+  sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: func @only_enforce_user_sharding_once_for_transitive_transfer
+func.func @only_enforce_user_sharding_once_for_transitive_transfer(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) attributes {topology = #topology} {
+    // CHECK: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+    // CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    // CHECK: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} %0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+    // CHECK-SAME: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+    %2 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} %1 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+    return %2 : !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+  }
+}
+
+
+// -----
+
+!mesh_1_tensor_8_2_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>>
+!mesh_2_tensor_8_2_f32 = !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>>
+#topology = #mpmd.topology<<"mesh1" : <["devices"=4]>>, <"mesh2" : <["devices"=4]>>>
+
+
+module {
+  sdy.mesh @mesh = <["devices"=4]>
+
+// CHECK-LABEL: func @transfer_operand_and_result_with_different_sharding
+func.func @transfer_operand_and_result_with_different_sharding(
+  %arg0: !mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices", ?}, {?}]>},
+  %arg1: !mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices", ?}, {?}]>}) ->
+  (!mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices"}, {}]>},
+  !mesh_2_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}) attributes {topology = #topology} {
+  %0 = mpmd.fragment<mesh="mesh1", origin=["add"], in_shardings=[<@mesh, [{"devices", ?}, {?}]>, <@mesh, [{"devices", ?}, {?}]>]> (%arg0, %arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"devices", ?}, {?}]>]>} (%arg2: tensor<8x2xf32>, %arg3: tensor<8x2xf32>) {
+    %2 = stablehlo.add %arg2, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"devices", ?}, {?}]>]>} : tensor<8x2xf32>
+    mpmd.return %2 : tensor<8x2xf32>
+  } : (!mesh_1_tensor_8_2_f32, !mesh_1_tensor_8_2_f32) -> !mesh_1_tensor_8_2_f32
+  // CHECK: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}]>]>} %0
+  // CHECK-SAME: (!mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>>) ->
+  // CHECK-SAME: !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>>
+  %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"devices", ?}, {?}]>]>} %0 : (!mesh_1_tensor_8_2_f32) -> !mesh_2_tensor_8_2_f32
+  return %0, %1 : !mesh_1_tensor_8_2_f32, !mesh_2_tensor_8_2_f32
+}
+
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=4, "y"=2]>>,<"m2": <["x"=4, "y"=2]>>>
+
+module {
+sdy.mesh @mesh = <["x"=4, "y"=2]>
+
+// CHECK-LABEL: func @enforce_user_arg_sharding_with_sub_axes
+// After propagation, the sharding of %arg0 has sub-axes.
+// Replace the sub-axes with empty axes.
+// CHECK-SAME: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {}]>},
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}
+func.func @enforce_user_arg_sharding_with_sub_axes(
+  // expected-warning@below {{Sub-axes sharding found for arg 0}}
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"y", "x":(1)2}, {}]>},
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x":(1)2}, {}]>}) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"y"}, {}]>, <@mesh, [{}, {}]>]>
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{}, {"x"}]>, <@mesh, [{?}, {?}]>]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+      mpmd.return %2 : tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=4, "y"=2]>>,<"m2": <["x"=4, "y"=2]>>>
+
+module {
+sdy.mesh @mesh = <["x"=4, "y"=2]>
+
+// CHECK-LABEL: func @enforce_user_result_sharding_with_sub_axes
+func.func @enforce_user_result_sharding_with_sub_axes(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x":(1)2}, {"y"}]>}, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.fragment<mesh="m1", origin=["producer"], out_shardings=[<@mesh, [{}, {"y"}]>, <@mesh, [{?}, {?}]>]> (%arg0)
+    %0:2 = mpmd.fragment<mesh="m1", origin=["producer"], out_shardings=[<@mesh, [{}, {"x"}]>, <@mesh, [{?}, {?}]>]> (%arg0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %2, %arg2 : tensor<4x8xf32>, tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    // expected-warning@below {{Sub-axes sharding found for result 0}}
+    return %0#0, %0#1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,  !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2, "y"=2]>>,<"m2": <["x"=2, "y"=2]>>>
+module {
+  sdy.mesh @mesh = <["x"=2, "y"=2]>
+
+// CHECK-LABEL: func @enforce_result_sharding_on_transfer_result_used_in_fragment(
+func.func @enforce_result_sharding_on_transfer_result_used_in_fragment(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) attributes {topology = #topology} {
+    // CHECK: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} %arg0 :
+    // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    // CHECK: mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"x"}, {}]>]>
+    %1 = mpmd.fragment<mesh="m1", origin=["producer"], in_shardings=[<@mesh, [{"y"}, {}]>]> (%0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %2: tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    return %0, %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/test/extract_reshard_from_inter_mesh_transfer.mlir b/shardy/dialect/mpmd/transforms/sharding_propagation/test/extract_reshard_from_inter_mesh_transfer.mlir
new file mode 100644
index 0000000..cadf9a2
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/test/extract_reshard_from_inter_mesh_transfer.mlir
@@ -0,0 +1,247 @@
+// RUN: mpmd_opt %s -mpmd-extract-reshards-from-inter-mesh-transfers -split-input-file 2>&1 | FileCheck %s
+
+module {
+sdy.mesh @mesh = <["x"=2, "y"=2]>
+
+// The local tensor at the destination has the same number of elements as the
+// local tensor at the source.
+// This causes the reshard to happen at consumer site.
+// CHECK-LABEL: func @reshard_on_consumer_when_same_local_type_size
+func.func @reshard_on_consumer_when_same_local_type_size(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>}
+{
+  // CHECK-NEXT: %[[TRANSFER_RESULT:.*]] = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  // CHECK-NEXT: %[[RESHARD_RESULT:.*]] = mpmd.fragment<mesh="m2", origin=[], out_shardings=[<@mesh, [{?}, {"x"}]>]> (%[[TRANSFER_RESULT]]) (%arg1: tensor<4x8xui32>) {
+  // CHECK-NEXT:     mpmd.return %arg1 : tensor<4x8xui32>
+  // CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %arg0: (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  func.return %0 : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+}
+
+// -----
+
+module {
+sdy.mesh @mesh = <["x"=2, "y"=4]>
+
+// The local tensor at the source has more elements than at the destination.
+// This causes the reshard to happen at producer site.
+// CHECK-LABEL: func @reshard_on_producer_when_local_type_smaller_on_producer
+func.func @reshard_on_producer_when_local_type_smaller_on_producer(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {?}]>}, !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>}
+{
+  // CHECK-NEXT: %[[RESHARD_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=[], out_shardings=[<@mesh, [{"y"}, {?}]>]> (%arg0) (%arg1: tensor<4x8xui32>) {
+  // CHECK-NEXT:   mpmd.return %arg1 : tensor<4x8xui32>
+  // CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  // CHECK-NEXT: %[[TRANSFER_RESULT:.*]] = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {?}]>]>} %0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  // CHECK-NEXT: return %[[TRANSFER_RESULT]], %arg0
+  func.return %0, %arg0 : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+}
+}
+
+// -----
+
+module {
+sdy.mesh @mesh = <["x"=2, "y"=2]>
+
+// Given the topology isn't homogeneous, no rewrite can be applied.
+// CHECK-LABEL: func @topology_isnt_homogeneous
+func.func @topology_isnt_homogeneous(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>, sharding=<@mesh, [{"x"}, {?}]>>)
+  -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["y"=2]>>>}
+{
+  // CHECK-NEXT: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"y"}]>]>} %arg0
+  // CHECK-NEXT: return
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"y"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>, sharding=<@mesh, [{"x"}, {?}]>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  func.return %0 : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+
+// CHECK-LABEL: func @intra_mesh_transfer_does_not_introduce_reshard
+func.func @intra_mesh_transfer_does_not_introduce_reshard(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  // CHECK-NEXT: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  func.return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+}
+
+// CHECK-LABEL: func @same_mesh_different_memory_kind
+func.func @same_mesh_different_memory_kind(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>, memory_kind="pinned_host"> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>>}
+{
+  // CHECK-NEXT: %[[TRANSFER_RESULT:.*]] = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 :
+  // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>, memory_kind="pinned_host">) ->
+  // CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  // CHECK-NEXT: %[[RESHARD_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=[], out_shardings=[<@mesh, [{?}, {"x"}]>]> (%[[TRANSFER_RESULT]]) (%arg1: tensor<4x8xui32>)
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>, memory_kind="pinned_host">) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  func.return %0 : !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+}
+
+// CHECK-LABEL: func @no_reshard_when_sharding_matches
+func.func @no_reshard_when_sharding_matches(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>}
+{
+  // CHECK-NEXT: transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  func.return %0 : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+
+// CHECK-LABEL: func @no_reshard_on_null_and_replicated_sharding
+func.func @no_reshard_on_null_and_replicated_sharding(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>})
+  -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>}
+{
+  // CHECK-NOT: mpmd.fragment
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  func.return %0 : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+
+// All the tests above exercise the decision of whether to reshard at consumer
+// or producer site, and no-op behaviour cases. The following tests focus on
+// whether the value types should be immediately updated or inferred fragments
+// should be created.
+}
+
+// -----
+module {
+sdy.mesh @mesh = <["x"=2, "y"=2]>
+
+// CHECK-LABEL: func @reshard_on_consumer_fragments_and_with_new_fragment
+func.func @reshard_on_consumer_fragments_and_with_new_fragment(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"x"}]>})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>}
+{
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+
+  // Create a new reshard fragment as the transfer is used by another transfer
+  // and by the return op.
+  // CHECK-NEXT: %[[RESHARD:.*]] = mpmd.fragment<mesh="m2", origin=[], out_shardings=[<@mesh, [{?}, {"x"}]>]> (%[[TRANSFER]])
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %t = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+
+  // Update the type of the fragment's operand with the resharded type.
+  // CHECK-NEXT: %[[C1:.*]] = mpmd.fragment<mesh="m2", origin=[], in_shardings=[<@mesh, [{"x"}, {?}]>], out_shardings=[<@mesh, [{?}, {"x"}]>]>
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %c1 = mpmd.fragment<mesh="m2", origin=[], in_shardings=[<@mesh, [{?}, {"x"}]>], out_shardings=[<@mesh, [{?}, {"x"}]>]> (%t) (%arg1: tensor<4x8xui32>) {
+    mpmd.return %arg1 : tensor<4x8xui32>
+  } : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+
+  // CHECK-NEXT: %[[C2:.*]] = mpmd.fragment<mesh="m2", origin=[], in_shardings=[<@mesh, [{"x"}, {?}]>], out_shardings=[<@mesh, [{?}, {"x"}]>]>
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %c2 = mpmd.fragment<mesh="m2", origin=[], in_shardings=[<@mesh, [{?}, {"x"}]>], out_shardings=[<@mesh, [{?}, {"x"}]>]> (%t) (%arg1: tensor<4x8xui32>) {
+    mpmd.return %arg1 : tensor<4x8xui32>
+  } : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+
+  // Not a resharding transfer.
+  // This transfer, if updated, would reshard the tensor, which isn't desired.
+  // The resulting transfer must be identical to this one.
+  // CHECK-NEXT: %[[C3:.*]] = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %[[RESHARD]] : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %c3 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %t : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+
+  // CHECK-NEXT: return %[[RESHARD]]
+  func.return %t : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+
+}
+
+// -----
+
+module {
+sdy.mesh @mesh = <["x"=2, "y"=4]>
+
+// CHECK-LABEL: func @create_fragment_at_producer_when_used_by_another_transfer
+func.func @create_fragment_at_producer_when_used_by_another_transfer(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"x"}]>})
+  -> (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {?}]>}, !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>}
+{
+  // CHECK-NEXT: %[[PROD:.*]] = mpmd.fragment<mesh="m1", origin=[], in_shardings=[<@mesh, [{?}, {"x"}]>], out_shardings=[<@mesh, [{"y"}, {?}]>]> (%arg0)
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  %prod = mpmd.fragment<mesh="m1", origin=[], in_shardings=[<@mesh, [{?}, {"x"}]>], out_shardings=[<@mesh, [{"x"}, {?}]>]> (%arg0) (%arg1: tensor<4x8xui32>) {
+    mpmd.return %arg1 : tensor<4x8xui32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  // CHECK-NEXT: %[[RESHARD:.*]] =  mpmd.fragment<mesh="m1", origin=[], out_shardings=[<@mesh, [{"x"}, {?}]>]> (%[[PROD]])
+  // CHECK-NEXT:   mpmd.return
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {?}]>]>} %[[PROD]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {?}]>]>} %prod : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+
+  // Not a resharding transfer.
+  // This transfer, if updated, would reshard the tensor, which isn't desired.
+  // The resulting transfer must be identical to this one.
+  // CHECK-NEXT: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %[[RESHARD]] : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  %another_user_of_prod = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %prod : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+
+  // return %[[TRANSFER]], %[[RESHARD]]
+  func.return %0, %prod : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+}
+
+// CHECK-LABEL: func @create_fragment_when_producer_is_transfer
+func.func @create_fragment_when_producer_is_transfer(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {?}]>})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>}
+{
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  // CHECK-NEXT: %[[RESHARD:.*]] = mpmd.fragment<mesh="m1", origin=[], out_shardings=[<@mesh, [{"y"}, {?}]>]> (%[[TRANSFER]]) (%arg1: tensor<4x8xui32>) {
+  // CHECK-NEXT:    mpmd.return %arg1 : tensor<4x8xui32>
+  // CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  // CHECK-NEXT: %[[TRANSFER_RESULT:.*]] = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {?}]>]>} %1 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {?}]>]>} %0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  func.return %1 : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+
+// CHECK-LABEL: func @update_input_sharding_on_consumer_fragment_with_existing_shardings
+func.func @update_input_sharding_on_consumer_fragment_with_existing_shardings(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"x"}]>})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>}
+{
+  // CHECK-NEXT: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  // CHECK-NEXT: mpmd.fragment<mesh="m2", origin=[], in_shardings=[<@mesh, [{"x"}, {?}]>], out_shardings=[<@mesh, [{?}, {"x"}]>]>
+  %consumer = mpmd.fragment<mesh="m2", origin=[], in_shardings=[<@mesh, [{?}, {"x"}]>], out_shardings=[<@mesh, [{?}, {"x"}]>]> (%0) (%arg1: tensor<4x8xui32>) {
+    mpmd.return %arg1 : tensor<4x8xui32>
+  } : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  func.return %consumer : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+
+// CHECK-LABEL: func @update_input_sharding_on_consumer_fragment_with_no_existing_shardings
+func.func @update_input_sharding_on_consumer_fragment_with_no_existing_shardings(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"x"}]>})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2]>>, <"m2": <["x"=2]>>>}
+{
+  // CHECK-NEXT: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {?}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"x"}]>]>} %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  // CHECK-NEXT: mpmd.fragment<mesh="m2", origin=[], in_shardings=[<@mesh, [{"x"}, {?}]>], out_shardings=[<@mesh, [{?}, {"x"}]>]>
+  %consumer = mpmd.fragment<mesh="m2", origin=[], out_shardings=[<@mesh, [{?}, {"x"}]>]> (%0) (%arg1: tensor<4x8xui32>) {
+    mpmd.return %arg1 : tensor<4x8xui32>
+  } : (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  func.return %consumer : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+
+// CHECK-LABEL: func @update_sharding_on_producer_fragment
+func.func @update_sharding_on_producer_fragment(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  -> (!mpmd.mesh_tensor<"m2", tensor<4x8xui32>> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {?}]>})
+  attributes {"topology"=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>}
+{
+  // CHECK-NEXT: mpmd.fragment<mesh="m1", origin=["prod"], in_shardings=[<@mesh, [{?}, {"x"}]>], out_shardings=[<@mesh, [{"y"}, {?}]>]> (%arg0) (%arg1: tensor<4x8xui32>) {
+  // CHECK-NEXT:    mpmd.return %arg1 : tensor<4x8xui32>
+  // CHECK-NEXT: } : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  // CHECK-NEXT: mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {?}]>]>} %0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  %producer = mpmd.fragment<mesh="m1", origin=["prod"], in_shardings=[<@mesh, [{?}, {"x"}]>], out_shardings=[<@mesh, [{?}, {"x"}]>]> (%arg0) (%arg1: tensor<4x8xui32>) {
+    mpmd.return %arg1 : tensor<4x8xui32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xui32>>
+  %0 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {?}]>]>} %producer : (!mpmd.mesh_tensor<"m1", tensor<4x8xui32>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+  func.return %0 : !mpmd.mesh_tensor<"m2", tensor<4x8xui32>>
+}
+
+}
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/test/sharding_propagation_pipeline.mlir b/shardy/dialect/mpmd/transforms/sharding_propagation/test/sharding_propagation_pipeline.mlir
new file mode 100644
index 0000000..b67f6a2
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/test/sharding_propagation_pipeline.mlir
@@ -0,0 +1,767 @@
+// RUN: mpmd_opt %s -mpmd-sharding-propagation-pipeline -split-input-file -mlir-diagnostic-verbosity-level=errors 2>&1 | FileCheck %s
+
+module {
+sdy.mesh @mesh = <["x"=4]>
+
+// CHECK-LABEL: @simple_propagation_within_fragment
+func.func public @simple_propagation_within_fragment(
+  %arg0: !mpmd.mesh_tensor<"mesh1", tensor<16x3x5xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {}]>},
+  %arg1: !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>>
+  {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {}]>})
+  -> (!mpmd.mesh_tensor<"mesh1", tensor<16x10x5xf32>>)
+  attributes {topology = #mpmd.topology<<"mesh1" : <["x"=4]>>, <"mesh2" : <["x"=4]>>>} {
+  // CHECK: %[[FRAG1:.*]] = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%arg0, %arg1) (%arg2: tensor<16x3x5xf32>, %arg3: tensor<16x10x3xf32>) {
+  // CHECK:   %[[CST:.*]] = stablehlo.constant dense<1.000000e+00> : tensor<16x3x5xf32>
+  // CHECK:   %[[ADD:.*]] = stablehlo.add %arg2, %[[CST]]
+  // CHECK:   %[[DOT:.*]] = stablehlo.dot_general
+  // CHECK:   mpmd.return %[[DOT]] : tensor<16x10x5xf32>
+  // CHECK: }
+  // CHECK: (!mpmd.mesh_tensor<"mesh1", tensor<16x3x5xf32>, sharding=<@mesh, [{"x"}, {}, {}]>>,
+  // CHECK: !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>, sharding=<@mesh, [{"x"}, {}, {}]>>) ->
+  // CHECK: !mpmd.mesh_tensor<"mesh1", tensor<16x10x5xf32>, sharding=<@mesh, [{"x"}, {}, {}]>>
+  %0 = mpmd.fragment<mesh="mesh1", origin=["stage1"]> (%arg0, %arg1) (%arg2: tensor<16x3x5xf32>, %arg3: tensor<16x10x3xf32>) {
+    %cst = stablehlo.constant dense<1.000000e+00> : tensor<16x3x5xf32>
+    %1 = stablehlo.add %arg2, %cst : tensor<16x3x5xf32>
+    %4 = stablehlo.dot_general %arg3, %1, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<16x10x3xf32>, tensor<16x3x5xf32>) -> tensor<16x10x5xf32>
+    mpmd.return %4 : tensor<16x10x5xf32>
+  } : (!mpmd.mesh_tensor<"mesh1", tensor<16x3x5xf32>>, !mpmd.mesh_tensor<"mesh1", tensor<16x10x3xf32>>) -> !mpmd.mesh_tensor<"mesh1", tensor<16x10x5xf32>>
+  return %0 : !mpmd.mesh_tensor<"mesh1", tensor<16x10x5xf32>>
+}
+
+}
+
+// -----
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=4]>
+
+// CHECK-LABEL: @two_fragments_one_producer_one_consumer
+func.func @two_fragments_one_producer_one_consumer(
+   %arg0: !mesh_1_tensor_4_8_f32
+   {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+   %arg1: !mesh_1_tensor_4_8_f32)
+  -> !mesh_1_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>
+    >} {
+  // CHECK: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+  // CHECK: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+  // CHECK: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %2 = mpmd.fragment<mesh="m1", origin=["consumer"]> (%0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %3 = stablehlo.multiply %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  func.return %2 : !mesh_1_tensor_4_8_f32
+}
+}
+
+// -----
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=4]>
+
+// CHECK-LABEL: @fragment_with_transfer
+func.func @fragment_with_transfer(
+   %arg0: !mesh_1_tensor_4_8_f32
+   {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+   %arg1: !mesh_1_tensor_4_8_f32)
+  -> !mesh_2_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+  // CHECK: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK: %1 = mpmd.transfer %0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) -> !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  // CHECK: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  func.return %1 : !mesh_2_tensor_4_8_f32
+}
+}
+
+
+// -----
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=4]>
+
+// CHECK-LABEL: @fragment_with_transfer_backward_propagation
+func.func @fragment_with_transfer_backward_propagation(
+  // CHECK: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  // CHECK: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+  // CHECK: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+   %arg0: !mesh_1_tensor_4_8_f32,
+   %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_2_tensor_4_8_f32  {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK: mpmd.fragment<mesh="m1", origin=["producer"]>
+  // CHECK: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+  // CHECK: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0, %arg1)
+    (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK: mpmd.transfer %0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+  // CHECK: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+
+  func.return %1 : !mesh_2_tensor_4_8_f32
+}
+}
+
+
+// -----
+
+#homogenous_topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+!mesh_1_tensor_16_32_f32 = !mpmd.mesh_tensor<"m1", tensor<16x32xf32>>
+!mesh_2_tensor_16_32_f32 = !mpmd.mesh_tensor<"m2", tensor<16x32xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=8]>
+
+// CHECK-LABEL: func @identify_function_with_arg_sharding
+// CHECK: (%arg0: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {}]>>,
+// CHECK:  %arg1: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+// CHECK: (!mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {}]>>,
+// CHECK:  !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+func.func @identify_function_with_arg_sharding(
+  %arg0: !mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}], replicated = {"x"}>},
+  %arg1: !mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>}
+)
+  -> (!mesh_1_tensor_16_32_f32,
+      !mesh_1_tensor_16_32_f32)
+  attributes {topology=#homogenous_topology} {
+  func.return %arg0, %arg1 : !mesh_1_tensor_16_32_f32, !mesh_1_tensor_16_32_f32
+}
+}
+
+// -----
+
+#homogenous_topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+!mesh_1_tensor_16_32_f32 = !mpmd.mesh_tensor<"m1", tensor<16x32xf32>>
+!mesh_2_tensor_16_32_f32 = !mpmd.mesh_tensor<"m2", tensor<16x32xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=8]>
+
+// CHECK-LABEL: func @identify_function_with_result_sharding
+// CHECK: (%arg0: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {"x"}]>>,
+// CHECK: %arg1: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+// CHECK: (!mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {"x"}]>>,
+// CHECK: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+func.func @identify_function_with_result_sharding(
+  %arg0: !mesh_1_tensor_16_32_f32,
+  %arg1: !mesh_1_tensor_16_32_f32
+)
+  -> (!mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"x"}]>},
+      !mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>})
+  attributes {topology=#homogenous_topology} {
+  func.return %arg0, %arg1 : !mesh_1_tensor_16_32_f32, !mesh_1_tensor_16_32_f32
+}
+}
+
+// -----
+#homogenous_topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+!mesh_1_tensor_16_32_f32 = !mpmd.mesh_tensor<"m1", tensor<16x32xf32>>
+!mesh_2_tensor_16_32_f32 = !mpmd.mesh_tensor<"m2", tensor<16x32xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=8]>
+
+// CHECK-LABEL: @propagates_to_func_args_and_inter_mesh_transfers
+func.func @propagates_to_func_args_and_inter_mesh_transfers(%arg0: !mesh_1_tensor_16_32_f32)
+  -> !mesh_2_tensor_16_32_f32 attributes {topology=#homogenous_topology} {
+  // CHECK-NEXT: %[[FRAG1:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1
+  // CHECK-NEXT:   mpmd.return %[[ADD]] : tensor<16x32xf32>
+  // CHECK-NEXT: }
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %[[FRAG1]] : {{.*}}"m1"{{.*}}<@mesh, [{"x"}, {}]>{{.*}} -> {{.*}}"m2"{{.*}}<@mesh, [{"x"}, {}]>
+  // CHECK-NEXT: %[[FRAG2:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[TRANSFER]]) (%arg1
+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1
+  // CHECK-NEXT:   mpmd.return %[[ADD]] : tensor<16x32xf32>
+  // CHECK-NEXT: }
+  // CHECK-NEXT: return %[[FRAG2]] : {{.*}}"m2"{{.*}}<@mesh, [{"x"}, {}]>
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<16x32xf32>) {
+    %4 = stablehlo.add %arg1, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x", ?}, {?}]>]>} : tensor<16x32xf32>
+    mpmd.return %4 : tensor<16x32xf32>
+  } : (!mesh_1_tensor_16_32_f32) -> (!mesh_1_tensor_16_32_f32)
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor_16_32_f32) -> !mesh_2_tensor_16_32_f32
+  %2 = mpmd.fragment<mesh="m2", origin=[]> (%1) (%arg1: tensor<16x32xf32>) {
+    %5 = stablehlo.add %arg1, %arg1 : tensor<16x32xf32>
+    mpmd.return %5 : tensor<16x32xf32>
+  } : (!mesh_2_tensor_16_32_f32) -> (!mesh_2_tensor_16_32_f32)
+  func.return %2 : !mesh_2_tensor_16_32_f32
+}
+}
+
+// -----
+
+#homogenous_topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+!mesh_1_tensor_16_32_f32 = !mpmd.mesh_tensor<"m1", tensor<16x32xf32>>
+!mesh_2_tensor_16_32_f32 = !mpmd.mesh_tensor<"m2", tensor<16x32xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=8]>
+
+// CHECK-LABEL: func @pass_func_arg_shardings_into_frag_through_transfers
+func.func @pass_func_arg_shardings_into_frag_through_transfers(
+  %arg0: !mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>})
+  -> (!mesh_2_tensor_16_32_f32) attributes {topology=#homogenous_topology} {
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>) -> !mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  // CHECK-NEXT: mpmd.fragment<mesh="m2", origin=[]>
+  // CHECK: } : (!mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>) -> !mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %1 = mpmd.transfer %arg0 : (!mesh_1_tensor_16_32_f32) -> !mesh_2_tensor_16_32_f32
+  %2 = mpmd.fragment<mesh="m2", origin=[]> (%1) (%arg1: tensor<16x32xf32>) {
+    %5 = stablehlo.add %arg1, %arg1 : tensor<16x32xf32>
+    mpmd.return %5 : tensor<16x32xf32>
+  } : (!mesh_2_tensor_16_32_f32) -> (!mesh_2_tensor_16_32_f32)
+  func.return %2 : !mesh_2_tensor_16_32_f32
+}
+}
+
+// -----
+
+#homogenous_topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+!mesh_1_tensor_16_32_f32 = !mpmd.mesh_tensor<"m1", tensor<16x32xf32>>
+!mesh_2_tensor_16_32_f32 = !mpmd.mesh_tensor<"m2", tensor<16x32xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=8]>
+
+// CHECK-LABEL: func @preserves_replicated_through_transfer(
+// CHECK-SAME:      %arg0: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {}]>>
+// CHECK-SAME:      %arg1: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>
+// CHECK-SAME:      -> (!mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+// CHECK-SAME:          !mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{}, {}]>>)
+func.func @preserves_replicated_through_transfer(
+  %arg0: !mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}], replicated = {"x"}>},
+  %arg1: !mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>}
+)
+  -> (!mesh_2_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>},
+      !mesh_2_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}], replicated = {"x"}>})
+  attributes {topology=#homogenous_topology} {
+// CHECK-NEXT: %[[RESHARD1:.*]] = mpmd.fragment
+// CHECK: } : (!mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {}]>>) -> !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>
+// CHECK-NEXT: mpmd.transfer %[[RESHARD1]] :
+// CHECK-NEXT: mpmd.transfer %arg1 :
+// CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>
+// CHECK-NEXT: %[[RESHARD2:.*]] = mpmd.fragment
+// CHECK: } : (!mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>) -> !mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{}, {}]>>
+  %0 = mpmd.transfer %arg0 : (!mesh_1_tensor_16_32_f32) -> !mesh_2_tensor_16_32_f32
+  %1 = mpmd.transfer %arg1 : (!mesh_1_tensor_16_32_f32) -> !mesh_2_tensor_16_32_f32
+  func.return %0, %1 : !mesh_2_tensor_16_32_f32, !mesh_2_tensor_16_32_f32
+}
+
+}
+
+
+// -----
+
+!mesh_1_tensor_8_2_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>>
+!mesh_2_tensor_8_2_f32 = !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>>
+#topology = #mpmd.topology<<"mesh1" : <["devices"=4]>>, <"mesh2" : <["devices"=4]>>>
+module {
+sdy.mesh @mesh = <["devices"=4]>
+
+// CHECK-LABEL: func @introduce_reshard_for_transfer_operand_and_result_with_different_sharding
+func.func @introduce_reshard_for_transfer_operand_and_result_with_different_sharding(
+  %arg0: !mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices", ?}, {?}]>},
+  %arg1: !mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices", ?}, {?}]>}) ->
+  (!mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices"}, {}]>},
+  !mesh_2_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}) attributes {topology = #topology} {
+  %0 = mpmd.fragment<mesh="mesh1", origin=["add"]> (%arg0, %arg1) (%arg2: tensor<8x2xf32>, %arg3: tensor<8x2xf32>) {
+    %2 = stablehlo.add %arg2, %arg3 : tensor<8x2xf32>
+    mpmd.return %2 : tensor<8x2xf32>
+  } : (!mesh_1_tensor_8_2_f32, !mesh_1_tensor_8_2_f32) -> !mesh_1_tensor_8_2_f32
+  // CHECK: %[[FRAG:.*]] = mpmd.fragment<mesh="mesh1", origin=["add"]> (%arg0, %arg1) (%arg2: tensor<8x2xf32>, %arg3: tensor<8x2xf32>) {
+  // CHECK: %[[TRANSFER:.*]] = mpmd.transfer %0 : (!mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>) -> !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>
+  // CHECK: %[[RESHARD:.*]] = mpmd.fragment<mesh="mesh2", origin=[]> (%[[TRANSFER]]) (%arg2: tensor<8x2xf32>) {
+  // CHECK: mpmd.return %arg2 : tensor<8x2xf32>
+  // CHECK: } : (!mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>) -> !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>, sharding=<@mesh, [{}, {}]>>
+  // CHECK: return %[[FRAG]], %[[RESHARD]] : !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>, !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>, sharding=<@mesh, [{}, {}]>>
+  %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"devices", ?}, {?}]>]>} %0 : (!mesh_1_tensor_8_2_f32) -> !mesh_2_tensor_8_2_f32
+  return %0, %1 : !mesh_1_tensor_8_2_f32, !mesh_2_tensor_8_2_f32
+}
+
+}
+
+// -----
+
+!mesh_1_tensor_8_2_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>>
+!mesh_2_tensor_8_2_f32 = !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>>
+#topology = #mpmd.topology<<"mesh1" : <["devices"=4]>>, <"mesh2" : <["devices"=4]>>>
+
+module {
+sdy.mesh @mesh = <["devices"=4]>
+
+// CHECK-LABEL: func @transfer_result_less_sharded_than_operand
+func.func @transfer_result_less_sharded_than_operand(
+  %arg0: !mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices", ?}, {?}]>},
+  %arg1: !mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices", ?}, {?}]>})
+  ->
+  (!mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices"}, {}]>}, !mesh_2_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>})
+  attributes {topology = #topology}
+{
+// CHECK-NEXT: %[[ADD:.*]] = mpmd.fragment<mesh="mesh1", origin=["add"]> (%arg0, %arg1)
+// CHECK-NEXT:   add
+// CHECK-NEXT:   mpmd.return
+// CHECK-NEXT: }
+// CHECK-SAME: (!mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>,
+// CHECK-SAME: !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>
+
+// CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %[[ADD]]
+// CHECK-SAME:  (!mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>)
+// CHECK-SAME:  -> !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>
+
+// CHECK-NEXT: %[[RESHARD:.*]] = mpmd.fragment<mesh="mesh2", origin=[]> (%[[TRANSFER]])
+// CHECK: } : (!mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>)
+// CHECK-SAME: -> !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>, sharding=<@mesh, [{}, {}]>>
+
+// CHECK-NEXT: %[[ADD]], %[[RESHARD]]
+
+  %0 = mpmd.fragment<mesh="mesh1", origin=["add"], in_shardings=[<@mesh, [{"devices", ?}, {?}]>, <@mesh, [{"devices", ?}, {?}]>], out_shardings=[<@mesh, [{"devices", ?}, {?}]>]> (%arg0, %arg1) (%arg2: tensor<8x2xf32>, %arg3: tensor<8x2xf32>) {
+    %2 = stablehlo.add %arg2, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"devices", ?}, {?}]>]>} : tensor<8x2xf32>
+    mpmd.return %2 : tensor<8x2xf32>
+  } : (!mesh_1_tensor_8_2_f32, !mesh_1_tensor_8_2_f32) -> !mesh_1_tensor_8_2_f32
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor_8_2_f32) -> !mesh_2_tensor_8_2_f32
+  return %0, %1 : !mesh_1_tensor_8_2_f32, !mesh_2_tensor_8_2_f32
+}
+
+}
+
+// -----
+
+!mesh_1_tensor_8_2_f32 = !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>>
+!mesh_2_tensor_8_2_f32 = !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>>
+#topology = #mpmd.topology<<"mesh1" : <["devices"=4]>>, <"mesh2" : <["devices"=4]>>>
+
+module {
+sdy.mesh @mesh = <["devices"=4]>
+
+// CHECK-LABEL: func @transfer_result_more_sharded_than_operand
+func.func @transfer_result_more_sharded_than_operand(
+  %arg0: !mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices", ?}, {?}]>},
+  %arg1: !mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices", ?}, {?}]>})
+  ->
+  (!mesh_1_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, !mesh_2_tensor_8_2_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"devices"}, {}]>})
+  attributes {topology = #topology}
+{
+// CHECK-NEXT: %[[ADD:.*]] = mpmd.fragment<mesh="mesh1", origin=["add"]> (%arg0, %arg1)
+// CHECK-NEXT:   add
+// CHECK-NEXT:   mpmd.return
+// CHECK-NEXT: }
+// CHECK-SAME: (!mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>,
+// CHECK-SAME: !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>) ->
+// CHECK-SAME: !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>
+
+// CHECK-NEXT: %[[RESHARD:.*]] = mpmd.fragment<mesh="mesh1", origin=[]> (%[[ADD]])
+// CHECK: } : (!mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>)
+// CHECK-SAME: -> !mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{}, {}]>>
+
+// CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %[[ADD]]
+// CHECK-SAME:  (!mpmd.mesh_tensor<"mesh1", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>)
+// CHECK-SAME:  -> !mpmd.mesh_tensor<"mesh2", tensor<8x2xf32>, sharding=<@mesh, [{"devices"}, {}]>>
+
+// CHECK-NEXT: %[[RESHARD]], %[[TRANSFER]]
+
+  %0 = mpmd.fragment<mesh="mesh1", origin=["add"], in_shardings=[<@mesh, [{"devices", ?}, {?}]>, <@mesh, [{"devices", ?}, {?}]>]> (%arg0, %arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"devices", ?}, {?}]>]>} (%arg2: tensor<8x2xf32>, %arg3: tensor<8x2xf32>) {
+    %2 = stablehlo.add %arg2, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"devices", ?}, {?}]>]>} : tensor<8x2xf32>
+    mpmd.return %2 : tensor<8x2xf32>
+  } : (!mesh_1_tensor_8_2_f32, !mesh_1_tensor_8_2_f32) -> !mesh_1_tensor_8_2_f32
+  %1 = mpmd.transfer {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"devices", ?}, {?}]>]>} %0 : (!mesh_1_tensor_8_2_f32) -> !mesh_2_tensor_8_2_f32
+  return %0, %1 : !mesh_1_tensor_8_2_f32, !mesh_2_tensor_8_2_f32
+}
+
+}
+
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: @for_loop_with_fragment_nested
+func.func @for_loop_with_fragment_nested(
+   %arg0: !mesh_1_tensor_4_8_f32
+   {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {?}]>},
+   %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_2_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+    // CHECK: mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, unroll_factor = 3 : ui32}
+    // CHECK-SAME: (%arg2: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+    // CHECK-SAME: %arg3: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>, %index: tensor<ui32>)
+    %0:2 = mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, unroll_factor = 3 : ui32} (
+        %arg2: !mesh_1_tensor_4_8_f32, %arg3: !mesh_1_tensor_4_8_f32, %index: tensor<ui32>) {
+        // CHECK: mpmd.fragment<mesh="m1", origin=["producer"]>
+        // CHECK: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+        // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+        // CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+        %fragment_result:2 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg2)
+        (%arg4: tensor<4x8xf32>) {
+          %3 = stablehlo.add %arg4, %arg4 : tensor<4x8xf32>
+          mpmd.return %3, %3 : tensor<4x8xf32>, tensor<4x8xf32>
+        } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+        mpmd.return %fragment_result#0, %fragment_result#1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+  } : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+  %1 = mpmd.transfer %0#0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  func.return %1, %0#1 : !mesh_2_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+}
+
+// -----
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: @for_loop_sharding_from_op_within_loop
+func.func @for_loop_sharding_from_op_within_loop(
+// CHECK: (%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+// CHECK-SAME: (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+   %arg0: !mesh_1_tensor_4_8_f32,
+   %arg1: !mesh_1_tensor_4_8_f32)
+  -> (!mesh_2_tensor_4_8_f32, !mesh_1_tensor_4_8_f32) attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+    // CHECK: mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, unroll_factor = 3 : ui32}
+    // CHECK-SAME: (%arg2: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+    // CHECK-SAME: %arg3: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>, %index: tensor<ui32>)
+    %0:2 = mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, unroll_factor = 3 : ui32} (
+        %arg2: !mesh_1_tensor_4_8_f32, %arg3: !mesh_1_tensor_4_8_f32, %index: tensor<ui32>) {
+        // CHECK: mpmd.fragment<mesh="m1", origin=["producer"]>
+        // CHECK: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>) ->
+        // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+        // CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+        %fragment_result:2 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg2)
+        (%arg4: tensor<4x8xf32>) {
+          %3 = stablehlo.add %arg4, %arg4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : tensor<4x8xf32>
+          mpmd.return %3, %3 : tensor<4x8xf32>, tensor<4x8xf32>
+        } : (!mesh_1_tensor_4_8_f32) -> (!mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32)
+        mpmd.return %fragment_result#0, %fragment_result#1 : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+  } : !mesh_1_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+  %1 = mpmd.transfer %0#0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  func.return %1, %0#1 : !mesh_2_tensor_4_8_f32, !mesh_1_tensor_4_8_f32
+}
+}
+
+// -----
+
+#homogenous_topology = #mpmd.topology<<"m1": <["x"=8]>>, <"m2": <["x"=8]>>>
+
+!mesh_1_tensor_16_32_f32 = !mpmd.mesh_tensor<"m1", tensor<16x32xf32>>
+!mesh_2_tensor_16_32_f32 = !mpmd.mesh_tensor<"m2", tensor<16x32xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=8]>
+
+// CHECK-LABEL: func @introduce_reshard_for_arg
+func.func @introduce_reshard_for_arg(
+  %arg0: !mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>})
+  -> (!mesh_2_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) attributes {topology=#homogenous_topology} {
+  // CHECK:  %[[RESHARD:.*]] = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg1: tensor<16x32xf32>) {
+  // CHECK-NEXT:    mpmd.return %arg1 : tensor<16x32xf32>
+  // CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {}]>>) ->
+  // CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  // CHECK-NEXT: %[[TRANSFER:.*]] = mpmd.transfer %[[RESHARD]] : (!mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+  // CHECK-SAME: -> !mpmd.mesh_tensor<"m2", tensor<16x32xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %1 = mpmd.transfer %arg0 : (!mesh_1_tensor_16_32_f32) -> !mesh_2_tensor_16_32_f32
+  %2 = mpmd.fragment<mesh="m2", origin=[]> (%1) (%arg1: tensor<16x32xf32>) {
+    mpmd.return %arg1 : tensor<16x32xf32>
+  } : (!mesh_2_tensor_16_32_f32) -> (!mesh_2_tensor_16_32_f32)
+  func.return %2 : !mesh_2_tensor_16_32_f32
+}
+}
+
+// -----
+
+#homogenous_topology = #mpmd.topology<<"m1": <["x"=8, "y"=1]>>>
+
+!mesh_1_tensor_16_32_f32 = !mpmd.mesh_tensor<"m1", tensor<16x32xf32>>
+
+module {
+// CHECK: sdy.mesh @mesh = <["x"=8, "y"=1]>
+sdy.mesh @mesh = <["x"=8, "y"=1]>
+
+// CHECK-LABEL: func @axis_sized_one_should_be_removed_from_sharding_not_mesh
+func.func @axis_sized_one_should_be_removed_from_sharding_not_mesh(
+// CHECK: (%arg0: !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {"x"}]>>)
+// CHECK-SAME: -> !mpmd.mesh_tensor<"m1", tensor<16x32xf32>, sharding=<@mesh, [{}, {"x"}]>>
+  %arg0: !mesh_1_tensor_16_32_f32 {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {"x"}]>}
+)
+  -> (!mesh_1_tensor_16_32_f32)
+  attributes {topology=#homogenous_topology} {
+  func.return %arg0 : !mesh_1_tensor_16_32_f32
+}
+}
+
+// -----
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=5]>
+
+// CHECK-LABEL: @non_divisible_sharding_after_propagation_turns_to_replicated
+func.func @non_divisible_sharding_after_propagation_turns_to_replicated(
+   %arg0: !mesh_1_tensor_4_8_f32
+   {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+   %arg1: !mesh_1_tensor_4_8_f32)
+  -> !mesh_2_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=5]>>,
+      <"m2": <["x"=5]>>
+    >} {
+  // CHECK: %[[FRAGMENT_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]>
+  // CHECK: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{}, {}]>>)
+  // CHECK-SAME: -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{}, {}]>>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  func.return %1 : !mesh_2_tensor_4_8_f32
+}
+}
+
+// -----
+
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=16]>
+
+// CHECK-LABEL: @partly_divisible_sharding_after_propagation_turns_to_subaxis_sharding
+func.func @partly_divisible_sharding_after_propagation_turns_to_subaxis_sharding(
+   %arg0: !mesh_1_tensor_4_8_f32
+   {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+   %arg1: !mesh_1_tensor_4_8_f32)
+  -> !mesh_2_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=16]>>,
+      <"m2": <["x"=16]>>
+    >} {
+  // CHECK: %[[FRAGMENT_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]>
+  // CHECK: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{}, {}]>>) ->
+  // CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x":(1)4}, {}]>>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg2: tensor<4x8xf32>) {
+    %3 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  // CHECK: %[[TRANSFER_RESULT:.*]] = mpmd.transfer %[[FRAGMENT_RESULT]] :
+  // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x":(1)4}, {}]>>) ->
+  // CHECK-SAME: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x":(1)4}, {}]>>
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  // CHECK:  %[[RESHARD_RESULT:.*]] = mpmd.fragment<mesh="m2", origin=[]> (%[[TRANSFER_RESULT]])
+  // CHECK: (!mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{"x":(1)4}, {}]>>) ->
+  // CHECK-SAME: !mpmd.mesh_tensor<"m2", tensor<4x8xf32>, sharding=<@mesh, [{}, {}]>>
+  func.return %1 : !mesh_2_tensor_4_8_f32
+}
+}
+
+// -----
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: @sharding_constraint_preserved
+func.func @sharding_constraint_preserved(
+   %arg0: !mesh_1_tensor_4_8_f32
+   {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>})
+  -> !mesh_2_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[FRAGMENT_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0) (%arg1: tensor<4x8xf32>) {
+  // CHECK-NEXT:     %[[CONSTRAINT:.*]] = sdy.sharding_constraint %arg1 <@mesh, [{}, {"x"}]>
+  // CHECK-NEXT:     %[[ADD:.*]] = stablehlo.add %[[CONSTRAINT]], %[[CONSTRAINT]]
+  // CHECK-NEXT:     mpmd.return %[[ADD]]
+  // CHECK-NEXT:   } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+  // CHECK-SAME:   -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{}, {"x"}]>>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    %2 = sdy.sharding_constraint %arg1 <@mesh, [{}, {"x"}]> : tensor<4x8xf32>
+    %3 = stablehlo.add %2, %2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  func.return %1 : !mesh_2_tensor_4_8_f32
+}
+}
+
+// -----
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+
+module {
+sdy.mesh @mesh = <["x"=2]>
+
+// CHECK-LABEL: @sharding_group_and_propagation_barrier_preserved
+func.func @sharding_group_and_propagation_barrier_preserved(
+   %arg0: !mesh_1_tensor_4_8_f32
+   {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>})
+  -> !mesh_2_tensor_4_8_f32 attributes {
+    "topology"=#mpmd.topology<
+      <"m1": <["x"=2]>>,
+      <"m2": <["x"=2]>>
+    >} {
+  // CHECK-NEXT: %[[FRAGMENT_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0) (%arg1: tensor<4x8xf32>) {
+  // CHECK-NEXT:     sdy.sharding_group %arg1 group_id=0
+  // CHECK-NEXT:     %[[BARRIER:.*]] = sdy.propagation_barrier %arg1 allowed_direction=NONE
+  // CHECK-NEXT:     %[[ADD:.*]] = stablehlo.add %[[BARRIER]], %[[BARRIER]]
+  // CHECK-NEXT:     sdy.sharding_group %[[ADD]] group_id=0
+  // CHECK-NEXT:     mpmd.return %[[ADD]]
+  // CHECK-NEXT:   } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+  // CHECK-SAME:   -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>
+  %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0)
+    (%arg1: tensor<4x8xf32>) {
+    sdy.sharding_group %arg1 group_id=0 : tensor<4x8xf32>
+    %2 = sdy.propagation_barrier %arg1 allowed_direction=NONE : tensor<4x8xf32>
+    %3 = stablehlo.add %2, %2 : tensor<4x8xf32>
+    sdy.sharding_group %3 group_id=0 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mesh_1_tensor_4_8_f32) -> !mesh_1_tensor_4_8_f32
+  %1 = mpmd.transfer %0 : (!mesh_1_tensor_4_8_f32) -> !mesh_2_tensor_4_8_f32
+  func.return %1 : !mesh_2_tensor_4_8_f32
+}
+}
+
+// -----
+!mesh_1_tensor = !mpmd.mesh_tensor<"m1", tensor<4xf32>>
+!mesh_2_tensor = !mpmd.mesh_tensor<"m2", tensor<4xf32>>
+
+module {
+// CHECK: sdy.mesh @mesh = <["x"=2, "y"=4]>
+sdy.mesh @mesh = <["x"=2, "y"=4]>
+// CHECK-LABEL: func @return_value_used_in_another_fragment
+func.func @return_value_used_in_another_fragment(%arg0: !mesh_1_tensor {sdy.sharding = #sdy.sharding<@mesh, [{"y"}]>}, %arg1: !mesh_2_tensor) -> (!mesh_2_tensor {sdy.sharding = #sdy.sharding<@mesh, [{"x"}]>}, !mesh_2_tensor) attributes {
+  "topology"=#mpmd.topology<<"m1": <["x"=2, "y"=4]>>, <"m2": <["x"=2, "y"=4]>>>
+} {
+  // CHECK: %[[TRANSFER_RESULT:.*]] = mpmd.transfer %arg0 :
+  // CHECK-SAME: (!mpmd.mesh_tensor<"m1", tensor<4xf32>, sharding=<@mesh, [{"y"}]>>)
+  // CHECK-SAME: -> !mpmd.mesh_tensor<"m2", tensor<4xf32>, sharding=<@mesh, [{"y"}]>>
+  %0 = mpmd.transfer %arg0 : (!mesh_1_tensor) -> !mesh_2_tensor // %arg0 is sharded by y axis
+  // CHECK: mpmd.fragment<mesh="m2", origin=[]> (%[[TRANSFER_RESULT]]) (%arg2: tensor<4xf32>) {
+  // CHECK-NEXT:     mpmd.return %arg2 : tensor<4xf32>
+  // CHECK-NEXT:  } : (!mpmd.mesh_tensor<"m2", tensor<4xf32>, sharding=<@mesh, [{"y"}]>>)
+  // CHECK-SAME: -> !mpmd.mesh_tensor<"m2", tensor<4xf32>, sharding=<@mesh, [{"x"}]>>
+  %1 = mpmd.fragment<mesh="m2", origin=["f2"]> (%0, %arg1) (%arg2: tensor<4xf32>, %arg3: tensor<4xf32>) {
+    %1 = stablehlo.add %arg2, %arg3 : tensor<4xf32>
+    mpmd.return %1 : tensor<4xf32>
+  } : (!mesh_2_tensor, !mesh_2_tensor) -> !mesh_2_tensor
+  return %0, %1 : !mesh_2_tensor, !mesh_2_tensor // %0 is sharded by x
+}
+}
+
+// -----
+
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2, "y"=2]>>,<"m2": <["x"=2, "y"=2]>>>
+module {
+  sdy.mesh @mesh = <["x"=2, "y"=2]>
+
+// CHECK-LABEL: func @transfer_result_used_in_fragment_and_returned_with_user_specified_sharding(
+func.func @transfer_result_used_in_fragment_and_returned_with_user_specified_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>},
+  !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {}]>}) attributes {topology = #topology} {
+    // CHECK: %[[TRANSFER_RESULT:.*]] = mpmd.transfer %arg0 : ({{.*}}sharding=<@mesh, [{"x"}, {}]>>) ->
+    // CHECK-SAME: {{.*}}sharding=<@mesh, [{"x"}, {}]>>
+    %0 = mpmd.transfer %arg0 : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    // CHECK: %[[FRAGMENT_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]>
+    // CHECK: ({{.*}}sharding=<@mesh, [{"x"}, {}]>>) ->
+    // CHECK-SAME: {{.*}}sharding=<@mesh, [{"y"}, {}]>>
+    %1 = mpmd.fragment<mesh="m1", origin=["producer"]> (%0) (%arg2: tensor<4x8xf32>) {
+            %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %2: tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    // CHECK: return %[[TRANSFER_RESULT]], %[[FRAGMENT_RESULT]] : {{.*}}sharding=<@mesh, [{"x"}, {}]>>,
+    // CHECK-SAME: {{.*}}sharding=<@mesh, [{"y"}, {}]>>
+    return %0, %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  }
+}
+
+// -----
+!mesh_1_tensor_4_8_f32 = !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+!mesh_2_tensor_4_8_f32 = !mpmd.mesh_tensor<"m2", tensor<4x8xf32>>
+#topology = #mpmd.topology<<"m1": <["x"=2, "y"=2]>>,<"m2": <["x"=2, "y"=2]>>>
+module {
+  sdy.mesh @mesh = <["x"=2, "y"=2]>
+
+// CHECK-LABEL: func @fragment_result_used_in_fragment_and_returned_with_user_specified_sharding(
+// CHECK: %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>,
+// CHECK-SAME: %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+// CHECK-SAME: -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"y"}, {}]>>,
+// CHECK-SAME: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>, sharding=<@mesh, [{"x"}, {}]>>)
+func.func @fragment_result_used_in_fragment_and_returned_with_user_specified_sharding(
+  %arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>,
+  %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) ->
+  (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {}]>},
+  !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) attributes {topology = #topology} {
+    // CHECK: %[[PRODUCER_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["producer"]>
+    %0 = mpmd.fragment<mesh="m1", origin=["producer"]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg3 : tensor<4x8xf32>
+      mpmd.return %2: tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    // CHECK: {{.*}}sharding=<@mesh, [{"x"}, {}]>>,
+    // CHECK-SAME: {{.*}}sharding=<@mesh, [{"x"}, {}]>>)
+    // CHECK-SAME: -> {{.*}}sharding=<@mesh, [{"y"}, {}]>>
+    // CHECK: %[[CONSUMER_RESULT:.*]] = mpmd.fragment<mesh="m1", origin=["consumer"]> (%[[PRODUCER_RESULT]])
+    %1 = mpmd.fragment<mesh="m1", origin=["consumer"]> (%0) (%arg2: tensor<4x8xf32>) {
+      %2 = stablehlo.add %arg2, %arg2 : tensor<4x8xf32>
+      mpmd.return %2: tensor<4x8xf32>
+    } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+    // CHECK: ({{.*}}sharding=<@mesh, [{"y"}, {}]>>) ->
+    // CHECK-SAME: {{.*}}sharding=<@mesh, [{"x"}, {}]>>
+    // CHECK: return %[[PRODUCER_RESULT]], %[[CONSUMER_RESULT]] :
+    // CHECK-SAME: {{.*}}sharding=<@mesh, [{"y"}, {}]>>, {{.*}}sharding=<@mesh, [{"x"}, {}]>>
+    return %0, %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+    }
+}
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/test/sink_sdy_data_flow_edges.mlir b/shardy/dialect/mpmd/transforms/sharding_propagation/test/sink_sdy_data_flow_edges.mlir
new file mode 100644
index 0000000..553ee65
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/test/sink_sdy_data_flow_edges.mlir
@@ -0,0 +1,90 @@
+// RUN: mpmd_opt %s -sdy-sink-data-flow-edges 2>&1 | FileCheck %s
+
+sdy.mesh @mesh = <["x"=4, "y"=2]>
+// CHECK-LABEL: func @one_edge_for_arg_and_one_for_result
+func.func @one_edge_for_arg_and_one_for_result(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>>} {
+  // CHECK-NOT: sdy.data_flow_edge
+  // CHECK:  mpmd.fragment<mesh="m1", origin=[], in_shardings=[<@mesh, [{"x"}, {}]>], out_shardings=[<@mesh, [{"x", ?}, {?}]>]> (%arg0)
+  %0 = mpmd.fragment<mesh="m1", origin=[]> (%arg0) (%arg2: tensor<4x8xf32>) {
+    %2 = sdy.data_flow_edge %arg2 sharding=<@mesh, [{"x"}, {}]> : tensor<4x8xf32>
+    %3 = stablehlo.add %2, %2 : tensor<4x8xf32>
+    mpmd.return %3 : tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %1 = sdy.data_flow_edge %0 sharding=<@mesh, [{"x", ?}, {?}]> : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  return %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @multiple_edges_for_args_and_results
+func.func @multiple_edges_for_args_and_results(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>>} {
+  // CHECK-NOT: sdy.data_flow_edge
+  // CHECK: mpmd.fragment<mesh="m1", origin=[], in_shardings=[<@mesh, [{"x"}, {}]>, <@mesh, [{}, {"x"}]>], out_shardings=[<@mesh, [{"x", ?}, {?}]>, <@mesh, [{?}, {"x", ?}]>]> (%arg0, %arg1)
+  %0:2 = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %3 = sdy.data_flow_edge %arg2 sharding=<@mesh, [{"x"}, {}]> : tensor<4x8xf32>
+    %4 = sdy.data_flow_edge %arg3 sharding=<@mesh, [{}, {"x"}]> : tensor<4x8xf32>
+    %5 = stablehlo.add %3, %4 : tensor<4x8xf32>
+    mpmd.return %5, %5 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+  %1 = sdy.data_flow_edge %0#0 sharding=<@mesh, [{"x", ?}, {?}]> : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %2 = sdy.data_flow_edge %0#1 sharding=<@mesh, [{?}, {"x", ?}]> : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  return %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @some_edges_have_shardings
+func.func @some_edges_have_shardings(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>>} {
+  // CHECK-NOT: sdy.data_flow_edge
+  // CHECK: mpmd.fragment<mesh="m1", origin=[], out_shardings=[<@mesh, [{"x", ?}, {?}]>, <@mesh, [{?}, {"x", ?}]>]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>)
+  %0:2 = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %3 = sdy.data_flow_edge %arg2 : tensor<4x8xf32>
+    %4 = sdy.data_flow_edge %arg3 : tensor<4x8xf32>
+    %5 = stablehlo.add %3, %4 : tensor<4x8xf32>
+    mpmd.return %5, %5 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+  %1 = sdy.data_flow_edge %0#0 sharding=<@mesh, [{"x", ?}, {?}]> : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %2 = sdy.data_flow_edge %0#1 sharding=<@mesh, [{?}, {"x", ?}]> : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  return %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @not_all_args_have_an_edge
+func.func @not_all_args_have_an_edge(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>>} {
+  // CHECK-NOT: sdy.data_flow_edge
+  // CHECK: mpmd.fragment<mesh="m1", origin=[], in_shardings=[<@mesh, [{"x"}, {}]>, <@mesh, [{?}, {?}]>], out_shardings=[<@mesh, [{"x", ?}, {?}]>, <@mesh, [{?}, {"x", ?}]>]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>)
+  %0:2 = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %3 = sdy.data_flow_edge %arg2 sharding=<@mesh, [{"x"}, {}]> : tensor<4x8xf32>
+    %5 = stablehlo.add %3, %arg3 : tensor<4x8xf32>
+    mpmd.return %5, %5 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+  %1 = sdy.data_flow_edge %0#0 sharding=<@mesh, [{"x", ?}, {?}]> : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %2 = sdy.data_flow_edge %0#1 sharding=<@mesh, [{?}, {"x", ?}]> : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  return %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @no_shardings_on_edges
+func.func @no_shardings_on_edges(%arg0: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, %arg1: !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> !mpmd.mesh_tensor<"m1", tensor<4x8xf32>> attributes {topology = #mpmd.topology<<"m1" : <["x"=2]>>>} {
+  // CHECK-NOT: sdy.data_flow_edge
+  // CHECK-NOT: sdy.sharding
+  // CHECK: mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg1)
+  %0:2 = mpmd.fragment<mesh="m1", origin=[]> (%arg0, %arg1) (%arg2: tensor<4x8xf32>, %arg3: tensor<4x8xf32>) {
+    %3 = sdy.data_flow_edge %arg2 : tensor<4x8xf32>
+    %4 = sdy.data_flow_edge %arg3 : tensor<4x8xf32>
+    %5 = stablehlo.add %3, %4 : tensor<4x8xf32>
+    mpmd.return %5, %5 : tensor<4x8xf32>, tensor<4x8xf32>
+  } : (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>) -> (!mpmd.mesh_tensor<"m1", tensor<4x8xf32>>, !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>)
+  %1 = sdy.data_flow_edge %0#0 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  %2 = sdy.data_flow_edge %0#1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+  return %1 : !mpmd.mesh_tensor<"m1", tensor<4x8xf32>>
+}
+
+// CHECK-LABEL: func @for_loop
+func.func @for_loop(%arg0: tensor<10xui32>, %arg1: tensor<10xui32>) -> (tensor<10xui32>, tensor<10xui32>) attributes {topology = #mpmd.topology<<"m1" : <["x"=4, "y"=2]>>>} {
+  // CHECK: mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x", ?}]>, <@mesh, [{"y", ?}]>]>, unroll_factor = 3 : ui32}
+  %0:2 = mpmd.for (%arg0, %arg1) {iterations = 12 : ui32, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x", ?}]>, <@mesh, [{"x", ?}]>]>, unroll_factor = 3 : ui32} (%arg2: tensor<10xui32>, %arg3: tensor<10xui32>, %index: tensor<ui32>) {
+    %3 = stablehlo.broadcast_in_dim %index, dims = [] : (tensor<ui32>) -> tensor<10xui32>
+    %4 = stablehlo.add %arg2, %3 : tensor<10xui32>
+    %5 = stablehlo.add %arg2, %arg3 : tensor<10xui32>
+    mpmd.return %4, %5 : tensor<10xui32>, tensor<10xui32>
+  } : tensor<10xui32>, tensor<10xui32>
+  // CHECK-NOT: sdy.data_flow_edge
+  %1 = sdy.data_flow_edge %0#0 sharding=<@mesh, [{"x", ?}]> : tensor<10xui32>
+  %2 = sdy.data_flow_edge %0#1 sharding=<@mesh, [{"y", ?}]> : tensor<10xui32>
+  return %1, %2 : tensor<10xui32>, tensor<10xui32>
+}
diff --git a/shardy/dialect/mpmd/transforms/sharding_propagation/utils.h b/shardy/dialect/mpmd/transforms/sharding_propagation/utils.h
new file mode 100644
index 0000000..bd72f8c
--- /dev/null
+++ b/shardy/dialect/mpmd/transforms/sharding_propagation/utils.h
@@ -0,0 +1,45 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef SHARDY_DIALECT_MPMD_TRANSFORMS_SHARDING_PROPAGATION_UTILS_H_
+#define SHARDY_DIALECT_MPMD_TRANSFORMS_SHARDING_PROPAGATION_UTILS_H_
+
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/Support/LLVM.h"
+#include "shardy/dialect/mpmd/ir/dialect.h"
+#include "shardy/dialect/sdy/ir/dialect.h"
+
+namespace mlir::mpmd {
+
+// Updates the in_shardings attr of fragment users of the result of `transfer`.
+inline void UpdateFragmentUserInShardings(
+    mlir::mpmd::TransferOp transfer, mlir::sdy::TensorShardingAttr sharding) {
+  for (mlir::Operation* user : transfer.getResult().getUsers()) {
+    if (auto fragment_user = mlir::dyn_cast<mlir::mpmd::FragmentOp>(user)) {
+      for (auto [operand_number, input] :
+           llvm::enumerate(fragment_user.getInputs())) {
+        if (input == transfer.getResult()) {
+          fragment_user.setInputSharding(operand_number, sharding);
+        }
+      }
+    }
+  }
+}
+
+}  // namespace mlir::mpmd
+
+#endif  // SHARDY_DIALECT_MPMD_TRANSFORMS_SHARDING_PROPAGATION_UTILS_H_
+
diff --git a/shardy/lit.cfg.py b/shardy/lit.cfg.py
index e8ce83c..fa81f43 100644
--- a/shardy/lit.cfg.py
+++ b/shardy/lit.cfg.py
@@ -35,6 +35,7 @@ config.environment['FILECHECK_OPTS'] = '-enable-var-scope'
 # Make LLVM and Shardy tools available in RUN directives
 tools = [
     'FileCheck',
+    'mpmd_opt',
     'sdy_opt',
 ]
 tool_dirs = [
diff --git a/shardy/tools/BUILD b/shardy/tools/BUILD
index f7769dd..a51ddd0 100644
--- a/shardy/tools/BUILD
+++ b/shardy/tools/BUILD
@@ -38,3 +38,18 @@ cc_binary(
         "@llvm-project//mlir:TranslateLib",
     ],
 )
+
+cc_binary(
+    name = "mpmd_opt",
+    srcs = ["mpmd_opt_main.cc"],
+    deps = [
+        "//shardy/dialect/mpmd/ir:register",
+        "//shardy/dialect/mpmd/transforms:passes",
+        "//shardy/dialect/sdy/transforms:passes",
+        "@llvm-project//mlir:AllPassesAndDialects",
+        "@llvm-project//mlir:FuncExtensions",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:MlirOptLib",
+        "@llvm-project//mlir:QuantOps",
+    ],
+)
diff --git a/shardy/tools/mpmd_opt_main.cc b/shardy/tools/mpmd_opt_main.cc
new file mode 100644
index 0000000..8519b1e
--- /dev/null
+++ b/shardy/tools/mpmd_opt_main.cc
@@ -0,0 +1,47 @@
+/* Copyright 2025 The MPMD Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// MLIR `opt` tool for driving MPMD transformations.
+//
+// Usage:
+//   mpmd_opt <file> <llvm options>
+
+#include "mlir/Dialect/Func/Extensions/AllExtensions.h"
+#include "mlir/Dialect/Quant/IR/Quant.h"
+#include "mlir/IR/DialectRegistry.h"
+#include "mlir/InitAllPasses.h"
+#include "mlir/Tools/mlir-opt/MlirOptMain.h"
+#include "shardy/dialect/mpmd/ir/register.h"
+#include "shardy/dialect/sdy/transforms/passes.h"
+#include "shardy/dialect/mpmd/transforms/passes.h"
+
+
+int main(int argc, char** argv) {
+  mlir::registerAllPasses();
+
+  mlir::DialectRegistry registry;
+  mlir::mpmd::registerAllDialects(registry);
+  registry.insert<mlir::quant::QuantDialect>();
+  mlir::func::registerAllExtensions(registry);
+
+  // Register all SDY passes and pipelines.
+  mlir::sdy::registerAllSdyPassesAndPipelines();
+
+  // Register all MPMD passes and pipelines.
+  mlir::mpmd::registerAllMpmdPassesAndPipelines();
+
+  return mlir::asMainReturnCode(
+      mlir::MlirOptMain(argc, argv, "MPMD pass driver\n", registry));
+}
