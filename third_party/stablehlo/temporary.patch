diff --ruN a/stablehlo/docs/spec.md b/stablehlo/docs/spec.md
--- stablehlo/docs/spec.md
+++ stablehlo/docs/spec.md
@@ -1947,8 +1947,8 @@
 #### Semantics
 
 Encapsulates an operation made up (composed) of other StableHLO operations,
-taking `inputs` and `composite_attributes` and producing `results`. The
-semantics of the op are implemented by the `decomposition` attribute. The
+taking `inputs`, `composite_attributes` and `regions` and producing `results`.
+The semantics of the op are implemented by the `decomposition` attribute. The
 `composite` op can be replaced with its decomposition without changing program
 semantics. In cases where inlining the decomposition does not provide the same
 op semantics, prefer using `custom_call`.
@@ -1956,15 +1956,20 @@
 The `version` field (defaults to `0`) is used to denote when a composite's
 semantics change.
 
-#### Inputs
-
-| Label | Name                   | Type                      |
-|-------|------------------------|---------------------------|
-| (I1)  | `inputs`               | variadic number of values |
-| (I2)  | `name`                 | constant of type `string` |
-| (I3)  | `composite_attributes` | attribute dictionary      |
-| (I4)  | `decomposition`        | constant of type `string` |
-| (I5)  | `version`              | constant of type `si32`   |
+The intent of `regions` is to only be used to model ops with bodies (e.g.
+`my_op` may have a body like `while` or `reduce`). If the decomposition is
+inlined, the `regions` are ignored.
+
+#### Inputs
+
+| Label | Name                   | Type                         |
+|-------|------------------------|------------------------------|
+| (I1)  | `inputs`               | variadic number of values    |
+| (I2)  | `name`                 | constant of type `string`    |
+| (I3)  | `composite_attributes` | attribute dictionary         |
+| (I4)  | `decomposition`        | constant of type `string`    |
+| (I5)  | `version`              | constant of type `si32`      |
+| (I6)  | `regions`              | variadic number of functions |
 
 #### Outputs
 
diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.cpp b/stablehlo/stablehlo/dialect/ChloOps.cpp
--- stablehlo/stablehlo/dialect/ChloOps.cpp
+++ stablehlo/stablehlo/dialect/ChloOps.cpp
@@ -866,10 +866,8 @@
 }
 
 LogicalResult ScanOp::verify() {
-  if (getInits().size() != getCarries().size()) {
-    return emitOpError() << "requires the number of inits ("
-                         << getInits().size() << ") and carries ("
-                         << getCarries().size() << ") to be equal";
+  if (getInputs().empty() && getOutputs().empty()) {
+    return emitOpError() << "at least one of inputs or outputs must be present";
   }
 
   // Check that the scan dimension is in bounds for all operands. Also check
diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.td b/stablehlo/stablehlo/dialect/ChloOps.td
--- stablehlo/stablehlo/dialect/ChloOps.td
+++ stablehlo/stablehlo/dialect/ChloOps.td
@@ -978,6 +978,9 @@
         setNameFn(region.getArgument(i), i < getInputs().size() ? "input" : "carry");
       }
     }
+    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
+      return succeeded(mlir::verifyCompatibleShapes(l, r));
+    }
   }];
 
   let hasCustomAssemblyFormat = 1;
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -47,6 +47,7 @@
 #include "llvm/ADT/TypeSwitch.h"
 #include "llvm/ADT/iterator_range.h"
 #include "llvm/Support/Casting.h"
+#include "llvm/Support/Debug.h"
 #include "llvm/Support/FormatVariadic.h"
 #include "llvm/Support/LogicalResult.h"
 #include "llvm/Support/MathExtras.h"
@@ -91,6 +92,8 @@
 #include "stablehlo/dialect/StablehloOps.h.inc"
 #include "stablehlo/dialect/TypeInference.h"
 
+#define DEBUG_TYPE "stablehlo"
+
 // Include order matters
 #define GET_TYPEDEF_CLASSES
 #include "stablehlo/dialect/StablehloTypeDefs.cpp.inc"
@@ -729,6 +732,16 @@
                           getPrecisionConfig(), getResult());
 }
 
+LogicalResult DotOp::inferReturnTypeComponents(
+    MLIRContext*, std::optional<Location> location, ValueShapeRange operands,
+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,
+    SmallVectorImpl<ShapedTypeComponents>& inferredReturnShapes) {
+  DotOp::Adaptor adaptor(operands, attributes, properties, regions);
+  auto lhsType = mlir::cast<RankedTensorType>(adaptor.getLhs().getType());
+  auto rhsType = mlir::cast<RankedTensorType>(adaptor.getRhs().getType());
+  return hlo::inferDotOp(location, lhsType, rhsType, {}, inferredReturnShapes);
+}
+
 // PrecisionConfig - std::optional attribute, print the array as raw enums
 //
 // {precision_config = [#stablehlo<precision DEFAULT>,
@@ -856,6 +869,35 @@
       getDotDimensionNumbersAttr().getRhsContractingDimensions(),
       getPrecisionConfig(), isDefaultPrecisionConfig, hasAlgorithmSpecified,
       getResult());
+}
+
+LogicalResult DotGeneralOp::inferReturnTypeComponents(
+    MLIRContext*, std::optional<Location> location, ValueShapeRange operands,
+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,
+    SmallVectorImpl<ShapedTypeComponents>& inferredReturnShapes) {
+  DotGeneralOp::Adaptor adaptor(operands, attributes, properties, regions);
+  LLVM_DEBUG(llvm::dbgs() << "DotGeneralOp::inferReturnTypeComponents\n");
+  LLVM_DEBUG(llvm::dbgs() << "attributes: " << attributes << "\n");
+  LLVM_DEBUG(llvm::dbgs() << "properties: " << properties << "\n");
+
+  ArrayRef<int64_t> lhsBatchingDimensions;
+  ArrayRef<int64_t> rhsBatchingDimensions;
+  ArrayRef<int64_t> lhsContractingDimensions;
+  ArrayRef<int64_t> rhsContractingDimensions;
+  if (adaptor.getDotDimensionNumbersAttr()) {
+    lhsBatchingDimensions =
+        adaptor.getDotDimensionNumbersAttr().getLhsBatchingDimensions();
+    rhsBatchingDimensions =
+        adaptor.getDotDimensionNumbersAttr().getRhsBatchingDimensions();
+    lhsContractingDimensions =
+        adaptor.getDotDimensionNumbersAttr().getLhsContractingDimensions();
+    rhsContractingDimensions =
+        adaptor.getDotDimensionNumbersAttr().getRhsContractingDimensions();
+  }
+  return hlo::inferDotGeneralOp(
+      location, adaptor.getLhs().getType(), adaptor.getRhs().getType(),
+      lhsBatchingDimensions, rhsBatchingDimensions, lhsContractingDimensions,
+      rhsContractingDimensions, {}, inferredReturnShapes);
 }
 
 LogicalResult DotGeneralOp::reifyReturnTypeShapes(
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.td b/stablehlo/stablehlo/dialect/StablehloOps.td
--- stablehlo/stablehlo/dialect/StablehloOps.td
+++ stablehlo/stablehlo/dialect/StablehloOps.td
@@ -56,9 +56,9 @@
 include "stablehlo/dialect/StablehloAttrs.td"
 include "stablehlo/dialect/StablehloTypes.td"
 
-class StableHLO_ShapedInterfaceOp<string mnemonic, list<Trait> traits> :
+class StableHLO_ShapedInterfaceOp<string mnemonic, list<Trait> traits, list<string> extraMethodOverrides = []> :
     StableHLO_Op<mnemonic, traits # [DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
-    ["reifyReturnTypeShapes"]>]> {}
+    ["reifyReturnTypeShapes"] # extraMethodOverrides>]> {}
 
 class StableHLO_ResourceBase<string resourceKind> :
     Resource<!strconcat("::mlir::stablehlo::side_effects::", resourceKind)> {}
@@ -2369,6 +2369,10 @@
     The `version` field (defaults to `0`) is used to denote when a composite's
     semantics change.
 
+    The intent of `composite_regions` is to only be used to model ops with
+    bodies, regions are thrown away if the fallback decomposition function gets
+    inlined.
+
     See:
     https://github.com/openxla/stablehlo/blob/main/docs/spec.md#composite
 
@@ -2391,9 +2395,31 @@
     FlatSymbolRefAttr:$decomposition, /*composite_i4*/
     DefaultValuedOptionalAttr<I32Attr, "0">:$version /*composite_i5*/
   );
+  let regions = (region VariadicRegion<AnyRegion>:$composite_regions);
   let results = (outs Variadic<HLO_TensorOrPerAxisQuantizedTensorOrTokenOrTuple>);
 
-  let assemblyFormat = "$name $inputs attr-dict `:` functional-type(operands, results)";
+  let builders = [
+    OpBuilder<(ins
+      "::mlir::TypeRange":$resultTypes, "::mlir::ValueRange":$inputs,
+      "::mlir::ArrayRef<::mlir::NamedAttribute>":$attributes
+    ), [{
+      build($_builder, $_state, resultTypes, inputs, attributes,
+            /*regionsCount=*/0);
+    }]>,
+    OpBuilder<(ins
+      "::mlir::TypeRange":$resultTypes, "::mlir::ValueRange":$inputs,
+      "::mlir::StringRef":$name, "::mlir::DictionaryAttr":$attributes,
+      "::mlir::StringRef":$decomposition, CArg<"uint32_t", "0">:$version
+    ), [{
+      build($_builder, $_state, resultTypes, inputs, name, attributes,
+            decomposition, version, /*regionsCount=*/0);
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    $name operands (` ` `(` $composite_regions^ `)`)? attr-dict
+    `:` functional-type(operands, results)
+  }];
 }
 
 def StableHLO_ConvolutionOp : StableHLO_Op<"convolution",
@@ -2557,7 +2583,7 @@
   }];
 }
 
-def StableHLO_DotOp: StableHLO_Op<"dot", [Pure]> {
+def StableHLO_DotOp: StableHLO_Op<"dot", [Pure, DeclareOpInterfaceMethods<InferShapedTypeOpInterface, ["inferReturnTypeComponents"]>]> {
   let summary = "Dot operation";
   let description = [{
     This operation is on its way out of StableHLO, so it is not included in
@@ -2587,7 +2613,8 @@
 }
 
 def StableHLO_DotGeneralOp: StableHLO_ShapedInterfaceOp<"dot_general",
-    [ConditionallySpeculatable, NoMemoryEffect]> {
+    [ConditionallySpeculatable, NoMemoryEffect,
+     DeclareOpInterfaceMethods<InferShapedTypeOpInterface, ["inferReturnTypeComponents"]>]> {
   let summary = "DotGeneral operation";
   let description = [{
     Computes dot products between slices of `lhs` and slices of `rhs` and
diff --ruN a/stablehlo/stablehlo/dialect/TypeInference.cpp b/stablehlo/stablehlo/dialect/TypeInference.cpp
--- stablehlo/stablehlo/dialect/TypeInference.cpp
+++ stablehlo/stablehlo/dialect/TypeInference.cpp
@@ -48,6 +48,7 @@
 #include "llvm/ADT/StringRef.h"
 #include "llvm/ADT/Twine.h"
 #include "llvm/ADT/iterator_range.h"
+#include "llvm/Support/Debug.h"
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Support/Regex.h"
 #include "llvm/Support/raw_ostream.h"
@@ -74,6 +75,8 @@
 #include "stablehlo/dialect/AssemblyFormat.h"
 #include "stablehlo/dialect/Base.h"
 
+#define DEBUG_TYPE "stablehlo-type-inference"
+
 namespace mlir {
 namespace hlo {
 namespace {
@@ -1246,9 +1249,11 @@
   if (!arrayAttr) return success();
   return arrayAttr.size() <= 2
              ? success()
-             : emitOptionalError(loc,
-                                 "expects precision config to be empty or have "
-                                 "<= 2 elements.");
+             : emitOptionalError(
+                   loc,
+                   "expects precision config to be empty or have "
+                   "<= 2 elements, got " +
+                       std::to_string(arrayAttr.getValue().size()));
 }
 
 LogicalResult verifyConvolutionAttributes(
@@ -2413,24 +2418,60 @@
   }
 
   // Infer the output dimensions of the operation.
-  SmallVector<int64_t> dimensions;
   auto lhsRankedType = cast<RankedTensorType>(lhsType);
   auto rhsRankedType = cast<RankedTensorType>(rhsType);
   auto lhsShape = lhsRankedType.getShape();
   auto rhsShape = rhsRankedType.getShape();
-  for (const int64_t lhsBatchingDim : lhsBatchingDimensions)
-    dimensions.push_back(lhsShape[lhsBatchingDim]);
-  for (int64_t i = 0; i < lhsRankedType.getRank(); i++)
+
+  SmallVector<int64_t> lhsBounds =
+      to_vector(encodingToBounds(lhsRankedType.getEncoding()));
+  SmallVector<int64_t> rhsBounds =
+      to_vector(encodingToBounds(rhsRankedType.getEncoding()));
+
+  SmallVector<int64_t> inferredDimensions;
+  SmallVector<int64_t> inferredBounds;
+
+  for (size_t i = 0; i < lhsBatchingDimensions.size(); ++i) {
+    auto lhsDim = lhsBatchingDimensions[i];
+    auto rhsDim = rhsBatchingDimensions[i];
+    int64_t lhsBound =
+        lhsBounds.empty() ? ShapedType::kDynamic : lhsBounds[lhsDim];
+    int64_t rhsBound =
+        rhsBounds.empty() ? ShapedType::kDynamic : rhsBounds[rhsDim];
+    auto inferredDimAndBoundOrErr = inferMostSpecificDimAndBound(
+        location, i, lhsShape[lhsDim], rhsShape[rhsDim], lhsBound, rhsBound);
+    if (failed(inferredDimAndBoundOrErr)) {
+      return failure();
+    }
+    inferredDimensions.push_back(inferredDimAndBoundOrErr->first);
+    inferredBounds.push_back(inferredDimAndBoundOrErr->second);
+  }
+
+  for (int64_t i = 0; i < lhsRankedType.getRank(); i++) {
     if (!llvm::is_contained(lhsBatchingDimensions, i) &&
-        !llvm::is_contained(lhsContractingDimensions, i))
-      dimensions.push_back(lhsShape[i]);
-  for (int64_t i = 0; i < rhsRankedType.getRank(); i++)
+        !llvm::is_contained(lhsContractingDimensions, i)) {
+      inferredDimensions.push_back(lhsShape[i]);
+      inferredBounds.push_back(lhsBounds.empty() ? ShapedType::kDynamic
+                                                 : lhsBounds[i]);
+    }
+  }
+
+  for (int64_t i = 0; i < rhsRankedType.getRank(); i++) {
     if (!llvm::is_contained(rhsBatchingDimensions, i) &&
-        !llvm::is_contained(rhsContractingDimensions, i))
-      dimensions.push_back(rhsShape[i]);
-
-  // dot_general_c12
-  inferredReturnShapes.emplace_back(dimensions);
+        !llvm::is_contained(rhsContractingDimensions, i)) {
+      inferredDimensions.push_back(rhsShape[i]);
+      inferredBounds.push_back(rhsBounds.empty() ? ShapedType::kDynamic
+                                                 : rhsBounds[i]);
+    }
+  }
+
+  Attribute outputEncoding = lhsRankedType.getEncoding()
+                                 ? lhsRankedType.getEncoding()
+                                 : rhsRankedType.getEncoding();
+
+  Attribute boundsAttr = boundsToEncoding(outputEncoding, inferredBounds);
+  inferredReturnShapes.emplace_back(inferredDimensions, /*elementType=*/nullptr,
+                                    boundsAttr);
   return success();
 }
 
diff --ruN a/stablehlo/stablehlo/dialect/Version.h b/stablehlo/stablehlo/dialect/Version.h
--- stablehlo/stablehlo/dialect/Version.h
+++ stablehlo/stablehlo/dialect/Version.h
@@ -38,7 +38,7 @@
   static FailureOr<Version> fromString(llvm::StringRef versionRef);
 
   /// Return a Version representing the current VHLO dialect version.
-  static Version getCurrentVersion() { return Version(1, 13, 8); }
+  static Version getCurrentVersion() { return Version(1, 14, 0); }
 
   /// Return a Version representing the minimum supported VHLO dialect version.
   static Version getMinimumVersion() { return Version(0, 9, 0); }
diff --ruN a/stablehlo/stablehlo/dialect/VhloBytecode.cpp b/stablehlo/stablehlo/dialect/VhloBytecode.cpp
--- stablehlo/stablehlo/dialect/VhloBytecode.cpp
+++ stablehlo/stablehlo/dialect/VhloBytecode.cpp
@@ -16,6 +16,7 @@
 #include "stablehlo/dialect/VhloBytecode.h"
 
 #include <cassert>
+#include <cstddef>
 #include <cstdint>
 #include <utility>
 
@@ -26,6 +27,7 @@
 #include "llvm/Support/Compiler.h"
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MathExtras.h"
 #include "llvm/Support/raw_ostream.h"
 #include "mlir/Bytecode/BytecodeImplementation.h"
 #include "mlir/IR/Attributes.h"
@@ -933,12 +935,82 @@
 // TensorV1Attr
 //===----------------------------------------------------------------------===//
 
+namespace {
+
+bool isBooleanType(Type type) {
+  auto tensorType = dyn_cast<RankedTensorV1Type>(type);
+  return tensorType && isa<BooleanV1Type>(tensorType.getElementType());
+}
+
+static LogicalResult readVhloTensorV1Attr(DialectBytecodeReader& reader,
+                                          Type type,
+                                          SmallVectorImpl<char>& rawData) {
+  ArrayRef<char> blob;
+  if (failed(reader.readBlob(blob))) return failure();
+
+  // If the type is not i1, just copy the blob.
+
+  if (!isBooleanType(type)) {
+    rawData.append(blob.begin(), blob.end());
+    return success();
+  }
+
+  // Check to see if this is using the packed format.
+  // Note: this could be asserted instead as this should be the case. But we
+  // did have period where the unpacked was being serialized, this enables
+  // consuming those still and the check for which case we are in is pretty
+  // cheap.
+  size_t numElements = cast<RankedTensorV1Type>(type).getNumElements();
+  size_t packedSize = llvm::divideCeil(numElements, 8);
+  if (blob.size() == packedSize && blob.size() != numElements) {
+    // Unpack the blob.
+    rawData.resize(numElements);
+    for (size_t i = 0; i < numElements; ++i)
+      rawData[i] = (blob[i / 8] & (1 << (i % 8))) ? 1 : 0;
+    return success();
+  }
+  // Otherwise, fallback to the default behavior.
+  rawData.append(blob.begin(), blob.end());
+  return success();
+}
+
+static void writeVhloTensorV1Attr(DialectBytecodeWriter& writer,
+                                  vhlo::TensorV1Attr attr) {
+  if (!isBooleanType(attr.getType())) {
+    writer.writeOwnedBlob(attr.getData());
+    return;
+  }
+
+  // Pack the data if i1
+  SmallVector<char> data;
+  ArrayRef<char> rawData = attr.getData();
+  auto numElements = cast<RankedTensorV1Type>(attr.getType()).getNumElements();
+
+  // If the attribute is a splat, we can just splat the value directly.
+  bool isSplat = rawData.size() == 1 && numElements > 1;
+  if (isSplat) {
+    data.resize(1);
+    data[0] = rawData[0] ? 0xFF : 0x00;
+    writer.writeUnownedBlob(data);
+    return;
+  }
+
+  data.resize(llvm::divideCeil(numElements, 8));
+  // Otherwise, pack the data manually.
+  for (size_t i = 0; i < numElements; ++i)
+    if (rawData[i]) data[i / 8] |= (1 << (i % 8));
+  writer.writeUnownedBlob(data);
+}
+
+}  // namespace
+
 TensorV1Attr VhloBytecodeInterface::readTensorV1Attr(
     DialectBytecodeReader& reader) const {
   LOG_READ_CALL;
   Type type;
-  ArrayRef<char> blob;
-  if (failed(reader.readType(type)) || failed(reader.readBlob(blob)))
+  SmallVector<char> blob;
+  if (failed(reader.readType(type)) ||
+      failed(readVhloTensorV1Attr(reader, type, blob)))
     return TensorV1Attr();
   return TensorV1Attr::get(getContext(), type, blob);
 }
@@ -947,7 +1019,7 @@
                                   DialectBytecodeWriter& writer) const {
   writer.writeVarInt(vhlo_encoding::kTensorV1Attr);
   writer.writeType(attr.getType());
-  writer.writeOwnedBlob(attr.getData());
+  writeVhloTensorV1Attr(writer, attr);
 }
 
 //===----------------------------------------------------------------------===//
diff --ruN a/stablehlo/stablehlo/dialect/VhloDialect.td b/stablehlo/stablehlo/dialect/VhloDialect.td
--- stablehlo/stablehlo/dialect/VhloDialect.td
+++ stablehlo/stablehlo/dialect/VhloDialect.td
@@ -52,11 +52,11 @@
       1.11.0: Allow (de)serializing VHLO programs mixed with potentially unstable dialects.
       1.12.0: Add `source_target_pairs` attribute to `send` and `recv` ops.
       1.13.0: Extend `custom_call` op to support `buffer` types.
+      1.14.0: Introduce `composite` op with regions.
   }];
 
   let useDefaultAttributePrinterParser = 0;
   let useDefaultTypePrinterParser = 0;
-  let usePropertiesForAttributes = 1;
 }
 
 #endif  // STABLEHLO_DIALECT_VHLO_DIALECT
diff --ruN a/stablehlo/stablehlo/dialect/VhloOps.td b/stablehlo/stablehlo/dialect/VhloOps.td
--- stablehlo/stablehlo/dialect/VhloOps.td
+++ stablehlo/stablehlo/dialect/VhloOps.td
@@ -326,7 +326,7 @@
   let results = (outs VHLO_AnyType:$result);
 }
 
-def VHLO_CompositeOpV1 : VHLO_Op<"composite_v1", "0.19.0", "current"> {
+def VHLO_CompositeOpV1 : VHLO_Op<"composite_v1", "0.19.0", "1.13.0"> {
   let arguments = (ins
     Variadic<VHLO_AnyType>:$inputs,
     VHLO_AnyAttr:$name,
@@ -334,6 +334,18 @@
     VHLO_AnyAttr:$decomposition,
     VHLO_AnyAttr:$version
   );
+  let results = (outs Variadic<VHLO_AnyType>:$results);
+}
+
+def VHLO_CompositeOpV2 : VHLO_Op<"composite_v2", "1.14.0", "current"> {
+  let arguments = (ins
+    Variadic<VHLO_AnyType>:$inputs,
+    VHLO_AnyAttr:$name,
+    VHLO_AnyAttr:$composite_attributes,
+    VHLO_AnyAttr:$decomposition,
+    VHLO_AnyAttr:$version
+  );
+  let regions = (region VariadicRegion<VHLO_AnyRegion>:$composite_regions);
   let results = (outs Variadic<VHLO_AnyType>:$results);
 }
 
diff --ruN a/stablehlo/stablehlo/integrations/python/ChloModule.cpp b/stablehlo/stablehlo/integrations/python/ChloModule.cpp
--- stablehlo/stablehlo/integrations/python/ChloModule.cpp
+++ stablehlo/stablehlo/integrations/python/ChloModule.cpp
@@ -11,6 +11,7 @@
 limitations under the License.
 ==============================================================================*/
 
+#include "llvm/ADT/STLExtras.h"
 #include "mlir-c/IR.h"
 #include "mlir/Bindings/Python/NanobindAdaptors.h"
 #include "nanobind/nanobind.h"
diff --ruN a/stablehlo/stablehlo/integrations/python/StablehloModule.cpp b/stablehlo/stablehlo/integrations/python/StablehloModule.cpp
--- stablehlo/stablehlo/integrations/python/StablehloModule.cpp
+++ stablehlo/stablehlo/integrations/python/StablehloModule.cpp
@@ -13,6 +13,7 @@
 
 #include <vector>
 
+#include "llvm/ADT/STLExtras.h"
 #include "mlir-c/IR.h"
 #include "mlir-c/Support.h"
 #include "mlir/Bindings/Python/NanobindAdaptors.h"
diff --ruN a/stablehlo/stablehlo/integrations/python/mlir/dialects/stablehlo.py b/stablehlo/stablehlo/integrations/python/mlir/dialects/stablehlo.py
--- stablehlo/stablehlo/integrations/python/mlir/dialects/stablehlo.py
+++ stablehlo/stablehlo/integrations/python/mlir/dialects/stablehlo.py
@@ -17,3 +17,11 @@
 # pylint: disable=wildcard-import,relative-beyond-top-level,g-import-not-at-top
 from ._stablehlo_ops_gen import *
 from .._mlir_libs._stablehlo import *
+
+from . import _stablehlo_ops_gen
+
+class CompositeOp(_stablehlo_ops_gen.CompositeOp):
+  def __init__(self, *args, **kwargs):
+    if "num_composite_regions" not in kwargs:
+      kwargs["num_composite_regions"] = 0
+    super().__init__(*args, **kwargs)
diff --ruN a/stablehlo/stablehlo/reference/InterpreterOps.td b/stablehlo/stablehlo/reference/InterpreterOps.td
--- stablehlo/stablehlo/reference/InterpreterOps.td
+++ stablehlo/stablehlo/reference/InterpreterOps.td
@@ -31,8 +31,6 @@
     Dialect to implement interpreter-specific functionality, outside of the
     StableHLO spec.
   }];
-
-  let usePropertiesForAttributes = 1;
 }
 
 def Interpreter_ArrayOfFlatSymbolRefArrayAttr :
diff --ruN a/stablehlo/stablehlo/testdata/convert_element_type_int8_100_100.mlir b/stablehlo/stablehlo/testdata/convert_element_type_int8_100_100.mlir
--- stablehlo/stablehlo/testdata/convert_element_type_int8_100_100.mlir
+++ stablehlo/stablehlo/testdata/convert_element_type_int8_100_100.mlir
@@ -19,7 +19,7 @@
     return %c : tensor<100x100xi8>
   }
   func.func private @expected() -> (tensor<100x100xi1> {mhlo.layout_mode = "default"}) {
-    %c = stablehlo.constant dense<"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tensor<100x100xi1>
+    %c = stablehlo.constant dense<"0x00010101000101010001010100010101010001010001000101010101010001010101000100010101010001010101010100010001000101010101010100010101000101010000010100000101010100010001010100000001000101010101010101010100010100000101000101010100010100000101010101010100010100010101000001010101010001000000010001010001010001000001010101010001010101010101010100010101010101010001010101010100010100010101010101010101010000010001010101010001010001000101010001000001010101010001010100010101010101000001010101010100000101000101010101010101010101010101010101010101010001000101010100010101010101000001010000010101000000010101010001010101010101010101010000010101000101010101010101000001010000010001010101000101010001000100010100010101010100010100010101000001010001010101010100010101010100000101010100010001010101010101000101010101010101000000010101010100010001010100010101010101010101000101000101010100010000010101010100010001010001000101010100000001010101010101000100010001000100010000000101010101000100010001010000010001010101010101010101010101010101010101000001010101010101010101010101010101010101000100000101010101000101000101010101010101000101010101010101010100000101010101000101010101010101000101010101010101010101010101000100010101010100010100000101010100010000000100010101010101000000010101010101010000000101010001010001010100010101010101000101010001000101010101010101000001010100010101010101010100010101010101000101000100010101010001010101000101010100000001010001010101000001000100010101000101010101000001010101010101010100010101010101010100010101000101010100010100000101010101010101010101010101000101010100010101010101010101010101000101000101000101010101010101010101010001010101010101000000010101010101000101000001010001010101010101000001010101010001010101000001010001010101010100010101010001000101010001010101000101010100010101010101000001010101010101010101010101010100010101010101000101000001010001010101010001010101010000000101010101010001000101000001010101000101000101010100010101010100010101000101000101010101010001010101000101000000010101000100000001000100010001010101010101010101010000010101010100010101010001010100010101010001010000010101000101010101010101010100010101010001010000010100010101000001000100010001010101010100010001000001010001010101000101010101010101010101010101010101010101010100000100010101010100000000010001010101010001010101010001010101010101010100010101010001010101010101010101010101010101010001010100010000010001000100010000000101010001010101010100010100010100000000000101010101010001010100010101010101000000000001000101010100000001010101010001010101010000010001010001010001010100010101000101000100010001000101000000010101010101010100000101010000010001000001010001010101010101010100010101000101010100010101010101010101010101010100010001010100000001010101010000000101000100010101010001000101010101000101010101010101010100010000010001010101010101000000010001010101010000010101010100000101010101010100010001000101010001010000010000010101000101010100010000010100000001010101010101010100010100000001010101010000010100000001010101010101010001010001010101000001010101010101010100010100010001010101010001010101010000000101000100010001010100010100010000010101010101010101010000010101010100010000010001000100000101010000010001010101010101010101010101010001010101010100010001010101010100000101010001010001000100010001010001010101000001010101010101000101010100010101000100010101010101010100010101010001000000010100000101000100000001000100010000010100010101010101000101010101010101000101010101010101010101010000010101010100010101000101000100010000010101000101000101000100010000010101010001010000010101010101010001010101000100000001000000000101010100000100010001000100000100010101010101010101010101000101010000010100010101010101010101010100000101010001010101010100010101010101010101000001010101000101010000000100010101000101010001010001000101000100010001010101010101000100010101010101000000000101000101000101010000010001010101000100010100000101010101000101000101010100010101010101010001010101000001010101010101010101010101010000000101010100010001010101000101010101010100010101010101010101010100000100010001010101010100010100000101010101000001010001010001010101010101010101010100010101010001010101010001000101010101010101010101010101010100010100010101010101010101000001010100010100000100010101010001010101010000010001010101000100010101010001000101010100010100010001010100010100010100010000000101010101010101000100010100010001010000010101000101010101010001010101010101010101000101010101010000010101010001010100010101010100010101000100010101010101010101010101010100010100010101010001010101010101000101000101010101000100000101010101000101010101000101010101010101010001000101010001010101010101010000010101000101010100000001010101010000010101000000000001010100010101000100000100010001010001000101010000010001010101010100000101010101010100010000010100000101010101010101010100010101010101010000010000010101010101010101010101010001010001000101010100010101010000010001000101010101010101010101010001010100010101010101010000010100010100010100010101000001010101010001000101000100010101010101010101000101010101010101010101000101010100010100010101000100010101000101010101010100010001010101010000000100010001010101010101000101010101010100000000010101010101010101010101010100000101010101010100000001010101010100010101010000010100010101010101010101000001010101010101010101000100010101010101010000010001000001000101010001010101010100000101010101010100010101000101010101010101010001010101010101010101000100010100000101010101010101000001010000000101000100010101010001010101010001010001010101010001010101000101010101010101010001010100010000010101010100010000010001010101010101010000010101010101010100010101010101010100010101000000000101010001010001010001010101000101000101010101010001000101010101010101010100010101000101010101000100010000000100010101000101010101010100010101010101010101000101000001010100010101010101010100000101010101010101000101010101000100010000010001010100010101000101010001000101010101000101000100010101010101010101010101010100010101000101010001010000010101000101010101010101010101010000010001010100010101010001010100010100000100000100010001000101010100010100010101010001010001000000010000010001010101000100000101010100010101010100010101010001010001010101010101010101000001000101010001010101010100010101010101000101000001000101000100010100000000010101010001010101010101010101010001010101010001000100010101010101010101010000010001010101010101000001010101000101010101010101000001010001010001010101000101010100010101010101000101010100010101010101010100010101010101010001010101010101000101010100000101000101000001000100000101010001010101000101010100010101000100000101010001000101000000000101000101000001010001010101000001000001010101010101010101010101010101010101000100010100010101000101000000010000010000010001010101010101010001010001000101010101010100010101010001000101010100010101000001010100000000000000010000010001010101000001010000000001010101000101000001010000010000010100010101010100010001010101010100000101010101010100000100010001000101000101010100010001000001010101000100010101010100010001000101010101010101010001010001010101000100010001000101000100010101010100010001010101010001000101000101010101010100000001000101010001010101010101010101010101000101010101000101010101010101000100010101010001010001010101010000000001010001010101010100010101010101010001010001000101000101010101000101000101010101010100010100010001010100010101010101010101010101010101010101010101000101010100010101010001000100000000010101010001010101010100010001010101000101010101000001010101010101010101010101000101000101010001010101010101010100000101010100000101010001010001000001010101010100010101010101010100010000000100010101010101010100010101000101010101010000010101000001010101000101000101010101010100010100000000010100010101010101000101010101010100000101000101010101010101000100010001000100000101010100010101000101000101000101000100010001010100010000010001010101000001010100010101010101010001010101000101010001010000010101010101010101010101010101010101000101000101010001010101010101000100010101010101000100010001010101010001010101010101010001010101010101010101000000010101010101000101010101000100010101010101000100010101010100000101010001010000010100010101010100010101010101010100010101000101010101000101010101000101010101010100010101010100000101010001000000010101000101010001010101000001010100000101010101010001010101010100010101000000000101010101010001010100010100000000010101000100010100010101010000000001010101010101000101000100010101010100010001010101010100000100010101000101010100000001010001000101000001010101010101000101000100010101000000010100010100010100010100010001010100010101010000010000010101000101010001010001000101010101010101010101010100010101010101010101010000010101010100010101010101000101010100010101000000010001010101000100000101010101010100000000010001010101010101000101010001010101000101010101010001000101000101010001010101010100010101010100010101000101010001010101010101010001000101010101010101000101000101010001000101010100010101010101010101010100010101000000010000000101010101010101010101000101010101000101010100000001010001010101000101010100000100010101010100010000000001000100010001000100010101010101000101010000010101000101000100010001010001010101010100010101010101010101000101000001010000010101000101010101000101000101000001010001010101010101010101010101010100010101010101000101010101010101010100010101010101000001010101010101010101010101010101000100010100010100010001010100000000010101010101010101010101010101010101010101010101000001000100010001010101010101010101000001000100010001000101010000010101010001010101010001010001000101010001010101010100010100010100010001010100010101010101010101010100010101000101000101010001000001010101010000010100010101010101000001010001010100000101010101010001000000000101000101010001010101000101010101010100010000010101010101010000010000010100000100010100010101010001010101010000010101010001010101000101000101010001000100000101010101000101010100010101010101010100000100010101000101010101000101010100010100010101010100010000000101010101000101000101000001010000010101010100010001010100010000010101010101000100010101010000010101010101010001010101000101000101000101010101010101010101010101010101010001010101010001010101000101010101010101010101010101010101010001010101010001010101010101010101010001010101010100010001010100010101010100010001010101000000010100010101010100010101000100000100010101010100010100000101010101010101010100010101000001010101000101010001010000000101010101010001010001010101010100000100000100000101010101000101010101010001000101010100010101010101010100010001010101000101000101010101010001010001010101010101010101010101010101010100010101000101010101010101010101010001010101010101010100010101010100010000010100000100010100010001010001000101010000010101000101010101010101010101010100010101010101000001010001010101010101010101010101010101000000000001010101010101010100010001010001000101000100010100010001010101010101010000000101010001010001010101010100010101010101000001000001010101000100000101010100010101010001010101010101000100010101010101000101010101010100000101000101010101010001010001010101010100010101010101010100010101000001010101000101010100010000000101000001010001010001000000000101010000010100010100010101010101010001000101000101010100010101010101000101010101010000000100010000010000010101000101010101010100010100010001010000010101010101010101000000010001010001010100010101010100010000010101000100010101010101010001010101010101010101010101000100010101010101000101010100010001010101000100000101010101010101010001010101010101000100010101010101000100000101010101010101000101010001010101010101010101010001010001010101010101010101010100010101010101010100010001010101000000010101010100010001000101000100010001010101010101010101010101010101010001010101010100010101000100010001000001000100010101010101010000010100010001010101010001010101010101000101010000010100010001010001000101010100000101010001010101010100010101000001000101010101000101010101010000010101000101010101010101010101000101000101010100010001010001010101000000010101000101000101000101010101010000010101010000010101000001010001010101010100010001010101010000000101010001010101010000010101010101000001000101000001010101010100010101010100000001010101010101010101010101000101010101010000000000010101010100000001010001010101000101010100010001010001000101000101000101000000010001010101010001000101010101000101010101010101000001000101010100000100010101000001010101010101000100010000010001010100010100010101010101010101010101000101010001010100010100000101010101010101010001010101010101010101010101010101010000010000010001010101010101000000010001010101010101000101010101010101010101010001000001000101010101010101010001000101010001010001000100010100010101010100000101010100000000000101000101010101010100010101010100010100010101010101010101000100010100000101010101000100000001000101000101000101010101010101010001010101010001010101010000010001010101000100010101010101010101000001010101010101010001010000010101000001010001010100000100010100010101010100010101000101010001010101000100010000000100010101010101010101010001010101010101000101000101000001010000010101000100010101000101010001010100000001010100010101010001010101000001010001000101010100010101010001010000010001010100000101000101000101010101000000010101010101010100010100010101010101010101010000010101010100000101010100010101010001000101000001010001010101010001010101010101010101010101010000010101010101010100010001000100000101010100010101010101010101000101010101010101000101000101010101000101010001010101010101010000010101010100010100010101010101010000000000010101010101010001010100010101000100000100010101010100010101010101010100000001010100000101000001010101010101010001010101010101000000010001010101010001010101010000010001010100010101010100010100010100010101010100000101010100010001010101010101010001010101010101010100010001010101010101010101010101010001000101010001010100010101000101010001010101000001010101010100010000010101010101010101010101010100010101010100010101010101010101000100000001000101010101010101000001010101000100010101000100010100010101010100010001000101000100010101010101010101010101000101010101010100010101010101010001010101010100010101010100000100010001010001010101010100010101010001010000010101000001010101010100010101000001010100010000010101000101000101010100010101010101010101010100010101000101010000010001010100000100010101000101010000010101000101010100010001010001010101010101010001010100000100000001010001010101010101010101010100000100010101010001000100010000000100010101010101000001010101010100010100010101010100000000010101010100010001010101010100000101010101010101010001010101010000010101010100010101000001000100000100000100010001010001010101010101010101010101010100000101010100010101010100010100010100000001000101010101000100000101010101010101010001010000010101010101010101010101010101000101010100010101010101000001010000000101010101010001000101000101000101000100000100010001000001010101010100010101010000010100000001010101010101010101000101010100010100000000000100010001010101010101000101010100010101000100010001010101010101010100010101010001000101000001010001010101010001010101010101010001010101000101000100010101010101000101010101000101000101000001000101000100010101010101000001010101010101000100000101000001010101000101000001010000010101010101010101010100010101010001010000010101010001010101010101010101010101010101010101010101010101010100010101010101010001000000010101010100000100010101010100000100010001010001010101010101010101010101010101000001010101010101010101000001000100010101010101010001010101010101010101010101010001010101010000000001010001000001000101010101000001010101010101010101010001000100010001010101000101010101010101010101010101010101000000000101000101000101000101000101010001010101010001010001010101010100010101010101010101010100010001010101010100010101000101010000010101010101010101010101010101000101000100010001010101010101000000010101000100000101010101000101010101010001010101010101000000000101010001000101010101010001010001010101010101010101010101010101010101010101010101000101000001010001000000010000010100000101010101010101000101010001010100010100010101010101000001010101000001010001000001010101010101010101010101010001010101010001010100010101010000010101010000010101010101000100000101010101010101010101010101010101010000000000010101000000010101010001010100000101010101010101010000010100010101010101010101010001010101010101010101000101010000000001010100000101010001000101010001010000010101010101000001010101010001000101010101000101010101010101010001010101010101010001000101000101010100010100000100000101010101000101010101010101000000010100000101000101000101000101010001010100010101010101010101010001010001000101010101000101010101010100010101010001010101000100010101010100010101000001000101010101000101000101000101010101010101000101010100010101010101010101010100010101010101010101000101010101010100010101000100010000010001000001010101010001010100010101010101000101010100010001010001010101010101000101010000010101010101010100000101000001010101000100010101010100010101010101010101000101010101010001000101010101010101010101010000010101010101000101010100010001000100010101010101000001010100010101000100010101010101010100000101010101010001010101010001010100010001010001000101000001010101010101000101010101010100010101010101010001010000010101000001010001010101010100000101010001000001000100010001010101010101010001010101000101010101000101000101010100010101010101000101010101010001010001010000010101010000010101010101010101010001010100010101010101010001010101010001010100010100010100010101010101010101010101000100010101010001010101010000010101000001010100010001010101010101010100010100010000010101000101010101000101010100010001000101010101000101010101010100010000010001000101010101010100010101010101010100010101000101010100010001010101010101010100010101010100000000000001010101010100010101000100010100000101010101010101010101010101010101010101010001000100010001000001010101000001010101010001010101010101010101010101010101010100010100000101000101010101010100000101010101010100010001010000010101000101010001010001000100010101010100010101010001010101010101010101010001010001010100010101010001000101000001010000010100010100000101010100010100010000010101010001010101010101010100000101010001010101000101010101010100000101000001000101010101010100000001010100010101010001010001010101000101010101010100010100010100010101010101010000010001010101000000000001000101010000010101010100010101010001010101010101000101010101000101010100010101010101010101000101010101010101010101010101010101010100000101010100010101010100010100000100010101010101010100010001000101010001000100000101000101010101010100000101010101000101000001000101000101010101010101000001010000010101010101010000000100010101010101010101010001010001010001010001010001010001010101010101010001010101010001010001010101010101010100010100010001000101000101010001010101000001010101010101010100010000010001010100010101000101010101010100010000000101010101010101010101010100010101000001010100010001010101000101000100010001010001000000000001010101010001010101000101010101010101010001010101000101010100010001010001010101000100010101000001000100010101010101010101000100010101000000010101000101010101010101010001010101010100010001000100010100010101010000010101010101010001000101010001010101010001010101000100010101010101010001010101010100010101010100000101000101000101010101000101000101010100010100000101010101010001010100010001010000000001010101010000010101010100010100010101010101010101000101010101010001010101010001010001010001010101010101000101010100000100010001010101010000000101010100010101010001010101000101010000010101010101010000000100000101010100010001010101010100010000010000010001010101010100010100010001000100010000010101010100010100010101010101010101010001010101010000010101000001010001000101000101000101010100010000010100010100010001000101010101010100010101010101010101000101000101010101010001000001010001010101010101010101010101010101010101"> : tensor<100x100xi1>
     return %c : tensor<100x100xi1>
   }
 }
diff --ruN a/stablehlo/stablehlo/tests/CheckOps.td b/stablehlo/stablehlo/tests/CheckOps.td
--- stablehlo/stablehlo/tests/CheckOps.td
+++ stablehlo/stablehlo/tests/CheckOps.td
@@ -32,7 +32,6 @@
 
   let useDefaultAttributePrinterParser = 0;
   let useDefaultTypePrinterParser = 0;
-  let usePropertiesForAttributes = 1;
 }
 
 //===----------------------------------------------------------------------===//
diff --ruN a/stablehlo/stablehlo/tests/TestUtils.cpp b/stablehlo/stablehlo/tests/TestUtils.cpp
--- stablehlo/stablehlo/tests/TestUtils.cpp
+++ stablehlo/stablehlo/tests/TestUtils.cpp
@@ -16,6 +16,7 @@
 
 #include "stablehlo/tests/TestUtils.h"
 
+#include <cstdint>
 #include <utility>
 
 #include "llvm/ADT/STLExtras.h"
@@ -25,6 +26,7 @@
 #include "mlir/Dialect/Shape/IR/Shape.h"
 #include "mlir/IR/Attributes.h"
 #include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
 #include "mlir/IR/MLIRContext.h"
 #include "mlir/IR/Operation.h"
 #include "mlir/IR/OperationSupport.h"
@@ -114,6 +116,54 @@
   }
 };
 
+struct InferReturnShapedTypesPattern : public RewritePattern {
+  explicit InferReturnShapedTypesPattern(MLIRContext* context)
+      : RewritePattern("hlo_test_infer.get_return_type_components", 1,
+                       context) {}
+  LogicalResult matchAndRewrite(Operation* op,
+                                PatternRewriter& rewriter) const override {
+    if (op->getNumOperands() != 1) return failure();
+    auto* definingOp = op->getOperand(0).getDefiningOp();
+    auto definingOpInt =
+        llvm::dyn_cast_or_null<InferShapedTypeOpInterface>(definingOp);
+    if (!definingOpInt)
+      return rewriter.notifyMatchFailure(
+          op, "doesn't implement InferShapedTypeOpInterface");
+
+    SmallVector<ShapedTypeComponents> inferredComponents;
+    if (failed(definingOpInt.inferReturnTypeComponents(
+            op->getContext(), op->getLoc(), definingOp->getOperands(),
+            definingOp->getAttrDictionary(), definingOp->getPropertiesStorage(),
+            definingOp->getRegions(), inferredComponents)))
+      return rewriter.notifyMatchFailure(op,
+                                         "failed to infer return shaped types");
+
+    // Replace the op with another pass-through op with attributes added.
+    OperationState state(op->getLoc(), "hlo_test_infer.return_type_components",
+                         op->getOperands(), op->getResultTypes(),
+                         op->getAttrs());
+    auto* newOp = rewriter.create(state);
+    for (const auto& it : llvm::enumerate(inferredComponents))
+      newOp->setAttr((StringRef("types") + Twine(it.index())).str(),
+                     componentToAttribute(it.value(), rewriter));
+    rewriter.replaceOp(op, {newOp->getResults()});
+    return success();
+  }
+  Attribute componentToAttribute(const ShapedTypeComponents& component,
+                                 PatternRewriter& rewriter) const {
+    SmallVector<NamedAttribute, 2> attrs;
+    // Dummy tensor of index type with the same rank as the shaped type.
+    // use tensor so we get `?` in the printing for dynamic dims
+    ArrayRef<int64_t> shape = component.getDims();
+    Type elementType = component.getElementType();
+    Attribute encoding = component.getAttribute();
+    if (!elementType) {
+      elementType = rewriter.getIndexType();
+    }
+    return TypeAttr::get(RankedTensorType::get(shape, elementType, encoding));
+  }
+};
+
 LogicalResult checkSpeculatability(PatternRewriter& rewriter, Operation* op,
                                    mlir::Speculation::Speculatability spec) {
   if (op->getNumOperands() != 1) return failure();
@@ -190,6 +240,7 @@
     RewritePatternSet patterns(context);
     patterns.add<InferReturnTypesPattern>(context);
     patterns.add<ReifyReturnTypeShapesPattern>(context);
+    patterns.add<InferReturnShapedTypesPattern>(context);
     patterns_ = std::move(patterns);
     return success();
   }
diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
+++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
@@ -238,168 +238,168 @@
 // CHECK:           %[[MAXIMUM_0:.*]] = stablehlo.maximum %[[ABS_0]], %[[ABS_1]] : tensor<?xf64>
 // CHECK:           %[[CONSTANT_0:.*]] = stablehlo.constant dense<1.7976931348623157E+308> : tensor<f64>
 // CHECK:           %[[SHAPE_OF_0:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_0:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_0]], %[[SHAPE_OF_0]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[SQRT_0:.*]] = stablehlo.sqrt %[[DYNAMIC_BROADCAST_IN_DIM_0]] : tensor<?xf64>
+// CHECK:           %[[ASSUMING_0:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_0]], %[[SHAPE_OF_0]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[SQRT_0:.*]] = stablehlo.sqrt %[[ASSUMING_0]] : tensor<?xf64>
 // CHECK:           %[[CONSTANT_1:.*]] = stablehlo.constant dense<8.000000e+00> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_1:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_1:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_1]], %[[SHAPE_OF_1]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[DIVIDE_0:.*]] = stablehlo.divide %[[SQRT_0]], %[[DYNAMIC_BROADCAST_IN_DIM_1]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_3:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[ASSUMING_1:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_1]], %[[SHAPE_OF_3]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[DIVIDE_0:.*]] = stablehlo.divide %[[SQRT_0]], %[[ASSUMING_1]] : tensor<?xf64>
 // CHECK:           %[[COMPARE_0:.*]] = stablehlo.compare  GE, %[[MAXIMUM_0]], %[[DIVIDE_0]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[CONSTANT_2:.*]] = stablehlo.constant dense<1.000000e+00> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_2:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_2:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_2]], %[[SHAPE_OF_2]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[COMPARE_1:.*]] = stablehlo.compare  LE, %[[ABS_0]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[SHAPE_OF_5:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[ASSUMING_2:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_2]], %[[SHAPE_OF_5]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[COMPARE_1:.*]] = stablehlo.compare  LE, %[[ABS_0]], %[[ASSUMING_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[CONSTANT_3:.*]] = stablehlo.constant dense<5.000000e-01> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_3:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_3:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_3]], %[[SHAPE_OF_3]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[ADD_0:.*]] = stablehlo.add %[[ABS_0]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_7:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[ASSUMING_3:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_3]], %[[SHAPE_OF_7]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[ADD_0:.*]] = stablehlo.add %[[ABS_0]], %[[ASSUMING_2]] : tensor<?xf64>
 // CHECK:           %[[ABS_2:.*]] = stablehlo.abs %[[ADD_0]] : tensor<?xf64>
 // CHECK:           %[[MAXIMUM_1:.*]] = stablehlo.maximum %[[ABS_2]], %[[ABS_1]] : tensor<?xf64>
 // CHECK:           %[[MINIMUM_0:.*]] = stablehlo.minimum %[[ABS_2]], %[[ABS_1]] : tensor<?xf64>
 // CHECK:           %[[COMPARE_2:.*]] = stablehlo.compare  EQ, %[[MAXIMUM_1]], %[[MINIMUM_0]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[CONSTANT_4:.*]] = stablehlo.constant dense<1.4142135623730951> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_4:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_4:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_4]], %[[SHAPE_OF_4]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[MULTIPLY_0:.*]] = stablehlo.multiply %[[DYNAMIC_BROADCAST_IN_DIM_4]], %[[MAXIMUM_1]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_9:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[ASSUMING_4:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_4]], %[[SHAPE_OF_9]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[MULTIPLY_0:.*]] = stablehlo.multiply %[[ASSUMING_4]], %[[MAXIMUM_1]] : tensor<?xf64>
 // CHECK:           %[[DIVIDE_1:.*]] = stablehlo.divide %[[MINIMUM_0]], %[[MAXIMUM_1]] : tensor<?xf64>
 // CHECK:           %[[MULTIPLY_1:.*]] = stablehlo.multiply %[[DIVIDE_1]], %[[DIVIDE_1]] : tensor<?xf64>
-// CHECK:           %[[ADD_1:.*]] = stablehlo.add %[[DYNAMIC_BROADCAST_IN_DIM_2]], %[[MULTIPLY_1]] : tensor<?xf64>
+// CHECK:           %[[ADD_1:.*]] = stablehlo.add %[[ASSUMING_2]], %[[MULTIPLY_1]] : tensor<?xf64>
 // CHECK:           %[[SQRT_1:.*]] = stablehlo.sqrt %[[ADD_1]] : tensor<?xf64>
-// CHECK:           %[[COMPARE_3:.*]] = stablehlo.compare  EQ, %[[SQRT_1]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[COMPARE_3:.*]] = stablehlo.compare  EQ, %[[SQRT_1]], %[[ASSUMING_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[CONSTANT_5:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_5:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_5:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_5]], %[[SHAPE_OF_5]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[COMPARE_4:.*]] = stablehlo.compare  GT, %[[MULTIPLY_1]], %[[DYNAMIC_BROADCAST_IN_DIM_5]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[SHAPE_OF_11:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[ASSUMING_5:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_5]], %[[SHAPE_OF_11]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[COMPARE_4:.*]] = stablehlo.compare  GT, %[[MULTIPLY_1]], %[[ASSUMING_5]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[AND_0:.*]] = stablehlo.and %[[COMPARE_3]], %[[COMPARE_4]] : tensor<?xi1>
 // CHECK:           %[[MULTIPLY_2:.*]] = stablehlo.multiply %[[MAXIMUM_1]], %[[MULTIPLY_1]] : tensor<?xf64>
 // CHECK:           %[[CONSTANT_6:.*]] = stablehlo.constant dense<2.000000e+00> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_6:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_6:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_6]], %[[SHAPE_OF_6]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[DIVIDE_2:.*]] = stablehlo.divide %[[MULTIPLY_2]], %[[DYNAMIC_BROADCAST_IN_DIM_6]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_13:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[ASSUMING_6:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_6]], %[[SHAPE_OF_13]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[DIVIDE_2:.*]] = stablehlo.divide %[[MULTIPLY_2]], %[[ASSUMING_6]] : tensor<?xf64>
 // CHECK:           %[[ADD_2:.*]] = stablehlo.add %[[MAXIMUM_1]], %[[DIVIDE_2]] : tensor<?xf64>
 // CHECK:           %[[MULTIPLY_3:.*]] = stablehlo.multiply %[[MAXIMUM_1]], %[[SQRT_1]] : tensor<?xf64>
-// CHECK:           %[[SELECT_0:.*]] = stablehlo.select %[[AND_0]], %[[ADD_2]], %[[MULTIPLY_3]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[SELECT_1:.*]] = stablehlo.select %[[COMPARE_2]], %[[MULTIPLY_0]], %[[SELECT_0]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[SUBTRACT_0:.*]] = stablehlo.subtract %[[ABS_0]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : tensor<?xf64>
+// CHECK:           %[[ASSUMING_7:.*]] = stablehlo.select %[[AND_0]], %[[ADD_2]], %[[MULTIPLY_3]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ASSUMING_8:.*]] = stablehlo.select %[[COMPARE_2]], %[[MULTIPLY_0]], %[[ASSUMING_7]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[SUBTRACT_0:.*]] = stablehlo.subtract %[[ABS_0]], %[[ASSUMING_2]] : tensor<?xf64>
 // CHECK:           %[[ABS_3:.*]] = stablehlo.abs %[[SUBTRACT_0]] : tensor<?xf64>
 // CHECK:           %[[MAXIMUM_2:.*]] = stablehlo.maximum %[[ABS_3]], %[[ABS_1]] : tensor<?xf64>
 // CHECK:           %[[MINIMUM_1:.*]] = stablehlo.minimum %[[ABS_3]], %[[ABS_1]] : tensor<?xf64>
 // CHECK:           %[[COMPARE_5:.*]] = stablehlo.compare  EQ, %[[MAXIMUM_2]], %[[MINIMUM_1]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
-// CHECK:           %[[MULTIPLY_4:.*]] = stablehlo.multiply %[[DYNAMIC_BROADCAST_IN_DIM_4]], %[[MAXIMUM_2]] : tensor<?xf64>
+// CHECK:           %[[MULTIPLY_4:.*]] = stablehlo.multiply %[[ASSUMING_4]], %[[MAXIMUM_2]] : tensor<?xf64>
 // CHECK:           %[[DIVIDE_3:.*]] = stablehlo.divide %[[MINIMUM_1]], %[[MAXIMUM_2]] : tensor<?xf64>
 // CHECK:           %[[MULTIPLY_5:.*]] = stablehlo.multiply %[[DIVIDE_3]], %[[DIVIDE_3]] : tensor<?xf64>
-// CHECK:           %[[ADD_3:.*]] = stablehlo.add %[[DYNAMIC_BROADCAST_IN_DIM_2]], %[[MULTIPLY_5]] : tensor<?xf64>
+// CHECK:           %[[ADD_3:.*]] = stablehlo.add %[[ASSUMING_2]], %[[MULTIPLY_5]] : tensor<?xf64>
 // CHECK:           %[[SQRT_2:.*]] = stablehlo.sqrt %[[ADD_3]] : tensor<?xf64>
-// CHECK:           %[[COMPARE_6:.*]] = stablehlo.compare  EQ, %[[SQRT_2]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
-// CHECK:           %[[COMPARE_7:.*]] = stablehlo.compare  GT, %[[MULTIPLY_5]], %[[DYNAMIC_BROADCAST_IN_DIM_5]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[COMPARE_6:.*]] = stablehlo.compare  EQ, %[[SQRT_2]], %[[ASSUMING_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[COMPARE_7:.*]] = stablehlo.compare  GT, %[[MULTIPLY_5]], %[[ASSUMING_5]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[AND_1:.*]] = stablehlo.and %[[COMPARE_6]], %[[COMPARE_7]] : tensor<?xi1>
 // CHECK:           %[[MULTIPLY_6:.*]] = stablehlo.multiply %[[MAXIMUM_2]], %[[MULTIPLY_5]] : tensor<?xf64>
-// CHECK:           %[[DIVIDE_4:.*]] = stablehlo.divide %[[MULTIPLY_6]], %[[DYNAMIC_BROADCAST_IN_DIM_6]] : tensor<?xf64>
+// CHECK:           %[[DIVIDE_4:.*]] = stablehlo.divide %[[MULTIPLY_6]], %[[ASSUMING_6]] : tensor<?xf64>
 // CHECK:           %[[ADD_4:.*]] = stablehlo.add %[[MAXIMUM_2]], %[[DIVIDE_4]] : tensor<?xf64>
 // CHECK:           %[[MULTIPLY_7:.*]] = stablehlo.multiply %[[MAXIMUM_2]], %[[SQRT_2]] : tensor<?xf64>
-// CHECK:           %[[SELECT_2:.*]] = stablehlo.select %[[AND_1]], %[[ADD_4]], %[[MULTIPLY_7]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[SELECT_3:.*]] = stablehlo.select %[[COMPARE_5]], %[[MULTIPLY_4]], %[[SELECT_2]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[ADD_5:.*]] = stablehlo.add %[[SELECT_1]], %[[SELECT_3]] : tensor<?xf64>
-// CHECK:           %[[MULTIPLY_8:.*]] = stablehlo.multiply %[[DYNAMIC_BROADCAST_IN_DIM_3]], %[[ADD_5]] : tensor<?xf64>
+// CHECK:           %[[ASSUMING_9:.*]] = stablehlo.select %[[AND_1]], %[[ADD_4]], %[[MULTIPLY_7]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ASSUMING_10:.*]] = stablehlo.select %[[COMPARE_5]], %[[MULTIPLY_4]], %[[ASSUMING_9]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ADD_5:.*]] = stablehlo.add %[[ASSUMING_8]], %[[ASSUMING_10]] : tensor<?xf64>
+// CHECK:           %[[MULTIPLY_8:.*]] = stablehlo.multiply %[[ASSUMING_3]], %[[ADD_5]] : tensor<?xf64>
 // CHECK:           %[[ADD_6:.*]] = stablehlo.add %[[MULTIPLY_8]], %[[ABS_0]] : tensor<?xf64>
-// CHECK:           %[[MULTIPLY_9:.*]] = stablehlo.multiply %[[DYNAMIC_BROADCAST_IN_DIM_3]], %[[ADD_6]] : tensor<?xf64>
+// CHECK:           %[[MULTIPLY_9:.*]] = stablehlo.multiply %[[ASSUMING_3]], %[[ADD_6]] : tensor<?xf64>
 // CHECK:           %[[MULTIPLY_10:.*]] = stablehlo.multiply %[[ABS_1]], %[[ABS_1]] : tensor<?xf64>
-// CHECK:           %[[ADD_7:.*]] = stablehlo.add %[[SELECT_1]], %[[ADD_0]] : tensor<?xf64>
+// CHECK:           %[[ADD_7:.*]] = stablehlo.add %[[ASSUMING_8]], %[[ADD_0]] : tensor<?xf64>
 // CHECK:           %[[DIVIDE_5:.*]] = stablehlo.divide %[[MULTIPLY_10]], %[[ADD_7]] : tensor<?xf64>
-// CHECK:           %[[SUBTRACT_1:.*]] = stablehlo.subtract %[[SELECT_3]], %[[SUBTRACT_0]] : tensor<?xf64>
+// CHECK:           %[[SUBTRACT_1:.*]] = stablehlo.subtract %[[ASSUMING_10]], %[[SUBTRACT_0]] : tensor<?xf64>
 // CHECK:           %[[ADD_8:.*]] = stablehlo.add %[[DIVIDE_5]], %[[SUBTRACT_1]] : tensor<?xf64>
 // CHECK:           %[[MULTIPLY_11:.*]] = stablehlo.multiply %[[MULTIPLY_9]], %[[ADD_8]] : tensor<?xf64>
 // CHECK:           %[[SQRT_3:.*]] = stablehlo.sqrt %[[MULTIPLY_11]] : tensor<?xf64>
 // CHECK:           %[[DIVIDE_6:.*]] = stablehlo.divide %[[MULTIPLY_9]], %[[ADD_7]] : tensor<?xf64>
-// CHECK:           %[[ADD_9:.*]] = stablehlo.add %[[SELECT_3]], %[[SUBTRACT_0]] : tensor<?xf64>
+// CHECK:           %[[ADD_9:.*]] = stablehlo.add %[[ASSUMING_10]], %[[SUBTRACT_0]] : tensor<?xf64>
 // CHECK:           %[[DIVIDE_7:.*]] = stablehlo.divide %[[MULTIPLY_9]], %[[ADD_9]] : tensor<?xf64>
 // CHECK:           %[[ADD_10:.*]] = stablehlo.add %[[DIVIDE_6]], %[[DIVIDE_7]] : tensor<?xf64>
 // CHECK:           %[[SQRT_4:.*]] = stablehlo.sqrt %[[ADD_10]] : tensor<?xf64>
 // CHECK:           %[[MULTIPLY_12:.*]] = stablehlo.multiply %[[ABS_1]], %[[SQRT_4]] : tensor<?xf64>
-// CHECK:           %[[SELECT_4:.*]] = stablehlo.select %[[COMPARE_1]], %[[SQRT_3]], %[[MULTIPLY_12]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[SELECT_5:.*]] = stablehlo.select %[[COMPARE_0]], %[[ABS_1]], %[[SELECT_4]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ASSUMING_11:.*]] = stablehlo.select %[[COMPARE_1]], %[[SQRT_3]], %[[MULTIPLY_12]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ASSUMING_12:.*]] = stablehlo.select %[[COMPARE_0]], %[[ABS_1]], %[[ASSUMING_11]] : tensor<?xi1>, tensor<?xf64>
 // CHECK:           %[[CONSTANT_7:.*]] = stablehlo.constant dense<1.000000e+12> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_7:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_7:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_7]], %[[SHAPE_OF_7]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[MULTIPLY_13:.*]] = stablehlo.multiply %[[DIVIDE_0]], %[[DYNAMIC_BROADCAST_IN_DIM_7]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_50:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_25:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_7]], %[[SHAPE_OF_50]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[MULTIPLY_13:.*]] = stablehlo.multiply %[[DIVIDE_0]], %[[DYNAMIC_BROADCAST_IN_DIM_25]] : tensor<?xf64>
 // CHECK:           %[[COMPARE_8:.*]] = stablehlo.compare  LT, %[[ABS_0]], %[[MULTIPLY_13]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[CONSTANT_8:.*]] = stablehlo.constant dense<9.9999999999999995E-7> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_8:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_8:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_8]], %[[SHAPE_OF_8]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[MULTIPLY_14:.*]] = stablehlo.multiply %[[DIVIDE_0]], %[[DYNAMIC_BROADCAST_IN_DIM_8]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_51:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_26:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_8]], %[[SHAPE_OF_51]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[MULTIPLY_14:.*]] = stablehlo.multiply %[[DIVIDE_0]], %[[DYNAMIC_BROADCAST_IN_DIM_26]] : tensor<?xf64>
 // CHECK:           %[[CONSTANT_9:.*]] = stablehlo.constant dense<1.000000e+02> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_9:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_9:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_9]], %[[SHAPE_OF_9]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[MULTIPLY_15:.*]] = stablehlo.multiply %[[DIVIDE_0]], %[[DYNAMIC_BROADCAST_IN_DIM_9]] : tensor<?xf64>
-// CHECK:           %[[SELECT_6:.*]] = stablehlo.select %[[COMPARE_8]], %[[MULTIPLY_14]], %[[MULTIPLY_15]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[COMPARE_9:.*]] = stablehlo.compare  GE, %[[ABS_1]], %[[SELECT_6]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
-// CHECK:           %[[SELECT_7:.*]] = stablehlo.select %[[COMPARE_9]], %[[ABS_1]], %[[ABS_0]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[SELECT_8:.*]] = stablehlo.select %[[COMPARE_9]], %[[SELECT_6]], %[[DIVIDE_0]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[COMPARE_10:.*]] = stablehlo.compare  GE, %[[SELECT_7]], %[[SELECT_8]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
-// CHECK:           %[[LOG_0:.*]] = stablehlo.log %[[DYNAMIC_BROADCAST_IN_DIM_6]] : tensor<?xf64>
-// CHECK:           %[[LOG_1:.*]] = stablehlo.log %[[SELECT_7]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_52:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_27:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_9]], %[[SHAPE_OF_52]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[MULTIPLY_15:.*]] = stablehlo.multiply %[[DIVIDE_0]], %[[DYNAMIC_BROADCAST_IN_DIM_27]] : tensor<?xf64>
+// CHECK:           %[[ASSUMING_13:.*]] = stablehlo.select %[[COMPARE_8]], %[[MULTIPLY_14]], %[[MULTIPLY_15]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[COMPARE_9:.*]] = stablehlo.compare  GE, %[[ABS_1]], %[[ASSUMING_13]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[ASSUMING_14:.*]] = stablehlo.select %[[COMPARE_9]], %[[ABS_1]], %[[ABS_0]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ASSUMING_15:.*]] = stablehlo.select %[[COMPARE_9]], %[[ASSUMING_13]], %[[DIVIDE_0]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[COMPARE_10:.*]] = stablehlo.compare  GE, %[[ASSUMING_14]], %[[ASSUMING_15]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[LOG_0:.*]] = stablehlo.log %[[ASSUMING_6]] : tensor<?xf64>
+// CHECK:           %[[LOG_1:.*]] = stablehlo.log %[[ASSUMING_14]] : tensor<?xf64>
 // CHECK:           %[[ADD_11:.*]] = stablehlo.add %[[LOG_0]], %[[LOG_1]] : tensor<?xf64>
 // CHECK:           %[[CONSTANT_10:.*]] = stablehlo.constant dense<0x7FF0000000000000> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_10:.*]] = shape.shape_of %[[IMAG_0]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_10:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_10]], %[[SHAPE_OF_10]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[COMPARE_11:.*]] = stablehlo.compare  EQ, %[[ABS_1]], %[[DYNAMIC_BROADCAST_IN_DIM_10]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[SHAPE_OF_71:.*]] = shape.shape_of %[[IMAG_0]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_37:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_10]], %[[SHAPE_OF_71]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[COMPARE_11:.*]] = stablehlo.compare  EQ, %[[ABS_1]], %[[DYNAMIC_BROADCAST_IN_DIM_37]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[NOT_0:.*]] = stablehlo.not %[[COMPARE_11]] : tensor<?xi1>
 // CHECK:           %[[AND_2:.*]] = stablehlo.and %[[COMPARE_9]], %[[NOT_0]] : tensor<?xi1>
 // CHECK:           %[[DIVIDE_8:.*]] = stablehlo.divide %[[ABS_0]], %[[ABS_1]] : tensor<?xf64>
-// CHECK:           %[[SELECT_9:.*]] = stablehlo.select %[[AND_2]], %[[DIVIDE_8]], %[[DYNAMIC_BROADCAST_IN_DIM_5]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[MULTIPLY_16:.*]] = stablehlo.multiply %[[SELECT_9]], %[[SELECT_9]] : tensor<?xf64>
+// CHECK:           %[[ASSUMING_16:.*]] = stablehlo.select %[[AND_2]], %[[DIVIDE_8]], %[[ASSUMING_5]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[MULTIPLY_16:.*]] = stablehlo.multiply %[[ASSUMING_16]], %[[ASSUMING_16]] : tensor<?xf64>
 // CHECK:           %[[LOG_PLUS_ONE_0:.*]] = stablehlo.log_plus_one %[[MULTIPLY_16]] : tensor<?xf64>
-// CHECK:           %[[MULTIPLY_17:.*]] = stablehlo.multiply %[[DYNAMIC_BROADCAST_IN_DIM_3]], %[[LOG_PLUS_ONE_0]] : tensor<?xf64>
+// CHECK:           %[[MULTIPLY_17:.*]] = stablehlo.multiply %[[ASSUMING_3]], %[[LOG_PLUS_ONE_0]] : tensor<?xf64>
 // CHECK:           %[[ADD_12:.*]] = stablehlo.add %[[ADD_11]], %[[MULTIPLY_17]] : tensor<?xf64>
 // CHECK:           %[[CONSTANT_11:.*]] = stablehlo.constant dense<2.2250738585072014E-308> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_11:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_11:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_11]], %[[SHAPE_OF_11]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[SQRT_5:.*]] = stablehlo.sqrt %[[DYNAMIC_BROADCAST_IN_DIM_11]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_78:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_41:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_11]], %[[SHAPE_OF_78]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[SQRT_5:.*]] = stablehlo.sqrt %[[DYNAMIC_BROADCAST_IN_DIM_41]] : tensor<?xf64>
 // CHECK:           %[[CONSTANT_12:.*]] = stablehlo.constant dense<4.000000e+00> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_12:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_12:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_12]], %[[SHAPE_OF_12]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[MULTIPLY_18:.*]] = stablehlo.multiply %[[SQRT_5]], %[[DYNAMIC_BROADCAST_IN_DIM_12]] : tensor<?xf64>
+// CHECK:           %[[SHAPE_OF_79:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_42:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_12]], %[[SHAPE_OF_79]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[MULTIPLY_18:.*]] = stablehlo.multiply %[[SQRT_5]], %[[DYNAMIC_BROADCAST_IN_DIM_42]] : tensor<?xf64>
 // CHECK:           %[[COMPARE_12:.*]] = stablehlo.compare  LT, %[[ABS_1]], %[[MULTIPLY_18]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
-// CHECK:           %[[COMPARE_13:.*]] = stablehlo.compare  LT, %[[ABS_0]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[COMPARE_13:.*]] = stablehlo.compare  LT, %[[ABS_0]], %[[ASSUMING_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[AND_3:.*]] = stablehlo.and %[[COMPARE_12]], %[[COMPARE_13]] : tensor<?xi1>
 // CHECK:           %[[MULTIPLY_19:.*]] = stablehlo.multiply %[[ADD_0]], %[[SUBTRACT_0]] : tensor<?xf64>
-// CHECK:           %[[ADD_13:.*]] = stablehlo.add %[[MULTIPLY_8]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : tensor<?xf64>
+// CHECK:           %[[ADD_13:.*]] = stablehlo.add %[[MULTIPLY_8]], %[[ASSUMING_2]] : tensor<?xf64>
 // CHECK:           %[[DIVIDE_9:.*]] = stablehlo.divide %[[MULTIPLY_19]], %[[ADD_13]] : tensor<?xf64>
 // CHECK:           %[[NEGATE_0:.*]] = stablehlo.negate %[[DIVIDE_9]] : tensor<?xf64>
-// CHECK:           %[[COMPARE_14:.*]] = stablehlo.compare  GE, %[[ABS_0]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
-// CHECK:           %[[MULTIPLY_20:.*]] = stablehlo.multiply %[[DYNAMIC_BROADCAST_IN_DIM_3]], %[[MULTIPLY_10]] : tensor<?xf64>
+// CHECK:           %[[COMPARE_14:.*]] = stablehlo.compare  GE, %[[ABS_0]], %[[ASSUMING_2]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[MULTIPLY_20:.*]] = stablehlo.multiply %[[ASSUMING_3]], %[[MULTIPLY_10]] : tensor<?xf64>
 // CHECK:           %[[DIVIDE_10:.*]] = stablehlo.divide %[[MULTIPLY_20]], %[[ADD_7]] : tensor<?xf64>
-// CHECK:           %[[MULTIPLY_21:.*]] = stablehlo.multiply %[[DYNAMIC_BROADCAST_IN_DIM_3]], %[[ADD_9]] : tensor<?xf64>
+// CHECK:           %[[MULTIPLY_21:.*]] = stablehlo.multiply %[[ASSUMING_3]], %[[ADD_9]] : tensor<?xf64>
 // CHECK:           %[[ADD_14:.*]] = stablehlo.add %[[DIVIDE_10]], %[[MULTIPLY_21]] : tensor<?xf64>
 // CHECK:           %[[CONSTANT_13:.*]] = stablehlo.constant dense<1.500000e+00> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_13:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_13:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_13]], %[[SHAPE_OF_13]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[COMPARE_15:.*]] = stablehlo.compare  LE, %[[MULTIPLY_8]], %[[DYNAMIC_BROADCAST_IN_DIM_13]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[SHAPE_OF_80:.*]] = shape.shape_of %[[REAL_1]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_43:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_13]], %[[SHAPE_OF_80]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[COMPARE_15:.*]] = stablehlo.compare  LE, %[[MULTIPLY_8]], %[[DYNAMIC_BROADCAST_IN_DIM_43]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[DIVIDE_11:.*]] = stablehlo.divide %[[MULTIPLY_20]], %[[SUBTRACT_1]] : tensor<?xf64>
 // CHECK:           %[[ADD_15:.*]] = stablehlo.add %[[DIVIDE_10]], %[[DIVIDE_11]] : tensor<?xf64>
-// CHECK:           %[[SUBTRACT_2:.*]] = stablehlo.subtract %[[MULTIPLY_8]], %[[DYNAMIC_BROADCAST_IN_DIM_2]] : tensor<?xf64>
-// CHECK:           %[[SELECT_10:.*]] = stablehlo.select %[[COMPARE_15]], %[[ADD_15]], %[[SUBTRACT_2]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[SELECT_11:.*]] = stablehlo.select %[[COMPARE_14]], %[[ADD_14]], %[[SELECT_10]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[SELECT_12:.*]] = stablehlo.select %[[AND_3]], %[[NEGATE_0]], %[[SELECT_11]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[MULTIPLY_22:.*]] = stablehlo.multiply %[[SELECT_12]], %[[ADD_13]] : tensor<?xf64>
+// CHECK:           %[[SUBTRACT_2:.*]] = stablehlo.subtract %[[MULTIPLY_8]], %[[ASSUMING_2]] : tensor<?xf64>
+// CHECK:           %[[ASSUMING_17:.*]] = stablehlo.select %[[COMPARE_15]], %[[ADD_15]], %[[SUBTRACT_2]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ASSUMING_18:.*]] = stablehlo.select %[[COMPARE_14]], %[[ADD_14]], %[[ASSUMING_17]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ASSUMING_19:.*]] = stablehlo.select %[[AND_3]], %[[NEGATE_0]], %[[ASSUMING_18]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[MULTIPLY_22:.*]] = stablehlo.multiply %[[ASSUMING_19]], %[[ADD_13]] : tensor<?xf64>
 // CHECK:           %[[SQRT_6:.*]] = stablehlo.sqrt %[[MULTIPLY_22]] : tensor<?xf64>
 // CHECK:           %[[DIVIDE_12:.*]] = stablehlo.divide %[[ABS_1]], %[[SQRT_6]] : tensor<?xf64>
-// CHECK:           %[[ADD_16:.*]] = stablehlo.add %[[SELECT_12]], %[[SQRT_6]] : tensor<?xf64>
+// CHECK:           %[[ADD_16:.*]] = stablehlo.add %[[ASSUMING_19]], %[[SQRT_6]] : tensor<?xf64>
 // CHECK:           %[[LOG_PLUS_ONE_1:.*]] = stablehlo.log_plus_one %[[ADD_16]] : tensor<?xf64>
-// CHECK:           %[[SELECT_13:.*]] = stablehlo.select %[[AND_3]], %[[DIVIDE_12]], %[[LOG_PLUS_ONE_1]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[SELECT_14:.*]] = stablehlo.select %[[COMPARE_10]], %[[ADD_12]], %[[SELECT_13]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[COMPLEX_0:.*]] = stablehlo.complex %[[SELECT_5]], %[[SELECT_14]] : tensor<?xcomplex<f64>>
+// CHECK:           %[[ASSUMING_20:.*]] = stablehlo.select %[[AND_3]], %[[DIVIDE_12]], %[[LOG_PLUS_ONE_1]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[ASSUMING_21:.*]] = stablehlo.select %[[COMPARE_10]], %[[ADD_12]], %[[ASSUMING_20]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[COMPLEX_0:.*]] = stablehlo.complex %[[ASSUMING_12]], %[[ASSUMING_21]] : tensor<?xcomplex<f64>>
 // CHECK:           %[[REAL_2:.*]] = stablehlo.real %[[COMPLEX_0]] : (tensor<?xcomplex<f64>>) -> tensor<?xf64>
 // CHECK:           %[[VAL_0:.*]] = stablehlo.atan2 %[[REAL_0]], %[[REAL_2]] : tensor<?xf64>
 // CHECK:           %[[IMAG_1:.*]] = stablehlo.imag %[[ARG0]] : (tensor<?xcomplex<f64>>) -> tensor<?xf64>
 // CHECK:           %[[CONSTANT_14:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f64>
-// CHECK:           %[[SHAPE_OF_14:.*]] = shape.shape_of %[[REAL_0]] : tensor<?xf64> -> tensor<1xindex>
-// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_14:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_14]], %[[SHAPE_OF_14]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
-// CHECK:           %[[COMPARE_16:.*]] = stablehlo.compare  LT, %[[IMAG_1]], %[[DYNAMIC_BROADCAST_IN_DIM_14]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
+// CHECK:           %[[SHAPE_OF_111:.*]] = shape.shape_of %[[REAL_0]] : tensor<?xf64> -> tensor<1xindex>
+// CHECK:           %[[DYNAMIC_BROADCAST_IN_DIM_59:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CONSTANT_14]], %[[SHAPE_OF_111]], dims = [] : (tensor<f64>, tensor<1xindex>) -> tensor<?xf64>
+// CHECK:           %[[COMPARE_16:.*]] = stablehlo.compare  LT, %[[IMAG_1]], %[[DYNAMIC_BROADCAST_IN_DIM_59]] : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xi1>
 // CHECK:           %[[IMAG_2:.*]] = stablehlo.imag %[[COMPLEX_0]] : (tensor<?xcomplex<f64>>) -> tensor<?xf64>
 // CHECK:           %[[NEGATE_1:.*]] = stablehlo.negate %[[IMAG_2]] : tensor<?xf64>
-// CHECK:           %[[SELECT_15:.*]] = stablehlo.select %[[COMPARE_16]], %[[NEGATE_1]], %[[IMAG_2]] : tensor<?xi1>, tensor<?xf64>
-// CHECK:           %[[COMPLEX_1:.*]] = stablehlo.complex %[[VAL_0]], %[[SELECT_15]] : tensor<?xcomplex<f64>>
+// CHECK:           %[[ASSUMING_22:.*]] = stablehlo.select %[[COMPARE_16]], %[[NEGATE_1]], %[[IMAG_2]] : tensor<?xi1>, tensor<?xf64>
+// CHECK:           %[[COMPLEX_1:.*]] = stablehlo.complex %[[VAL_0]], %[[ASSUMING_22]] : tensor<?xcomplex<f64>>
 // CHECK:           return %[[COMPLEX_1]] : tensor<?xcomplex<f64>>
 // CHECK:         }
 func.func @asin_complex_f64_dynamic(%arg : tensor<?xcomplex<f64>>) -> tensor<?xcomplex<f64>> {
@@ -4064,26 +4064,27 @@
 // CHECK-LABEL:   func.func @dyn_top_k(
 // CHECK-SAME:      %[[ARG0:.*]]: tensor<?x5x?xi1>) -> (tensor<?x5x2xi1>, tensor<?x5x2xi32>) {
 // CHECK:           %[[GET_DIMENSION_SIZE_0:.*]] = stablehlo.get_dimension_size %[[ARG0]], dim = 0 : (tensor<?x5x?xi1>) -> tensor<i32>
-// CHECK:           %[[RESHAPE_0:.*]] = stablehlo.reshape %[[GET_DIMENSION_SIZE_0]] : (tensor<i32>) -> tensor<1xi32>
-// CHECK:           %[[GET_DIMENSION_SIZE_1:.*]] = stablehlo.get_dimension_size %[[ARG0]], dim = 1 : (tensor<?x5x?xi1>) -> tensor<i32>
-// CHECK:           %[[RESHAPE_1:.*]] = stablehlo.reshape %[[GET_DIMENSION_SIZE_1]] : (tensor<i32>) -> tensor<1xi32>
+// CHECK:           %[[CONVERT_0:.*]] = stablehlo.convert %[[GET_DIMENSION_SIZE_0]] : (tensor<i32>) -> tensor<i64>
+// CHECK:           %[[RESHAPE_0:.*]] = stablehlo.reshape %[[CONVERT_0]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK:           %[[C_1:.*]] = stablehlo.constant dense<5> : tensor<i64>
+// CHECK:           %[[RESHAPE_1:.*]] = stablehlo.reshape %[[C_1]] : (tensor<i64>) -> tensor<1xi64>
 // CHECK:           %[[GET_DIMENSION_SIZE_2:.*]] = stablehlo.get_dimension_size %[[ARG0]], dim = 2 : (tensor<?x5x?xi1>) -> tensor<i32>
-// CHECK:           %[[RESHAPE_2:.*]] = stablehlo.reshape %[[GET_DIMENSION_SIZE_2]] : (tensor<i32>) -> tensor<1xi32>
-// CHECK:           %[[CONCATENATE_0:.*]] = stablehlo.concatenate %[[RESHAPE_0]], %[[RESHAPE_1]], %[[RESHAPE_2]], dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
-// CHECK:           %[[CONSTANT_0:.*]] = stablehlo.constant dense<2> : tensor<i32>
-// CHECK:           %[[RESHAPE_3:.*]] = stablehlo.reshape %[[CONSTANT_0]] : (tensor<i32>) -> tensor<1xi32>
-// CHECK:           %[[CONCATENATE_1:.*]] = stablehlo.concatenate %[[RESHAPE_0]], %[[RESHAPE_1]], %[[RESHAPE_3]], dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
-// CHECK:           %[[DYNAMIC_IOTA_0:.*]] = stablehlo.dynamic_iota %[[CONCATENATE_0]], dim = 2 : (tensor<3xi32>) -> tensor<?x5x?xi32>
+// CHECK:           %[[CONVERT_2:.*]] = stablehlo.convert %[[GET_DIMENSION_SIZE_2]] : (tensor<i32>) -> tensor<i64>
+// CHECK:           %[[RESHAPE_2:.*]] = stablehlo.reshape %[[CONVERT_2]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK:           %[[CONCATENATE_0:.*]] = stablehlo.concatenate %[[RESHAPE_0]], %[[RESHAPE_1]], %[[RESHAPE_2]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           %[[CONSTANT_0:.*]] = stablehlo.constant dense<2> : tensor<i64>
+// CHECK:           %[[RESHAPE_3:.*]] = stablehlo.reshape %[[CONSTANT_0]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK:           %[[CONCATENATE_1:.*]] = stablehlo.concatenate %[[RESHAPE_0]], %[[RESHAPE_1]], %[[RESHAPE_3]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           %[[DYNAMIC_IOTA_0:.*]] = stablehlo.dynamic_iota %[[CONCATENATE_0]], dim = 2 : (tensor<3xi64>) -> tensor<?x5x?xi32>
 // CHECK:           %[[VAL_0:.*]]:2 = "stablehlo.sort"(%[[ARG0]], %[[DYNAMIC_IOTA_0]]) <{dimension = 2 : i64, is_stable = true}> ({
 // CHECK:           ^bb0(%[[VAL_1:.*]]: tensor<i1>, %[[VAL_2:.*]]: tensor<i1>, %[[VAL_3:.*]]: tensor<i32>, %[[VAL_4:.*]]: tensor<i32>):
 // CHECK:             %[[COMPARE_0:.*]] = stablehlo.compare  GT, %[[VAL_1]], %[[VAL_2]] : (tensor<i1>, tensor<i1>) -> tensor<i1>
 // CHECK:             stablehlo.return %[[COMPARE_0]] : tensor<i1>
 // CHECK:           }) : (tensor<?x5x?xi1>, tensor<?x5x?xi32>) -> (tensor<?x5x?xi1>, tensor<?x5x?xi32>)
 // CHECK:           %[[CONSTANT_1:.*]] = stablehlo.constant dense<0> : tensor<3xi64>
-// CHECK:           %[[CONVERT_0:.*]] = stablehlo.convert %[[CONCATENATE_1]] : (tensor<3xi32>) -> tensor<3xi64>
 // CHECK:           %[[CONSTANT_2:.*]] = stablehlo.constant dense<1> : tensor<3xi64>
-// CHECK:           %[[REAL_DYNAMIC_SLICE_0:.*]] = stablehlo.real_dynamic_slice %[[VAL_5:.*]]#0, %[[CONSTANT_1]], %[[CONVERT_0]], %[[CONSTANT_2]] : (tensor<?x5x?xi1>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<?x5x2xi1>
-// CHECK:           %[[REAL_DYNAMIC_SLICE_1:.*]] = stablehlo.real_dynamic_slice %[[VAL_5]]#1, %[[CONSTANT_1]], %[[CONVERT_0]], %[[CONSTANT_2]] : (tensor<?x5x?xi32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<?x5x2xi32>
+// CHECK:           %[[REAL_DYNAMIC_SLICE_0:.*]] = stablehlo.real_dynamic_slice %[[VAL_5:.*]]#0, %[[CONSTANT_1]], %[[CONCATENATE_1]], %[[CONSTANT_2]] : (tensor<?x5x?xi1>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<?x5x2xi1>
+// CHECK:           %[[REAL_DYNAMIC_SLICE_1:.*]] = stablehlo.real_dynamic_slice %[[VAL_5]]#1, %[[CONSTANT_1]], %[[CONCATENATE_1]], %[[CONSTANT_2]] : (tensor<?x5x?xi32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<?x5x2xi32>
 // CHECK:           return %[[REAL_DYNAMIC_SLICE_0]], %[[REAL_DYNAMIC_SLICE_1]] : tensor<?x5x2xi1>, tensor<?x5x2xi32>
 // CHECK:         }
 func.func @dyn_top_k(%arg0: tensor<?x5x?xi1>) -> (tensor<?x5x2xi1>, tensor<?x5x2xi32>) {
@@ -5061,53 +5062,51 @@
 // CHECK-SAME:      %[[ARG0:.*]]: tensor<2x11x5xf32>,
 // CHECK-SAME:      %[[ARG1:.*]]: tensor<3x2x5x7xf32>,
 // CHECK-SAME:      %[[ARG2:.*]]: tensor<3xi64>) -> tensor<2x11x7xf32> {
-// CHECK:           %[[IOTA_0:.*]] = stablehlo.iota dim = 1 : tensor<1x11x1xi64>
-// CHECK:           %[[CONSTANT_0:.*]] = stablehlo.constant dense<0> : tensor<1xi64>
-// CHECK:           %[[CONSTANT_1:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<2x11x7xf32>
-// CHECK:           %[[CONSTANT_2:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<2x11x7xf32>
+// CHECK-DAG:       %[[IOTA_0:.*]] = stablehlo.iota dim = 1 : tensor<1x11x1xi64>
+// CHECK-DAG:       %[[CONSTANT_0:.*]] = stablehlo.constant dense<0> : tensor<i64>
+// CHECK:           %[[CONSTANT_1:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<2x11x5xf32>
 // CHECK:           %[[SLICE_0:.*]] = stablehlo.slice %[[ARG2]] [0:1] : (tensor<3xi64>) -> tensor<1xi64>
-// CHECK:           %[[BROADCAST_IN_DIM_0:.*]] = stablehlo.broadcast_in_dim %[[CONSTANT_0]], dims = [0] : (tensor<1xi64>) -> tensor<1x11x1xi64>
-// CHECK:           %[[COMPARE_0:.*]] = stablehlo.compare  LE, %[[BROADCAST_IN_DIM_0]], %[[IOTA_0]] : (tensor<1x11x1xi64>, tensor<1x11x1xi64>) -> tensor<1x11x1xi1>
-// CHECK:           %[[ADD_0:.*]] = stablehlo.add %[[CONSTANT_0]], %[[SLICE_0]] : tensor<1xi64>
-// CHECK:           %[[BROADCAST_IN_DIM_1:.*]] = stablehlo.broadcast_in_dim %[[ADD_0]], dims = [0] : (tensor<1xi64>) -> tensor<1x11x1xi64>
-// CHECK:           %[[COMPARE_1:.*]] = stablehlo.compare  LT, %[[IOTA_0]], %[[BROADCAST_IN_DIM_1]] : (tensor<1x11x1xi64>, tensor<1x11x1xi64>) -> tensor<1x11x1xi1>
-// CHECK:           %[[AND_0:.*]] = stablehlo.and %[[COMPARE_0]], %[[COMPARE_1]] : tensor<1x11x1xi1>
-// CHECK:           %[[BROADCAST_IN_DIM_2:.*]] = stablehlo.broadcast_in_dim %[[AND_0]], dims = [0, 1, 2] : (tensor<1x11x1xi1>) -> tensor<2x11x7xi1>
+// CHECK:           %[[RESHAPE_0:.*]] = stablehlo.reshape %[[SLICE_0]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:           %[[ADD_0:.*]] = stablehlo.add %[[CONSTANT_0]], %[[RESHAPE_0]] : tensor<i64>
+// CHECK:           %[[BROADCAST_IN_DIM_0:.*]] = stablehlo.broadcast_in_dim %[[CONSTANT_0]], dims = [] : (tensor<i64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_1:.*]] = stablehlo.broadcast_in_dim %[[ADD_0]], dims = [] : (tensor<i64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_2:.*]] = stablehlo.broadcast_in_dim %[[IOTA_0]], dims = [0, 1, 2] : (tensor<1x11x1xi64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[COMPARE_0:.*]] = stablehlo.compare  GE, %[[BROADCAST_IN_DIM_2]], %[[BROADCAST_IN_DIM_0]] : (tensor<2x11x5xi64>, tensor<2x11x5xi64>) -> tensor<2x11x5xi1>
+// CHECK:           %[[COMPARE_1:.*]] = stablehlo.compare  LT, %[[BROADCAST_IN_DIM_2]], %[[BROADCAST_IN_DIM_1]] : (tensor<2x11x5xi64>, tensor<2x11x5xi64>) -> tensor<2x11x5xi1>
+// CHECK:           %[[AND_0:.*]] = stablehlo.and %[[COMPARE_0]], %[[COMPARE_1]] : tensor<2x11x5xi1>
+// CHECK:           %[[SELECT_0:.*]] = stablehlo.select %[[AND_0]], %[[ARG0]], %[[CONSTANT_1]] : tensor<2x11x5xi1>, tensor<2x11x5xf32>
 // CHECK:           %[[SLICE_1:.*]] = stablehlo.slice %[[ARG1]] [0:1, 0:2, 0:5, 0:7] : (tensor<3x2x5x7xf32>) -> tensor<1x2x5x7xf32>
-// CHECK:           %[[RESHAPE_0:.*]] = stablehlo.reshape %[[SLICE_1]] : (tensor<1x2x5x7xf32>) -> tensor<2x5x7xf32>
-// CHECK:           %[[DOT_GENERAL_0:.*]] = stablehlo.dot_general %[[ARG0]], %[[RESHAPE_0]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x11x5xf32>, tensor<2x5x7xf32>) -> tensor<2x11x7xf32>
-// CHECK:           %[[SELECT_0:.*]] = stablehlo.select %[[BROADCAST_IN_DIM_2]], %[[DOT_GENERAL_0]], %[[CONSTANT_2]] : tensor<2x11x7xi1>, tensor<2x11x7xf32>
-// CHECK:           %[[ADD_1:.*]] = stablehlo.add %[[CONSTANT_1]], %[[SELECT_0]] : tensor<2x11x7xf32>
-// CHECK:           %[[ADD_2:.*]] = stablehlo.add %[[CONSTANT_0]], %[[SLICE_0]] : tensor<1xi64>
+// CHECK:           %[[RESHAPE_1:.*]] = stablehlo.reshape %[[SLICE_1]] : (tensor<1x2x5x7xf32>) -> tensor<2x5x7xf32>
+// CHECK:           %[[DOT_GENERAL_0:.*]] = stablehlo.dot_general %[[SELECT_0]], %[[RESHAPE_1]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x11x5xf32>, tensor<2x5x7xf32>) -> tensor<2x11x7xf32>
 // CHECK:           %[[SLICE_2:.*]] = stablehlo.slice %[[ARG2]] [1:2] : (tensor<3xi64>) -> tensor<1xi64>
-// CHECK:           %[[BROADCAST_IN_DIM_3:.*]] = stablehlo.broadcast_in_dim %[[ADD_2]], dims = [0] : (tensor<1xi64>) -> tensor<1x11x1xi64>
-// CHECK:           %[[COMPARE_2:.*]] = stablehlo.compare  LE, %[[BROADCAST_IN_DIM_3]], %[[IOTA_0]] : (tensor<1x11x1xi64>, tensor<1x11x1xi64>) -> tensor<1x11x1xi1>
-// CHECK:           %[[ADD_3:.*]] = stablehlo.add %[[ADD_2]], %[[SLICE_2]] : tensor<1xi64>
-// CHECK:           %[[BROADCAST_IN_DIM_4:.*]] = stablehlo.broadcast_in_dim %[[ADD_3]], dims = [0] : (tensor<1xi64>) -> tensor<1x11x1xi64>
-// CHECK:           %[[COMPARE_3:.*]] = stablehlo.compare  LT, %[[IOTA_0]], %[[BROADCAST_IN_DIM_4]] : (tensor<1x11x1xi64>, tensor<1x11x1xi64>) -> tensor<1x11x1xi1>
-// CHECK:           %[[AND_1:.*]] = stablehlo.and %[[COMPARE_2]], %[[COMPARE_3]] : tensor<1x11x1xi1>
-// CHECK:           %[[BROADCAST_IN_DIM_5:.*]] = stablehlo.broadcast_in_dim %[[AND_1]], dims = [0, 1, 2] : (tensor<1x11x1xi1>) -> tensor<2x11x7xi1>
+// CHECK:           %[[RESHAPE_2:.*]] = stablehlo.reshape %[[SLICE_2]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:           %[[ADD_1:.*]] = stablehlo.add %[[ADD_0]], %[[RESHAPE_2]] : tensor<i64>
+// CHECK:           %[[BROADCAST_IN_DIM_3:.*]] = stablehlo.broadcast_in_dim %[[ADD_0]], dims = [] : (tensor<i64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_4:.*]] = stablehlo.broadcast_in_dim %[[ADD_1]], dims = [] : (tensor<i64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_5:.*]] = stablehlo.broadcast_in_dim %[[IOTA_0]], dims = [0, 1, 2] : (tensor<1x11x1xi64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[COMPARE_2:.*]] = stablehlo.compare  GE, %[[BROADCAST_IN_DIM_5]], %[[BROADCAST_IN_DIM_3]] : (tensor<2x11x5xi64>, tensor<2x11x5xi64>) -> tensor<2x11x5xi1>
+// CHECK:           %[[COMPARE_3:.*]] = stablehlo.compare  LT, %[[BROADCAST_IN_DIM_5]], %[[BROADCAST_IN_DIM_4]] : (tensor<2x11x5xi64>, tensor<2x11x5xi64>) -> tensor<2x11x5xi1>
+// CHECK:           %[[AND_1:.*]] = stablehlo.and %[[COMPARE_2]], %[[COMPARE_3]] : tensor<2x11x5xi1>
+// CHECK:           %[[SELECT_1:.*]] = stablehlo.select %[[AND_1]], %[[ARG0]], %[[CONSTANT_1]] : tensor<2x11x5xi1>, tensor<2x11x5xf32>
 // CHECK:           %[[SLICE_3:.*]] = stablehlo.slice %[[ARG1]] [1:2, 0:2, 0:5, 0:7] : (tensor<3x2x5x7xf32>) -> tensor<1x2x5x7xf32>
-// CHECK:           %[[RESHAPE_1:.*]] = stablehlo.reshape %[[SLICE_3]] : (tensor<1x2x5x7xf32>) -> tensor<2x5x7xf32>
-// CHECK:           %[[DOT_GENERAL_1:.*]] = stablehlo.dot_general %[[ARG0]], %[[RESHAPE_1]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x11x5xf32>, tensor<2x5x7xf32>) -> tensor<2x11x7xf32>
-// CHECK:           %[[SELECT_1:.*]] = stablehlo.select %[[BROADCAST_IN_DIM_5]], %[[DOT_GENERAL_1]], %[[CONSTANT_2]] : tensor<2x11x7xi1>, tensor<2x11x7xf32>
-// CHECK:           %[[ADD_4:.*]] = stablehlo.add %[[ADD_1]], %[[SELECT_1]] : tensor<2x11x7xf32>
-// CHECK:           %[[ADD_5:.*]] = stablehlo.add %[[ADD_2]], %[[SLICE_2]] : tensor<1xi64>
+// CHECK:           %[[RESHAPE_3:.*]] = stablehlo.reshape %[[SLICE_3]] : (tensor<1x2x5x7xf32>) -> tensor<2x5x7xf32>
+// CHECK:           %[[DOT_GENERAL_1:.*]] = stablehlo.dot_general %[[SELECT_1]], %[[RESHAPE_3]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x11x5xf32>, tensor<2x5x7xf32>) -> tensor<2x11x7xf32>
+// CHECK:           %[[ADD_2:.*]] = stablehlo.add %[[DOT_GENERAL_0]], %[[DOT_GENERAL_1]] : tensor<2x11x7xf32>
 // CHECK:           %[[SLICE_4:.*]] = stablehlo.slice %[[ARG2]] [2:3] : (tensor<3xi64>) -> tensor<1xi64>
-// CHECK:           %[[BROADCAST_IN_DIM_6:.*]] = stablehlo.broadcast_in_dim %[[ADD_5]], dims = [0] : (tensor<1xi64>) -> tensor<1x11x1xi64>
-// CHECK:           %[[COMPARE_4:.*]] = stablehlo.compare  LE, %[[BROADCAST_IN_DIM_6]], %[[IOTA_0]] : (tensor<1x11x1xi64>, tensor<1x11x1xi64>) -> tensor<1x11x1xi1>
-// CHECK:           %[[ADD_6:.*]] = stablehlo.add %[[ADD_5]], %[[SLICE_4]] : tensor<1xi64>
-// CHECK:           %[[BROADCAST_IN_DIM_7:.*]] = stablehlo.broadcast_in_dim %[[ADD_6]], dims = [0] : (tensor<1xi64>) -> tensor<1x11x1xi64>
-// CHECK:           %[[COMPARE_5:.*]] = stablehlo.compare  LT, %[[IOTA_0]], %[[BROADCAST_IN_DIM_7]] : (tensor<1x11x1xi64>, tensor<1x11x1xi64>) -> tensor<1x11x1xi1>
-// CHECK:           %[[AND_2:.*]] = stablehlo.and %[[COMPARE_4]], %[[COMPARE_5]] : tensor<1x11x1xi1>
-// CHECK:           %[[BROADCAST_IN_DIM_8:.*]] = stablehlo.broadcast_in_dim %[[AND_2]], dims = [0, 1, 2] : (tensor<1x11x1xi1>) -> tensor<2x11x7xi1>
+// CHECK:           %[[RESHAPE_4:.*]] = stablehlo.reshape %[[SLICE_4]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:           %[[ADD_3:.*]] = stablehlo.add %[[ADD_1]], %[[RESHAPE_4]] : tensor<i64>
+// CHECK:           %[[BROADCAST_IN_DIM_6:.*]] = stablehlo.broadcast_in_dim %[[ADD_1]], dims = [] : (tensor<i64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_7:.*]] = stablehlo.broadcast_in_dim %[[ADD_3]], dims = [] : (tensor<i64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_8:.*]] = stablehlo.broadcast_in_dim %[[IOTA_0]], dims = [0, 1, 2] : (tensor<1x11x1xi64>) -> tensor<2x11x5xi64>
+// CHECK:           %[[COMPARE_4:.*]] = stablehlo.compare  GE, %[[BROADCAST_IN_DIM_8]], %[[BROADCAST_IN_DIM_6]] : (tensor<2x11x5xi64>, tensor<2x11x5xi64>) -> tensor<2x11x5xi1>
+// CHECK:           %[[COMPARE_5:.*]] = stablehlo.compare  LT, %[[BROADCAST_IN_DIM_8]], %[[BROADCAST_IN_DIM_7]] : (tensor<2x11x5xi64>, tensor<2x11x5xi64>) -> tensor<2x11x5xi1>
+// CHECK:           %[[AND_2:.*]] = stablehlo.and %[[COMPARE_4]], %[[COMPARE_5]] : tensor<2x11x5xi1>
+// CHECK:           %[[SELECT_2:.*]] = stablehlo.select %[[AND_2]], %[[ARG0]], %[[CONSTANT_1]] : tensor<2x11x5xi1>, tensor<2x11x5xf32>
 // CHECK:           %[[SLICE_5:.*]] = stablehlo.slice %[[ARG1]] [2:3, 0:2, 0:5, 0:7] : (tensor<3x2x5x7xf32>) -> tensor<1x2x5x7xf32>
-// CHECK:           %[[RESHAPE_2:.*]] = stablehlo.reshape %[[SLICE_5]] : (tensor<1x2x5x7xf32>) -> tensor<2x5x7xf32>
-// CHECK:           %[[DOT_GENERAL_2:.*]] = stablehlo.dot_general %[[ARG0]], %[[RESHAPE_2]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x11x5xf32>, tensor<2x5x7xf32>) -> tensor<2x11x7xf32>
-// CHECK:           %[[SELECT_2:.*]] = stablehlo.select %[[BROADCAST_IN_DIM_8]], %[[DOT_GENERAL_2]], %[[CONSTANT_2]] : tensor<2x11x7xi1>, tensor<2x11x7xf32>
-// CHECK:           %[[ADD_7:.*]] = stablehlo.add %[[ADD_4]], %[[SELECT_2]] : tensor<2x11x7xf32>
-// CHECK:           %[[ADD_8:.*]] = stablehlo.add %[[ADD_5]], %[[SLICE_4]] : tensor<1xi64>
-// CHECK:           return %[[ADD_7]] : tensor<2x11x7xf32>
+// CHECK:           %[[RESHAPE_5:.*]] = stablehlo.reshape %[[SLICE_5]] : (tensor<1x2x5x7xf32>) -> tensor<2x5x7xf32>
+// CHECK:           %[[DOT_GENERAL_2:.*]] = stablehlo.dot_general %[[SELECT_2]], %[[RESHAPE_5]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x11x5xf32>, tensor<2x5x7xf32>) -> tensor<2x11x7xf32>
+// CHECK:           %[[ADD_4:.*]] = stablehlo.add %[[ADD_2]], %[[DOT_GENERAL_2]] : tensor<2x11x7xf32>
+// CHECK:           return %[[ADD_4]] : tensor<2x11x7xf32>
 // CHECK:         }
 func.func @ragged_dot_mode_1(%lhs : tensor<2x11x5xf32>, %rhs : tensor<3x2x5x7xf32>, %group_sizes : tensor<3xi64>) -> tensor<2x11x7xf32> {
   %0 = "chlo.ragged_dot"(%lhs, %rhs, %group_sizes) {
@@ -5123,3 +5122,144 @@
   } : (tensor<2x11x5xf32>, tensor<3x2x5x7xf32>, tensor<3xi64>) -> tensor<2x11x7xf32>
   func.return %0 : tensor<2x11x7xf32>
 }
+
+// -----
+
+// CHECK-LABEL:   func.func @ragged_dot_mode_2(
+// CHECK-SAME:      %[[ARG0:.*]]: tensor<2x3x5xf32>,
+// CHECK-SAME:      %[[ARG1:.*]]: tensor<2x5x7xf32>,
+// CHECK-SAME:      %[[ARG2:.*]]: tensor<2x4xi64>) -> tensor<4x2x3x7xf32> {
+// CHECK:           %[[CONSTANT_0:.*]] = stablehlo.constant dense<0> : tensor<2xi64>
+// CHECK:           %[[IOTA_0:.*]] = stablehlo.iota dim = 2 : tensor<1x1x5xi64>
+// CHECK:           %[[CONSTANT_1:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<2x3x5xf32>
+// CHECK:           %[[SLICE_0:.*]] = stablehlo.slice %[[ARG2]] [0:2, 0:1] : (tensor<2x4xi64>) -> tensor<2x1xi64>
+// CHECK:           %[[RESHAPE_0:.*]] = stablehlo.reshape %[[SLICE_0]] : (tensor<2x1xi64>) -> tensor<2xi64>
+// CHECK:           %[[ADD_0:.*]] = stablehlo.add %[[CONSTANT_0]], %[[RESHAPE_0]] : tensor<2xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_0:.*]] = stablehlo.broadcast_in_dim %[[CONSTANT_0]], dims = [0] : (tensor<2xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_1:.*]] = stablehlo.broadcast_in_dim %[[ADD_0]], dims = [0] : (tensor<2xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_2:.*]] = stablehlo.broadcast_in_dim %[[IOTA_0]], dims = [0, 1, 2] : (tensor<1x1x5xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[COMPARE_0:.*]] = stablehlo.compare  GE, %[[BROADCAST_IN_DIM_2]], %[[BROADCAST_IN_DIM_0]] : (tensor<2x3x5xi64>, tensor<2x3x5xi64>) -> tensor<2x3x5xi1>
+// CHECK:           %[[COMPARE_1:.*]] = stablehlo.compare  LT, %[[BROADCAST_IN_DIM_2]], %[[BROADCAST_IN_DIM_1]] : (tensor<2x3x5xi64>, tensor<2x3x5xi64>) -> tensor<2x3x5xi1>
+// CHECK:           %[[AND_0:.*]] = stablehlo.and %[[COMPARE_0]], %[[COMPARE_1]] : tensor<2x3x5xi1>
+// CHECK:           %[[SELECT_0:.*]] = stablehlo.select %[[AND_0]], %[[ARG0]], %[[CONSTANT_1]] : tensor<2x3x5xi1>, tensor<2x3x5xf32>
+// CHECK:           %[[DOT_GENERAL_0:.*]] = stablehlo.dot_general %[[SELECT_0]], %[[ARG1]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x3x5xf32>, tensor<2x5x7xf32>) -> tensor<2x3x7xf32>
+// CHECK:           %[[SLICE_1:.*]] = stablehlo.slice %[[ARG2]] [0:2, 1:2] : (tensor<2x4xi64>) -> tensor<2x1xi64>
+// CHECK:           %[[RESHAPE_1:.*]] = stablehlo.reshape %[[SLICE_1]] : (tensor<2x1xi64>) -> tensor<2xi64>
+// CHECK:           %[[ADD_1:.*]] = stablehlo.add %[[ADD_0]], %[[RESHAPE_1]] : tensor<2xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_3:.*]] = stablehlo.broadcast_in_dim %[[ADD_0]], dims = [0] : (tensor<2xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_4:.*]] = stablehlo.broadcast_in_dim %[[ADD_1]], dims = [0] : (tensor<2xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_5:.*]] = stablehlo.broadcast_in_dim %[[IOTA_0]], dims = [0, 1, 2] : (tensor<1x1x5xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[COMPARE_2:.*]] = stablehlo.compare  GE, %[[BROADCAST_IN_DIM_5]], %[[BROADCAST_IN_DIM_3]] : (tensor<2x3x5xi64>, tensor<2x3x5xi64>) -> tensor<2x3x5xi1>
+// CHECK:           %[[COMPARE_3:.*]] = stablehlo.compare  LT, %[[BROADCAST_IN_DIM_5]], %[[BROADCAST_IN_DIM_4]] : (tensor<2x3x5xi64>, tensor<2x3x5xi64>) -> tensor<2x3x5xi1>
+// CHECK:           %[[AND_1:.*]] = stablehlo.and %[[COMPARE_2]], %[[COMPARE_3]] : tensor<2x3x5xi1>
+// CHECK:           %[[SELECT_1:.*]] = stablehlo.select %[[AND_1]], %[[ARG0]], %[[CONSTANT_1]] : tensor<2x3x5xi1>, tensor<2x3x5xf32>
+// CHECK:           %[[DOT_GENERAL_1:.*]] = stablehlo.dot_general %[[SELECT_1]], %[[ARG1]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x3x5xf32>, tensor<2x5x7xf32>) -> tensor<2x3x7xf32>
+// CHECK:           %[[SLICE_2:.*]] = stablehlo.slice %[[ARG2]] [0:2, 2:3] : (tensor<2x4xi64>) -> tensor<2x1xi64>
+// CHECK:           %[[RESHAPE_2:.*]] = stablehlo.reshape %[[SLICE_2]] : (tensor<2x1xi64>) -> tensor<2xi64>
+// CHECK:           %[[ADD_2:.*]] = stablehlo.add %[[ADD_1]], %[[RESHAPE_2]] : tensor<2xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_6:.*]] = stablehlo.broadcast_in_dim %[[ADD_1]], dims = [0] : (tensor<2xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_7:.*]] = stablehlo.broadcast_in_dim %[[ADD_2]], dims = [0] : (tensor<2xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_8:.*]] = stablehlo.broadcast_in_dim %[[IOTA_0]], dims = [0, 1, 2] : (tensor<1x1x5xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[COMPARE_4:.*]] = stablehlo.compare  GE, %[[BROADCAST_IN_DIM_8]], %[[BROADCAST_IN_DIM_6]] : (tensor<2x3x5xi64>, tensor<2x3x5xi64>) -> tensor<2x3x5xi1>
+// CHECK:           %[[COMPARE_5:.*]] = stablehlo.compare  LT, %[[BROADCAST_IN_DIM_8]], %[[BROADCAST_IN_DIM_7]] : (tensor<2x3x5xi64>, tensor<2x3x5xi64>) -> tensor<2x3x5xi1>
+// CHECK:           %[[AND_2:.*]] = stablehlo.and %[[COMPARE_4]], %[[COMPARE_5]] : tensor<2x3x5xi1>
+// CHECK:           %[[SELECT_2:.*]] = stablehlo.select %[[AND_2]], %[[ARG0]], %[[CONSTANT_1]] : tensor<2x3x5xi1>, tensor<2x3x5xf32>
+// CHECK:           %[[DOT_GENERAL_2:.*]] = stablehlo.dot_general %[[SELECT_2]], %[[ARG1]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x3x5xf32>, tensor<2x5x7xf32>) -> tensor<2x3x7xf32>
+// CHECK:           %[[SLICE_3:.*]] = stablehlo.slice %[[ARG2]] [0:2, 3:4] : (tensor<2x4xi64>) -> tensor<2x1xi64>
+// CHECK:           %[[RESHAPE_3:.*]] = stablehlo.reshape %[[SLICE_3]] : (tensor<2x1xi64>) -> tensor<2xi64>
+// CHECK:           %[[ADD_3:.*]] = stablehlo.add %[[ADD_2]], %[[RESHAPE_3]] : tensor<2xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_9:.*]] = stablehlo.broadcast_in_dim %[[ADD_2]], dims = [0] : (tensor<2xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_10:.*]] = stablehlo.broadcast_in_dim %[[ADD_3]], dims = [0] : (tensor<2xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[BROADCAST_IN_DIM_11:.*]] = stablehlo.broadcast_in_dim %[[IOTA_0]], dims = [0, 1, 2] : (tensor<1x1x5xi64>) -> tensor<2x3x5xi64>
+// CHECK:           %[[COMPARE_6:.*]] = stablehlo.compare  GE, %[[BROADCAST_IN_DIM_11]], %[[BROADCAST_IN_DIM_9]] : (tensor<2x3x5xi64>, tensor<2x3x5xi64>) -> tensor<2x3x5xi1>
+// CHECK:           %[[COMPARE_7:.*]] = stablehlo.compare  LT, %[[BROADCAST_IN_DIM_11]], %[[BROADCAST_IN_DIM_10]] : (tensor<2x3x5xi64>, tensor<2x3x5xi64>) -> tensor<2x3x5xi1>
+// CHECK:           %[[AND_3:.*]] = stablehlo.and %[[COMPARE_6]], %[[COMPARE_7]] : tensor<2x3x5xi1>
+// CHECK:           %[[SELECT_3:.*]] = stablehlo.select %[[AND_3]], %[[ARG0]], %[[CONSTANT_1]] : tensor<2x3x5xi1>, tensor<2x3x5xf32>
+// CHECK:           %[[DOT_GENERAL_3:.*]] = stablehlo.dot_general %[[SELECT_3]], %[[ARG1]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x3x5xf32>, tensor<2x5x7xf32>) -> tensor<2x3x7xf32>
+// CHECK:           %[[RESHAPE_4:.*]] = stablehlo.reshape %[[DOT_GENERAL_0]] : (tensor<2x3x7xf32>) -> tensor<1x2x3x7xf32>
+// CHECK:           %[[RESHAPE_5:.*]] = stablehlo.reshape %[[DOT_GENERAL_1]] : (tensor<2x3x7xf32>) -> tensor<1x2x3x7xf32>
+// CHECK:           %[[RESHAPE_6:.*]] = stablehlo.reshape %[[DOT_GENERAL_2]] : (tensor<2x3x7xf32>) -> tensor<1x2x3x7xf32>
+// CHECK:           %[[RESHAPE_7:.*]] = stablehlo.reshape %[[DOT_GENERAL_3]] : (tensor<2x3x7xf32>) -> tensor<1x2x3x7xf32>
+// CHECK:           %[[CONCATENATE_0:.*]] = stablehlo.concatenate %[[RESHAPE_4]], %[[RESHAPE_5]], %[[RESHAPE_6]], %[[RESHAPE_7]], dim = 0 : (tensor<1x2x3x7xf32>, tensor<1x2x3x7xf32>, tensor<1x2x3x7xf32>, tensor<1x2x3x7xf32>) -> tensor<4x2x3x7xf32>
+// CHECK:           return %[[CONCATENATE_0]] : tensor<4x2x3x7xf32>
+// CHECK:         }
+func.func @ragged_dot_mode_2(%lhs : tensor<2x3x5xf32>, %rhs : tensor<2x5x7xf32>, %group_sizes : tensor<2x4xi64>) -> tensor<4x2x3x7xf32> {
+  %0 = "chlo.ragged_dot"(%lhs, %rhs, %group_sizes) {
+    ragged_dot_dimension_numbers = #chlo.ragged_dot<
+      lhs_batching_dimensions = [0],
+      rhs_batching_dimensions = [0],
+      lhs_contracting_dimensions = [2],
+      rhs_contracting_dimensions = [1],
+      lhs_ragged_dimensions = [2],
+      rhs_group_dimensions = []
+    >,
+    precision_config = [#chlo<precision DEFAULT>, #chlo<precision DEFAULT>]
+  } : (tensor<2x3x5xf32>, tensor<2x5x7xf32>, tensor<2x4xi64>) -> tensor<4x2x3x7xf32>
+  func.return %0 : tensor<4x2x3x7xf32>
+}
+
+// CHECK-LABEL:   func.func @ragged_dot_mode_3(
+// CHECK-SAME:      %[[ARG0:.*]]: tensor<2x3x5xf32>,
+// CHECK-SAME:      %[[ARG1:.*]]: tensor<2x5x7xf32>,
+// CHECK-SAME:      %[[ARG2:.*]]: tensor<2xi64>) -> tensor<2x3x7xf32> {
+// CHECK:           %[[DOT_GENERAL_0:.*]] = stablehlo.dot_general %[[ARG0]], %[[ARG1]], batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x3x5xf32>, tensor<2x5x7xf32>) -> tensor<2x3x7xf32>
+// CHECK:           return %[[DOT_GENERAL_0]] : tensor<2x3x7xf32>
+// CHECK:         }
+func.func @ragged_dot_mode_3(%lhs : tensor<2x3x5xf32>, %rhs : tensor<2x5x7xf32>, %group_sizes : tensor<2xi64>) -> tensor<2x3x7xf32> {
+  %0 = "chlo.ragged_dot"(%lhs, %rhs, %group_sizes) {
+    ragged_dot_dimension_numbers = #chlo.ragged_dot<
+      lhs_batching_dimensions = [0],
+      rhs_batching_dimensions = [0],
+      lhs_contracting_dimensions = [2],
+      rhs_contracting_dimensions = [1],
+      lhs_ragged_dimensions = [0],
+      rhs_group_dimensions = []
+    >,
+    precision_config = [#chlo<precision DEFAULT>, #chlo<precision DEFAULT>]
+  } : (tensor<2x3x5xf32>, tensor<2x5x7xf32>, tensor<2xi64>) -> tensor<2x3x7xf32>
+  func.return %0 : tensor<2x3x7xf32>
+}
+
+// -----
+
+// CHECK-LABEL:   func.func @scan(
+// CHECK-SAME:      %[[ARG0:.*]]: tensor<2xi32>,
+// CHECK-SAME:      %[[ARG1:.*]]: tensor<i32>) -> (tensor<2xi32>, tensor<i32>) {
+// CHECK-DAG:       %[[GET_DIMENSION_SIZE:.*]] = stablehlo.get_dimension_size %[[ARG0]], dim = 0 : (tensor<2xi32>) -> tensor<i32>
+// CHECK-DAG:       %[[CONVERT:.*]] = stablehlo.convert %[[GET_DIMENSION_SIZE]] : (tensor<i32>) -> tensor<i64>
+// CHECK-DAG:       %[[C0_I64:.*]] = stablehlo.constant dense<0> : tensor<i64>
+// CHECK-DAG:       %[[C0_I32:.*]] = stablehlo.constant dense<0> : tensor<i32>
+// CHECK-DAG:       %[[BROADCAST:.*]] = stablehlo.broadcast %[[C0_I32]], sizes = [2] : (tensor<i32>) -> tensor<2xi32>
+// CHECK:           %[[WHILE:.*]]:3 = stablehlo.while(%[[ITER:.*]] = %[[C0_I64]], %[[ACC:.*]] = %[[ARG1]], %[[OUT:.*]] = %[[BROADCAST]]) : tensor<i64>, tensor<i32>, tensor<2xi32>
+// CHECK:             cond {
+// CHECK:               %[[CMP:.*]] = stablehlo.compare  LT, %[[ITER]], %[[CONVERT]] : (tensor<i64>, tensor<i64>) -> tensor<i1>
+// CHECK:               stablehlo.return %[[CMP]] : tensor<i1>
+// CHECK:             } do {
+// CHECK-DAG:           %[[C0_I64_2:.*]] = stablehlo.constant dense<0> : tensor<i64>
+// CHECK-DAG:           %[[RESHAPE_ITER:.*]] = stablehlo.reshape %[[ITER]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK-DAG:           %[[CONCAT_START:.*]] = stablehlo.concatenate %[[RESHAPE_ITER]], dim = 0 : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:           %[[C1_I64:.*]] = stablehlo.constant dense<1> : tensor<i64>
+// CHECK-DAG:           %[[ITER_PLUS_1:.*]] = stablehlo.add %[[ITER]], %[[C1_I64]] : tensor<i64>
+// CHECK-DAG:           %[[RESHAPE_LIMIT:.*]] = stablehlo.reshape %[[ITER_PLUS_1]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK-DAG:           %[[CONCAT_LIMIT:.*]] = stablehlo.concatenate %[[RESHAPE_LIMIT]], dim = 0 : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:           %[[STRIDES:.*]] = stablehlo.constant dense<1> : tensor<1xi64>
+// CHECK-DAG:           %[[SLICE:.*]] = stablehlo.real_dynamic_slice %[[ARG0]], %[[CONCAT_START]], %[[CONCAT_LIMIT]], %[[STRIDES]] : (tensor<2xi32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi32>
+// CHECK-DAG:           %[[INPUT_ELEM:.*]] = stablehlo.reshape %[[SLICE]] : (tensor<1xi32>) -> tensor<i32>
+// CHECK-DAG:           %[[ADD_RES:.*]] = stablehlo.add %[[INPUT_ELEM]], %[[ACC]] : tensor<i32>
+// CHECK-DAG:           %[[RESHAPE_RES:.*]] = stablehlo.reshape %[[ADD_RES]] : (tensor<i32>) -> tensor<1xi32>
+// CHECK-DAG:           %[[C0_I64_3:.*]] = stablehlo.constant dense<0> : tensor<i64>
+// CHECK-DAG:           %[[UPDATE:.*]] = stablehlo.dynamic_update_slice %[[OUT]], %[[RESHAPE_RES]], %[[ITER]] : (tensor<2xi32>, tensor<1xi32>, tensor<i64>) -> tensor<2xi32>
+// CHECK-DAG:           %[[C1_I64_2:.*]] = stablehlo.constant dense<1> : tensor<i64>
+// CHECK-DAG:           %[[NEXT_ITER:.*]] = stablehlo.add %[[ITER]], %[[C1_I64_2]] : tensor<i64>
+// CHECK:               stablehlo.return %[[NEXT_ITER]], %[[ADD_RES]], %[[UPDATE]] : tensor<i64>, tensor<i32>, tensor<2xi32>
+// CHECK:             }
+// CHECK:           return %[[WHILE]]#2, %[[WHILE]]#1 : tensor<2xi32>, tensor<i32>
+// CHECK:         }
+func.func @scan(%arg0: tensor<2xi32>, %arg1: tensor<i32>) -> (tensor<2xi32>, tensor<i32>) {
+  %0:2 = chlo.scan(%arg0) inits(%arg1) dimension=0 {
+  ^bb0(%scan_arg0: tensor<i32>, %scan_arg1: tensor<i32>):
+    %1 = stablehlo.add %scan_arg0, %scan_arg1 : tensor<i32>
+    stablehlo.return %1, %1 : tensor<i32>, tensor<i32>
+  } : (tensor<2xi32>, tensor<i32>) -> (tensor<2xi32>, tensor<i32>)
+  func.return %0#0, %0#1 : tensor<2xi32>, tensor<i32>
+}
diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_dynamic.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_dynamic.mlir
--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_dynamic.mlir
+++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_dynamic.mlir
@@ -0,0 +1,82 @@
+// RUN: stablehlo-opt --chlo-legalize-to-stablehlo --split-input-file %s | FileCheck %s
+
+// CHECK-LABEL: func.func @constant_like_dynamic
+// CHECK-SAME:      %[[ARG0:.*]]: tensor<?x?xf32>) -> tensor<?x?xf32> {
+// CHECK-DAG:       %[[CST:.*]] = stablehlo.constant dense<1.000000e+00> : tensor<f32>
+// CHECK-DAG:       %[[SHAPE:.*]] = shape.shape_of %[[ARG0]] : tensor<?x?xf32> -> tensor<2xindex>
+// CHECK:           %[[RES:.*]] = stablehlo.dynamic_broadcast_in_dim %[[CST]], %[[SHAPE]], dims = [] : (tensor<f32>, tensor<2xindex>) -> tensor<?x?xf32>
+// CHECK:           return %[[RES]] : tensor<?x?xf32>
+func.func @constant_like_dynamic(%arg0: tensor<?x?xf32>) -> tensor<?x?xf32> {
+  %0 = "chlo.constant_like"(%arg0) { value = 1.0 : f32 } : (tensor<?x?xf32>) -> tensor<?x?xf32>
+  func.return %0 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func.func @top_k_dynamic
+// CHECK-SAME:      %[[ARG0:.*]]: tensor<?x?xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+// CHECK-DAG:       %[[GET_DIM0:.*]] = stablehlo.get_dimension_size %[[ARG0]], dim = 0 : (tensor<?x?xf32>) -> tensor<i32>
+// CHECK-DAG:       %[[CONVERT0:.*]] = stablehlo.convert %[[GET_DIM0]] : (tensor<i32>) -> tensor<i64>
+// CHECK-DAG:       %[[RESHAPE0:.*]] = stablehlo.reshape %[[CONVERT0]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK-DAG:       %[[GET_DIM1:.*]] = stablehlo.get_dimension_size %[[ARG0]], dim = 1 : (tensor<?x?xf32>) -> tensor<i32>
+// CHECK-DAG:       %[[CONVERT1:.*]] = stablehlo.convert %[[GET_DIM1]] : (tensor<i32>) -> tensor<i64>
+// CHECK-DAG:       %[[RESHAPE1:.*]] = stablehlo.reshape %[[CONVERT1]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK:           %[[CONCAT_SHAPE:.*]] = stablehlo.concatenate %[[RESHAPE0]], %[[RESHAPE1]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       %[[C5:.*]] = stablehlo.constant dense<5> : tensor<i64>
+// CHECK-DAG:       %[[RESHAPE_C5:.*]] = stablehlo.reshape %[[C5]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK-DAG:       %[[LIMIT:.*]] = stablehlo.concatenate %[[RESHAPE0]], %[[RESHAPE_C5]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK:           %[[IOTA:.*]] = stablehlo.dynamic_iota %[[CONCAT_SHAPE]], dim = 1 : (tensor<2xi64>) -> tensor<?x?xi32>
+// CHECK:           %[[SORT:.*]]:2 = "stablehlo.sort"(%[[ARG0]], %[[IOTA]])
+// CHECK:           %[[C0:.*]] = stablehlo.constant dense<0> : tensor<2xi64>
+// CHECK:           %[[C1:.*]] = stablehlo.constant dense<1> : tensor<2xi64>
+// CHECK:           %[[SLICE_VAL:.*]] = stablehlo.real_dynamic_slice %[[SORT]]#0, %[[C0]], %[[LIMIT]], %[[C1]] : (tensor<?x?xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<?x?xf32>
+// CHECK:           %[[SLICE_IDX:.*]] = stablehlo.real_dynamic_slice %[[SORT]]#1, %[[C0]], %[[LIMIT]], %[[C1]] : (tensor<?x?xi32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<?x?xi32>
+// CHECK:           return %[[SLICE_VAL]], %[[SLICE_IDX]] : tensor<?x?xf32>, tensor<?x?xi32>
+func.func @top_k_dynamic(%arg0: tensor<?x?xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  %0:2 = chlo.top_k(%arg0, k=5) : tensor<?x?xf32> -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  func.return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func.func @ragged_dot_dynamic
+// CHECK-SAME:      %[[LHS:.*]]: tensor<?x?x?xf32>,
+// CHECK-SAME:      %[[RHS:.*]]: tensor<3x2x5x7xf32>,
+// CHECK-SAME:      %[[GROUP_SIZES:.*]]: tensor<3xi64>) -> tensor<?x?x?xf32> {
+// CHECK:           %[[C0_I64:.*]] = stablehlo.constant dense<0> : tensor<i64>
+// CHECK-DAG:       %[[C1_I64:.*]] = stablehlo.constant dense<1> : tensor<1xi64>
+// CHECK-DAG:       %[[DIM1_LHS:.*]] = stablehlo.get_dimension_size %[[LHS]], dim = 1 : (tensor<?x?x?xf32>) -> tensor<i32>
+// CHECK-DAG:       %[[CONVERT_DIM1:.*]] = stablehlo.convert %[[DIM1_LHS]] : (tensor<i32>) -> tensor<i64>
+// CHECK-DAG:       %[[RESHAPE_DIM1:.*]] = stablehlo.reshape %[[CONVERT_DIM1]] : (tensor<i64>) -> tensor<1xi64>
+// CHECK:           %[[SHAPE_I64:.*]] = stablehlo.concatenate %{{.*}}, %[[RESHAPE_DIM1]], %{{.*}}, dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           %[[IOTA:.*]] = stablehlo.dynamic_iota %[[SHAPE_I64]], dim = 1 : (tensor<3xi64>) -> tensor<1x?x1xi64>
+// CHECK:           %[[ZERO:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
+// CHECK:           %[[SHAPE_LHS:.*]] = shape.shape_of %[[LHS]] : tensor<?x?x?xf32> -> tensor<3xindex>
+// CHECK:           %[[BROADCAST_ZERO:.*]] = stablehlo.dynamic_broadcast_in_dim %[[ZERO]], %{{.*}}, dims = [] : (tensor<f32>, tensor<3xindex>) -> tensor<?x?x?xf32>
+// CHECK:           %[[SLICE_G0:.*]] = stablehlo.slice %[[GROUP_SIZES]] [0:1] : (tensor<3xi64>) -> tensor<1xi64>
+// CHECK:           %[[RESHAPE_G0:.*]] = stablehlo.reshape %[[SLICE_G0]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:           %[[ADD_LIMIT:.*]] = stablehlo.add %[[C0_I64]], %[[RESHAPE_G0]] : tensor<i64>
+// CHECK:           %[[BROADCAST_START:.*]] = stablehlo.dynamic_broadcast_in_dim %[[C0_I64]], %{{.*}}, dims = [] : (tensor<i64>, tensor<3xindex>) -> tensor<?x?x?xi64>
+// CHECK:           %[[BROADCAST_LIMIT:.*]] = stablehlo.dynamic_broadcast_in_dim %[[ADD_LIMIT]], %{{.*}}, dims = [] : (tensor<i64>, tensor<3xindex>) -> tensor<?x?x?xi64>
+// CHECK:           %[[BROADCAST_IOTA:.*]] = stablehlo.dynamic_broadcast_in_dim %[[IOTA]], %{{.*}}, dims = [0, 1, 2] : (tensor<1x?x1xi64>, tensor<3xindex>) -> tensor<?x?x?xi64>
+// CHECK:           %[[COMPARE_START:.*]] = stablehlo.compare  GE, %[[BROADCAST_IOTA]], %[[BROADCAST_START]] : (tensor<?x?x?xi64>, tensor<?x?x?xi64>) -> tensor<?x?x?xi1>
+// CHECK:           %[[COMPARE_LIMIT:.*]] = stablehlo.compare  LT, %[[BROADCAST_IOTA]], %[[BROADCAST_LIMIT]] : (tensor<?x?x?xi64>, tensor<?x?x?xi64>) -> tensor<?x?x?xi1>
+// CHECK:           %[[AND:.*]] = stablehlo.and %[[COMPARE_START]], %[[COMPARE_LIMIT]] : tensor<?x?x?xi1>
+// CHECK:           %[[SELECT:.*]] = stablehlo.select %[[AND]], %[[LHS]], %[[BROADCAST_ZERO]] : tensor<?x?x?xi1>, tensor<?x?x?xf32>
+// CHECK:           %[[SLICE_RHS0:.*]] = stablehlo.slice %[[RHS]] [0:1, 0:2, 0:5, 0:7] : (tensor<3x2x5x7xf32>) -> tensor<1x2x5x7xf32>
+// CHECK:           %[[RESHAPE_RHS0:.*]] = stablehlo.reshape %[[SLICE_RHS0]] : (tensor<1x2x5x7xf32>) -> tensor<2x5x7xf32>
+// CHECK:           %[[DOT0:.*]] = stablehlo.dot_general %[[SELECT]], %[[RESHAPE_RHS0]]
+func.func @ragged_dot_dynamic(%lhs : tensor<?x?x?xf32>, %rhs : tensor<3x2x5x7xf32>, %group_sizes : tensor<3xi64>) -> tensor<?x?x?xf32> {
+  %0 = "chlo.ragged_dot"(%lhs, %rhs, %group_sizes) {
+    ragged_dot_dimension_numbers = #chlo.ragged_dot<
+      lhs_batching_dimensions = [0],
+      rhs_batching_dimensions = [1],
+      lhs_contracting_dimensions = [2],
+      rhs_contracting_dimensions = [2],
+      lhs_ragged_dimensions = [1],
+      rhs_group_dimensions = [0]
+    >,
+    precision_config = [#chlo<precision DEFAULT>, #chlo<precision DEFAULT>]
+  } : (tensor<?x?x?xf32>, tensor<3x2x5x7xf32>, tensor<3xi64>) -> tensor<?x?x?xf32>
+  func.return %0 : tensor<?x?x?xf32>
+}
diff --ruN a/stablehlo/stablehlo/tests/infer_stablehlo.mlir b/stablehlo/stablehlo/tests/infer_stablehlo.mlir
--- stablehlo/stablehlo/tests/infer_stablehlo.mlir
+++ stablehlo/stablehlo/tests/infer_stablehlo.mlir
@@ -1807,7 +1807,8 @@
 // CHECK-LABEL: func @dot_bounds
 func.func @dot_bounds(%arg0: tensor<?x12xf32, #stablehlo.bounds<64, ?>>, %arg1: tensor<12x?xf32, #stablehlo.bounds<?, 64>>) -> tensor<?x?xf32, #stablehlo.bounds<64, 64>> {
   %0 = stablehlo.dot %arg0, %arg1, precision = [HIGHEST, HIGHEST] : (tensor<?x12xf32, #stablehlo.bounds<64, ?>>, tensor<12x?xf32, #stablehlo.bounds<?, 64>>) -> tensor<?x?xf32, #stablehlo.bounds<64, 64>>
-  // CHECK: return {{.*}} : tensor<?x?xf32, #stablehlo.bounds<64, 64>>
+  // CHECK: types0 = tensor<?x?xindex, #stablehlo.bounds<64, 64>>
+  %1 = "hlo_test_infer.get_return_type_components"(%0): (tensor<?x?xf32, #stablehlo.bounds<64, 64>>) -> tensor<2xindex>
   return %0 : tensor<?x?xf32, #stablehlo.bounds<64, 64>>
 }
 
@@ -1833,6 +1834,26 @@
   } : (tensor<?x?x?xf32>, tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
   %1 = "hlo_test_infer.reify_return_type_shapes"(%result): (tensor<?x?x?xf32>) -> tensor<3xindex>
   func.return %1: tensor<3xindex>
+}
+
+// -----
+
+// CHECK-LABEL: func @dot_general_bounds
+func.func @dot_general_bounds(%arg0: tensor<?x12xf32, #stablehlo.bounds<64, ?>>, %arg1: tensor<12x?xf32, #stablehlo.bounds<?, 64>>) -> tensor<?x?xf32, #stablehlo.bounds<64, 64>> {
+  %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [HIGHEST, HIGHEST] : (tensor<?x12xf32, #stablehlo.bounds<64, ?>>, tensor<12x?xf32, #stablehlo.bounds<?, 64>>) -> tensor<?x?xf32, #stablehlo.bounds<64, 64>>
+  // CHECK: types0 = tensor<?x?xindex, #stablehlo.bounds<64, 64>>
+  %1 = "hlo_test_infer.get_return_type_components"(%0): (tensor<?x?xf32, #stablehlo.bounds<64, 64>>) -> tensor<2xindex>
+  return %0 : tensor<?x?xf32, #stablehlo.bounds<64, 64>>
+}
+
+// -----
+
+// CHECK-LABEL: func @dot_general_bounds_another
+func.func @dot_general_bounds_another(%arg0: tensor<10x?x12xf32, #stablehlo.bounds<?, 64, ?>>, %arg1: tensor<10x12x?xf32, #stablehlo.bounds<?, ?, 64>>) -> tensor<3xindex> {
+  %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<10x?x12xf32, #stablehlo.bounds<?, 64, ?>>, tensor<10x12x?xf32, #stablehlo.bounds<?, ?, 64>>) -> tensor<10x?x?xf32, #stablehlo.bounds<?, 64, 64>>
+  // CHECK: types0 = tensor<10x?x?xindex, #stablehlo.bounds<?, 64, 64>>
+  %1 = "hlo_test_infer.get_return_type_components"(%0): (tensor<10x?x?xf32, #stablehlo.bounds<?, 64, 64>>) -> tensor<3xindex>
+  func.return %1 : tensor<3xindex>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_13_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_13_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_13_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_13_0.mlir
@@ -1,10 +1,10 @@
-// RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --mlir-print-op-generic --split-input-file %s | FileCheck %s
-// RUN: stablehlo-translate --serialize --target=current %s | stablehlo-translate --deserialize | stablehlo-opt > %t.0
-// RUN: stablehlo-opt %s > %t.1
+// RUN: stablehlo-opt --mlir-print-op-generic %s.bc | FileCheck %s
+// RUN: stablehlo-translate --deserialize %s.bc | stablehlo-translate --serialize --target=1.13.0 | stablehlo-opt --mlir-print-op-generic | FileCheck %s
+// RUN: stablehlo-translate --deserialize %s.bc | stablehlo-opt > %t.0
+// RUN: stablehlo-opt --strip-debuginfo %s > %t.1
 // RUN: diff %t.0 %t.1
-// RUN: stablehlo-translate --serialize --target=current %s | stablehlo-opt --pass-pipeline='builtin.module(stablehlo-deserialize)' > %t.0
-// RUN: stablehlo-opt %s > %t.1
-// RUN: diff %t.0 %t.1
+// RUN: stablehlo-translate --serialize --target=1.13.0 --strip-debuginfo %s > %t.2
+// RUN: diff %s.bc %t.2
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo -emit-bytecode -debug-only=vhlo-bytecode %s 2>&1 | FileCheck --check-prefix=CHECK-WARN %s
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo -emit-bytecode %s | stablehlo-opt -debug-only=vhlo-bytecode 2>&1 | FileCheck --check-prefix=CHECK-WARN %s
 
@@ -416,6 +416,21 @@
   // CHECK: some.unregistered_attr
   %1 = stablehlo.cosine %arg0 {some.unregistered_attr = 1 : i32} : tensor<f32>
   return %1 : tensor<f32>
+}
+
+// Builtin attriubute tests
+
+// CHECK-LABEL: "byte_packed_boolean"
+func.func @byte_packed_boolean() -> (tensor<8xi1>, tensor<8xi1>, tensor<4xi1>, tensor<16xi1>) {
+  // CHECK: #vhlo.tensor_v1<dense<[true, false, false, false, false, false, false, false]
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<true> : tensor<8xi1>>
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<[true, false, false, false]> : tensor<4xi1>>
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<[true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false]> : tensor<16xi1>>
+  %c = stablehlo.constant dense<[true, false, false, false, false, false, false, false]> : tensor<8xi1>
+  %c_0 = stablehlo.constant dense<true> : tensor<8xi1>
+  %c_1 = stablehlo.constant dense<[true, false, false, false]> : tensor<4xi1>
+  %c_2 = stablehlo.constant dense<[true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false]> : tensor<16xi1>
+  return %c, %c_0, %c_1, %c_2 : tensor<8xi1>, tensor<8xi1>, tensor<4xi1>, tensor<16xi1>
 }
 
 // ============ DEFAULTS ============
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_14_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_14_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_14_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_14_0.mlir
@@ -0,0 +1,3045 @@
+// RUN: stablehlo-opt --mlir-print-op-generic %s.bc | FileCheck %s
+// RUN: stablehlo-translate --deserialize %s.bc | stablehlo-translate --serialize --target=1.14.0 | stablehlo-opt --mlir-print-op-generic | FileCheck %s
+// RUN: stablehlo-translate --deserialize %s.bc | stablehlo-opt > %t.0
+// RUN: stablehlo-opt --strip-debuginfo %s > %t.1
+// RUN: diff %t.0 %t.1
+// RUN: stablehlo-translate --serialize --target=1.14.0 --strip-debuginfo %s > %t.2
+// RUN: diff %s.bc %t.2
+// RUN: stablehlo-opt --stablehlo-legalize-to-vhlo -emit-bytecode -debug-only=vhlo-bytecode %s 2>&1 | FileCheck --check-prefix=CHECK-WARN %s
+// RUN: stablehlo-opt --stablehlo-legalize-to-vhlo -emit-bytecode %s | stablehlo-opt -debug-only=vhlo-bytecode 2>&1 | FileCheck --check-prefix=CHECK-WARN %s
+
+// CHECK-WARN-NOT: Not Implemented
+
+// ============ ATTRIBUTES ============
+
+// CHECK-LABEL: "attr_comparison_direction_eq"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_eq(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 EQ>
+    comparison_direction = #stablehlo<comparison_direction EQ>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_ne"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_ne(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 NE>
+    comparison_direction = #stablehlo<comparison_direction NE>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_ge"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_ge(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 GE>
+    comparison_direction = #stablehlo<comparison_direction GE>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_gt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_gt(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 GT>
+    comparison_direction = #stablehlo<comparison_direction GT>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_le"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_le(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 LE>
+    comparison_direction = #stablehlo<comparison_direction LE>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_lt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_lt(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 LT>
+    comparison_direction = #stablehlo<comparison_direction LT>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_notype"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_notype(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>
+    // CHECK: compare_type = #vhlo<comparison_type_v1 NOTYPE>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_float"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_float(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    // CHECK: compare_type = #vhlo<comparison_type_v1 FLOAT>,
+    compare_type = #stablehlo<comparison_type FLOAT>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_totalorder"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_totalorder(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    // CHECK: compare_type = #vhlo<comparison_type_v1 TOTALORDER>,
+    compare_type = #stablehlo<comparison_type TOTALORDER>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_signed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_signed(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    // CHECK: compare_type = #vhlo<comparison_type_v1 SIGNED>,
+    compare_type = #stablehlo<comparison_type SIGNED>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_unsigned"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_unsigned(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    // CHECK: compare_type = #vhlo<comparison_type_v1 UNSIGNED>,
+    compare_type = #stablehlo<comparison_type UNSIGNED>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// ConvDimensionNumbers aka #stablehlo.conv is covered below.
+
+// CHECK-LABEL: "attr_custom_call_api_version_unspecified"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_custom_call_api_version_unspecified(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    // CHECK: api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>
+    api_version = 0 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_custom_call_api_version_original"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_custom_call_api_version_original(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    // CHECK: api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>
+    api_version = 1 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_custom_call_api_version_status_returning"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_custom_call_api_version_status_returning(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    // CHECK: api_version = #vhlo<api_version_v1 API_VERSION_STATUS_RETURNING>
+    api_version = 2 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_custom_call_api_version_status_returning_unified"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_custom_call_api_version_status_returning_unified(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    // CHECK: api_version = #vhlo<api_version_v1 API_VERSION_STATUS_RETURNING_UNIFIED>
+    api_version = 3 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_dict"
+// CHECK: #vhlo.dict_v1<{#vhlo.string_v1<"attr1"> = #vhlo.integer_v1<1 : i32>, #vhlo.string_v1<"attr2"> = #vhlo.integer_v1<2 : i32>}
+func.func @attr_dict() attributes {stablehlo.attr = {attr1 = 1 : i32, attr2 = 2 : i32}} {
+  return
+}
+
+// CHECK-LABEL: "attr_custom_call_api_version_typed_ffi"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+// CHECK: api_version = #vhlo<api_version_v1 API_VERSION_TYPED_FFI>
+// CHECK-SAME: backend_config = #vhlo.dict_v1<{#vhlo.string_v1<"bar"> = #vhlo.integer_v1<42 : i32>}>
+func.func @attr_custom_call_api_version_typed_ffi(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    backend_config= {bar = 42 : i32},
+    api_version = 4 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+
+// CHECK-LABEL: "attr_custom_call_api_version_typed_ffi_no_backend_config"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+// CHECK: api_version = #vhlo<api_version_v1 API_VERSION_TYPED_FFI>
+// CHECK-SAME: backend_config = #vhlo.dict_v1<{}>
+func.func @attr_custom_call_api_version_typed_ffi_no_backend_config(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    api_version = 4 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// DotDimensionNumbers aka #stablehlo.dot is covered below.
+
+// CHECK-LABEL: "attr_fft_type_fft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_fft_type_fft(%arg0: tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>> {
+  %0 = "stablehlo.fft"(%arg0) {
+    // CHECK: fft_type = #vhlo<fft_type_v1 FFT>
+    fft_type = #stablehlo<fft_type FFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>>
+  func.return %0 : tensor<16xcomplex<f32>>
+}
+
+// CHECK-LABEL: "attr_fft_type_ifft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_fft_type_ifft(%arg0: tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>> {
+  %0 = "stablehlo.fft"(%arg0) {
+    // CHECK: fft_type = #vhlo<fft_type_v1 IFFT>
+    fft_type = #stablehlo<fft_type IFFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>>
+  func.return %0 : tensor<16xcomplex<f32>>
+}
+
+// CHECK-LABEL: "attr_fft_type_rfft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_fft_type_rfft(%arg0: tensor<16xf32>) -> tensor<9xcomplex<f32>> {
+  %0 = "stablehlo.fft"(%arg0) {
+    // CHECK: fft_type = #vhlo<fft_type_v1 RFFT>
+    fft_type = #stablehlo<fft_type RFFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<16xf32>) -> tensor<9xcomplex<f32>>
+  func.return %0 : tensor<9xcomplex<f32>>
+}
+
+// CHECK-LABEL: "attr_fft_type_irfft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_fft_type_irfft(%arg0: tensor<9xcomplex<f32>>) -> tensor<16xf32> {
+  %0 = "stablehlo.fft"(%arg0) {
+    // CHECK: fft_type = #vhlo<fft_type_v1 IRFFT>
+    fft_type = #stablehlo<fft_type IRFFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<9xcomplex<f32>>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "exponential_HIGHEST"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}
+func.func @exponential_HIGHEST(%arg0: tensor<8x16xf32>) -> tensor<8x16xf32> {
+  %0 = "stablehlo.exponential"(%arg0) {
+    // CHECK: result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 HIGHEST>>
+    result_accuracy = #stablehlo.result_accuracy<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #stablehlo.result_accuracy_mode<HIGHEST>>
+  } : (tensor<8x16xf32>) -> tensor<8x16xf32>
+  func.return %0 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: "exponential_TOLERANCE"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}
+func.func @exponential_TOLERANCE(%arg0: tensor<8x16xf32>) -> tensor<8x16xf32> {
+  %0 = "stablehlo.exponential"(%arg0) {
+    // CHECK: result_accuracy = #vhlo.result_accuracy_v1<atol = 1.000000e-05, rtol = 0.000000e+00, ulps = 1, mode = #vhlo<result_accuracy_mode_v1 TOLERANCE>>
+    result_accuracy = #stablehlo.result_accuracy<atol = 1.000000e-5, rtol = 0.000000e+00, ulps = 1, mode = #stablehlo.result_accuracy_mode<TOLERANCE>>
+  } : (tensor<8x16xf32>) -> tensor<8x16xf32>
+  func.return %0 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: "exponential_DEFAULT"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}
+func.func @exponential_DEFAULT(%arg0: tensor<8x16xf32>) -> tensor<8x16xf32> {
+  %0 = "stablehlo.exponential"(%arg0) {
+    // CHECK: result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>
+    result_accuracy = #stablehlo.result_accuracy<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #stablehlo.result_accuracy_mode<DEFAULT>>
+  } : (tensor<8x16xf32>) -> tensor<8x16xf32>
+  func.return %0 : tensor<8x16xf32>
+}
+
+// GatherDimensionNumbers aka #stablehlo.gather is covered below.
+
+// CHECK-LABEL: "attr_precision_config_default"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_precision_config_default(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  %0 = "stablehlo.dot"(%arg0, %arg1) {
+    // CHECK: precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "attr_precision_config_high"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_precision_config_high(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  %0 = "stablehlo.dot"(%arg0, %arg1) {
+    // CHECK: precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGH>, #vhlo<precision_v1 HIGH>]>
+    precision_config = [#stablehlo<precision HIGH>, #stablehlo<precision HIGH>]
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "attr_precision_config_highest"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_precision_config_highest(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  %0 = "stablehlo.dot"(%arg0, %arg1) {
+    // CHECK: precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "attr_rng_algorithm_default"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_rng_algorithm_default(%arg0: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  %0:2 = "stablehlo.rng_bit_generator"(%arg0) {
+    // CHECK: rng_algorithm = #vhlo<rng_algorithm_v1 DEFAULT>
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "attr_rng_algorithm_three_fry"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_rng_algorithm_three_fry(%arg0: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  %0:2 = "stablehlo.rng_bit_generator"(%arg0) {
+    // CHECK: rng_algorithm = #vhlo<rng_algorithm_v1 THREE_FRY>
+    rng_algorithm = #stablehlo<rng_algorithm THREE_FRY>
+  } : (tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "attr_rng_algorithm_philox"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_rng_algorithm_philox(%arg0: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  %0:2 = "stablehlo.rng_bit_generator"(%arg0) {
+    // CHECK: rng_algorithm = #vhlo<rng_algorithm_v1 PHILOX>
+    rng_algorithm = #stablehlo<rng_algorithm PHILOX>
+  } : (tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "attr_rng_distribution_uniform"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @attr_rng_distribution_uniform(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<0xindex>) -> tensor<f32> {
+  %0 = "stablehlo.rng"(%arg0, %arg1, %arg2) {
+    // CHECK: rng_distribution = #vhlo<rng_distribution_v1 UNIFORM>
+    rng_distribution = #stablehlo<rng_distribution UNIFORM>
+  } : (tensor<f32>, tensor<f32>, tensor<0xindex>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_rng_distribution_normal"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @attr_rng_distribution_normal(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<0xindex>) -> tensor<f32> {
+  %0 = "stablehlo.rng"(%arg0, %arg1, %arg2) {
+    // CHECK: rng_distribution = #vhlo<rng_distribution_v1 NORMAL>
+    rng_distribution = #stablehlo<rng_distribution NORMAL>
+  } : (tensor<f32>, tensor<f32>, tensor<0xindex>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// ScatterDimensionNumbers aka #stablehlo.scatter is covered below.
+
+// CHECK-LABEL: "attr_transpose_no_transpose"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_transpose_no_transpose(%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) ->  tensor<16x16xf32> {
+  %0 = "stablehlo.triangular_solve"(%arg0, %arg1) {
+    left_side = true,
+    lower = true,
+    unit_diagonal = true,
+    // transpose_a = #vhlo<transpose_v1 NO_TRANSPOSE>,
+    transpose_a = #stablehlo<transpose NO_TRANSPOSE>
+  } : (tensor<16x16xf32>, tensor<16x16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "attr_transpose_transpose"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_transpose_transpose(%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) ->  tensor<16x16xf32> {
+  %0 = "stablehlo.triangular_solve"(%arg0, %arg1) {
+    left_side = true,
+    lower = true,
+    unit_diagonal = true,
+    // transpose_a = #vhlo<transpose_v1 TRANSPOSE>,
+    transpose_a = #stablehlo<transpose TRANSPOSE>
+  } : (tensor<16x16xf32>, tensor<16x16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "attr_transpose_adjoint"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_transpose_adjoint(%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) ->  tensor<16x16xf32> {
+  %0 = "stablehlo.triangular_solve"(%arg0, %arg1) {
+    left_side = true,
+    lower = true,
+    unit_diagonal = true,
+    // transpose_a = #vhlo<transpose_v1 ADJOINT>,
+    transpose_a = #stablehlo<transpose ADJOINT>
+  } : (tensor<16x16xf32>, tensor<16x16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// TypeExtensionsAttr aka #stablehlo.type_extensions is covered below.
+
+// CHECK-LABEL: "attr_type_extensions_bounds"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_type_extensions_bounds(%arg0: tensor<?x?xf32, #stablehlo.type_extensions<bounds = [16, ?]>>) -> tensor<?x?xf32, #stablehlo.type_extensions<bounds = [16, ?]>> {
+  // CHECK: "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<?x?x!vhlo.f32_v1, #vhlo.type_extensions_v1<bounds = [16, ?]>>) -> ()
+  func.return %arg0 : tensor<?x?xf32, #stablehlo.type_extensions<bounds = [16, ?]>>
+}
+
+// CHECK-LABEL: "attr_frontend_attributes"
+func.func @attr_frontend_attributes(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: some.unregistered_attr
+  %1 = stablehlo.cosine %arg0 {some.unregistered_attr = 1 : i32} : tensor<f32>
+  return %1 : tensor<f32>
+}
+
+// Builtin attriubute tests
+
+// CHECK-LABEL: "byte_packed_boolean"
+func.func @byte_packed_boolean() -> (tensor<8xi1>, tensor<8xi1>, tensor<4xi1>, tensor<4xi1>, tensor<16xi1>) {
+  // CHECK: #vhlo.tensor_v1<dense<[true, false, false, false, false, false, false, false]
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<true> : tensor<8xi1>>
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<[true, false, false, false]> : tensor<4xi1>>
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<true> : tensor<4xi1>>
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<[true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false]> : tensor<16xi1>>
+  %c = stablehlo.constant dense<[true, false, false, false, false, false, false, false]> : tensor<8xi1>
+  %c_0 = stablehlo.constant dense<true> : tensor<8xi1>
+  %c_1 = stablehlo.constant dense<[true, false, false, false]> : tensor<4xi1>
+  %c_2 = stablehlo.constant dense<true> : tensor<4xi1>
+  %c_3 = stablehlo.constant dense<[true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false]> : tensor<16xi1>
+  return %c, %c_0, %c_1, %c_2, %c_3 : tensor<8xi1>, tensor<8xi1>, tensor<4xi1>, tensor<4xi1>, tensor<16xi1>
+}
+
+// ============ DEFAULTS ============
+
+// CHECK-LABEL: "default_all_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_all_gather(%arg0: tensor<16x8xf32>) -> tensor<16x16xf32> {
+  //               CHECK: "vhlo.all_gather_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   all_gather_dim = #vhlo.integer_v1<1 : i64>
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<false>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.all_gather"(%arg0) {
+    all_gather_dim = 1 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<16x8xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "default_all_gather_variadic"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_all_gather_variadic(%arg0: tensor<16x8xf32>, %arg1: tensor<16x8xf32>) -> (tensor<16x16xf32>, tensor<16x16xf32>) {
+  %0:2 = "stablehlo.all_gather"(%arg0, %arg1) {
+    all_gather_dim = 1 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<16x8xf32>, tensor<16x8xf32>) -> (tensor<16x16xf32>, tensor<16x16xf32>)
+  func.return %0#0, %0#1 : tensor<16x16xf32>, tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "default_all_reduce"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_all_reduce(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.all_reduce_v2"(%[[ARG0]])
+  //          CHECK-SAME: <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<false>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+
+  %0 = "stablehlo.all_reduce"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.add"(%arg1, %arg2) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "default_all_to_all"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_all_to_all(%arg0: tensor<4x16xf32>) -> tensor<16x4xf32> {
+  //               CHECK: "vhlo.all_to_all_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  //          CHECK-SAME:   concat_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>>,
+  //          CHECK-SAME:   split_count = #vhlo.integer_v1<4 : i64>
+  //          CHECK-SAME:   split_dimension = #vhlo.integer_v1<1 : i64>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<4x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x4x!vhlo.f32_v1>
+  %0 = "stablehlo.all_to_all"(%arg0) {
+    split_dimension = 1 : i64,
+    concat_dimension = 0 : i64,
+    split_count = 4 : i64,
+    replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>
+  } : (tensor<4x16xf32>) -> tensor<16x4xf32>
+  func.return %0 : tensor<16x4xf32>
+}
+
+// CHECK-LABEL: "default_all_to_all_variadic"
+func.func @default_all_to_all_variadic(%arg0: tensor<4x16xf32>, %arg1: tensor<5x16xf32>) -> (tensor<16x4xf32>, tensor<20x4xf32>) {
+  %0:2 = "stablehlo.all_to_all"(%arg0, %arg1) {
+    split_dimension = 1 : i64,
+    concat_dimension = 0 : i64,
+    split_count = 4 : i64,
+    replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>
+  } : (tensor<4x16xf32>, tensor<5x16xf32>) -> (tensor<16x4xf32>, tensor<20x4xf32>)
+  func.return %0#0, %0#1 : tensor<16x4xf32>, tensor<20x4xf32>
+}
+
+// CHECK-LABEL: "default_cholesky"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_cholesky(%arg0: tensor<1x16x16xf32>) -> tensor<1x16x16xf32> {
+  //      CHECK: "vhlo.cholesky_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   lower = #vhlo.bool_v1<false>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x16x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.cholesky"(%arg0) : (tensor<1x16x16xf32>) -> tensor<1x16x16xf32>
+  func.return %0 : tensor<1x16x16xf32>
+}
+
+// CHECK-LABEL: "default_collective_permute"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_collective_permute(%arg0: tensor<16x8xf32>) -> tensor<16x8xf32> {
+  //               CHECK: "vhlo.collective_permute_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x8x!vhlo.f32_v1>
+  %0 = "stablehlo.collective_permute"(%arg0) {
+    source_target_pairs = dense<[[0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>
+  } : (tensor<16x8xf32>) -> tensor<16x8xf32>
+  func.return %0 : tensor<16x8xf32>
+}
+
+// CHECK-LABEL: "default_collective_broadcast"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_collective_broadcast(%arg0: tensor<16x8xf32>) -> tensor<16x8xf32> {
+  //               CHECK: "vhlo.collective_broadcast_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0, 1]]> : tensor<1x2xi64>>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x8x!vhlo.f32_v1>
+  %0 = "stablehlo.collective_broadcast"(%arg0) {
+    replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>
+  } : (tensor<16x8xf32>) -> tensor<16x8xf32>
+  func.return %0 : tensor<16x8xf32>
+}
+
+// CHECK-LABEL: "default_compare"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_compare(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  //      CHECK: "vhlo.compare_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   compare_type = #vhlo<comparison_type_v1 NOTYPE>,
+  // CHECK-SAME:   comparison_direction = #vhlo<comparison_direction_v1 EQ>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "default_composite"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_composite(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.composite_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   composite_attributes = #vhlo.dict_v1<{}>
+  //          CHECK-SAME:   decomposition = #vhlo.string_v1<"composite_target">
+  //          CHECK-SAME:   name = #vhlo.string_v1<"stablehlo.composite_target">
+  //          CHECK-SAME:   version = #vhlo.integer_v1<0 : i64>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.composite"(%arg0) {
+    name = "stablehlo.composite_target",
+    decomposition = @composite_target
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "default_convolution"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_convolution(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>) -> tensor<1x6x6x16xf32> {
+  //      CHECK: "vhlo.convolution_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   batch_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   feature_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   input_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   input_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   input_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   kernel_input_feature_dimension = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   kernel_output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>,
+  // CHECK-SAME:   output_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   output_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   padding = #vhlo.tensor_v1<dense<0> : tensor<2x2xi64>>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>,
+  // CHECK-SAME:   rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>,
+  // CHECK-SAME:   window_reversal = #vhlo.tensor_v1<dense<false> : tensor<2xi1>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x6x6x16x!vhlo.f32_v1>
+  %0 = "stablehlo.convolution"(%arg0, %arg1) {
+    dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
+    feature_group_count = 1 : i64,
+    batch_group_count = 1 : i64
+  } : (tensor<1x8x8x207xf32>, tensor<3x3x207x16xf32>) -> tensor<1x6x6x16xf32>
+  func.return %0 : tensor<1x6x6x16xf32>
+}
+
+// CHECK-LABEL: "default_custom_call"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_custom_call(%arg0: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.custom_call_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>,
+  // CHECK-SAME:   backend_config = #vhlo.string_v1<"">,
+  // CHECK-SAME:   call_target_name = #vhlo.string_v1<"foo">,
+  // CHECK-SAME:   called_computations = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   has_side_effect = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   operand_layouts = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   output_operand_aliases = #vhlo.array_v1<[]>
+  // CHECK-SAME:   result_layouts = #vhlo.array_v1<[]>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo"
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "default_dot_general"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_dot_general(%arg0: tensor<8x8x16xf32>, %arg1: tensor<8x16x8xf32>) -> tensor<8x8x8xf32> {
+  //      CHECK: "vhlo.dot_general_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   accumulation_type = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>,
+  // CHECK-SAME:   rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>,
+  // CHECK-SAME:   rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<8x16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dot_general"(%arg0, %arg1) {
+    dot_dimension_numbers = #stablehlo.dot<
+      lhs_batching_dimensions = [0],
+      lhs_contracting_dimensions = [2],
+      rhs_batching_dimensions = [0],
+      rhs_contracting_dimensions = [1]
+    >
+  } : (tensor<8x8x16xf32>, tensor<8x16x8xf32>) -> tensor<8x8x8xf32>
+  func.return %0 : tensor<8x8x8xf32>
+}
+
+// CHECK-LABEL: "dot_general_algorithm"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @dot_general_algorithm(%arg0: tensor<8x8x16xf32>, %arg1: tensor<8x16x8xf32>) -> tensor<8x8x8xf32> {
+//      CHECK: "vhlo.dot_general_v2"(%[[ARG0]], %[[ARG1]]) <{
+// CHECK-SAME:   accumulation_type = #vhlo.type_v1<!vhlo.f32_v1>,
+// CHECK-SAME:   allow_imprecise_accumulation = #vhlo.bool_v1<false>,
+// CHECK-SAME:   lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+// CHECK-SAME:   lhs_component_count = #vhlo.integer_v1<1 : i64>,
+// CHECK-SAME:   lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+// CHECK-SAME:   lhs_precision_type = #vhlo.type_v1<!vhlo.tf31_v1>,
+// CHECK-SAME:   num_primitive_operations = #vhlo.integer_v1<1 : i64>,
+// CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>,
+// CHECK-SAME:   rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+// CHECK-SAME:   rhs_component_count = #vhlo.integer_v1<1 : i64>,
+// CHECK-SAME:   rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>,
+// CHECK-SAME:   rhs_precision_type = #vhlo.type_v1<!vhlo.tf31_v1>
+// CHECK-SAME: }> : (!vhlo.tensor_v1<8x8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<8x16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dot_general"(%arg0, %arg1) {
+    dot_dimension_numbers = #stablehlo.dot<
+      lhs_batching_dimensions = [0],
+      lhs_contracting_dimensions = [2],
+      rhs_batching_dimensions = [0],
+      rhs_contracting_dimensions = [1]
+    >,
+    algorithm = #stablehlo.dot_algorithm<
+      lhs_precision_type = tf32,
+      rhs_precision_type = tf32,
+      accumulation_type = f32,
+      lhs_component_count = 1,
+      rhs_component_count = 1,
+      num_primitive_operations = 1,
+      allow_imprecise_accumulation = false
+    >
+  } : (tensor<8x8x16xf32>, tensor<8x16x8xf32>) -> tensor<8x8x8xf32>
+  func.return %0 : tensor<8x8x8xf32>
+}
+
+// CHECK-LABEL: "default_dynamic_broadcast_in_dim"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_dynamic_broadcast_in_dim(%arg0: tensor<?x?xf32>, %arg1: tensor<2xindex>) -> tensor<?x?xf32> {
+  //      CHECK: "vhlo.dynamic_broadcast_in_dim_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   known_expanding_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   known_nonexpanding_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<?x?x!vhlo.f32_v1>, !vhlo.tensor_v1<2x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_broadcast_in_dim"(%arg0, %arg1) {
+    broadcast_dimensions = array<i64: 0, 1>
+  } : (tensor<?x?xf32>, tensor<2xindex>) -> tensor<?x?xf32>
+  func.return %0 : tensor<?x?xf32>
+}
+
+// CHECK-LABEL: "default_dynamic_conv"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @default_dynamic_conv(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>, %arg2: tensor<2x2xi64>) -> tensor<1x?x?x16xf32> {
+  //      CHECK: "vhlo.dynamic_conv_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   batch_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   feature_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   input_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   input_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   input_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   kernel_input_feature_dimension = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   kernel_output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>,
+  // CHECK-SAME:   output_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   output_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>,
+  // CHECK-SAME:   rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>,
+  // CHECK-SAME:   window_reversal = #vhlo.tensor_v1<dense<false> : tensor<2xi1>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<2x2x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
+    dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
+    feature_group_count = 1 : i64,
+    batch_group_count = 1 : i64
+  } : (tensor<1x8x8x207xf32>, tensor<3x3x207x16xf32>, tensor<2x2xi64>) -> tensor<1x?x?x16xf32>
+  func.return %0 : tensor<1x?x?x16xf32>
+}
+
+// CHECK-LABEL: "default_dynamic_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @default_dynamic_gather(%arg0 : tensor<2x4x9xf32>, %arg1 : tensor<1x5x2xi32>, %arg2 : tensor<3xi32>) -> tensor<1x5x8xf32> {
+  //      CHECK: "vhlo.dynamic_gather_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>, !vhlo.tensor_v1<3x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_gather"(%arg0, %arg1, %arg2) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [0, 1],
+      start_index_map = [0, 1],
+      index_vector_dim = 2
+    >
+  } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>, tensor<3xi32>) -> tensor<1x5x8xf32>
+  func.return %0 : tensor<1x5x8xf32>
+}
+
+func.func @default_func(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK:      "vhlo.func_v1"() <{
+  // CHECK-SAME:   arg_attrs = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   function_type = #vhlo.type_v1<!vhlo.func_v1<(!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>>>,
+  // CHECK-SAME:   res_attrs = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   sym_name = #vhlo.string_v1<"default_func">,
+  // CHECK-SAME:   sym_visibility = #vhlo.string_v1<"">
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG0:.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : () -> ()
+  func.return %arg0 : tensor<f32>
+}
+
+// CHECK-LABEL: "default_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_gather(%arg0 : tensor<2x4x9xf32>, %arg1 : tensor<1x5x2xi32>) -> tensor<1x5x1xf32> {
+  //      CHECK: "vhlo.gather_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   slice_sizes = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x1x!vhlo.f32_v1>
+  %0 = "stablehlo.gather"(%arg0, %arg1) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [0, 1],
+      start_index_map = [0, 1],
+      index_vector_dim = 2
+    >,
+    slice_sizes = array<i64: 1, 1, 1>
+  } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
+  func.return %0 : tensor<1x5x1xf32>
+}
+
+// CHECK-LABEL: "default_infeed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_infeed(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //               CHECK: "vhlo.infeed_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   infeed_config = #vhlo.string_v1<"">,
+  // CHECK-SAME{LITERAL}:   layout = #vhlo.array_v1<[]>
+  //          CHECK-SAME: }> : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  %0:2 = "stablehlo.infeed"(%arg0) : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
+
+// CHECK-LABEL: "default_outfeed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_outfeed(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.outfeed_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   outfeed_config = #vhlo.string_v1<"">
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.outfeed"(%arg0, %arg1) : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_recv_with_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_recv_with_source_target_pairs(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //      CHECK: "vhlo.recv_v2"(%[[ARG0]]) <{
+  // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   channel_type = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<false>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>>
+  // CHECK-SAME{LITERAL}: }> : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  %0:2 = "stablehlo.recv"(%arg0) {
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+    source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
+  } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
+
+// CHECK-LABEL: "default_send"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_send(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.send_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   channel_type = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<false>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>>
+  // CHECK-SAME{LITERAL}: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.send"(%arg0, %arg1) {
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+    source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
+  } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "default_reduce_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_reduce_scatter(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //               CHECK: "vhlo.reduce_scatter_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   scatter_dimension = #vhlo.integer_v1<0 : i64>
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<false>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_scatter"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.add"(%arg1, %arg2) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension = 0 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "default_reduce_window"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_reduce_window(%arg0: tensor<2x17x31x7xf32>, %arg1: tensor<f32>) -> tensor<2x16x30x7xf32> {
+  //               CHECK: "vhlo.reduce_window_v1"(%[[ARG0]], %[[ARG1]])  <{
+  //          CHECK-SAME:   base_dilations = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>,
+  // CHECK-SAME{LITERAL}:   padding = #vhlo.tensor_v1<dense<0> : tensor<4x2xi64>>,
+  //          CHECK-SAME:   window_dilations = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>,
+  //          CHECK-SAME:   window_dimensions = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  //          CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.maximum_v1"(%[[ARG2]], %[[ARG3]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<2x17x31x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x16x30x7x!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_window"(%arg0, %arg1) ({
+    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
+      %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>
+  } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
+  func.return %0 : tensor<2x16x30x7xf32>
+}
+
+// CHECK-LABEL: "default_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @default_scatter(%arg0: tensor<200x100x300xf32>, %arg1: tensor<10x2xi32>, %arg2: tensor<10x300xf32>) -> tensor<200x100x300xf32> {
+  //      CHECK: "vhlo.scatter_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   unique_indices = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   update_window_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG4:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG3]], %[[ARG4]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>, !vhlo.tensor_v1<10x2x!vhlo.i32_v1>, !vhlo.tensor_v1<10x300x!vhlo.f32_v1>) -> !vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>
+  %0 = "stablehlo.scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension_numbers = #stablehlo.scatter<
+      update_window_dims = [1],
+      inserted_window_dims = [0, 1],
+      scatter_dims_to_operand_dims = [0, 1],
+      index_vector_dim = 1
+    >
+  } : (tensor<200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) -> tensor<200x100x300xf32>
+  func.return %0 : tensor<200x100x300xf32>
+}
+
+// CHECK-LABEL: "default_select_and_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @default_select_and_scatter(%arg0: tensor<10x24x24x64xf32>, %arg1: tensor<10x23x23x64xf32>, %arg2: tensor<f32>) -> tensor<10x24x24x64xf32> {
+  //      CHECK: "vhlo.select_and_scatter_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   padding = #vhlo.tensor_v1<dense<0> : tensor<4x2xi64>>,
+  // CHECK-SAME:   window_dimensions = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG31:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG41:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL11:.*]] = "vhlo.compare_v1"(%[[ARG31]], %[[ARG41]]) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GE>}>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL11]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }, {
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG32:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG42:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL12:.*]] = "vhlo.add_v1"(%[[ARG32]], %[[ARG42]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL12]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>, !vhlo.tensor_v1<10x23x23x64x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>
+  %0 = "stablehlo.select_and_scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg3, %arg4) {compare_type = #stablehlo<comparison_type TOTALORDER>, comparison_direction = #stablehlo<comparison_direction GE>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }, {
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>
+  } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
+  func.return %0 : tensor<10x24x24x64xf32>
+}
+
+// CHECK-LABEL: "default_sort"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_sort(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.sort_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<-1 : i64>
+  // CHECK-SAME:   is_stable = #vhlo.bool_v1<false>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.compare_v1"(%[[ARG1]], %[[ARG2]]) <{compare_type = #vhlo<comparison_type_v1 FLOAT>, comparison_direction = #vhlo<comparison_direction_v1 GT>}>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.sort"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg1, %arg2) {compare_type = #stablehlo<comparison_type FLOAT>, comparison_direction = #stablehlo<comparison_direction GT>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }) : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// ============ OPS ============
+
+// CHECK-LABEL: "op_abs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_abs(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.abs_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.abs"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_add"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_add(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_after_all"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_after_all(%arg0: !stablehlo.token) -> !stablehlo.token {
+  // CHECK: "vhlo.after_all_v1"(%[[ARG0]]) : (!vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.after_all"(%arg0) : (!stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_all_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_all_gather(%arg0: tensor<16x8xf32>) -> tensor<16x16xf32> {
+  //               CHECK: "vhlo.all_gather_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   all_gather_dim = #vhlo.integer_v1<1 : i64>
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<true>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.all_gather"(%arg0) {
+    all_gather_dim = 1 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+    use_global_device_ids
+  } : (tensor<16x8xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "op_all_reduce"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_all_reduce(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.all_reduce_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<true>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.all_reduce"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.add"(%arg1, %arg2) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+    use_global_device_ids
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_all_reduce_with_promotable_types"
+func.func @op_all_reduce_with_promotable_types(%operand: tensor<f32>) -> tensor<f64> {
+  //  CHECK: "vhlo.all_reduce_v2"(%[[ARG0:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f64_v1>
+  %result = "stablehlo.all_reduce"(%operand) ({
+    ^bb0(%arg0: tensor<f64>, %arg1: tensor<f64>):
+      %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f64>, tensor<f64>) -> tensor<f64>
+      "stablehlo.return"(%0) : (tensor<f64>) -> ()
+  }) {
+    replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+    use_global_device_ids
+  } : (tensor<f32>) -> tensor<f64>
+
+  func.return %result : tensor<f64>
+}
+
+// CHECK-LABEL: "default_all_reduce_variadic"
+func.func @default_all_reduce_variadic(%arg0: tensor<f32>, %arg1: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  %0:2 = "stablehlo.all_reduce"(%arg0, %arg1) ({
+    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
+      %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> (tensor<f32>)
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<f32>, tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "op_all_to_all"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_all_to_all(%arg0: tensor<4x16xf32>) -> tensor<16x4xf32> {
+  //               CHECK: "vhlo.all_to_all_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  //          CHECK-SAME:   concat_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>>,
+  //          CHECK-SAME:   split_count = #vhlo.integer_v1<4 : i64>
+  //          CHECK-SAME:   split_dimension = #vhlo.integer_v1<1 : i64>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<4x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x4x!vhlo.f32_v1>
+  %0 = "stablehlo.all_to_all"(%arg0) {
+    split_dimension = 1 : i64,
+    concat_dimension = 0 : i64,
+    split_count = 4 : i64,
+    replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>
+  } : (tensor<4x16xf32>) -> tensor<16x4xf32>
+  func.return %0 : tensor<16x4xf32>
+}
+
+// CHECK-LABEL: "op_and"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_and(%arg0: tensor<i1>, %arg1: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.and_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.and"(%arg0, %arg1) : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_atan2"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_atan2(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.atan2_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.atan2"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_batch_norm_grad"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}}, %[[ARG3:.*]]: {{.*}}, %[[ARG4:.*]]: {{.*}})
+func.func @op_batch_norm_grad(%arg0: tensor<16x16x16x16xf32>, %arg1: tensor<16xf32>, %arg2: tensor<16xf32>, %arg3: tensor<16xf32>, %arg4: tensor<16x16x16x16xf32>) -> (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>) {
+  //      CHECK: "vhlo.batch_norm_grad_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]], %[[ARG3]], %[[ARG4]]) <{
+  // CHECK-SAME:   epsilon = #vhlo.float_v1<1.000000e-03 : !vhlo.f32_v1>,
+  // CHECK-SAME:   feature_index = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>) -> (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>)
+  %0:3 = "stablehlo.batch_norm_grad"(%arg0, %arg1, %arg2, %arg3, %arg4) {
+    epsilon = 0.001 : f32,
+    feature_index = 0 : i64
+  } : (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>, tensor<16xf32>, tensor<16x16x16x16xf32>) -> (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>)
+  func.return %0#0, %0#1, %0#2 : tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_batch_norm_inference"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}}, %[[ARG3:.*]]: {{.*}}, %[[ARG4:.*]]: {{.*}})
+func.func @op_batch_norm_inference(%arg0: tensor<16x16x16x16xf32>, %arg1: tensor<16xf32>, %arg2: tensor<16xf32>, %arg3: tensor<16xf32>, %arg4: tensor<16xf32>) -> tensor<16x16x16x16xf32> {
+  //      CHECK: "vhlo.batch_norm_inference_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]], %[[ARG3]], %[[ARG4]]) <{
+  // CHECK-SAME:   epsilon = #vhlo.float_v1<1.000000e-03 : !vhlo.f32_v1>,
+  // CHECK-SAME:   feature_index = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.batch_norm_inference"(%arg0, %arg1, %arg2, %arg3, %arg4) {
+    epsilon = 0.001 : f32,
+    feature_index = 0 : i64
+  } : (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>, tensor<16xf32>, tensor<16xf32>) -> tensor<16x16x16x16xf32>
+  func.return %0 : tensor<16x16x16x16xf32>
+}
+
+// CHECK-LABEL: "op_batch_norm_training"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_batch_norm_training(%arg0: tensor<16x16x16x16xf32>, %arg1: tensor<16xf32>, %arg2: tensor<16xf32>) -> (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>) {
+  //      CHECK: "vhlo.batch_norm_training_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   epsilon = #vhlo.float_v1<1.000000e-03 : !vhlo.f32_v1>,
+  // CHECK-SAME:   feature_index = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>) -> (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>)
+  %0:3 = "stablehlo.batch_norm_training"(%arg0, %arg1, %arg2) {
+    epsilon = 0.001 : f32,
+    feature_index = 0 : i64
+  } : (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>) -> (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>)
+  func.return %0#0, %0#1, %0#2 : tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_bitcast_convert"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_bitcast_convert(%arg0: tensor<i32>) -> tensor<f32> {
+  // CHECK: "vhlo.bitcast_convert_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.bitcast_convert"(%arg0) : (tensor<i32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_broadcast_in_dim"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_broadcast_in_dim(%arg0: tensor<16xf32>) -> tensor<16x16xf32> {
+  //      CHECK: "vhlo.broadcast_in_dim_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.broadcast_in_dim"(%arg0) {
+    broadcast_dimensions = array<i64: 1>
+  } : (tensor<16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "op_broadcast"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_broadcast(%arg0: tensor<16xf32>) -> tensor<16x16xf32> {
+  //      CHECK: "vhlo.broadcast_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   broadcast_sizes = #vhlo.tensor_v1<dense<16> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.broadcast"(%arg0) {
+    broadcast_sizes = array<i64: 16>
+  } : (tensor<16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "op_case"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_case(%arg0: tensor<i32>, %arg1: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.case_v1"(%[[ARG0]]) ({
+  // CHECK-NEXT:   "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.case"(%arg0) ({
+    "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+  }) : (tensor<i32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_cbrt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_cbrt(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.cbrt_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.cbrt"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_ceil"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_ceil(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.ceil_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.ceil"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_cholesky"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_cholesky(%arg0: tensor<1x16x16xf32>) -> tensor<1x16x16xf32> {
+  //      CHECK: "vhlo.cholesky_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   lower = #vhlo.bool_v1<true>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x16x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.cholesky"(%arg0) {
+    lower = true
+  } : (tensor<1x16x16xf32>) -> tensor<1x16x16xf32>
+  func.return %0 : tensor<1x16x16xf32>
+}
+
+// CHECK-LABEL: "op_clamp"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_clamp(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.clamp_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.clamp"(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_count_leading_zeros"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_count_leading_zeros(%arg0: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.count_leading_zeros_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.count_leading_zeros"(%arg0) : (tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_collective_permute"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_collective_permute(%arg0: tensor<16x8xf32>) -> tensor<16x8xf32> {
+  //               CHECK: "vhlo.collective_permute_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x8x!vhlo.f32_v1>
+  %0 = "stablehlo.collective_permute"(%arg0) {
+    source_target_pairs = dense<[[0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>
+  } : (tensor<16x8xf32>) -> tensor<16x8xf32>
+  func.return %0 : tensor<16x8xf32>
+}
+
+// CHECK-LABEL: "op_compare"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_compare(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  //      CHECK: "vhlo.compare_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   compare_type = #vhlo<comparison_type_v1 TOTALORDER>,
+  // CHECK-SAME:   comparison_direction = #vhlo<comparison_direction_v1 EQ>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    compare_type = #stablehlo<comparison_type TOTALORDER>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_complex"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_complex(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<complex<f32>> {
+  // CHECK: "vhlo.complex_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>
+  %0 = "stablehlo.complex"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<complex<f32>>
+  func.return %0 : tensor<complex<f32>>
+}
+
+// CHECK-LABEL: "op_composite"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_composite(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.composite_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"my_int"> = #vhlo.integer_v1<1 : i64>, #vhlo.string_v1<"my_string"> = #vhlo.string_v1<"foo">}>
+  //          CHECK-SAME:   decomposition = #vhlo.string_v1<"composite_target">
+  //          CHECK-SAME:   name = #vhlo.string_v1<"stablehlo.composite_target">
+  //          CHECK-SAME:   version = #vhlo.integer_v1<1 : i32>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.composite"(%arg0) {
+    name = "stablehlo.composite_target",
+    decomposition = @composite_target,
+    version = 1 : i32,
+    composite_attributes = {
+      my_int = 1 : i64,
+      my_string = "foo"
+    }
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "composite_regions"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @composite_regions(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.composite_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   composite_attributes = #vhlo.dict_v1<{}>
+  //          CHECK-SAME:   decomposition = #vhlo.string_v1<"composite_target">
+  //          CHECK-SAME:   name = #vhlo.string_v1<"stablehlo.composite_target">
+  //          CHECK-SAME:   version = #vhlo.integer_v1<1 : i32>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT: ^bb0(%[[ARG1:.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:   "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.composite"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>):
+      "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+  }) {
+    name = "stablehlo.composite_target",
+    decomposition = @composite_target,
+    version = 1 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_concatenate"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_concatenate(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.concatenate_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x!vhlo.f32_v1>, !vhlo.tensor_v1<8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.concatenate"(%arg0, %arg1) {
+    dimension = 0 : i64
+  } : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_constant"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_constant(%arg0: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.constant_v1"() <{
+  // CHECK-SAME:   value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>
+  // CHECK-SAME: }> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.constant"() {
+    value = dense<0.0> : tensor<f32>
+  } : () -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_convert"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_convert(%arg0: tensor<i32>) -> tensor<f32> {
+  // CHECK: "vhlo.convert_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.convert"(%arg0) : (tensor<i32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_convolution"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_convolution(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>) -> tensor<1x7x7x16xf32> {
+  //      CHECK: "vhlo.convolution_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   batch_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   feature_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   input_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   input_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   input_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   kernel_input_feature_dimension = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   kernel_output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   lhs_dilation = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>,
+  // CHECK-SAME:   output_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   output_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   padding = #vhlo.tensor_v1<dense<1> : tensor<2x2xi64>>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>,
+  // CHECK-SAME:   rhs_dilation = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>,
+  // CHECK-SAME:   window_reversal = #vhlo.tensor_v1<dense<true> : tensor<2xi1>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
+  %0 = "stablehlo.convolution"(%arg0, %arg1) {
+    window_strides = array<i64: 2, 2>,
+    padding = dense<1> : tensor<2x2xi64>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
+    dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
+    feature_group_count = 1 : i64,
+    batch_group_count = 1 : i64,
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<1x8x8x207xf32>, tensor<3x3x207x16xf32>) -> tensor<1x7x7x16xf32>
+  func.return %0 : tensor<1x7x7x16xf32>
+}
+
+// CHECK-LABEL: "op_cosine"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_cosine(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.cosine_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.cosine"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_create_token"
+func.func @op_create_token() -> !stablehlo.token {
+  // CHECK: "vhlo.create_token_v1"() : () -> !vhlo.token_v1
+  %0 = "stablehlo.create_token"() : () -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_cross_replica_sum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_cross_replica_sum(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.cross-replica-sum_v1"(%[[ARG0]]) <{
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.cross-replica-sum"(%arg0) {
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_custom_call"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_custom_call(%arg0: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.custom_call_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   api_version = #vhlo<api_version_v1 API_VERSION_STATUS_RETURNING>,
+  // CHECK-SAME:   backend_config = #vhlo.string_v1<"\08\03\1A\02">,
+  // CHECK-SAME:   call_target_name = #vhlo.string_v1<"foo">,
+  // CHECK-SAME:   called_computations = #vhlo.array_v1<[#vhlo.string_v1<"foo">]>,
+  // CHECK-SAME:   has_side_effect = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   operand_layouts = #vhlo.array_v1<[#vhlo.tensor_v1<dense<> : tensor<0xindex>>]>,
+  // CHECK-SAME:   output_operand_aliases = #vhlo.array_v1<[
+  // CHECK-SAME:     #vhlo.output_operand_alias_v1<
+  // CHECK-SAME:       outputTupleIndices = [],
+  // CHECK-SAME:       operandIndex = 0,
+  // CHECK-SAME:       operandTupleIndices = []>]>
+  // CHECK-SAME:   result_layouts = #vhlo.array_v1<[#vhlo.tensor_v1<dense<> : tensor<0xindex>>]>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    has_side_effect = true,
+    backend_config = "\08\03\1A\02",
+    api_version = 2 : i32,
+    called_computations = [@foo],
+    operand_layouts = [dense<> : tensor<0xindex>],
+    output_operand_aliases = [
+      #stablehlo.output_operand_alias<output_tuple_indices = [],
+                                 operand_index = 0,
+                                 operand_tuple_indices = []>],
+    result_layouts = [dense<> : tensor<0xindex>]
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_custom_call_empty_result_layout"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func public @op_custom_call_empty_result_layout(%arg0: tensor<i64>) -> tensor<i64> {
+  // %0 = "vhlo.custom_call_v1"(%arg0) <{>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tuple_v1<>
+  //      CHECK: "vhlo.custom_call_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   api_version = #vhlo<api_version_v1 API_VERSION_STATUS_RETURNING>,
+  // CHECK-SAME:   backend_config = #vhlo.string_v1<"">,
+  // CHECK-SAME:   call_target_name = #vhlo.string_v1<"empty_output">,
+  // CHECK-SAME:   called_computations = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   has_side_effect = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   operand_layouts = #vhlo.array_v1<[#vhlo.tensor_v1<dense<> : tensor<0xindex>>]>,
+  // CHECK-SAME:   output_operand_aliases = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   result_layouts = #vhlo.array_v1<[]>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tuple_v1<>
+  %0 = "stablehlo.custom_call"(%arg0) <{
+    api_version = 2 : i32,
+    call_target_name = "empty_output",
+    has_side_effect = true,
+    operand_layouts = [dense<> : tensor<0xindex>],
+    result_layouts = []
+  }> : (tensor<i64>) -> tuple<>
+  return %arg0 : tensor<i64>
+}
+
+// CHECK-LABEL: "op_divide"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_divide(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.divide_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.divide"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_dot_general"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dot_general(%arg0: tensor<8x8x16xf32>, %arg1: tensor<8x16x8xf32>) -> tensor<8x8x8xf32> {
+  //      CHECK: "vhlo.dot_general_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   accumulation_type = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>,
+  // CHECK-SAME:   rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>,
+  // CHECK-SAME:   rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<8x16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dot_general"(%arg0, %arg1) {
+    dot_dimension_numbers = #stablehlo.dot<
+      lhs_batching_dimensions = [0],
+      lhs_contracting_dimensions = [2],
+      rhs_batching_dimensions = [0],
+      rhs_contracting_dimensions = [1]
+    >,
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<8x8x16xf32>, tensor<8x16x8xf32>) -> tensor<8x8x8xf32>
+  func.return %0 : tensor<8x8x8xf32>
+}
+
+// CHECK-LABEL: "op_dot"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dot(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  //      CHECK: "vhlo.dot_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dot"(%arg0, %arg1) {
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_broadcast_in_dim"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dynamic_broadcast_in_dim(%arg0: tensor<?x?xf32>, %arg1: tensor<2xindex>) -> tensor<?x?xf32> {
+  //      CHECK: "vhlo.dynamic_broadcast_in_dim_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   known_expanding_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   known_nonexpanding_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<?x?x!vhlo.f32_v1>, !vhlo.tensor_v1<2x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_broadcast_in_dim"(%arg0, %arg1) {
+    broadcast_dimensions = array<i64: 0, 1>,
+    known_expanding_dimensions = array<i64: 0>,
+    known_nonexpanding_dimensions = array<i64: 1>
+  } : (tensor<?x?xf32>, tensor<2xindex>) -> tensor<?x?xf32>
+  func.return %0 : tensor<?x?xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_conv"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_dynamic_conv(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>, %arg2: tensor<2x2xi64>) -> tensor<1x?x?x16xf32> {
+  //      CHECK: "vhlo.dynamic_conv_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   batch_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   feature_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   input_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   input_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   input_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   kernel_input_feature_dimension = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   kernel_output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   lhs_dilation = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>,
+  // CHECK-SAME:   output_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   output_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>,
+  // CHECK-SAME:   rhs_dilation = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>,
+  // CHECK-SAME:   window_reversal = #vhlo.tensor_v1<dense<true> : tensor<2xi1>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<2x2x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
+    window_strides = array<i64: 2, 2>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
+    dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
+    feature_group_count = 1 : i64,
+    batch_group_count = 1 : i64,
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<1x8x8x207xf32>, tensor<3x3x207x16xf32>, tensor<2x2xi64>) -> tensor<1x?x?x16xf32>
+  func.return %0 : tensor<1x?x?x16xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_dynamic_gather(%arg0 : tensor<2x4x9xf32>, %arg1 : tensor<1x5x2xi32>, %arg2 : tensor<3xi32>) -> tensor<1x5x8xf32> {
+  //      CHECK: "vhlo.dynamic_gather_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>, !vhlo.tensor_v1<3x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_gather"(%arg0, %arg1, %arg2) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [0, 1],
+      start_index_map = [0, 1],
+      index_vector_dim = 2
+    >,
+    indices_are_sorted = true
+  } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>, tensor<3xi32>) -> tensor<1x5x8xf32>
+  func.return %0 : tensor<1x5x8xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_gather_with_batching_dims"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_dynamic_gather_with_batching_dims(%arg0 : tensor<5x2x4x9xf32>, %arg1 : tensor<1x5x2xi32>, %arg2 : tensor<4xi32>) -> tensor<1x5x8xf32> {
+  //      CHECK: "vhlo.dynamic_gather_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<5x2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_gather"(%arg0, %arg1, %arg2) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [1, 2],
+      operand_batching_dims = [0],
+      start_indices_batching_dims = [1],
+      start_index_map = [1, 2],
+      index_vector_dim = 2
+    >,
+    indices_are_sorted = true
+  } : (tensor<5x2x4x9xf32>, tensor<1x5x2xi32>, tensor<4xi32>) -> tensor<1x5x8xf32>
+  func.return %0 : tensor<1x5x8xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_iota"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_dynamic_iota(%arg0: tensor<1xindex>) -> tensor<?xf32> {
+  //      CHECK: "vhlo.dynamic_iota_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   iota_dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_iota"(%arg0) {
+    iota_dimension = 0 : i64
+  } : (tensor<1xindex>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_pad"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}}, %[[ARG3:.*]]: {{.*}}, %[[ARG4:.*]]: {{.*}})
+func.func @op_dynamic_pad(%arg0: tensor<?xf32>, %arg1: tensor<f32>, %arg2: tensor<1xindex>, %arg3: tensor<1xindex>, %arg4: tensor<1xindex>) -> tensor<?xf32> {
+  // CHECK: "vhlo.dynamic_pad_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]], %[[ARG3]], %[[ARG4]]) : (!vhlo.tensor_v1<?x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_pad"(%arg0, %arg1, %arg2, %arg3, %arg4) : (tensor<?xf32>, tensor<f32>, tensor<1xindex>, tensor<1xindex>, tensor<1xindex>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_reshape"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dynamic_reshape(%arg0: tensor<16xf32>, %arg1: tensor<2xindex>) -> tensor<?x?xf32> {
+  // CHECK: "vhlo.dynamic_reshape_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<2x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_reshape"(%arg0, %arg1) : (tensor<16xf32>, tensor<2xindex>) -> tensor<?x?xf32>
+  func.return %0 : tensor<?x?xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_slice"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dynamic_slice(%arg0: tensor<16xf32>, %arg1: tensor<i64>) -> tensor<4xf32> {
+  //      CHECK: "vhlo.dynamic_slice_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   slice_sizes = #vhlo.tensor_v1<dense<4> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<4x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_slice"(%arg0, %arg1) {
+    slice_sizes = array<i64: 4>
+  } : (tensor<16xf32>, tensor<i64>) -> tensor<4xf32>
+  func.return %0 : tensor<4xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_update_slice"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_dynamic_update_slice(%arg0: tensor<16xf32>, %arg1: tensor<4xf32>, %arg2: tensor<i64>) -> tensor<16xf32> {
+  // CHECK: "vhlo.dynamic_update_slice_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_update_slice"(%arg0, %arg1, %arg2) : (tensor<16xf32>, tensor<4xf32>, tensor<i64>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_einsum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_einsum(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  //      CHECK: "vhlo.einsum_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   einsum_config = #vhlo.string_v1<"ab,bc->ac">
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.einsum"(%arg0, %arg1) {
+    einsum_config = "ab,bc->ac"
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "op_exponential_minus_one"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_exponential_minus_one(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.exponential_minus_one_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.exponential_minus_one"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_exponential"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_exponential(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.exponential_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.exponential"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_fft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_fft(%arg0: tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>> {
+  //      CHECK: "vhlo.fft_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   fft_length = #vhlo.tensor_v1<dense<16> : tensor<1xi64>>,
+  // CHECK-SAME:   fft_type = #vhlo<fft_type_v1 FFT>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.complex_v1<!vhlo.f32_v1>>) -> !vhlo.tensor_v1<16x!vhlo.complex_v1<!vhlo.f32_v1>>
+  %0 = "stablehlo.fft"(%arg0) {
+    fft_type = #stablehlo<fft_type FFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>>
+  func.return %0 : tensor<16xcomplex<f32>>
+}
+
+// CHECK-LABEL: "op_floor"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_floor(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.floor_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.floor"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+func.func private @op_func(%arg0: tensor<f32> {stablehlo.arg = "0"}) -> (tensor<f32> {stablehlo.result = "0"}) {
+  // CHECK:      "vhlo.func_v1"() <{
+  // CHECK-SAME:   arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"stablehlo.arg"> = #vhlo.string_v1<"0">}>]>,
+  // CHECK-SAME:   function_type = #vhlo.type_v1<!vhlo.func_v1<(!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>>>,
+  // CHECK-SAME:   res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"stablehlo.result"> = #vhlo.string_v1<"0">}>]>,
+  // CHECK-SAME:   sym_name = #vhlo.string_v1<"op_func">,
+  // CHECK-SAME:   sym_visibility = #vhlo.string_v1<"private">
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG0:.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : () -> ()
+
+  func.return %arg0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_gather(%arg0 : tensor<2x4x9xf32>, %arg1 : tensor<1x5x2xi32>) -> tensor<1x5x1xf32> {
+  //      CHECK: "vhlo.gather_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   slice_sizes = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x1x!vhlo.f32_v1>
+  %0 = "stablehlo.gather"(%arg0, %arg1) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [0, 1],
+      start_index_map = [0, 1],
+      index_vector_dim = 2
+    >,
+    slice_sizes = array<i64: 1, 1, 1>,
+    indices_are_sorted = true
+  } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
+  func.return %0 : tensor<1x5x1xf32>
+}
+
+// CHECK-LABEL: "op_gather_with_batching_dims"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_gather_with_batching_dims(%arg0 : tensor<5x2x4x9xf32>, %arg1 : tensor<1x5x2xi32>) -> tensor<1x5x1xf32> {
+  //      CHECK: "vhlo.gather_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   slice_sizes = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<5x2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x1x!vhlo.f32_v1>
+  %0 = "stablehlo.gather"(%arg0, %arg1) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [1, 2],
+      operand_batching_dims = [0],
+      start_indices_batching_dims = [1],
+      start_index_map = [1, 2],
+      index_vector_dim = 2
+    >,
+    slice_sizes = array<i64: 1, 1, 1, 1>,
+    indices_are_sorted = true
+  } : (tensor<5x2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
+  func.return %0 : tensor<1x5x1xf32>
+}
+
+// CHECK-LABEL: "op_get_dimension_size"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_get_dimension_size(%arg0: tensor<?xf32>) -> tensor<i32> {
+  //      CHECK: "vhlo.get_dimension_size_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<?x!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.get_dimension_size"(%arg0) {
+    dimension = 0 : i64
+  } : (tensor<?xf32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_get_tuple_element"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_get_tuple_element(%arg0: tuple<tensor<f32>, tensor<i32>>) -> tensor<f32> {
+  //      CHECK: "vhlo.get_tuple_element_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   index = #vhlo.integer_v1<0 : i32>
+  // CHECK-SAME: }> : (!vhlo.tuple_v1<!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.get_tuple_element"(%arg0) {
+    index = 0 : i32
+  } : (tuple<tensor<f32>, tensor<i32>>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_if"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_if(%arg0: tensor<i1>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.if_v1"(%[[ARG0]]) ({
+  // CHECK-NEXT:   "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }, {
+  // CHECK-NEXT:   "vhlo.return_v1"(%[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.if"(%arg0) ({
+    "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+  }, {
+    "stablehlo.return"(%arg2) : (tensor<f32>) -> ()
+  }) : (tensor<i1>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_imag"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_imag(%arg0: tensor<complex<f32>>) -> tensor<f32> {
+  // CHECK: "vhlo.imag_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.imag"(%arg0) : (tensor<complex<f32>>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_infeed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_infeed(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //               CHECK: "vhlo.infeed_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   infeed_config = #vhlo.string_v1<"foo">,
+  // CHECK-SAME{LITERAL}:   layout = #vhlo.array_v1<[#vhlo.array_v1<[]>]>
+  //          CHECK-SAME: }> : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  %0:2 = "stablehlo.infeed"(%arg0) {
+    infeed_config = "foo",
+    layout = [[]]
+  } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
+
+// CHECK-LABEL: "op_iota"
+func.func @op_iota() -> tensor<16xf32> {
+  //      CHECK: "vhlo.iota_v1"() <{
+  // CHECK-SAME:   iota_dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : () -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.iota"() {
+    iota_dimension = 0 : i64
+  } : () -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_is_finite"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_is_finite(%arg0: tensor<f32>) -> tensor<i1> {
+  // CHECK: "vhlo.is_finite_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.is_finite"(%arg0) : (tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_log"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_log(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.log_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.log"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_log_plus_one"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_log_plus_one(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.log_plus_one_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.log_plus_one"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_logistic"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_logistic(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.logistic_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.logistic"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_map"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_map(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.map_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.abs_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.map"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>):
+      %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    dimensions = array<i64: 0>
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_maximum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_maximum(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.maximum_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.maximum"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_minimum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_minimum(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.minimum_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.minimum"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_multiply"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_multiply(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.multiply_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.multiply"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_negate"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_negate(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.negate_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.negate"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_not"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_not(%arg0: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.not_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.not"(%arg0) : (tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_optimization_barrier"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_optimization_barrier(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.optimization_barrier_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.optimization_barrier"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_or"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_or(%arg0: tensor<i1>, %arg1: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.or_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.or"(%arg0, %arg1) : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_outfeed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_outfeed(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.outfeed_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   outfeed_config = #vhlo.string_v1<"foo">
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.outfeed"(%arg0, %arg1) {
+    outfeed_config = "foo"
+  } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_pad"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_pad(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.pad_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   edge_padding_high = #vhlo.tensor_v1<dense<4> : tensor<1xi64>>,
+  // CHECK-SAME:   edge_padding_low = #vhlo.tensor_v1<dense<4> : tensor<1xi64>>,
+  // CHECK-SAME:   interior_padding = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.pad"(%arg0, %arg1) {
+    edge_padding_high = array<i64: 4>,
+    edge_padding_low = array<i64: 4>,
+    interior_padding = array<i64: 0>
+  } : (tensor<8xf32>, tensor<f32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_popcnt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_popcnt(%arg0: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.popcnt_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.popcnt"(%arg0) : (tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_power"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_power(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.power_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.power"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_real_dynamic_slice"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}}, %[[ARG3:.*]]: {{.*}})
+func.func @op_real_dynamic_slice(%arg0: tensor<?xf32>, %arg1: tensor<1xindex>, %arg2: tensor<1xindex>, %arg3: tensor<1xindex>) -> tensor<?xf32> {
+  // CHECK: "vhlo.real_dynamic_slice_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]], %[[ARG3]]) : (!vhlo.tensor_v1<?x!vhlo.f32_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x!vhlo.f32_v1>
+  %0 = "stablehlo.real_dynamic_slice"(%arg0, %arg1, %arg2, %arg3) : (tensor<?xf32>, tensor<1xindex>, tensor<1xindex>, tensor<1xindex>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// CHECK-LABEL: "op_real"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_real(%arg0: tensor<complex<f32>>) -> tensor<f32> {
+  // CHECK: "vhlo.real_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.real"(%arg0) : (tensor<complex<f32>>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_recv_no_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_recv_no_source_target_pairs(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //      CHECK: "vhlo.recv_v2"(%[[ARG0]]) <{
+  // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   channel_type = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<> : tensor<0xi64>
+  // CHECK-SAME{LITERAL}: }> : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  %0:2 = "stablehlo.recv"(%arg0) {
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 3>,
+    is_host_transfer = true
+  } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
+
+// CHECK-LABEL: "op_reduce"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_reduce(%arg0: tensor<16xf32>, %arg1: tensor<f32>) -> tensor<f32> {
+  //  CHECK: "vhlo.reduce_v1"(%[[ARG0]], %[[ARG1]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.reduce"(%arg0, %arg1) ({
+    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
+      %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    dimensions = array<i64: 0>
+  } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_reduce_precision"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_reduce_precision(%arg0: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.reduce_precision_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   exponent_bits = #vhlo.integer_v1<8 : i32>
+  // CHECK-SAME:   mantissa_bits = #vhlo.integer_v1<10 : i32>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_precision"(%arg0) {
+    exponent_bits = 8 : i32,
+    mantissa_bits = 10 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK_lABEL: "op_reduce_with_promotable_types"
+func.func @op_reduce_with_promotable_types(%arg0: tensor<4x4xf32>, %arg1 : tensor<f32>)
+    -> (tensor<4xf64>) {
+  //  CHECK: "vhlo.reduce_v1"(%[[ARG0:.*]], %[[ARG1:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<4x4x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<4x!vhlo.f64_v1>
+  %0 = "stablehlo.reduce"(%arg0, %arg1) ({
+  ^bb0(%arg2: tensor<f64>, %arg3: tensor<f64> ):
+    %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
+    "stablehlo.return"(%1) : (tensor<f64>) -> ()
+
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+
+  func.return %0: tensor<4xf64>
+}
+
+// CHECK-LABEL: "op_reduce_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_reduce_scatter(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //               CHECK: "vhlo.reduce_scatter_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   scatter_dimension = #vhlo.integer_v1<0 : i64>
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<true>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_scatter"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.add"(%arg1, %arg2) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension = 0 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+    use_global_device_ids
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK_lABEL: "op_reduce_scatter_with_promotable_types"
+func.func @op_reduce_scatter_with_promotable_types(%data: tensor<4x16xf32>) -> tensor<4x4xf64> {
+  //  CHECK: "vhlo.reduce_scatter_v1"(%[[ARG0:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<4x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<4x4x!vhlo.f64_v1>
+  %0 = "stablehlo.reduce_scatter"(%data) ({
+    ^bb0(%arg2: tensor<f64>, %arg3: tensor<f64>):
+    %1 = stablehlo.add %arg2, %arg3 : tensor<f64>
+    "stablehlo.return"(%1) : (tensor<f64>) -> ()
+  }) {replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>,
+      scatter_dimension = 1 : i64,
+      channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+      use_global_device_ids} : (tensor<4x16xf32>) -> tensor<4x4xf64>
+  func.return %0 : tensor<4x4xf64>
+}
+
+
+// CHECK-LABEL: "op_reduce_window"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_reduce_window(%arg0: tensor<2x17x31x7xf32>, %arg1: tensor<f32>) -> tensor<2x9x16x7xf32> {
+  //               CHECK: "vhlo.reduce_window_v1"(%[[ARG0]], %[[ARG1]]) <{
+  //          CHECK-SAME:   base_dilations = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  // CHECK-SAME{LITERAL}:   padding = #vhlo.tensor_v1<dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>>,
+  //          CHECK-SAME:   window_dilations = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  //          CHECK-SAME:   window_dimensions = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  //          CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<[1, 4, 4, 1]> : tensor<4xi64>>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.maximum_v1"(%[[ARG2]], %[[ARG3]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<2x17x31x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x9x16x7x!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_window"(%arg0, %arg1) ({
+    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
+      %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
+    padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
+  } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
+  func.return %0 : tensor<2x9x16x7xf32>
+}
+
+// CHECK-LABEL: "op_reduce_window_with_promotable_types"
+func.func @op_reduce_window_with_promotable_types(%arg0: tensor<4x2xf32>,
+    %arg1: tensor<4x2xf32>, %init0: tensor<f32>, %init1: tensor<f32>) ->
+    (tensor<2x2xf64>, tensor<2x2xf32>) {
+  //  CHECK: "vhlo.reduce_window_v1"(%[[ARG0:.*]], %[[ARG1:.*]], %[[ARG2:.*]], %[[ARG3:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG4:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]], %[[VAL2:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<4x2x!vhlo.f32_v1>, !vhlo.tensor_v1<4x2x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> (!vhlo.tensor_v1<2x2x!vhlo.f64_v1>, !vhlo.tensor_v1<2x2x!vhlo.f32_v1>)
+  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
+         ^bb0(%a0: tensor<f64>, %a1: tensor<f32>, %b0: tensor<f64>,
+                %b1: tensor<f32>):
+              %2 = stablehlo.add %a0, %b0 : tensor<f64>
+              %3 = stablehlo.add %a1, %b1 : tensor<f32>
+              "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
+            })
+         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
+         window_dimensions = array<i64: 5, 1>,
+         window_strides = array<i64: 3, 1> }
+         : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
+              (tensor<2x2xf64>, tensor<2x2xf32>)
+  func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
+}
+
+// CHECK-LABEL: "op_remainder"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_remainder(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.remainder_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.remainder"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_replica_id"
+func.func @op_replica_id() -> tensor<ui32> {
+  // CHECK: "vhlo.replica_id_v1"() : () -> !vhlo.tensor_v1<!vhlo.ui32_v1>
+  %0 = "stablehlo.replica_id"() : () -> tensor<ui32>
+  func.return %0 : tensor<ui32>
+}
+
+// CHECK-LABEL: "op_partition_id"
+func.func @op_partition_id() -> tensor<ui32> {
+  // CHECK: "vhlo.partition_id_v1"() : () -> !vhlo.tensor_v1<!vhlo.ui32_v1>
+  %0 = "stablehlo.partition_id"() : () -> tensor<ui32>
+  func.return %0 : tensor<ui32>
+}
+
+// CHECK-LABEL: "op_reshape"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_reshape(%arg0: tensor<16xf32>) -> tensor<4x4xf32> {
+  // CHECK: "vhlo.reshape_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<4x4x!vhlo.f32_v1>
+  %0 = "stablehlo.reshape"(%arg0) : (tensor<16xf32>) -> tensor<4x4xf32>
+  func.return %0 : tensor<4x4xf32>
+}
+
+// CHECK-LABEL: "op_return"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_return(%arg0: tensor<i32>, %arg1: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.case_v1"(%[[ARG0]]) ({
+  // CHECK-NEXT:   "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.case"(%arg0) ({
+    "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+  }) : (tensor<i32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_reverse"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_reverse(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.reverse_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.reverse"(%arg0) {
+    dimensions = array<i64: 0>
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_rng_bit_generator"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_rng_bit_generator(%arg0: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  //      CHECK: "vhlo.rng_bit_generator_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   rng_algorithm = #vhlo<rng_algorithm_v1 PHILOX>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>)
+  %0:2 = "stablehlo.rng_bit_generator"(%arg0) {
+    rng_algorithm = #stablehlo<rng_algorithm PHILOX>
+  } : (tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "op_rng"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_rng(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<0xindex>) -> tensor<f32> {
+  //      CHECK: "vhlo.rng_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   rng_distribution = #vhlo<rng_distribution_v1 NORMAL>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<0x!vhlo.index_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.rng"(%arg0, %arg1, %arg2) {
+    rng_distribution = #stablehlo<rng_distribution NORMAL>
+  } : (tensor<f32>, tensor<f32>, tensor<0xindex>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_round_nearest_afz"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_round_nearest_afz(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.round_nearest_afz_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.round_nearest_afz"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_round_nearest_even"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_round_nearest_even(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.round_nearest_even_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.round_nearest_even"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_rsqrt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_rsqrt(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.rsqrt_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.rsqrt"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_scatter(%arg0: tensor<200x100x300xf32>, %arg1: tensor<10x2xi32>, %arg2: tensor<10x300xf32>) -> tensor<200x100x300xf32> {
+  //      CHECK: "vhlo.scatter_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   unique_indices = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   update_window_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG4:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG3]], %[[ARG4]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>, !vhlo.tensor_v1<10x2x!vhlo.i32_v1>, !vhlo.tensor_v1<10x300x!vhlo.f32_v1>) -> !vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>
+  %0 = "stablehlo.scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension_numbers = #stablehlo.scatter<
+      update_window_dims = [1],
+      inserted_window_dims = [0, 1],
+      scatter_dims_to_operand_dims = [0, 1],
+      index_vector_dim = 1
+    >,
+    indices_are_sorted = true,
+    unique_indices = true
+  } : (tensor<200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) -> tensor<200x100x300xf32>
+  func.return %0 : tensor<200x100x300xf32>
+}
+
+// CHECK-LABEL: "op_scatter_with_batching_dims"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_scatter_with_batching_dims(%arg0: tensor<10x200x100x300xf32>, %arg1: tensor<10x2xi32>, %arg2: tensor<10x300xf32>) -> tensor<10x200x100x300xf32> {
+  //      CHECK: "vhlo.scatter_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   input_batching_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   inserted_window_dims = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_indices_batching_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   unique_indices = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   update_window_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG4:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG3]], %[[ARG4]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<10x200x100x300x!vhlo.f32_v1>, !vhlo.tensor_v1<10x2x!vhlo.i32_v1>, !vhlo.tensor_v1<10x300x!vhlo.f32_v1>) -> !vhlo.tensor_v1<10x200x100x300x!vhlo.f32_v1>
+  %0 = "stablehlo.scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension_numbers = #stablehlo.scatter<
+      update_window_dims = [1],
+      inserted_window_dims = [1, 2],
+      input_batching_dims = [0],
+      scatter_dims_to_operand_dims = [1, 2],
+      scatter_indices_batching_dims = [0],
+      index_vector_dim = 1
+    >,
+    indices_are_sorted = true,
+    unique_indices = true
+  } : (tensor<10x200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) -> tensor<10x200x100x300xf32>
+  func.return %0 : tensor<10x200x100x300xf32>
+}
+
+// CHECK_lABEL: "op_scatter_with_promotable_types"
+func.func @op_scatter_with_promotable_types(%input_tensor: tensor<200x100x300xf32>,
+    %scatter_indices: tensor<10x2xi32>, %updates: tensor<10x300xf32>) ->
+      tensor<200x100x300xf64> {
+  //  CHECK: "vhlo.scatter_v2"(%[[ARG0:.*]], %[[ARG1:.*]], %[[ARG2:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>, !vhlo.tensor_v1<10x2x!vhlo.i32_v1>, !vhlo.tensor_v1<10x300x!vhlo.f32_v1>) -> !vhlo.tensor_v1<200x100x300x!vhlo.f64_v1>
+  %0 = "stablehlo.scatter" (%input_tensor, %scatter_indices, %updates) ({
+  ^bb0(%lhs: tensor<f64>, %rhs: tensor<f64>):
+    %add = stablehlo.add %lhs, %rhs : tensor<f64>
+    "stablehlo.return"(%add) : (tensor<f64>) -> ()
+  }) {
+    scatter_dimension_numbers = #stablehlo.scatter<
+      update_window_dims = [1],
+      inserted_window_dims = [0, 1],
+      scatter_dims_to_operand_dims = [0, 1],
+      index_vector_dim = 1
+    >,
+    indices_are_sorted = true,
+    unique_indices = true
+  } : (tensor<200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) ->
+      tensor<200x100x300xf64>
+  func.return %0 : tensor<200x100x300xf64>
+}
+
+// CHECK-LABEL: "op_select_and_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_select_and_scatter(%arg0: tensor<10x24x24x64xf32>, %arg1: tensor<12x13x13x66xf32>, %arg2: tensor<f32>) -> tensor<10x24x24x64xf32> {
+  //      CHECK: "vhlo.select_and_scatter_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   padding = #vhlo.tensor_v1<dense<1> : tensor<4x2xi64>>,
+  // CHECK-SAME:   window_dimensions = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG31:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG41:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL11:.*]] = "vhlo.compare_v1"(%[[ARG31]], %[[ARG41]]) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL11]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }, {
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG32:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG42:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL12:.*]] = "vhlo.add_v1"(%[[ARG32]], %[[ARG42]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL12]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>, !vhlo.tensor_v1<12x13x13x66x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>
+  %0 = "stablehlo.select_and_scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg3, %arg4) {compare_type = #stablehlo<comparison_type TOTALORDER>, comparison_direction = #stablehlo<comparison_direction GE>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }, {
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
+    padding = dense<1> : tensor<4x2xi64>
+  } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
+  func.return %0 : tensor<10x24x24x64xf32>
+}
+
+// CHECK-LABEL: "op_select_and_scatter_with_promotable_types"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_select_and_scatter_with_promotable_types(%arg0: tensor<10x24x24x64xf32>, %arg1: tensor<12x13x13x66xf32>, %arg2: tensor<f32>) -> tensor<10x24x24x64xf64> {
+  // CHECK: "vhlo.select_and_scatter_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]])
+  // CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  // CHECK:     %[[VAL:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>, !vhlo.tensor_v1<!vhlo.f64_v1>) -> !vhlo.tensor_v1<!vhlo.f64_v1>
+  // CHECK:     "vhlo.return_v1"(%[[VAL]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  // CHECK: }) : (!vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>, !vhlo.tensor_v1<12x13x13x66x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<10x24x24x64x!vhlo.f64_v1>
+  %0 = "stablehlo.select_and_scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg3, %arg4) {compare_type = #stablehlo<comparison_type TOTALORDER>, comparison_direction = #stablehlo<comparison_direction GE>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }, {
+    ^bb0(%arg3: tensor<f64>, %arg4: tensor<f64>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f64>, tensor<f64>) -> tensor<f64>
+      "stablehlo.return"(%1) : (tensor<f64>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
+    padding = dense<1> : tensor<4x2xi64>
+  } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf64>
+  func.return %0 : tensor<10x24x24x64xf64>
+}
+
+// CHECK-LABEL: "op_select"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_select(%arg0: tensor<i1>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.select_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.select"(%arg0, %arg1, %arg2) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_send_no_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_send_no_source_target_pairs(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.send_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   channel_type = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME{LITERAL}: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.send"(%arg0, %arg1) {
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 2>,
+    is_host_transfer = true
+  } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_set_dimension_size"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_set_dimension_size(%arg0: tensor<?xf32>, %arg1: tensor<i32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.set_dimension_size_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<?x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.set_dimension_size"(%arg0, %arg1) {
+    dimension = 0 : i64
+  } : (tensor<?xf32>, tensor<i32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_shift_left"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_shift_left(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.shift_left_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.shift_left"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_shift_right_arithmetic"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_shift_right_arithmetic(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.shift_right_arithmetic_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.shift_right_arithmetic"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_shift_right_logical"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_shift_right_logical(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.shift_right_logical_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.shift_right_logical"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_sign"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_sign(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.sign_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.sign"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_sine"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_sine(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.sine_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.sine"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_slice"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_slice(%arg0: tensor<16xf32>) -> tensor<4xf32> {
+  //      CHECK: "vhlo.slice_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   limit_indices = #vhlo.tensor_v1<dense<4> : tensor<1xi64>>,
+  // CHECK-SAME:   start_indices = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   strides = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<4x!vhlo.f32_v1>
+  %0 = "stablehlo.slice"(%arg0) {
+    start_indices = array<i64: 0>,
+    limit_indices = array<i64: 4>,
+    strides = array<i64: 1>
+  } : (tensor<16xf32>) -> tensor<4xf32>
+  func.return %0 : tensor<4xf32>
+}
+
+// CHECK-LABEL: "op_sort"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_sort(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.sort_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME:   is_stable = #vhlo.bool_v1<true>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.compare_v1"(%[[ARG1]], %[[ARG2]]) <{compare_type = #vhlo<comparison_type_v1 FLOAT>, comparison_direction = #vhlo<comparison_direction_v1 GT>}>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.sort"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg1, %arg2) {compare_type = #stablehlo<comparison_type FLOAT>, comparison_direction = #stablehlo<comparison_direction GT>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }) {
+    dimension = 0 : i64,
+    is_stable = true
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_sqrt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_sqrt(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.sqrt_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.sqrt"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_subtract"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_subtract(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.subtract_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.subtract"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_tan"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_tan(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.tan_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.tan"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_tanh"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_tanh(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.tanh_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.tanh"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_torch_index_select"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_torch_index_select(%arg0: tensor<5x1x5xf32>, %arg1: tensor<2xi32>) ->  tensor<2x1x5xf32> {
+  //      CHECK: "vhlo.torch_index_select_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   batch_dims = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME:   dim = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<5x1x5x!vhlo.f32_v1>, !vhlo.tensor_v1<2x!vhlo.i32_v1>) -> !vhlo.tensor_v1<2x1x5x!vhlo.f32_v1>
+  %0 = "stablehlo.torch_index_select"(%arg0, %arg1) {
+    dim = 0 : i64,
+    batch_dims = 0 : i64
+  } : (tensor<5x1x5xf32>, tensor<2xi32>) -> tensor<2x1x5xf32>
+  func.return %0 : tensor<2x1x5xf32>
+}
+
+// CHECK-LABEL: "op_transpose"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_transpose(%arg0: tensor<16x8xf32>) ->  tensor<8x16xf32> {
+  //      CHECK: "vhlo.transpose_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x16x!vhlo.f32_v1>
+  %0 = "stablehlo.transpose"(%arg0) {
+    permutation = array<i64: 1, 0>
+  } : (tensor<16x8xf32>) -> tensor<8x16xf32>
+  func.return %0 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: "op_triangular_solve"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_triangular_solve(%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) ->  tensor<16x16xf32> {
+  //      CHECK: "vhlo.triangular_solve_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   left_side = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   lower = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   transpose_a = #vhlo<transpose_v1 NO_TRANSPOSE>,
+  // CHECK-SAME:   unit_diagonal = #vhlo.bool_v1<true>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.triangular_solve"(%arg0, %arg1) {
+    left_side = true,
+    lower = true,
+    unit_diagonal = true,
+    transpose_a = #stablehlo<transpose NO_TRANSPOSE>
+  } : (tensor<16x16xf32>, tensor<16x16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "op_tuple"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_tuple(%arg0: tensor<f32>) -> tuple<tensor<f32>> {
+  // CHECK: "vhlo.tuple_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tuple_v1<!vhlo.tensor_v1<!vhlo.f32_v1>>
+  %0 = "stablehlo.tuple"(%arg0) : (tensor<f32>) -> tuple<tensor<f32>>
+  func.return %0 : tuple<tensor<f32>>
+}
+
+// CHECK-LABEL: "op_unary_einsum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_unary_einsum(%arg0: tensor<8x16xf32>) -> tensor<8xf32> {
+  //      CHECK: "vhlo.unary_einsum_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   einsum_config = #vhlo.string_v1<"ab->a">
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x!vhlo.f32_v1>
+  %0 = "stablehlo.unary_einsum"(%arg0) {
+    einsum_config = "ab->a"
+  } : (tensor<8x16xf32>) -> tensor<8xf32>
+  func.return %0 : tensor<8xf32>
+}
+
+// CHECK-LABEL: "op_uniform_dequantize"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_uniform_dequantize(%arg0: tensor<!quant.uniform<i8:f32, 34.0:16>>) -> tensor<f32> {
+  // CHECK: "vhlo.uniform_dequantize_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.uniform_dequantize"(%arg0) : (tensor<!quant.uniform<i8:f32, 34.0:16>>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_uniform_quantize"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_uniform_quantize(%arg0: tensor<f32>) -> tensor<!quant.uniform<i8:f32, 34.0:16>> {
+  // CHECK: "vhlo.uniform_quantize_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>
+  %0 = "stablehlo.uniform_quantize"(%arg0) : (tensor<f32>) -> tensor<!quant.uniform<i8:f32, 34.0:16>>
+  func.return %0 : tensor<!quant.uniform<i8:f32, 34.0:16>>
+}
+
+// CHECK-LABEL: "op_while"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_while(%arg0: tensor<i1>) -> tensor<i1> {
+  //      CHECK: "vhlo.while_v1"(%[[ARG0]]) ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.bool_v1>):
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT:   }, {
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.bool_v1>)
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.while"(%arg0) ({
+    ^bb0(%arg1: tensor<i1>):
+      "stablehlo.return"(%arg1) : (tensor<i1>) -> ()
+    }, {
+    ^bb0(%arg1: tensor<i1>):
+      "stablehlo.return"(%arg1) : (tensor<i1>) -> ()
+  }) : (tensor<i1>) -> tensor<i1>
+  func.return %0: tensor<i1>
+}
+
+// CHECK-LABEL: "op_xor"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_xor(%arg0: tensor<i1>, %arg1: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.xor_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.xor"(%arg0, %arg1) : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// ============ TYPES ============
+
+// CHECK-LABEL: "type_i1"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i1(%arg0: tensor<i1>, %arg1: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.and_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.and"(%arg0, %arg1) : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "type_i2"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i2(%arg0: tensor<i2>, %arg1: tensor<i2>) -> tensor<i2> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i2_v1>, !vhlo.tensor_v1<!vhlo.i2_v1>) -> !vhlo.tensor_v1<!vhlo.i2_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i2>, tensor<i2>) -> tensor<i2>
+  func.return %0 : tensor<i2>
+}
+
+// CHECK-LABEL: "type_i4"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i4(%arg0: tensor<i4>, %arg1: tensor<i4>) -> tensor<i4> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i4_v1>, !vhlo.tensor_v1<!vhlo.i4_v1>) -> !vhlo.tensor_v1<!vhlo.i4_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i4>, tensor<i4>) -> tensor<i4>
+  func.return %0 : tensor<i4>
+}
+
+// CHECK-LABEL: "type_i8"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i8(%arg0: tensor<i8>, %arg1: tensor<i8>) -> tensor<i8> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i8_v1>, !vhlo.tensor_v1<!vhlo.i8_v1>) -> !vhlo.tensor_v1<!vhlo.i8_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i8>, tensor<i8>) -> tensor<i8>
+  func.return %0 : tensor<i8>
+}
+
+// CHECK-LABEL: "type_i16"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i16(%arg0: tensor<i16>, %arg1: tensor<i16>) -> tensor<i16> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i16_v1>, !vhlo.tensor_v1<!vhlo.i16_v1>) -> !vhlo.tensor_v1<!vhlo.i16_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i16>, tensor<i16>) -> tensor<i16>
+  func.return %0 : tensor<i16>
+}
+
+// CHECK-LABEL: "type_i32"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i32(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "type_i64"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i64(%arg0: tensor<i64>, %arg1: tensor<i64>) -> tensor<i64> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i64_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<!vhlo.i64_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>
+  func.return %0 : tensor<i64>
+}
+
+// CHECK-LABEL: "type_ui2"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui2(%arg0: tensor<ui2>, %arg1: tensor<ui2>) -> tensor<ui2> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui2_v1>, !vhlo.tensor_v1<!vhlo.ui2_v1>) -> !vhlo.tensor_v1<!vhlo.ui2_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui2>, tensor<ui2>) -> tensor<ui2>
+  func.return %0 : tensor<ui2>
+}
+
+// CHECK-LABEL: "type_ui4"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui4(%arg0: tensor<ui4>, %arg1: tensor<ui4>) -> tensor<ui4> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui4_v1>, !vhlo.tensor_v1<!vhlo.ui4_v1>) -> !vhlo.tensor_v1<!vhlo.ui4_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui4>, tensor<ui4>) -> tensor<ui4>
+  func.return %0 : tensor<ui4>
+}
+
+// CHECK-LABEL: "type_ui8"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui8(%arg0: tensor<ui8>, %arg1: tensor<ui8>) -> tensor<ui8> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui8_v1>, !vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<!vhlo.ui8_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui8>, tensor<ui8>) -> tensor<ui8>
+  func.return %0 : tensor<ui8>
+}
+
+// CHECK-LABEL: "type_ui16"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui16(%arg0: tensor<ui16>, %arg1: tensor<ui16>) -> tensor<ui16> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui16_v1>, !vhlo.tensor_v1<!vhlo.ui16_v1>) -> !vhlo.tensor_v1<!vhlo.ui16_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui16>, tensor<ui16>) -> tensor<ui16>
+  func.return %0 : tensor<ui16>
+}
+
+// CHECK-LABEL: "type_ui32"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui32(%arg0: tensor<ui32>, %arg1: tensor<ui32>) -> tensor<ui32> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui32_v1>, !vhlo.tensor_v1<!vhlo.ui32_v1>) -> !vhlo.tensor_v1<!vhlo.ui32_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui32>, tensor<ui32>) -> tensor<ui32>
+  func.return %0 : tensor<ui32>
+}
+
+// CHECK-LABEL: "type_ui64"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui64(%arg0: tensor<ui64>, %arg1: tensor<ui64>) -> tensor<ui64> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui64_v1>, !vhlo.tensor_v1<!vhlo.ui64_v1>) -> !vhlo.tensor_v1<!vhlo.ui64_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui64>, tensor<ui64>) -> tensor<ui64>
+  func.return %0 : tensor<ui64>
+}
+
+// CHECK-LABEL: "type_f4E2M1FN"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f4E2M1FN(%arg0: tensor<f4E2M1FN>, %arg1: tensor<f4E2M1FN>) -> tensor<f4E2M1FN> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f4E2M1FN_v1>, !vhlo.tensor_v1<!vhlo.f4E2M1FN_v1>) -> !vhlo.tensor_v1<!vhlo.f4E2M1FN_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f4E2M1FN>, tensor<f4E2M1FN>) -> tensor<f4E2M1FN>
+  func.return %0 : tensor<f4E2M1FN>
+}
+
+// CHECK-LABEL: "type_f6E2M3FN"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f6E2M3FN(%arg0: tensor<f6E2M3FN>, %arg1: tensor<f6E2M3FN>) -> tensor<f6E2M3FN> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f6E2M3FN_v1>, !vhlo.tensor_v1<!vhlo.f6E2M3FN_v1>) -> !vhlo.tensor_v1<!vhlo.f6E2M3FN_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f6E2M3FN>, tensor<f6E2M3FN>) -> tensor<f6E2M3FN>
+  func.return %0 : tensor<f6E2M3FN>
+}
+
+// CHECK-LABEL: "type_f6E3M2FN"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f6E3M2FN(%arg0: tensor<f6E3M2FN>, %arg1: tensor<f6E3M2FN>) -> tensor<f6E3M2FN> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f6E3M2FN_v1>, !vhlo.tensor_v1<!vhlo.f6E3M2FN_v1>) -> !vhlo.tensor_v1<!vhlo.f6E3M2FN_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f6E3M2FN>, tensor<f6E3M2FN>) -> tensor<f6E3M2FN>
+  func.return %0 : tensor<f6E3M2FN>
+}
+
+// CHECK-LABEL: "type_f8E3M4"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E3M4(%arg0: tensor<f8E3M4>, %arg1: tensor<f8E3M4>) -> tensor<f8E3M4> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E3M4_v1>, !vhlo.tensor_v1<!vhlo.f8E3M4_v1>) -> !vhlo.tensor_v1<!vhlo.f8E3M4_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E3M4>, tensor<f8E3M4>) -> tensor<f8E3M4>
+  func.return %0 : tensor<f8E3M4>
+}
+
+// CHECK-LABEL: "type_f8E4M3"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E4M3(%arg0: tensor<f8E4M3>, %arg1: tensor<f8E4M3>) -> tensor<f8E4M3> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E4M3_v1>, !vhlo.tensor_v1<!vhlo.f8E4M3_v1>) -> !vhlo.tensor_v1<!vhlo.f8E4M3_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E4M3>, tensor<f8E4M3>) -> tensor<f8E4M3>
+  func.return %0 : tensor<f8E4M3>
+}
+
+// CHECK-LABEL: "type_f8E4M3FN"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E4M3FN(%arg0: tensor<f8E4M3FN>, %arg1: tensor<f8E4M3FN>) -> tensor<f8E4M3FN> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E4M3FN_v1>, !vhlo.tensor_v1<!vhlo.f8E4M3FN_v1>) -> !vhlo.tensor_v1<!vhlo.f8E4M3FN_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E4M3FN>, tensor<f8E4M3FN>) -> tensor<f8E4M3FN>
+  func.return %0 : tensor<f8E4M3FN>
+}
+
+// CHECK-LABEL: "type_f8E5M2"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E5M2(%arg0: tensor<f8E5M2>, %arg1: tensor<f8E5M2>) -> tensor<f8E5M2> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E5M2_v1>, !vhlo.tensor_v1<!vhlo.f8E5M2_v1>) -> !vhlo.tensor_v1<!vhlo.f8E5M2_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E5M2>, tensor<f8E5M2>) -> tensor<f8E5M2>
+  func.return %0 : tensor<f8E5M2>
+}
+
+// CHECK-LABEL: "type_f8E4M3FNUZ"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E4M3FNUZ(%arg0: tensor<f8E4M3FNUZ>, %arg1: tensor<f8E4M3FNUZ>) -> tensor<f8E4M3FNUZ> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E4M3FNUZ_v1>, !vhlo.tensor_v1<!vhlo.f8E4M3FNUZ_v1>) -> !vhlo.tensor_v1<!vhlo.f8E4M3FNUZ_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E4M3FNUZ>, tensor<f8E4M3FNUZ>) -> tensor<f8E4M3FNUZ>
+  func.return %0 : tensor<f8E4M3FNUZ>
+}
+
+// CHECK-LABEL: "type_f8E4M3B11FNUZ"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E4M3B11FNUZ(%arg0: tensor<f8E4M3B11FNUZ>, %arg1: tensor<f8E4M3B11FNUZ>) -> tensor<f8E4M3B11FNUZ> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E4M3B11FNUZ_v1>, !vhlo.tensor_v1<!vhlo.f8E4M3B11FNUZ_v1>) -> !vhlo.tensor_v1<!vhlo.f8E4M3B11FNUZ_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E4M3B11FNUZ>, tensor<f8E4M3B11FNUZ>) -> tensor<f8E4M3B11FNUZ>
+  func.return %0 : tensor<f8E4M3B11FNUZ>
+}
+
+// CHECK-LABEL: "type_f8E5M2FNUZ"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E5M2FNUZ(%arg0: tensor<f8E5M2FNUZ>, %arg1: tensor<f8E5M2FNUZ>) -> tensor<f8E5M2FNUZ> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E5M2FNUZ_v1>, !vhlo.tensor_v1<!vhlo.f8E5M2FNUZ_v1>) -> !vhlo.tensor_v1<!vhlo.f8E5M2FNUZ_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E5M2FNUZ>, tensor<f8E5M2FNUZ>) -> tensor<f8E5M2FNUZ>
+  func.return %0 : tensor<f8E5M2FNUZ>
+}
+
+// CHECK-LABEL: "type_f8E8M0FNU"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E8M0FNU(%arg0: tensor<f8E8M0FNU>, %arg1: tensor<f8E8M0FNU>) -> tensor<f8E8M0FNU> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E8M0FNU_v1>, !vhlo.tensor_v1<!vhlo.f8E8M0FNU_v1>) -> !vhlo.tensor_v1<!vhlo.f8E8M0FNU_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E8M0FNU>, tensor<f8E8M0FNU>) -> tensor<f8E8M0FNU>
+  func.return %0 : tensor<f8E8M0FNU>
+}
+
+// CHECK-LABEL: "type_bf16"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_bf16(%arg0: tensor<bf16>, %arg1: tensor<bf16>) -> tensor<bf16> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
+  func.return %0 : tensor<bf16>
+}
+
+// CHECK-LABEL: "type_f16"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f16(%arg0: tensor<f16>, %arg1: tensor<f16>) -> tensor<f16> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f16_v1>, !vhlo.tensor_v1<!vhlo.f16_v1>) -> !vhlo.tensor_v1<!vhlo.f16_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f16>, tensor<f16>) -> tensor<f16>
+  func.return %0 : tensor<f16>
+}
+
+// CHECK-LABEL: "type_f32"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f32(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "type_f64"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f64(%arg0: tensor<f64>, %arg1: tensor<f64>) -> tensor<f64> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>, !vhlo.tensor_v1<!vhlo.f64_v1>) -> !vhlo.tensor_v1<!vhlo.f64_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f64>, tensor<f64>) -> tensor<f64>
+  func.return %0 : tensor<f64>
+}
+
+// CHECK-LABEL: "type_complex_f32"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_complex_f32(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>) -> tensor<complex<f32>> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>, !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>) -> !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<complex<f32>>, tensor<complex<f32>>) -> tensor<complex<f32>>
+  func.return %0 : tensor<complex<f32>>
+}
+
+// CHECK-LABEL: "type_complex_f64"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_complex_f64(%arg0: tensor<complex<f64>>, %arg1: tensor<complex<f64>>) -> tensor<complex<f64>> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f64_v1>>, !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f64_v1>>) -> !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f64_v1>>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<complex<f64>>, tensor<complex<f64>>) -> tensor<complex<f64>>
+  func.return %0 : tensor<complex<f64>>
+}
+
+// CHECK-LABEL: "type_tf32"
+// CHECK: #vhlo.type_v1<!vhlo.tf31_v1>
+func.func @type_tf32() attributes {stablehlo.attr = tf32 } {
+  return
+}
+
+// CHECK-LABEL: "type_none"
+// CHECK: #vhlo.type_v1<!vhlo.none_v1>
+func.func @type_none() attributes {stablehlo.attr = none } {
+  return
+}
+
+// CHECK-LABEL: "type_dynamism_ranked"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_dynamism_ranked(%arg0: tensor<?xf32>) -> tensor<?xf32> {
+  // CHECK: "vhlo.abs_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<?x!vhlo.f32_v1>) -> !vhlo.tensor_v1<?x!vhlo.f32_v1>
+  %0 = "stablehlo.abs"(%arg0) : (tensor<?xf32>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// CHECK-LABEL: "type_per_tensor_quantization"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_per_tensor_quantization(%arg0: tensor<!quant.uniform<i8:f32, 34.0:16>>, %arg1: tensor<!quant.uniform<i8:f32, 34.0:16>>) -> tensor<!quant.uniform<i8:f32, 34.0:16>> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>, !vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>) -> !vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<!quant.uniform<i8:f32, 34.0:16>>, tensor<!quant.uniform<i8:f32, 34.0:16>>) -> tensor<!quant.uniform<i8:f32, 34.0:16>>
+  func.return %0 : tensor<!quant.uniform<i8:f32, 34.0:16>>
+}
+
+// CHECK-LABEL: "type_per_axis_quantization"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_per_axis_quantization(%arg0: tensor<2x!quant.uniform<i8:f32:0, {34.0:16, 34.0:16}>>) -> tensor<2x!quant.uniform<i8:f32:0, {34.0:16, 34.0:16}>> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG0]]) : (!vhlo.tensor_v1<2x!vhlo.quant_per_axis_v1<!vhlo.i8_v1:!vhlo.f32_v1, 0, [3.400000e+01, 3.400000e+01], [16, 16], -128:127, 1>>, !vhlo.tensor_v1<2x!vhlo.quant_per_axis_v1<!vhlo.i8_v1:!vhlo.f32_v1, 0, [3.400000e+01, 3.400000e+01], [16, 16], -128:127, 1>>) -> !vhlo.tensor_v1<2x!vhlo.quant_per_axis_v1<!vhlo.i8_v1:!vhlo.f32_v1, 0, [3.400000e+01, 3.400000e+01], [16, 16], -128:127, 1>>
+  %0 = stablehlo.add %arg0, %arg0 : tensor<2x!quant.uniform<i8:f32:0, {34.0:16, 34.0:16}>>
+  func.return %0 : tensor<2x!quant.uniform<i8:f32:0, {34.0:16, 34.0:16}>>
+}
+
+//       CHECK: function_type = #vhlo.type_v1<!vhlo.func_v1<(!vhlo.token_v1) -> !vhlo.token_v1>>
+// CHECK-LABEL: "type_token_callee"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_token_callee(%arg0: !stablehlo.token) -> !stablehlo.token {
+  // CHECK: "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.token_v1) -> ()
+  return %arg0 : !stablehlo.token
+}
+
+//       CHECK: function_type = #vhlo.type_v1<!vhlo.func_v1<(!vhlo.token_v1) -> !vhlo.token_v1>>
+// CHECK-LABEL: "type_token_caller"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_token_caller(%arg0: !stablehlo.token) -> !stablehlo.token {
+  // CHECK:      "vhlo.call_v1"(%[[ARG0]]) <{callee = #vhlo.string_v1<"type_token_callee">}
+  // CHECK-SAME: (!vhlo.token_v1) -> !vhlo.token_v1
+  %0 = func.call @type_token_callee(%arg0) : (!stablehlo.token) -> !stablehlo.token
+  return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "type_tuple"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_tuple(%arg0: tuple<tensor<f32>>) -> tuple<!stablehlo.token> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo"
+  // CHECK: (!vhlo.tuple_v1<!vhlo.tensor_v1<!vhlo.f32_v1>>) -> !vhlo.tuple_v1<!vhlo.token_v1>
+  } : (tuple<tensor<f32>>) -> tuple<!stablehlo.token>
+  return %0 : tuple<!stablehlo.token>
+}
+
+// CHECK-LABEL: type_buffer_function_input_output
+// CHECK-NEXT: (%[[ARG0:.*]]: !vhlo.buffer_v1<2x!vhlo.f32_v1>)
+func.func @type_buffer_function_input_output(%arg0: memref<2xf32>) -> memref<2xf32> {
+  // CHECK: "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.buffer_v1<2x!vhlo.f32_v1>) -> ()
+  func.return %arg0 : memref<2xf32>
+}
+
+// CHECK-LABEL: type_buffer_special_custom_calls
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_buffer_special_custom_calls(%arg0: tensor<2xf32>) -> tensor<2xf32> {
+  //               CHECK: %[[CALL0:.*]] = "vhlo.custom_call_v1"(%[[ARG0]])
+  //          CHECK-SAME: call_target_name = #vhlo.string_v1<"Pin">
+  //          CHECK-SAME: : (!vhlo.tensor_v1<2x!vhlo.f32_v1>) -> !vhlo.buffer_v1<2x!vhlo.f32_v1>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "Pin",
+    api_version = 4 : i32
+  } : (tensor<2xf32>) -> memref<2xf32>
+  //               CHECK: %{{.*}} = "vhlo.custom_call_v1"(%[[CALL0]])
+  //          CHECK-SAME: call_target_name = #vhlo.string_v1<"Unpin">
+  //          CHECK-SAME: : (!vhlo.buffer_v1<2x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x!vhlo.f32_v1>
+  %1 = "stablehlo.custom_call"(%0) {
+    call_target_name = "Unpin",
+    api_version = 4 : i32
+  } : (memref<2xf32>) -> tensor<2xf32>
+  func.return %1 : tensor<2xf32>
+}
+
+// ============ DEPENDENCIES  ============
+
+func.func @composite_target(%arg0: tensor<f32>) -> tensor<f32> {
+  return %arg0: tensor<f32>
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
@@ -416,6 +416,23 @@
   // CHECK: some.unregistered_attr
   %1 = stablehlo.cosine %arg0 {some.unregistered_attr = 1 : i32} : tensor<f32>
   return %1 : tensor<f32>
+}
+
+// Builtin attriubute tests
+
+// CHECK-LABEL: "byte_packed_boolean"
+func.func @byte_packed_boolean() -> (tensor<8xi1>, tensor<8xi1>, tensor<4xi1>, tensor<4xi1>, tensor<16xi1>) {
+  // CHECK: #vhlo.tensor_v1<dense<[true, false, false, false, false, false, false, false]
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<true> : tensor<8xi1>>
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<[true, false, false, false]> : tensor<4xi1>>
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<true> : tensor<4xi1>>
+  // CHECK-NEXT: #vhlo.tensor_v1<dense<[true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false]> : tensor<16xi1>>
+  %c = stablehlo.constant dense<[true, false, false, false, false, false, false, false]> : tensor<8xi1>
+  %c_0 = stablehlo.constant dense<true> : tensor<8xi1>
+  %c_1 = stablehlo.constant dense<[true, false, false, false]> : tensor<4xi1>
+  %c_2 = stablehlo.constant dense<true> : tensor<4xi1>
+  %c_3 = stablehlo.constant dense<[true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false]> : tensor<16xi1>
+  return %c, %c_0, %c_1, %c_2, %c_3 : tensor<8xi1>, tensor<8xi1>, tensor<4xi1>, tensor<4xi1>, tensor<16xi1>
 }
 
 // ============ DEFAULTS ============
@@ -553,7 +570,7 @@
 // CHECK-LABEL: "default_composite"
 // CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
 func.func @default_composite(%arg0: tensor<f32>) -> tensor<f32> {
-  //               CHECK: "vhlo.composite_v1"(%[[ARG0]]) <{
+  //               CHECK: "vhlo.composite_v2"(%[[ARG0]]) <{
   //          CHECK-SAME:   composite_attributes = #vhlo.dict_v1<{}>
   //          CHECK-SAME:   decomposition = #vhlo.string_v1<"composite_target">
   //          CHECK-SAME:   name = #vhlo.string_v1<"stablehlo.composite_target">
@@ -1271,7 +1288,7 @@
 // CHECK-LABEL: "op_composite"
 // CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
 func.func @op_composite(%arg0: tensor<f32>) -> tensor<f32> {
-  //               CHECK: "vhlo.composite_v1"(%[[ARG0]]) <{
+  //               CHECK: "vhlo.composite_v2"(%[[ARG0]]) <{
   //          CHECK-SAME:   composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"my_int"> = #vhlo.integer_v1<1 : i64>, #vhlo.string_v1<"my_string"> = #vhlo.string_v1<"foo">}>
   //          CHECK-SAME:   decomposition = #vhlo.string_v1<"composite_target">
   //          CHECK-SAME:   name = #vhlo.string_v1<"stablehlo.composite_target">
@@ -1282,9 +1299,32 @@
     decomposition = @composite_target,
     version = 1 : i32,
     composite_attributes = {
-      my_string = "foo",
-      my_int = 1 : i64
+      my_int = 1 : i64,
+      my_string = "foo"
     }
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "composite_regions"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @composite_regions(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.composite_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   composite_attributes = #vhlo.dict_v1<{}>
+  //          CHECK-SAME:   decomposition = #vhlo.string_v1<"composite_target">
+  //          CHECK-SAME:   name = #vhlo.string_v1<"stablehlo.composite_target">
+  //          CHECK-SAME:   version = #vhlo.integer_v1<1 : i32>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT: ^bb0(%[[ARG1:.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:   "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.composite"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>):
+      "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+  }) {
+    name = "stablehlo.composite_target",
+    decomposition = @composite_target,
+    version = 1 : i32
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade.1_13_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade.1_13_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade.1_13_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade.1_13_0.mlir
@@ -0,0 +1,22 @@
+// RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.13.0' %s | FileCheck %s
+
+// CompositeOp was changed in v1.14.0 to have regions.
+// Ensure that serializing for 1.13.0 uses composite_v1 (no regions).
+
+// CHECK-LABEL: vhlo.func_v1 @composite_op
+// CHECK-NEXT: "vhlo.composite_v1"(%arg0) <{
+// CHECK-NOT: regions
+// CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+func.func @composite_op(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.composite"(%arg0) {
+    name = "test.composite",
+    composite_attributes = {},
+    decomposition = @decomposition,
+    version = 1 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+func.func @decomposition(%arg0: tensor<f32>) -> tensor<f32> {
+  func.return %arg0 : tensor<f32>
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_18_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_18_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_18_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_18_0.mlir
@@ -2,7 +2,7 @@
 
 // expected-error @-3 {{failed to convert VHLO to v0.18.0}}
 func.func @composite(%arg0: tensor<f32>) -> tensor<f32> {
-  // expected-error @+1 {{failed to legalize operation 'vhlo.composite_v1' that was explicitly marked illegal}}
+  // expected-error @+1 {{failed to legalize operation 'vhlo.composite_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.composite"(%arg0) {
     name = "stablehlo.composite_target",
     decomposition = @composite_target
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_13_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_13_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_13_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_13_0.mlir
@@ -0,0 +1,23 @@
+// RUN: not stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.13.0' %s 2>&1 | FileCheck %s
+
+// CompositeOp with regions cannot be downgraded to v1.13.0 (v1).
+
+module {
+  func.func @composite_with_regions(%arg0: tensor<f32>) -> tensor<f32> {
+    // CHECK: failed to legalize operation 'vhlo.composite_v2'
+    %0 = "stablehlo.composite"(%arg0) ({
+      ^bb0(%arg1: tensor<f32>):
+        "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+    }) {
+      name = "test.composite",
+      composite_attributes = {},
+      decomposition = @decomposition,
+      version = 1 : i32
+    } : (tensor<f32>) -> tensor<f32>
+    func.return %0 : tensor<f32>
+  }
+
+  func.func @decomposition(%arg0: tensor<f32>) -> tensor<f32> {
+    func.return %arg0 : tensor<f32>
+  }
+}
diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
@@ -2069,6 +2069,156 @@
 
 namespace {
 
+Value createInitialStartValue(OpBuilder& builder, Location loc,
+                              Value groupSizes, RankedTensorType groupSizesTy,
+                              ArrayRef<int64_t> batchShape) {
+  int64_t groupSizesRank = groupSizesTy.getRank();
+  if (ShapedType::isDynamicShape(batchShape)) {
+    Value groupSizesShape = shape::ShapeOfOp::create(builder, loc, groupSizes);
+    Value batchShapeVal = mlir::stablehlo::SliceOp::create(
+        builder, loc, groupSizesShape, builder.getDenseI64ArrayAttr({0}),
+        builder.getDenseI64ArrayAttr({groupSizesRank - 1}),
+        builder.getDenseI64ArrayAttr({1}));
+    Value zero = mlir::stablehlo::ConstantOp::create(
+        builder, loc, builder.getZeroAttr(builder.getI64Type()));
+    return mlir::stablehlo::DynamicBroadcastInDimOp::create(
+        builder, loc, RankedTensorType::get(batchShape, builder.getI64Type()),
+        zero, batchShapeVal, builder.getDenseI64ArrayAttr({}));
+  }
+  return mlir::stablehlo::ConstantOp::create(
+      builder, loc,
+      builder.getZeroAttr(
+          RankedTensorType::get(batchShape, builder.getI64Type())));
+}
+
+Value getGroupSize(OpBuilder& builder, Location loc, Value groupSizes,
+                   RankedTensorType groupSizesTy, int64_t index,
+                   ArrayRef<int64_t> batchShape) {
+  int64_t groupSizesRank = groupSizesTy.getRank();
+  int64_t groupDimIdx = groupSizesRank - 1;
+  SmallVector<int64_t> startIndices(groupSizesRank, 0);
+  startIndices[groupDimIdx] = index;
+  SmallVector<int64_t> limitIndices(groupSizesTy.getShape());
+  limitIndices[groupDimIdx] = index + 1;
+  SmallVector<int64_t> strides(groupSizesRank, 1);
+
+  Value groupSizeSlice;
+  if (groupSizesTy.hasStaticShape()) {
+    groupSizeSlice = mlir::stablehlo::SliceOp::create(
+        builder, loc, groupSizes, builder.getDenseI64ArrayAttr(startIndices),
+        builder.getDenseI64ArrayAttr(limitIndices),
+        builder.getDenseI64ArrayAttr(strides));
+  } else {
+    auto startTensor = mlir::stablehlo::ConstantOp::create(
+        builder, loc,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({groupSizesRank}, builder.getI64Type()),
+            startIndices));
+    auto stridesTensor = mlir::stablehlo::ConstantOp::create(
+        builder, loc,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({groupSizesRank}, builder.getI64Type()),
+            strides));
+    SmallVector<Value> limitValues;
+    for (int64_t d = 0; d < groupSizesRank; ++d) {
+      if (d == groupDimIdx) {
+        limitValues.push_back(mlir::stablehlo::ConstantOp::create(
+            builder, loc, builder.getI64IntegerAttr(index + 1)));
+      } else {
+        Value dimSize = mlir::stablehlo::GetDimensionSizeOp::create(
+            builder, loc, groupSizes, d);
+        if (!dimSize.getType().isInteger(64)) {
+          dimSize = mlir::stablehlo::ConvertOp::create(
+              builder, loc, builder.getI64Type(), dimSize);
+        }
+        limitValues.push_back(dimSize);
+      }
+    }
+    SmallVector<Value> limitTensorParts;
+    for (Value v : limitValues) {
+      limitTensorParts.push_back(mlir::stablehlo::ReshapeOp::create(
+          builder, loc, RankedTensorType::get({1}, builder.getI64Type()), v));
+    }
+    Value limitTensor = mlir::stablehlo::ConcatenateOp::create(
+        builder, loc,
+        RankedTensorType::get({groupSizesRank}, builder.getI64Type()),
+        limitTensorParts, /*dimension=*/0);
+
+    SmallVector<int64_t> sliceShape(groupSizesTy.getShape());
+    sliceShape[groupDimIdx] = 1;
+    groupSizeSlice = mlir::stablehlo::RealDynamicSliceOp::create(
+        builder, loc,
+        RankedTensorType::get(sliceShape, groupSizesTy.getElementType()),
+        groupSizes, startTensor, limitTensor, stridesTensor);
+  }
+
+  Value groupSize = mlir::stablehlo::ReshapeOp::create(
+      builder, loc,
+      RankedTensorType::get(batchShape, groupSizesTy.getElementType()),
+      groupSizeSlice);
+
+  if (!cast<ShapedType>(groupSize.getType()).getElementType().isInteger(64)) {
+    groupSize = mlir::stablehlo::ConvertOp::create(
+        builder, loc, RankedTensorType::get(batchShape, builder.getI64Type()),
+        groupSize);
+  }
+  return groupSize;
+}
+
+Value applyRaggedMask(OpBuilder& builder, Location loc, Value lhs, Value zero,
+                      Value start, Value limit, Value iota,
+                      RankedTensorType lhsTy,
+                      ArrayRef<int64_t> groupSizesShape) {
+  int64_t groupSizesRank = groupSizesShape.size();
+  SmallVector<int64_t> broadcastDims;
+  for (int64_t d = 0; d < groupSizesRank - 1; ++d) {
+    broadcastDims.push_back(d);
+  }
+
+  auto broadcastToLhs = [&](Value v) -> Value {
+    if (lhsTy.hasStaticShape()) {
+      return mlir::stablehlo::BroadcastInDimOp::create(
+          builder, loc,
+          RankedTensorType::get(lhsTy.getShape(), builder.getI64Type()), v,
+          builder.getDenseI64ArrayAttr(broadcastDims));
+    }
+    Value lhsShape = shape::ShapeOfOp::create(builder, loc, lhs);
+    return mlir::stablehlo::DynamicBroadcastInDimOp::create(
+        builder, loc,
+        RankedTensorType::get(lhsTy.getShape(), builder.getI64Type()), v,
+        lhsShape, builder.getDenseI64ArrayAttr(broadcastDims));
+  };
+
+  Value startBroadcast = broadcastToLhs(start);
+  Value limitBroadcast = broadcastToLhs(limit);
+
+  SmallVector<int64_t> iotaBroadcastDims;
+  for (int64_t d = 0; d < lhsTy.getRank(); ++d) iotaBroadcastDims.push_back(d);
+  Value iotaBroadcast;
+  if (lhsTy.hasStaticShape()) {
+    iotaBroadcast = mlir::stablehlo::BroadcastInDimOp::create(
+        builder, loc,
+        RankedTensorType::get(lhsTy.getShape(), builder.getI64Type()), iota,
+        builder.getDenseI64ArrayAttr(iotaBroadcastDims));
+  } else {
+    Value lhsShape = shape::ShapeOfOp::create(builder, loc, lhs);
+    iotaBroadcast = mlir::stablehlo::DynamicBroadcastInDimOp::create(
+        builder, loc,
+        RankedTensorType::get(lhsTy.getShape(), builder.getI64Type()), iota,
+        lhsShape, builder.getDenseI64ArrayAttr(iotaBroadcastDims));
+  }
+
+  Value geStart = mlir::stablehlo::CompareOp::create(
+      builder, loc, iotaBroadcast, startBroadcast,
+      mlir::stablehlo::ComparisonDirection::GE);
+  Value ltLimit = mlir::stablehlo::CompareOp::create(
+      builder, loc, iotaBroadcast, limitBroadcast,
+      mlir::stablehlo::ComparisonDirection::LT);
+  Value mask = mlir::stablehlo::AndOp::create(builder, loc, geStart, ltLimit);
+
+  return mlir::stablehlo::SelectOp::create(builder, loc, mask, lhs, zero);
+}
+
 ArrayAttr convertPrecisionConfig(mlir::ArrayAttr precisionConfig,
                                  ConversionPatternRewriter& rewriter) {
   std::vector<Attribute> precisions;
@@ -2123,131 +2273,158 @@
   }
   RankedTensorType lhsTy = cast<RankedTensorType>(lhs.getType());
   RankedTensorType rhsTy = cast<RankedTensorType>(rhs.getType());
-  int64_t lhsRank = lhsTy.getRank();
   int64_t rhsRank = rhsTy.getRank();
-  auto outDType = op.getResult().getType().getElementType();
-
-  int64_t m = lhsTy.getShape()[lhsTy.getRank() - 2];
-  int64_t k = lhsTy.getShape()[lhsTy.getRank() - 1];
-  int64_t g = rhsTy.getShape()[0];
-  int64_t n = rhsTy.getShape()[rhsTy.getRank() - 1];
-
-  std::vector<int64_t> outDims = {m, n};
-  std::vector<int64_t> iotaShape = {m, 1};
-  auto iotaDim = 0;
-  std::vector<int64_t> rhsBatchingDims = {};
-  std::vector<int64_t> rhsContractingDims = {0};
-  std::vector<int64_t> rhsReshapedSliceShape = {k, n};
-
-  // If LHS has batching dimension, then decompose ragged dot based on shape
-  // [b, m, k], otherwise assume shape with no batch [m, k].
-  if (lhsRank == 3) {
-    int64_t b = lhsTy.getShape()[0];
-    outDims = {b, m, n};
-    iotaShape = {1, m, 1};
-    iotaDim = 1;
-    rhsBatchingDims = {0};
-    rhsContractingDims = {1};
-    rhsReshapedSliceShape = {b, k, n};
-  }
-
-  // result_iota = iota of shape [m, 1] or [1, m, 1]
-  Value resultIota = mlir::stablehlo::IotaOp::create(
-      rewriter, op.getLoc(),
-      RankedTensorType::get(iotaShape, rewriter.getI64Type()),
-      /*dimension=*/iotaDim);
-  Value start = mlir::stablehlo::ConstantOp::create(
-      rewriter, op.getLoc(),
-      rewriter.getZeroAttr(RankedTensorType::get({1}, rewriter.getI64Type())));
-
-  std::vector<int64_t> broadcastDimensions(lhsRank);
-  std::iota(broadcastDimensions.begin(), broadcastDimensions.end(), 0);
-
-  Value out = mlir::stablehlo::ConstantOp::create(
-      rewriter, op.getLoc(),
-      rewriter.getZeroAttr(RankedTensorType::get(outDims, outDType)));
-
-  Value outZeros = mlir::stablehlo::ConstantOp::create(
-      rewriter, op.getLoc(),
-      rewriter.getZeroAttr(RankedTensorType::get(outDims, outDType)));
+
+  int64_t lhsRaggedDimension =
+      raggedDotDimensionNumbers.getLhsRaggedDimensions()[0];
+  int64_t m = lhsTy.getShape()[lhsRaggedDimension];
+  // rhsGroupDimension identifies the group dimension in rhs.
+  int64_t g = rhsTy.getShape()[rhsGroupDimension];
+  if (g == ShapedType::kDynamic) {
+    return rewriter.notifyMatchFailure(op, "dynamic group count not supported");
+  }
+
+  RankedTensorType groupSizesTy = cast<RankedTensorType>(groupSizes.getType());
+  int64_t groupSizesRank = groupSizesTy.getRank();
+  int64_t groupDimIdx = groupSizesRank - 1;
+  SmallVector<int64_t> batchShape;
+  for (int64_t i = 0; i < groupDimIdx; ++i) {
+    batchShape.push_back(groupSizesTy.getDimSize(i));
+  }
+
+  // lhs is [..., m, k]. ragged dim is m.
+  // Identify ragged dimension in LHS.
+  // RaggedDotDimensionNumbers gives lhsRaggedDimensions.
+  // In Mode 1, it's non-contracting.
+  Value start = createInitialStartValue(rewriter, op.getLoc(), groupSizes,
+                                        groupSizesTy, batchShape);
+
+  // Create iota for ragged dimension (m).
+  SmallVector<int64_t> iotaShape(lhsTy.getRank(), 1);
+  iotaShape[lhsRaggedDimension] = m;
+  Value iotaM;
+  if (ShapedType::isDynamic(m)) {
+    SmallVector<Value> dims;
+    auto i64Type = rewriter.getI64Type();
+    for (int64_t d = 0; d < lhsTy.getRank(); ++d) {
+      if (d == lhsRaggedDimension) {
+        Value s = mlir::stablehlo::GetDimensionSizeOp::create(
+            rewriter, op.getLoc(), lhs, d);
+        s = mlir::stablehlo::ConvertOp::create(
+            rewriter, op.getLoc(), RankedTensorType::get({}, i64Type), s);
+        s = mlir::stablehlo::ReshapeOp::create(
+            rewriter, op.getLoc(), RankedTensorType::get({1}, i64Type), s);
+        dims.push_back(s);
+      } else {
+        Value s = mlir::stablehlo::ConstantOp::create(
+            rewriter, op.getLoc(), rewriter.getI64TensorAttr({1}));
+        dims.push_back(s);
+      }
+    }
+    Value iotaShapeVal = mlir::stablehlo::ConcatenateOp::create(
+        rewriter, op.getLoc(),
+        RankedTensorType::get({lhsTy.getRank()}, i64Type), dims, 0);
+    iotaM = mlir::stablehlo::DynamicIotaOp::create(
+        rewriter, op.getLoc(),
+        RankedTensorType::get(iotaShape, rewriter.getI64Type()), iotaShapeVal,
+        rewriter.getI64IntegerAttr(lhsRaggedDimension));
+  } else {
+    iotaM = mlir::stablehlo::IotaOp::create(
+        rewriter, op.getLoc(),
+        RankedTensorType::get(iotaShape, rewriter.getI64Type()),
+        rewriter.getI64IntegerAttr(lhsRaggedDimension));
+  }
+
+  Value zero;
+  if (lhsTy.hasStaticShape()) {
+    zero = mlir::stablehlo::ConstantOp::create(rewriter, op.getLoc(),
+                                               rewriter.getZeroAttr(lhsTy));
+  } else {
+    Value scalarZero = mlir::stablehlo::ConstantOp::create(
+        rewriter, op.getLoc(),
+        rewriter.getZeroAttr(
+            RankedTensorType::get({}, lhsTy.getElementType())));
+    Value lhsShape = shape::ShapeOfOp::create(rewriter, op.getLoc(), lhs);
+    zero = mlir::stablehlo::DynamicBroadcastInDimOp::create(
+        rewriter, op.getLoc(), lhsTy, scalarZero, lhsShape,
+        rewriter.getDenseI64ArrayAttr({}));
+  }
+
+  // Calculate DotDimensionNumbers for dot_general (lhs, rhs_slice).
+  auto getAdjustedDim = [&](int64_t dim) {
+    return dim > rhsGroupDimension ? dim - 1 : dim;
+  };
+  SmallVector<int64_t> rhsBatchingDims;
+  for (int64_t d : raggedDotDimensionNumbers.getRhsBatchingDimensions()) {
+    rhsBatchingDims.push_back(getAdjustedDim(d));
+  }
+  SmallVector<int64_t> rhsContractingDims;
+  for (int64_t d : raggedDotDimensionNumbers.getRhsContractingDimensions()) {
+    rhsContractingDims.push_back(getAdjustedDim(d));
+  }
+
+  Value out;
   for (auto i = 0; i < g; ++i) {
-    // groupSize = group_sizes[i]
-    Value groupSize = mlir::stablehlo::SliceOp::create(
-        rewriter, op.getLoc(),
-        RankedTensorType::get({1}, rewriter.getI64Type()), groupSizes,
-        /*startIndices=*/rewriter.getDenseI64ArrayAttr({i}),
-        /*limitIndices=*/rewriter.getDenseI64ArrayAttr({i + 1}),
-        /*strides=*/rewriter.getDenseI64ArrayAttr({1}));
-
-    Value startBroadcasted = mlir::stablehlo::BroadcastInDimOp::create(
-        rewriter, op.getLoc(), resultIota.getType(), start,
-        /*broadcast_dimensions=*/
-        rewriter.getDenseI64ArrayAttr(0));
-
-    // start <= result_iota
-    Value startLEResultIota = mlir::stablehlo::CompareOp::create(
-        rewriter, op.getLoc(), startBroadcasted, resultIota,
-        ComparisonDirection::LE);
-
-    // result_iota < (start + size)
-    Value resultIotaLTStartPlusGroupSize = mlir::stablehlo::CompareOp::create(
-        rewriter, op.getLoc(), resultIota,
-        mlir::stablehlo::BroadcastInDimOp::create(
-            rewriter, op.getLoc(), resultIota.getType(),
-            mlir::stablehlo::AddOp::create(rewriter, op.getLoc(), start,
-                                           groupSize),
-            /*broadcast_dimensions=*/rewriter.getDenseI64ArrayAttr(0)),
-        ComparisonDirection::LT);
-
-    // (start <= result_iota) & (result_iota < (start + size))
-    Value logicalAnd =
-        mlir::stablehlo::AndOp::create(rewriter, op.getLoc(), startLEResultIota,
-                                       resultIotaLTStartPlusGroupSize);
-    Value logicalAndBroadcasted = mlir::stablehlo::BroadcastInDimOp::create(
-        rewriter, op.getLoc(),
-        RankedTensorType::get(op.getResult().getType().getShape(),
-                              rewriter.getI1Type()),
-        logicalAnd,
-        /*broadcast_dimensions=*/
-        rewriter.getDenseI64ArrayAttr(broadcastDimensions));
-
-    // rhs_rehaped_slice = rhs[i, :, :, :]
-    std::vector<int64_t> rhs_start_indices(rhsTy.getRank(), 0);
-    rhs_start_indices[rhsGroupDimension] = i;
-    std::vector<int64_t> rhs_limit_indices = rhsTy.getShape();
-    rhs_limit_indices[rhsGroupDimension] = i + 1;
+    Value groupSize = getGroupSize(rewriter, op.getLoc(), groupSizes,
+                                   groupSizesTy, i, batchShape);
+
+    Value limit =
+        mlir::stablehlo::AddOp::create(rewriter, op.getLoc(), start, groupSize);
+
+    Value lhsMasked =
+        applyRaggedMask(rewriter, op.getLoc(), lhs, zero, start, limit, iotaM,
+                        lhsTy, groupSizesTy.getShape());
+
+    // Slice RHS.
+    SmallVector<int64_t> rhsStartIndices(rhsRank, 0);
+    rhsStartIndices[rhsGroupDimension] = i;
+    SmallVector<int64_t> rhsLimitIndices(rhsTy.getShape());
+    rhsLimitIndices[rhsGroupDimension] = i + 1;
+    SmallVector<int64_t> rhsStrides(rhsRank, 1);
     Value rhsSliced = mlir::stablehlo::SliceOp::create(
         rewriter, op.getLoc(), rhs,
-        /*startIndices=*/rewriter.getDenseI64ArrayAttr(rhs_start_indices),
-        /*limitIndices=*/rewriter.getDenseI64ArrayAttr(rhs_limit_indices),
-        /*strides=*/
-        rewriter.getDenseI64ArrayAttr(std::vector<int64_t>(rhsRank, 1)));
-    Value rhsReshapedSlice = mlir::stablehlo::ReshapeOp::create(
+        rewriter.getDenseI64ArrayAttr(rhsStartIndices),
+        rewriter.getDenseI64ArrayAttr(rhsLimitIndices),
+        rewriter.getDenseI64ArrayAttr(rhsStrides));
+
+    // Reshape RHS to remove group dim.
+    SmallVector<int64_t> rhsReshapedShape;
+    for (int64_t d = 0; d < rhsRank; ++d) {
+      if (d != rhsGroupDimension)
+        rhsReshapedShape.push_back(rhsTy.getShape()[d]);
+    }
+    Value rhsReshaped = mlir::stablehlo::ReshapeOp::create(
         rewriter, op.getLoc(),
-        RankedTensorType::get(rhsReshapedSliceShape, rhsTy.getElementType()),
+        RankedTensorType::get(rhsReshapedShape, rhsTy.getElementType()),
         rhsSliced);
 
-    // Einsum of (b)mk,(b)kn->(b)mn
+    // Dot General.
+    auto resultType = op.getResult().getType();
+    // If output is dynamic, we can't infer result type of dot_general trivially
+    // without shape inference. But dot_general shape inference handles it.
+    // However, if we accumulate, `out` needs to be initialized.
+    // We peel the first iteration to initialize `out`.
+    SmallVector<NamedAttribute, 2> attributes;
+    attributes.push_back(rewriter.getNamedAttr(
+        "dot_dimension_numbers",
+        rewriter.getAttr<mlir::stablehlo::DotDimensionNumbersAttr>(
+            lhsBatchingDimensions, rhsBatchingDims, lhsContractingDimensions,
+            rhsContractingDims)));
+    if (precisionConfig.has_value()) {
+      attributes.push_back(
+          rewriter.getNamedAttr("precision_config", precisionConfig.value()));
+    }
     Value dotGeneral = mlir::stablehlo::DotGeneralOp::create(
-        rewriter, op.getLoc(), TypeRange{out.getType()},
-        ValueRange{lhs, rhsReshapedSlice},
-        ArrayRef<mlir::NamedAttribute>{
-            rewriter.getNamedAttr(
-                "dot_dimension_numbers",
-                rewriter.getAttr<mlir::stablehlo::DotDimensionNumbersAttr>(
-                    /*lhs_batching_dimensions=*/lhsBatchingDimensions,
-                    /*rhs_batching_dimensions=*/rhsBatchingDims,
-                    /*lhs_contracting_dimensions=*/lhsContractingDimensions,
-                    /*rhs_contracting_dimensions=*/rhsContractingDims)),
-            rewriter.getNamedAttr("precision_config",
-                                  precisionConfig.value())});
-
-    // Place the i'th dot_general to the corresponding position in the result.
-    Value select = mlir::stablehlo::SelectOp::create(
-        rewriter, op.getLoc(), logicalAndBroadcasted, dotGeneral, outZeros);
-    out = mlir::stablehlo::AddOp::create(rewriter, op.getLoc(), out, select);
-    start =
-        mlir::stablehlo::AddOp::create(rewriter, op.getLoc(), start, groupSize);
+        rewriter, op.getLoc(), resultType, ValueRange{lhsMasked, rhsReshaped},
+        attributes);
+
+    if (i == 0) {
+      out = dotGeneral;
+    } else {
+      out = mlir::stablehlo::AddOp::create(rewriter, op.getLoc(), out,
+                                           dotGeneral);
+    }
+    start = limit;
   }
   rewriter.replaceOp(op, {out});
   return success();
@@ -2260,7 +2437,158 @@
 //   result : [g, b, m, n]
 LogicalResult handleRaggedDotMode2(mlir::chlo::RaggedDotOp op,
                                    ConversionPatternRewriter& rewriter) {
-  return failure();
+  Value lhs = op.getLhs();
+  Value rhs = op.getRhs();
+  chlo::RaggedDotDimensionNumbersAttr raggedDotDimensionNumbers =
+      op.getRaggedDotDimensionNumbers();
+  ArrayRef<int64_t> lhsBatchingDimensions =
+      raggedDotDimensionNumbers.getLhsBatchingDimensions();
+  ArrayRef<int64_t> lhsContractingDimensions =
+      raggedDotDimensionNumbers.getLhsContractingDimensions();
+  ArrayRef<int64_t> rhsContractingDimensions =
+      raggedDotDimensionNumbers.getRhsContractingDimensions();
+  int64_t lhsRaggedDimension =
+      raggedDotDimensionNumbers.getLhsRaggedDimensions()[0];
+
+  auto groupSizes = op.getGroupSizes();
+  auto precisionConfig = op.getPrecisionConfig();
+  if (precisionConfig.has_value()) {
+    precisionConfig = convertPrecisionConfig(precisionConfig.value(), rewriter);
+  }
+  RankedTensorType lhsTy = cast<RankedTensorType>(lhs.getType());
+  RankedTensorType groupSizesTy = cast<RankedTensorType>(groupSizes.getType());
+  auto outDType = op.getResult().getType().getElementType();
+
+  int64_t groupSizesRank = groupSizesTy.getRank();
+  int64_t g = groupSizesTy.getDimSize(groupSizesRank - 1);
+  if (g == ShapedType::kDynamic) {
+    return rewriter.notifyMatchFailure(op, "dynamic group count not supported");
+  }
+
+  // Identify which contracting dimension is the ragged one.
+  int64_t rhsRaggedDimension = -1;
+  for (auto [lhsDim, rhsDim] :
+       llvm::zip(lhsContractingDimensions, rhsContractingDimensions)) {
+    if (lhsDim == lhsRaggedDimension) {
+      rhsRaggedDimension = rhsDim;
+      break;
+    }
+  }
+  if (rhsRaggedDimension == -1) return failure();
+
+  // Initialize start = 0.
+  // We need start to be broadcastable to [b].
+  // groupSizes can be [g] or [b, g].
+  int64_t groupDimIdx = groupSizesRank - 1;
+  SmallVector<int64_t> batchShape;
+  for (int64_t i = 0; i < groupDimIdx; ++i) {
+    batchShape.push_back(groupSizesTy.getDimSize(i));
+  }
+  Value start = createInitialStartValue(rewriter, op.getLoc(), groupSizes,
+                                        groupSizesTy, batchShape);
+
+  // Create iota for the contracting dimension (ragged dimension).
+  // lhs is [..., k].
+  int64_t k = lhsTy.getDimSize(lhsRaggedDimension);
+  // iotaShape: [1, ..., 1, k] (rank equal to lhs).
+  SmallVector<int64_t> iotaShape(lhsTy.getRank(), 1);
+  iotaShape[lhsRaggedDimension] = k;
+  Value iotaK;
+  if (ShapedType::isDynamic(k)) {
+    SmallVector<Value> dims;
+    auto i64Type = rewriter.getI64Type();
+    for (int64_t d = 0; d < lhsTy.getRank(); ++d) {
+      if (d == lhsRaggedDimension) {
+        Value s = mlir::stablehlo::GetDimensionSizeOp::create(
+            rewriter, op.getLoc(), lhs, d);
+        s = mlir::stablehlo::ConvertOp::create(
+            rewriter, op.getLoc(), RankedTensorType::get({}, i64Type), s);
+        s = mlir::stablehlo::ReshapeOp::create(
+            rewriter, op.getLoc(), RankedTensorType::get({1}, i64Type), s);
+        dims.push_back(s);
+      } else {
+        Value s = mlir::stablehlo::ConstantOp::create(
+            rewriter, op.getLoc(), rewriter.getI64TensorAttr({1}));
+        dims.push_back(s);
+      }
+    }
+    Value iotaShapeVal = mlir::stablehlo::ConcatenateOp::create(
+        rewriter, op.getLoc(),
+        RankedTensorType::get({lhsTy.getRank()}, i64Type), dims, 0);
+    iotaK = mlir::stablehlo::DynamicIotaOp::create(
+        rewriter, op.getLoc(),
+        RankedTensorType::get(iotaShape, rewriter.getI64Type()), iotaShapeVal,
+        rewriter.getI64IntegerAttr(lhsRaggedDimension));
+  } else {
+    iotaK = mlir::stablehlo::IotaOp::create(
+        rewriter, op.getLoc(),
+        RankedTensorType::get(iotaShape, rewriter.getI64Type()),
+        rewriter.getI64IntegerAttr(lhsRaggedDimension));
+  }
+
+  Value zero;
+  if (lhsTy.hasStaticShape()) {
+    zero = mlir::stablehlo::ConstantOp::create(rewriter, op.getLoc(),
+                                               rewriter.getZeroAttr(lhsTy));
+  } else {
+    Value scalarZero = mlir::stablehlo::ConstantOp::create(
+        rewriter, op.getLoc(),
+        rewriter.getZeroAttr(
+            RankedTensorType::get({}, lhsTy.getElementType())));
+    Value lhsShape = shape::ShapeOfOp::create(rewriter, op.getLoc(), lhs);
+    zero = mlir::stablehlo::DynamicBroadcastInDimOp::create(
+        rewriter, op.getLoc(), lhsTy, scalarZero, lhsShape,
+        rewriter.getDenseI64ArrayAttr({}));
+  }
+
+  SmallVector<Value> results;
+  for (int64_t i = 0; i < g; ++i) {
+    Value groupSize = getGroupSize(rewriter, op.getLoc(), groupSizes,
+                                   groupSizesTy, i, batchShape);
+
+    Value limit =
+        mlir::stablehlo::AddOp::create(rewriter, op.getLoc(), start, groupSize);
+
+    Value lhsMasked =
+        applyRaggedMask(rewriter, op.getLoc(), lhs, zero, start, limit, iotaK,
+                        lhsTy, groupSizesTy.getShape());
+
+    // Dot general.
+    auto resultShape = op.getResult().getType().getShape().drop_front();
+    SmallVector<NamedAttribute, 2> attributes;
+    attributes.push_back(rewriter.getNamedAttr(
+        "dot_dimension_numbers",
+        rewriter.getAttr<mlir::stablehlo::DotDimensionNumbersAttr>(
+            lhsBatchingDimensions,
+            raggedDotDimensionNumbers.getRhsBatchingDimensions(),
+            lhsContractingDimensions, rhsContractingDimensions)));
+    if (precisionConfig.has_value()) {
+      attributes.push_back(
+          rewriter.getNamedAttr("precision_config", precisionConfig.value()));
+    }
+    Value dotGeneral = mlir::stablehlo::DotGeneralOp::create(
+        rewriter, op.getLoc(), RankedTensorType::get(resultShape, outDType),
+        {lhsMasked, rhs}, attributes);
+
+    results.push_back(dotGeneral);
+    start = limit;
+  }
+
+  // Concatenate results along a new dimension 0.
+  // First broadcast each result to [1, ...].
+  SmallVector<Value> reshapedResults;
+  for (Value res : results) {
+    auto type = cast<RankedTensorType>(res.getType());
+    SmallVector<int64_t> newShape = {1};
+    newShape.append(type.getShape().begin(), type.getShape().end());
+    reshapedResults.push_back(mlir::stablehlo::ReshapeOp::create(
+        rewriter, op.getLoc(),
+        RankedTensorType::get(newShape, type.getElementType()), res));
+  }
+
+  rewriter.replaceOpWithNewOp<mlir::stablehlo::ConcatenateOp>(
+      op, reshapedResults, 0);
+  return success();
 }
 
 // Mode 3, where the ragged dimension is an lhs/rhs batch dim (b).
@@ -2270,7 +2598,31 @@
 //   result : [b, m, n]
 LogicalResult handleRaggedDotMode3(mlir::chlo::RaggedDotOp op,
                                    ConversionPatternRewriter& rewriter) {
-  return failure();
+  // Mode 3 is semantically equivalent to a batch dot_general because the
+  // grouping does not introduce dependencies between groups.
+  // We can just lower to stablehlo.dot_general.
+  auto precisionConfig = op.getPrecisionConfig();
+  if (precisionConfig.has_value()) {
+    precisionConfig = convertPrecisionConfig(precisionConfig.value(), rewriter);
+  }
+
+  auto raggedDotDimensionNumbers = op.getRaggedDotDimensionNumbers();
+  SmallVector<NamedAttribute, 2> attributes;
+  attributes.push_back(rewriter.getNamedAttr(
+      "dot_dimension_numbers",
+      rewriter.getAttr<mlir::stablehlo::DotDimensionNumbersAttr>(
+          raggedDotDimensionNumbers.getLhsBatchingDimensions(),
+          raggedDotDimensionNumbers.getRhsBatchingDimensions(),
+          raggedDotDimensionNumbers.getLhsContractingDimensions(),
+          raggedDotDimensionNumbers.getRhsContractingDimensions())));
+  if (precisionConfig.has_value()) {
+    attributes.push_back(
+        rewriter.getNamedAttr("precision_config", precisionConfig.value()));
+  }
+  rewriter.replaceOpWithNewOp<mlir::stablehlo::DotGeneralOp>(
+      op, op.getResult().getType(), ValueRange{op.getLhs(), op.getRhs()},
+      attributes);
+  return success();
 }
 
 }  // namespace
@@ -2291,6 +2643,333 @@
     } else {
       return handleRaggedDotMode3(op, rewriter);
     }
+  }
+};
+
+struct ConvertScanOp final : OpConversionPattern<mlir::chlo::ScanOp> {
+  using OpConversionPattern::OpConversionPattern;
+
+ private:
+  static FailureOr<Value> getScanDimensionSize(
+      ConversionPatternRewriter& rewriter, Location loc, mlir::chlo::ScanOp op,
+      ValueRange inputs, int64_t dim) {
+    if (!inputs.empty()) {
+      return GetDimensionSizeOp::create(rewriter, loc, inputs[0], dim)
+          .getResult();
+    }
+    auto resType = cast<RankedTensorType>(op.getResult(0).getType());
+    if (resType.isDynamicDim(dim)) {
+      return rewriter.notifyMatchFailure(
+          op,
+          "cannot determine scan dimension size from empty inputs and "
+          "dynamic result");
+    }
+    auto scan_dim_size = static_cast<int32_t>(resType.getDimSize(dim));
+    auto attr = DenseIntElementsAttr::get(
+        RankedTensorType::get({}, rewriter.getI32Type()), scan_dim_size);
+    return ConstantOp::create(rewriter, loc, attr).getResult();
+  }
+
+  static SmallVector<Value> getLimitScalars(ConversionPatternRewriter& rewriter,
+                                            Location loc, int64_t dim,
+                                            Value index, Value input) {
+    auto inputType = cast<RankedTensorType>(input.getType());
+    int64_t rank = inputType.getRank();
+    SmallVector<Value> limitScalars;
+    for (int64_t d = 0; d < rank; ++d) {
+      if (d == dim) {
+        Value indexPlusOne = AddOp::create(
+            rewriter, loc, index,
+            ConstantOp::create(
+                rewriter, loc,
+                DenseIntElementsAttr::get(
+                    RankedTensorType::get({}, rewriter.getI64Type()), {1LL})));
+        limitScalars.push_back(indexPlusOne);
+      } else {
+        Value dSize;
+        if (inputType.isDynamicDim(d)) {
+          dSize = GetDimensionSizeOp::create(rewriter, loc, input, d);
+        } else {
+          auto attr = DenseIntElementsAttr::get(
+              RankedTensorType::get({}, rewriter.getI32Type()),
+              static_cast<int32_t>(inputType.getDimSize(d)));
+          dSize = ConstantOp::create(rewriter, loc, attr);
+        }
+        dSize = ConvertOp::create(rewriter, loc, dSize, rewriter.getI64Type());
+        limitScalars.push_back(dSize);
+      }
+    }
+    return limitScalars;
+  }
+
+  static SmallVector<Value> createInitialValues(
+      ConversionPatternRewriter& rewriter, Location loc, mlir::chlo::ScanOp op,
+      ValueRange inputs, ValueRange inits) {
+    size_t numInputs = inputs.size();
+    size_t numCarries = inits.size();
+    size_t numScanOutputs = op.getNumResults() - numCarries;
+
+    Value zeroIndex = ConstantOp::create(
+        rewriter, loc,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({}, rewriter.getI64Type()), {0LL}));
+
+    SmallVector<Value> initialValues;
+    initialValues.reserve(1 + numCarries + numScanOutputs);
+    initialValues.push_back(zeroIndex);
+    initialValues.append(inits.begin(), inits.end());
+
+    // Initialize output arrays.
+    for (size_t i = 0; i < numScanOutputs; ++i) {
+      auto resultType = cast<RankedTensorType>(op.getResult(i).getType());
+      Value zero =
+          ConstantOp::create(rewriter, loc,
+                             rewriter.getZeroAttr(RankedTensorType::get(
+                                 {}, resultType.getElementType())));
+
+      if (resultType.hasStaticShape()) {
+        initialValues.push_back(BroadcastOp::create(
+            rewriter, loc, resultType, zero,
+            rewriter.getDenseI64ArrayAttr(resultType.getShape())));
+      } else {
+        if (numInputs == 0) {
+          // This should be caught by verification or earlier checks, but just
+          // in case.
+          return {};
+        }
+        Value refInput = inputs[i < numInputs ? i : 0];
+        Value refShape = shape::ShapeOfOp::create(rewriter, loc, refInput);
+        initialValues.push_back(DynamicBroadcastInDimOp::create(
+            rewriter, loc, resultType, zero, refShape,
+            rewriter.getDenseI64ArrayAttr({})));
+      }
+    }
+    return initialValues;
+  }
+
+  static void createConditionRegion(ConversionPatternRewriter& rewriter,
+                                    Location loc, WhileOp whileOp,
+                                    Value scanDimSize,
+                                    const SmallVector<Type>& whileTypes) {
+    OpBuilder::InsertionGuard guard(rewriter);
+    Block* condBlock = rewriter.createBlock(&whileOp.getCond());
+    for (Type t : whileTypes) condBlock->addArgument(t, loc);
+
+    Value index = condBlock->getArgument(0);
+    Value cond = CompareOp::create(rewriter, loc, index, scanDimSize,
+                                   ComparisonDirection::LT);
+    ReturnOp::create(rewriter, loc, cond);
+  }
+
+  static SmallVector<Value> sliceInputs(ConversionPatternRewriter& rewriter,
+                                        Location loc, int64_t dim, Value index,
+                                        ValueRange inputs) {
+    SmallVector<Value> slicedInputs;
+    for (Value input : inputs) {
+      auto inputType = cast<RankedTensorType>(input.getType());
+      int64_t rank = inputType.getRank();
+
+      auto build1DTensor = [&](const SmallVector<Value>& scalars) {
+        SmallVector<Value> parts;
+        auto i64Ty = RankedTensorType::get({1}, rewriter.getI64Type());
+        for (Value s : scalars) {
+          parts.push_back(ReshapeOp::create(rewriter, loc, i64Ty, s));
+        }
+        return ConcatenateOp::create(rewriter, loc, parts, 0);
+      };
+
+      Value zero =
+          ConstantOp::create(rewriter, loc, rewriter.getI64IntegerAttr(0));
+      SmallVector<Value> startScalars(rank, zero);
+      startScalars[dim] = index;
+
+      Value startTensor = build1DTensor(startScalars);
+
+      SmallVector<Value> limitScalars =
+          getLimitScalars(rewriter, loc, dim, index, input);
+      Value limitTensor = build1DTensor(limitScalars);
+
+      SmallVector<int64_t> strides(rank, 1);
+      Value stridesTensor = ConstantOp::create(
+          rewriter, loc,
+          DenseIntElementsAttr::get(
+              RankedTensorType::get({rank}, rewriter.getI64Type()), strides));
+
+      SmallVector<int64_t> sliceShape(inputType.getShape().begin(),
+                                      inputType.getShape().end());
+      sliceShape[dim] = 1;
+      auto sliceType =
+          RankedTensorType::get(sliceShape, inputType.getElementType());
+
+      Value slice =
+          RealDynamicSliceOp::create(rewriter, loc, sliceType, input,
+                                     startTensor, limitTensor, stridesTensor);
+
+      SmallVector<int64_t> resultShape;
+      for (int64_t d = 0; d < rank; ++d) {
+        if (d != dim) resultShape.push_back(sliceShape[d]);
+      }
+      Value reshaped = ReshapeOp::create(
+          rewriter, loc,
+          RankedTensorType::get(resultShape, inputType.getElementType()),
+          slice);
+      slicedInputs.push_back(reshaped);
+    }
+    return slicedInputs;
+  }
+
+  static SmallVector<Value> updateOutputs(ConversionPatternRewriter& rewriter,
+                                          Location loc, int64_t dim,
+                                          Value index,
+                                          ValueRange currentOutputs,
+                                          ValueRange newElements) {
+    SmallVector<Value> updatedOutputs;
+    size_t numScanOutputs = currentOutputs.size();
+    for (size_t i = 0; i < numScanOutputs; ++i) {
+      Value currentOutput = currentOutputs[i];
+      Value newElement = newElements[i];
+
+      auto outputType = cast<RankedTensorType>(currentOutput.getType());
+      SmallVector<int64_t> elementShape(outputType.getShape().begin(),
+                                        outputType.getShape().end());
+      elementShape[dim] = 1;
+      Value reshapedElement = ReshapeOp::create(
+          rewriter, loc,
+          RankedTensorType::get(elementShape, outputType.getElementType()),
+          newElement);
+
+      int64_t outRank = outputType.getRank();
+      SmallVector<Value> startScalars;
+      Value zero =
+          ConstantOp::create(rewriter, loc, rewriter.getI64IntegerAttr(0));
+      for (int64_t d = 0; d < outRank; ++d) {
+        if (d == dim)
+          startScalars.push_back(index);
+        else
+          startScalars.push_back(zero);
+      }
+
+      Value updated =
+          DynamicUpdateSliceOp::create(rewriter, loc, outputType, currentOutput,
+                                       reshapedElement, startScalars);
+      updatedOutputs.push_back(updated);
+    }
+    return updatedOutputs;
+  }
+
+  static void createBodyRegion(ConversionPatternRewriter& rewriter,
+                               Location loc, mlir::chlo::ScanOp op,
+                               WhileOp whileOp, ValueRange inputs, int64_t dim,
+                               const SmallVector<Type>& whileTypes) {
+    OpBuilder::InsertionGuard guard(rewriter);
+    Block* bodyBlock = rewriter.createBlock(&whileOp.getBody());
+    for (Type t : whileTypes) bodyBlock->addArgument(t, loc);
+
+    Value index = bodyBlock->getArgument(0);
+    size_t numCarries = op.getInits().size();
+    size_t numScanOutputs = op.getNumResults() - numCarries;
+
+    // Args: index (1), accs (numCarries), outputs (numScanOutputs)
+    size_t offset = 1;
+    auto argAccs = bodyBlock->getArguments().slice(offset, numCarries);
+    offset += numCarries;
+    auto argOutputs = bodyBlock->getArguments().slice(offset, numScanOutputs);
+
+    // Slice inputs.
+    SmallVector<Value> slicedInputs =
+        sliceInputs(rewriter, loc, dim, index, inputs);
+
+    // Inline Body.
+    Block& scanBody = op.getBody().front();
+    IRMapping mapping;
+    // inputs.size() is used as the number of inputs to map.
+    mapping.map(scanBody.getArguments().take_front(inputs.size()),
+                slicedInputs);
+    mapping.map(scanBody.getArguments().drop_front(inputs.size()), argAccs);
+
+    for (auto& nestedOp : scanBody.without_terminator()) {
+      rewriter.clone(nestedOp, mapping);
+    }
+
+    Operation* terminator = scanBody.getTerminator();
+    SmallVector<Value> bodyResults;
+    for (Value operand : terminator->getOperands()) {
+      bodyResults.push_back(mapping.lookup(operand));
+    }
+
+    auto newOutputElements = ArrayRef(bodyResults).take_front(numScanOutputs);
+    auto newAccs = ArrayRef(bodyResults).take_back(numCarries);
+
+    // Update Outputs.
+    SmallVector<Value> updatedOutputs =
+        updateOutputs(rewriter, loc, dim, index, argOutputs, newOutputElements);
+
+    Value oneIndex = ConstantOp::create(
+        rewriter, loc,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({}, rewriter.getI64Type()), {1LL}));
+    Value nextIndex = AddOp::create(rewriter, loc, index, oneIndex);
+
+    SmallVector<Value> yieldOperands;
+    yieldOperands.push_back(nextIndex);
+    yieldOperands.append(newAccs.begin(), newAccs.end());
+    yieldOperands.append(updatedOutputs.begin(), updatedOutputs.end());
+
+    ReturnOp::create(rewriter, loc, yieldOperands);
+  }
+
+  LogicalResult matchAndRewrite(
+      mlir::chlo::ScanOp op, OpAdaptor adaptor,
+      ConversionPatternRewriter& rewriter) const override {
+    Location loc = op.getLoc();
+    int64_t dim = op.getDimension();
+    ValueRange inputs = adaptor.getInputs();
+    ValueRange inits = adaptor.getInits();
+    size_t numCarries = inits.size();
+    size_t numScanOutputs = op.getNumResults() - numCarries;
+
+    // 1. Determine scan dimension size.
+    FailureOr<Value> scanDimSizeOrError =
+        getScanDimensionSize(rewriter, loc, op, inputs, dim);
+    if (failed(scanDimSizeOrError)) {
+      return failure();
+    }
+    Value scanDimSize = ConvertOp::create(
+        rewriter, loc, RankedTensorType::get({}, rewriter.getI64Type()),
+        *scanDimSizeOrError);
+
+    // 2. Initial values.
+    auto initialValues = createInitialValues(rewriter, loc, op, inputs, inits);
+    if (initialValues.empty() && numScanOutputs > 0) {
+      return rewriter.notifyMatchFailure(
+          op, "cannot determine dynamic output shape without inputs");
+    }
+
+    // 3. Create WhileOp.
+    SmallVector<Type> whileTypes;
+    for (Value v : initialValues) whileTypes.push_back(v.getType());
+
+    auto whileOp = WhileOp::create(rewriter, loc, whileTypes, initialValues);
+
+    // 4. Condition Region.
+    createConditionRegion(rewriter, loc, whileOp, scanDimSize, whileTypes);
+
+    // 5. Body Region.
+    createBodyRegion(rewriter, loc, op, whileOp, inputs, dim, whileTypes);
+
+    // 6. Extract Results.
+    SmallVector<Value> replacements;
+    size_t outputsStart = 1 + numCarries;
+    for (size_t i = 0; i < numScanOutputs; ++i) {
+      replacements.push_back(whileOp.getResult(outputsStart + i));
+    }
+    size_t accsStart = 1;
+    for (size_t i = 0; i < numCarries; ++i) {
+      replacements.push_back(whileOp.getResult(accsStart + i));
+    }
+
+    rewriter.replaceOp(op, replacements);
+    return success();
   }
 };
 
@@ -2361,29 +3040,41 @@
             : std::min(static_cast<int64_t>(op.getK()), lastDimSize);
     int64_t isDynamic = !operandType.hasStaticShape();
     auto i32Type = rewriter.getIntegerType(32);
+    auto i64Type = rewriter.getI64Type();
     Value opShapeValue, resultShapeValue;
     if (isDynamic) {
-      SmallVector<Value> sizesI32x1;
+      SmallVector<Value> sizesI64x1;
       for (auto i = 0; i < operandType.getRank(); ++i) {
-        auto sizeI32 = mlir::stablehlo::GetDimensionSizeOp::create(
-            rewriter, op.getLoc(), op.getOperand(), i);
-        auto sizeI32x1 = mlir::stablehlo::ReshapeOp::create(
-            rewriter, op.getLoc(), RankedTensorType::get({1}, i32Type),
-            sizeI32);
-        sizesI32x1.push_back(sizeI32x1);
+        Value sizeI64;
+        if (operandType.isDynamicDim(i)) {
+          auto sizeI32 = mlir::stablehlo::GetDimensionSizeOp::create(
+              rewriter, op.getLoc(), op.getOperand(), i);
+          sizeI64 = mlir::stablehlo::ConvertOp::create(
+              rewriter, op.getLoc(), RankedTensorType::get({}, i64Type),
+              sizeI32);
+        } else {
+          auto attr = DenseIntElementsAttr::get(
+              RankedTensorType::get({}, i64Type),
+              static_cast<int64_t>(operandType.getDimSize(i)));
+          sizeI64 =
+              mlir::stablehlo::ConstantOp::create(rewriter, op.getLoc(), attr);
+        }
+        auto sizeI64x1 = mlir::stablehlo::ReshapeOp::create(
+            rewriter, op.getLoc(), RankedTensorType::get({1}, i64Type),
+            sizeI64);
+        sizesI64x1.push_back(sizeI64x1);
       }
       opShapeValue = mlir::stablehlo::ConcatenateOp::create(
-          rewriter, op.getLoc(), sizesI32x1,
+          rewriter, op.getLoc(), sizesI64x1,
           /*dimension=*/0);
-      auto lastDimI32 = mlir::stablehlo::ConstantOp::create(
-          rewriter, op.getLoc(),
-          rewriter.getI32IntegerAttr(static_cast<int32_t>(lastDimResultSize)));
-      auto lastDimI32x1 = mlir::stablehlo::ReshapeOp::create(
-          rewriter, op.getLoc(), RankedTensorType::get({1}, i32Type),
-          lastDimI32);
-      sizesI32x1.back() = lastDimI32x1;
+      auto lastDimI64 = mlir::stablehlo::ConstantOp::create(
+          rewriter, op.getLoc(), rewriter.getI64IntegerAttr(lastDimResultSize));
+      auto lastDimI64x1 = mlir::stablehlo::ReshapeOp::create(
+          rewriter, op.getLoc(), RankedTensorType::get({1}, i64Type),
+          lastDimI64);
+      sizesI64x1.back() = lastDimI64x1;
       resultShapeValue = mlir::stablehlo::ConcatenateOp::create(
-          rewriter, op.getLoc(), sizesI32x1,
+          rewriter, op.getLoc(), sizesI64x1,
           /*dimension=*/0);
     }
 
@@ -2426,18 +3117,12 @@
       Value startIndices = mlir::stablehlo::ConstantOp::create(
           rewriter, op.getLoc(),
           DenseIntElementsAttr::get(indicesTy, beginIndices));
-      Value lastIndices = mlir::stablehlo::ConvertOp::create(
-          rewriter, op.getLoc(), resultShapeValue, rewriter.getI64Type());
+      Value lastIndices = resultShapeValue;
       Value stridesOp = mlir::stablehlo::ConstantOp::create(
           rewriter, op.getLoc(), DenseIntElementsAttr::get(indicesTy, strides));
 
-      SmallVector<int64_t> resultShape =
-          llvm::to_vector(operandType.getShape());
-      resultShape.back() = lastDimResultSize;
-      RankedTensorType resultType = RankedTensorType::get(
-          resultShape, elementType, operandType.getEncoding());
-      RankedTensorType indexResultType =
-          RankedTensorType::get(resultShape, i32Type);
+      auto resultType = cast<RankedTensorType>(op.getResult(0).getType());
+      auto indexResultType = cast<RankedTensorType>(op.getResult(1).getType());
 
       values = mlir::stablehlo::RealDynamicSliceOp::create(
           rewriter, op.getLoc(), resultType, tupleFirstElement, startIndices,
@@ -2531,12 +3216,11 @@
 static void populateChloDecompositionPatterns(MLIRContext* context,
                                               RewritePatternSet* patterns) {
   populateWithGenerated(*patterns);
-  patterns
-      ->add<ConvertConstantOp, ConvertBesselI1eOp, ConvertCoshOp,
-            ConvertDigammaOp, ConvertErfOp, ConvertErfcOp, ConvertErfInvOp,
-            ConvertLgammaOp, ConvertNextAfterOp, ConvertPolygammaOp,
-            ConvertRaggedDotOp, ConvertSinhOp, ConvertTopKOp, ConvertZetaOp>(
-          context);
+  patterns->add<ConvertConstantOp, ConvertBesselI1eOp, ConvertCoshOp,
+                ConvertDigammaOp, ConvertErfOp, ConvertErfcOp, ConvertErfInvOp,
+                ConvertLgammaOp, ConvertNextAfterOp, ConvertPolygammaOp,
+                ConvertRaggedDotOp, ConvertScanOp, ConvertSinhOp, ConvertTopKOp,
+                ConvertZetaOp>(context);
 }
 }  // namespace
 
diff --ruN a/stablehlo/stablehlo/transforms/MapStablehloToVhlo.h b/stablehlo/stablehlo/transforms/MapStablehloToVhlo.h
--- stablehlo/stablehlo/transforms/MapStablehloToVhlo.h
+++ stablehlo/stablehlo/transforms/MapStablehloToVhlo.h
@@ -72,7 +72,7 @@
 MAP_STABLEHLO_TO_VHLO(CollectivePermuteOp, V1)
 MAP_STABLEHLO_TO_VHLO(CompareOp, V1)
 MAP_STABLEHLO_TO_VHLO(ComplexOp, V1)
-MAP_STABLEHLO_TO_VHLO(CompositeOp, V1)
+MAP_STABLEHLO_TO_VHLO(CompositeOp, V2)
 MAP_STABLEHLO_TO_VHLO(ConcatenateOp, V1)
 MAP_STABLEHLO_TO_VHLO(ConstantOp, V1)
 MAP_STABLEHLO_TO_VHLO(ConvertOp, V1)
diff --ruN a/stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp b/stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
--- stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
+++ stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
@@ -28,6 +28,7 @@
 #include "mlir/IR/BuiltinDialect.h"
 #include "mlir/IR/BuiltinOps.h"
 #include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/OpDefinition.h"
 #include "mlir/IR/PatternMatch.h"
 #include "mlir/IR/ValueRange.h"
 #include "mlir/Rewrite/FrozenRewritePatternSet.h"
@@ -1024,17 +1025,18 @@
     // stablehlo.case that uses a variadic number of regions which means an
     // additional argument for the generic builder.
     StablehloToVhloOp<StablehloOpTy> vhloOp;
-    if constexpr (std::is_same<StablehloOpTy, stablehlo::CaseOp>::value) {
-      vhloOp = vhlo::CaseOpV1::create(rewriter, stablehloOp.getLoc(), vhloTypes,
-                                      vhloOperands, vhloAttrs,
-                                      stablehloOp.getBranches().size());
+    if constexpr (StablehloOpTy::template hasTrait<
+                      OpTrait::VariadicRegions>()) {
+      vhloOp = StablehloToVhloOp<StablehloOpTy>::create(
+          rewriter, stablehloOp.getLoc(), vhloTypes, vhloOperands, vhloAttrs,
+          stablehloOp.getNumRegions());
     } else {
       vhloOp = StablehloToVhloOp<StablehloOpTy>::create(
           rewriter, stablehloOp.getLoc(), vhloTypes, vhloOperands, vhloAttrs);
     }
 
     for (auto [stablehloRegion, vhloRegion] :
-         llvm::zip(stablehloOp->getRegions(), vhloOp->getRegions())) {
+         llvm::zip_equal(stablehloOp->getRegions(), vhloOp->getRegions())) {
       rewriter.inlineRegionBefore(stablehloRegion, vhloRegion,
                                   vhloRegion.end());
       if (failed(rewriter.convertRegionTypes(&vhloRegion,
diff --ruN a/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
@@ -29,6 +29,7 @@
 #include "mlir/IR/BuiltinAttributes.h"
 #include "mlir/IR/BuiltinOps.h"
 #include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/OpDefinition.h"
 #include "mlir/IR/PatternMatch.h"
 #include "mlir/IR/ValueRange.h"
 #include "mlir/Rewrite/FrozenRewritePatternSet.h"
@@ -607,7 +608,7 @@
       if (!stablehloAttr) return specialFailure();
     }
   }
-  if constexpr (std::is_same<VhloOpTy, vhlo::CompositeOpV1>::value) {
+  if constexpr (std::is_same<VhloOpTy, vhlo::CompositeOpV2>::value) {
     if (vhloName == "decomposition") {
       stablehloAttr = convertSymbol(vhloAttr, typeConverter);
       if (!stablehloAttr) return specialFailure();
@@ -814,7 +815,7 @@
                                                vhlo::ComparisonTypeV1::NOTYPE)))
       eraseAttrs(vhloAttrs, "compare_type");
   }
-  if constexpr (std::is_same<VhloOpTy, vhlo::CompositeOpV1>::value) {
+  if constexpr (std::is_same<VhloOpTy, vhlo::CompositeOpV2>::value) {
     if (isInteger(vhloOp.getVersionAttr(), 0)) {
       eraseAttrs(vhloAttrs, "version");
     }
@@ -1002,15 +1003,14 @@
       }
     }
 
-    // Convert the VHLO operation to a StableHLO equivalent.
-    // This can almost be done in a generic fashion, except for
-    // vhlo.case that uses a variadic number of regions which means an
-    // additional argument for the generic builder.
+    // Convert the VHLO operation to a StableHLO equivalent. This can almost be
+    // done in a generic fashion, except for ops with a variadic number of
+    // regions which means an additional argument for the generic builder.
     VhloToStablehloOp<VhloOpTy> stablehloOp;
-    if constexpr (std::is_same<VhloOpTy, vhlo::CaseOpV1>::value) {
-      stablehloOp = stablehlo::CaseOp::create(
+    if constexpr (VhloOpTy::template hasTrait<OpTrait::VariadicRegions>()) {
+      stablehloOp = VhloToStablehloOp<VhloOpTy>::create(
           rewriter, vhloOp.getLoc(), stablehloTypes, stablehloOperands,
-          stablehloAttrs, vhloOp.getBranches().size());
+          stablehloAttrs, vhloOp.getNumRegions());
     } else {
       stablehloOp = VhloToStablehloOp<VhloOpTy>::create(
           rewriter, vhloOp.getLoc(), stablehloTypes, stablehloOperands,
@@ -1018,7 +1018,7 @@
     }
 
     for (auto [vhloRegion, stablehloRegion] :
-         llvm::zip(vhloOp->getRegions(), stablehloOp->getRegions())) {
+         llvm::zip_equal(vhloOp->getRegions(), stablehloOp->getRegions())) {
       rewriter.inlineRegionBefore(vhloRegion, stablehloRegion,
                                   stablehloRegion.end());
       if (failed(rewriter.convertRegionTypes(&stablehloRegion,
diff --ruN a/stablehlo/stablehlo/transforms/VhloToVersion.cpp b/stablehlo/stablehlo/transforms/VhloToVersion.cpp
--- stablehlo/stablehlo/transforms/VhloToVersion.cpp
+++ stablehlo/stablehlo/transforms/VhloToVersion.cpp
@@ -480,6 +480,36 @@
   }
 };
 
+struct CompositeOpV1ToV2 : public OpRewritePattern<CompositeOpV1> {
+  using OpRewritePattern<CompositeOpV1>::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(CompositeOpV1 op,
+                                PatternRewriter& rewriter) const override {
+    auto newOp = rewriter.replaceOpWithNewOp<CompositeOpV2>(
+        op, op->getResultTypes(), op.getInputs(), op.getName(),
+        op.getCompositeAttributes(), op.getDecomposition(), op.getVersion(),
+        /*composite_regionsCount=*/0);
+    copyDiscardableAttrs(op, newOp);
+    return success();
+  }
+};
+
+struct CompositeOpV2ToV1 : public OpRewritePattern<CompositeOpV2> {
+  using OpRewritePattern<CompositeOpV2>::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(CompositeOpV2 op,
+                                PatternRewriter& rewriter) const override {
+    if (!op.getCompositeRegions().empty()) {
+      return rewriter.notifyMatchFailure(op, "non-empty regions");
+    }
+    auto newOp = rewriter.replaceOpWithNewOp<CompositeOpV1>(
+        op, op->getResultTypes(), op.getInputs(), op.getName(),
+        op.getCompositeAttributes(), op.getDecomposition(), op.getVersion());
+    copyDiscardableAttrs(op, newOp);
+    return success();
+  }
+};
+
 #include "stablehlo/transforms/VhloToVersionPatterns.h.inc"
 
 }  // namespace
@@ -492,6 +522,7 @@
   vhlo::populateWithGenerated(*patterns);
   patterns->add<vhlo::ScatterOpV1ToV2, vhlo::ScatterOpV2ToV1>(context);
   patterns->add<vhlo::AllReduceOpV1ToV2, vhlo::AllReduceOpV2ToV1>(context);
+  patterns->add<vhlo::CompositeOpV1ToV2, vhlo::CompositeOpV2ToV1>(context);
 }
 
 }  // namespace stablehlo

